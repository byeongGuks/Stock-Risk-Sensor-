{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = ['?', '??', 'N/A', 'NA', 'nan', 'NaN', '-nan', '-NaN', 'null', '-']\n",
    "x_train = pd.read_csv('./data/track1/features/x_train_normal.csv', na_values = null_values)\n",
    "x_valid = pd.read_csv('./data/track1/features/x_valid_normal.csv', na_values = null_values)\n",
    "x_test = pd.read_csv('./data/track1/features/x_test_normal.csv', na_values = null_values)\n",
    "y_train = pd.read_csv('./data/track1/features/y_train_normal.csv', na_values = null_values)\n",
    "y_valid = pd.read_csv('./data/track1/features/y_valid_normal.csv', na_values = null_values)\n",
    "y_test = pd.read_csv('./data/track1/features/y_test_normal.csv', na_values = null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_valid = x_valid.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_test = x_test.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "y_train_bool = y_train['Y'] <-2.0\n",
    "y_valid_bool = y_valid['Y'] <-2.0\n",
    "y_test_bool = y_test['Y'] <-2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tree Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_feature_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "rfe_features_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "x_train_features = x_train[sfs_feature_list]\n",
    "x_valid_features = x_valid[sfs_feature_list]\n",
    "x_test_features = x_test[sfs_feature_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[1]\ttraining's binary_logloss: 0.68593\n",
      "[2]\ttraining's binary_logloss: 0.680011\n",
      "[3]\ttraining's binary_logloss: 0.674916\n",
      "[4]\ttraining's binary_logloss: 0.670883\n",
      "[5]\ttraining's binary_logloss: 0.667258\n",
      "[6]\ttraining's binary_logloss: 0.664002\n",
      "[7]\ttraining's binary_logloss: 0.6611\n",
      "[8]\ttraining's binary_logloss: 0.658771\n",
      "[9]\ttraining's binary_logloss: 0.656329\n",
      "[10]\ttraining's binary_logloss: 0.654196\n",
      "[11]\ttraining's binary_logloss: 0.652471\n",
      "[12]\ttraining's binary_logloss: 0.650882\n",
      "[13]\ttraining's binary_logloss: 0.649265\n",
      "[14]\ttraining's binary_logloss: 0.64794\n",
      "[15]\ttraining's binary_logloss: 0.646684\n",
      "[16]\ttraining's binary_logloss: 0.645487\n",
      "[17]\ttraining's binary_logloss: 0.644293\n",
      "[18]\ttraining's binary_logloss: 0.643337\n",
      "[19]\ttraining's binary_logloss: 0.642276\n",
      "[20]\ttraining's binary_logloss: 0.64132\n",
      "[21]\ttraining's binary_logloss: 0.640491\n",
      "[22]\ttraining's binary_logloss: 0.639765\n",
      "[23]\ttraining's binary_logloss: 0.63902\n",
      "[24]\ttraining's binary_logloss: 0.638244\n",
      "[25]\ttraining's binary_logloss: 0.637302\n",
      "[26]\ttraining's binary_logloss: 0.636636\n",
      "[27]\ttraining's binary_logloss: 0.635949\n",
      "[28]\ttraining's binary_logloss: 0.635384\n",
      "[29]\ttraining's binary_logloss: 0.634798\n",
      "[30]\ttraining's binary_logloss: 0.634222\n",
      "[31]\ttraining's binary_logloss: 0.6336\n",
      "[32]\ttraining's binary_logloss: 0.633031\n",
      "[33]\ttraining's binary_logloss: 0.632468\n",
      "[34]\ttraining's binary_logloss: 0.63191\n",
      "[35]\ttraining's binary_logloss: 0.631295\n",
      "[36]\ttraining's binary_logloss: 0.630744\n",
      "[37]\ttraining's binary_logloss: 0.630266\n",
      "[38]\ttraining's binary_logloss: 0.62976\n",
      "[39]\ttraining's binary_logloss: 0.629294\n",
      "[40]\ttraining's binary_logloss: 0.628809\n",
      "[41]\ttraining's binary_logloss: 0.628359\n",
      "[42]\ttraining's binary_logloss: 0.627802\n",
      "[43]\ttraining's binary_logloss: 0.627349\n",
      "[44]\ttraining's binary_logloss: 0.626987\n",
      "[45]\ttraining's binary_logloss: 0.626615\n",
      "[46]\ttraining's binary_logloss: 0.626166\n",
      "[47]\ttraining's binary_logloss: 0.625772\n",
      "[48]\ttraining's binary_logloss: 0.625331\n",
      "[49]\ttraining's binary_logloss: 0.624874\n",
      "[50]\ttraining's binary_logloss: 0.624458\n",
      "[51]\ttraining's binary_logloss: 0.624062\n",
      "[52]\ttraining's binary_logloss: 0.623689\n",
      "[53]\ttraining's binary_logloss: 0.623388\n",
      "[54]\ttraining's binary_logloss: 0.623039\n",
      "[55]\ttraining's binary_logloss: 0.622688\n",
      "[56]\ttraining's binary_logloss: 0.622268\n",
      "[57]\ttraining's binary_logloss: 0.621922\n",
      "[58]\ttraining's binary_logloss: 0.621554\n",
      "[59]\ttraining's binary_logloss: 0.621217\n",
      "[60]\ttraining's binary_logloss: 0.62082\n",
      "[61]\ttraining's binary_logloss: 0.620498\n",
      "[62]\ttraining's binary_logloss: 0.620156\n",
      "[63]\ttraining's binary_logloss: 0.619893\n",
      "[64]\ttraining's binary_logloss: 0.619571\n",
      "[65]\ttraining's binary_logloss: 0.619234\n",
      "[66]\ttraining's binary_logloss: 0.618855\n",
      "[67]\ttraining's binary_logloss: 0.61856\n",
      "[68]\ttraining's binary_logloss: 0.618223\n",
      "[69]\ttraining's binary_logloss: 0.617968\n",
      "[70]\ttraining's binary_logloss: 0.617621\n",
      "[71]\ttraining's binary_logloss: 0.617319\n",
      "[72]\ttraining's binary_logloss: 0.616981\n",
      "[73]\ttraining's binary_logloss: 0.616706\n",
      "[74]\ttraining's binary_logloss: 0.616457\n",
      "[75]\ttraining's binary_logloss: 0.616208\n",
      "[76]\ttraining's binary_logloss: 0.615883\n",
      "[77]\ttraining's binary_logloss: 0.615556\n",
      "[78]\ttraining's binary_logloss: 0.615371\n",
      "[79]\ttraining's binary_logloss: 0.615055\n",
      "[80]\ttraining's binary_logloss: 0.614686\n",
      "[81]\ttraining's binary_logloss: 0.614459\n",
      "[82]\ttraining's binary_logloss: 0.614091\n",
      "[83]\ttraining's binary_logloss: 0.613841\n",
      "[84]\ttraining's binary_logloss: 0.613623\n",
      "[85]\ttraining's binary_logloss: 0.613307\n",
      "[86]\ttraining's binary_logloss: 0.613013\n",
      "[87]\ttraining's binary_logloss: 0.612715\n",
      "[88]\ttraining's binary_logloss: 0.612431\n",
      "[89]\ttraining's binary_logloss: 0.612069\n",
      "[90]\ttraining's binary_logloss: 0.611823\n",
      "[91]\ttraining's binary_logloss: 0.611565\n",
      "[92]\ttraining's binary_logloss: 0.611329\n",
      "[93]\ttraining's binary_logloss: 0.611135\n",
      "[94]\ttraining's binary_logloss: 0.610805\n",
      "[95]\ttraining's binary_logloss: 0.610513\n",
      "[96]\ttraining's binary_logloss: 0.610248\n",
      "[97]\ttraining's binary_logloss: 0.609962\n",
      "[98]\ttraining's binary_logloss: 0.609671\n",
      "[99]\ttraining's binary_logloss: 0.60942\n",
      "[100]\ttraining's binary_logloss: 0.609174\n",
      "[101]\ttraining's binary_logloss: 0.608977\n",
      "[102]\ttraining's binary_logloss: 0.608805\n",
      "[103]\ttraining's binary_logloss: 0.608544\n",
      "[104]\ttraining's binary_logloss: 0.608228\n",
      "[105]\ttraining's binary_logloss: 0.607933\n",
      "[106]\ttraining's binary_logloss: 0.607618\n",
      "[107]\ttraining's binary_logloss: 0.607336\n",
      "[108]\ttraining's binary_logloss: 0.607116\n",
      "[109]\ttraining's binary_logloss: 0.606894\n",
      "[110]\ttraining's binary_logloss: 0.606611\n",
      "[111]\ttraining's binary_logloss: 0.606302\n",
      "[112]\ttraining's binary_logloss: 0.606125\n",
      "[113]\ttraining's binary_logloss: 0.605839\n",
      "[114]\ttraining's binary_logloss: 0.605649\n",
      "[115]\ttraining's binary_logloss: 0.605361\n",
      "[116]\ttraining's binary_logloss: 0.60511\n",
      "[117]\ttraining's binary_logloss: 0.604884\n",
      "[118]\ttraining's binary_logloss: 0.604604\n",
      "[119]\ttraining's binary_logloss: 0.604375\n",
      "[120]\ttraining's binary_logloss: 0.6041\n",
      "[121]\ttraining's binary_logloss: 0.603779\n",
      "[122]\ttraining's binary_logloss: 0.603629\n",
      "[123]\ttraining's binary_logloss: 0.603461\n",
      "[124]\ttraining's binary_logloss: 0.603289\n",
      "[125]\ttraining's binary_logloss: 0.602998\n",
      "[126]\ttraining's binary_logloss: 0.602701\n",
      "[127]\ttraining's binary_logloss: 0.60246\n",
      "[128]\ttraining's binary_logloss: 0.602203\n",
      "[129]\ttraining's binary_logloss: 0.601995\n",
      "[130]\ttraining's binary_logloss: 0.601805\n",
      "[131]\ttraining's binary_logloss: 0.601578\n",
      "[132]\ttraining's binary_logloss: 0.601397\n",
      "[133]\ttraining's binary_logloss: 0.601212\n",
      "[134]\ttraining's binary_logloss: 0.600968\n",
      "[135]\ttraining's binary_logloss: 0.600763\n",
      "[136]\ttraining's binary_logloss: 0.600595\n",
      "[137]\ttraining's binary_logloss: 0.60031\n",
      "[138]\ttraining's binary_logloss: 0.600084\n",
      "[139]\ttraining's binary_logloss: 0.599851\n",
      "[140]\ttraining's binary_logloss: 0.599619\n",
      "[141]\ttraining's binary_logloss: 0.599404\n",
      "[142]\ttraining's binary_logloss: 0.599202\n",
      "[143]\ttraining's binary_logloss: 0.598906\n",
      "[144]\ttraining's binary_logloss: 0.598676\n",
      "[145]\ttraining's binary_logloss: 0.598526\n",
      "[146]\ttraining's binary_logloss: 0.598298\n",
      "[147]\ttraining's binary_logloss: 0.598124\n",
      "[148]\ttraining's binary_logloss: 0.597913\n",
      "[149]\ttraining's binary_logloss: 0.597705\n",
      "[150]\ttraining's binary_logloss: 0.597582\n",
      "[151]\ttraining's binary_logloss: 0.597411\n",
      "[152]\ttraining's binary_logloss: 0.597217\n",
      "[153]\ttraining's binary_logloss: 0.596968\n",
      "[154]\ttraining's binary_logloss: 0.59674\n",
      "[155]\ttraining's binary_logloss: 0.59652\n",
      "[156]\ttraining's binary_logloss: 0.596376\n",
      "[157]\ttraining's binary_logloss: 0.596211\n",
      "[158]\ttraining's binary_logloss: 0.596088\n",
      "[159]\ttraining's binary_logloss: 0.595926\n",
      "[160]\ttraining's binary_logloss: 0.595741\n",
      "[161]\ttraining's binary_logloss: 0.595483\n",
      "[162]\ttraining's binary_logloss: 0.595257\n",
      "[163]\ttraining's binary_logloss: 0.595059\n",
      "[164]\ttraining's binary_logloss: 0.594812\n",
      "[165]\ttraining's binary_logloss: 0.594553\n",
      "[166]\ttraining's binary_logloss: 0.594357\n",
      "[167]\ttraining's binary_logloss: 0.594218\n",
      "[168]\ttraining's binary_logloss: 0.593991\n",
      "[169]\ttraining's binary_logloss: 0.593817\n",
      "[170]\ttraining's binary_logloss: 0.593606\n",
      "[171]\ttraining's binary_logloss: 0.593372\n",
      "[172]\ttraining's binary_logloss: 0.593193\n",
      "[173]\ttraining's binary_logloss: 0.592962\n",
      "[174]\ttraining's binary_logloss: 0.592772\n",
      "[175]\ttraining's binary_logloss: 0.592576\n",
      "[176]\ttraining's binary_logloss: 0.592354\n",
      "[177]\ttraining's binary_logloss: 0.592098\n",
      "[178]\ttraining's binary_logloss: 0.591914\n",
      "[179]\ttraining's binary_logloss: 0.591679\n",
      "[180]\ttraining's binary_logloss: 0.591473\n",
      "[181]\ttraining's binary_logloss: 0.591278\n",
      "[182]\ttraining's binary_logloss: 0.591103\n",
      "[183]\ttraining's binary_logloss: 0.590926\n",
      "[184]\ttraining's binary_logloss: 0.590685\n",
      "[185]\ttraining's binary_logloss: 0.590501\n",
      "[186]\ttraining's binary_logloss: 0.590272\n",
      "[187]\ttraining's binary_logloss: 0.590108\n",
      "[188]\ttraining's binary_logloss: 0.589808\n",
      "[189]\ttraining's binary_logloss: 0.589601\n",
      "[190]\ttraining's binary_logloss: 0.589375\n",
      "[191]\ttraining's binary_logloss: 0.58913\n",
      "[192]\ttraining's binary_logloss: 0.588848\n",
      "[193]\ttraining's binary_logloss: 0.588667\n",
      "[194]\ttraining's binary_logloss: 0.588509\n",
      "[195]\ttraining's binary_logloss: 0.588292\n",
      "[196]\ttraining's binary_logloss: 0.588132\n",
      "[197]\ttraining's binary_logloss: 0.587962\n",
      "[198]\ttraining's binary_logloss: 0.587736\n",
      "[199]\ttraining's binary_logloss: 0.587568\n",
      "[200]\ttraining's binary_logloss: 0.587288\n",
      "[201]\ttraining's binary_logloss: 0.587052\n",
      "[202]\ttraining's binary_logloss: 0.586921\n",
      "[203]\ttraining's binary_logloss: 0.586727\n",
      "[204]\ttraining's binary_logloss: 0.586535\n",
      "[205]\ttraining's binary_logloss: 0.586345\n",
      "[206]\ttraining's binary_logloss: 0.586207\n",
      "[207]\ttraining's binary_logloss: 0.586009\n",
      "[208]\ttraining's binary_logloss: 0.585883\n",
      "[209]\ttraining's binary_logloss: 0.58563\n",
      "[210]\ttraining's binary_logloss: 0.585446\n",
      "[211]\ttraining's binary_logloss: 0.58519\n",
      "[212]\ttraining's binary_logloss: 0.584945\n",
      "[213]\ttraining's binary_logloss: 0.584688\n",
      "[214]\ttraining's binary_logloss: 0.584455\n",
      "[215]\ttraining's binary_logloss: 0.584174\n",
      "[216]\ttraining's binary_logloss: 0.58403\n",
      "[217]\ttraining's binary_logloss: 0.583908\n",
      "[218]\ttraining's binary_logloss: 0.583693\n",
      "[219]\ttraining's binary_logloss: 0.583536\n",
      "[220]\ttraining's binary_logloss: 0.58332\n",
      "[221]\ttraining's binary_logloss: 0.583125\n",
      "[222]\ttraining's binary_logloss: 0.582936\n",
      "[223]\ttraining's binary_logloss: 0.582777\n",
      "[224]\ttraining's binary_logloss: 0.582577\n",
      "[225]\ttraining's binary_logloss: 0.582435\n",
      "[226]\ttraining's binary_logloss: 0.582211\n",
      "[227]\ttraining's binary_logloss: 0.581968\n",
      "[228]\ttraining's binary_logloss: 0.581762\n",
      "[229]\ttraining's binary_logloss: 0.581472\n",
      "[230]\ttraining's binary_logloss: 0.581305\n",
      "[231]\ttraining's binary_logloss: 0.581068\n",
      "[232]\ttraining's binary_logloss: 0.580849\n",
      "[233]\ttraining's binary_logloss: 0.580636\n",
      "[234]\ttraining's binary_logloss: 0.580513\n",
      "[235]\ttraining's binary_logloss: 0.580318\n",
      "[236]\ttraining's binary_logloss: 0.580184\n",
      "[237]\ttraining's binary_logloss: 0.579969\n",
      "[238]\ttraining's binary_logloss: 0.579763\n",
      "[239]\ttraining's binary_logloss: 0.579578\n",
      "[240]\ttraining's binary_logloss: 0.579359\n",
      "[241]\ttraining's binary_logloss: 0.579174\n",
      "[242]\ttraining's binary_logloss: 0.579063\n",
      "[243]\ttraining's binary_logloss: 0.578901\n",
      "[244]\ttraining's binary_logloss: 0.578647\n",
      "[245]\ttraining's binary_logloss: 0.578388\n",
      "[246]\ttraining's binary_logloss: 0.57821\n",
      "[247]\ttraining's binary_logloss: 0.578003\n",
      "[248]\ttraining's binary_logloss: 0.577786\n",
      "[249]\ttraining's binary_logloss: 0.577586\n",
      "[250]\ttraining's binary_logloss: 0.577375\n",
      "[251]\ttraining's binary_logloss: 0.577189\n",
      "[252]\ttraining's binary_logloss: 0.576934\n",
      "[253]\ttraining's binary_logloss: 0.576655\n",
      "[254]\ttraining's binary_logloss: 0.57639\n",
      "[255]\ttraining's binary_logloss: 0.576204\n",
      "[256]\ttraining's binary_logloss: 0.576046\n",
      "[257]\ttraining's binary_logloss: 0.575875\n",
      "[258]\ttraining's binary_logloss: 0.575678\n",
      "[259]\ttraining's binary_logloss: 0.575489\n",
      "[260]\ttraining's binary_logloss: 0.575276\n",
      "[261]\ttraining's binary_logloss: 0.575116\n",
      "[262]\ttraining's binary_logloss: 0.574943\n",
      "[263]\ttraining's binary_logloss: 0.574681\n",
      "[264]\ttraining's binary_logloss: 0.574532\n",
      "[265]\ttraining's binary_logloss: 0.574361\n",
      "[266]\ttraining's binary_logloss: 0.574215\n",
      "[267]\ttraining's binary_logloss: 0.574042\n",
      "[268]\ttraining's binary_logloss: 0.573921\n",
      "[269]\ttraining's binary_logloss: 0.573721\n",
      "[270]\ttraining's binary_logloss: 0.573536\n",
      "[271]\ttraining's binary_logloss: 0.573326\n",
      "[272]\ttraining's binary_logloss: 0.573115\n",
      "[273]\ttraining's binary_logloss: 0.572925\n",
      "[274]\ttraining's binary_logloss: 0.572737\n",
      "[275]\ttraining's binary_logloss: 0.572558\n",
      "[276]\ttraining's binary_logloss: 0.5724\n",
      "[277]\ttraining's binary_logloss: 0.572193\n",
      "[278]\ttraining's binary_logloss: 0.572076\n",
      "[279]\ttraining's binary_logloss: 0.57188\n",
      "[280]\ttraining's binary_logloss: 0.571628\n",
      "[281]\ttraining's binary_logloss: 0.571431\n",
      "[282]\ttraining's binary_logloss: 0.571308\n",
      "[283]\ttraining's binary_logloss: 0.571108\n",
      "[284]\ttraining's binary_logloss: 0.570932\n",
      "[285]\ttraining's binary_logloss: 0.570766\n",
      "[286]\ttraining's binary_logloss: 0.570643\n",
      "[287]\ttraining's binary_logloss: 0.570452\n",
      "[288]\ttraining's binary_logloss: 0.570336\n",
      "[289]\ttraining's binary_logloss: 0.570203\n",
      "[290]\ttraining's binary_logloss: 0.570092\n",
      "[291]\ttraining's binary_logloss: 0.569923\n",
      "[292]\ttraining's binary_logloss: 0.569745\n",
      "[293]\ttraining's binary_logloss: 0.569553\n",
      "[294]\ttraining's binary_logloss: 0.569325\n",
      "[295]\ttraining's binary_logloss: 0.569085\n",
      "[296]\ttraining's binary_logloss: 0.568966\n",
      "[297]\ttraining's binary_logloss: 0.568797\n",
      "[298]\ttraining's binary_logloss: 0.568562\n",
      "[299]\ttraining's binary_logloss: 0.568372\n",
      "[300]\ttraining's binary_logloss: 0.568192\n",
      "[301]\ttraining's binary_logloss: 0.568009\n",
      "[302]\ttraining's binary_logloss: 0.567908\n",
      "[303]\ttraining's binary_logloss: 0.567804\n",
      "[304]\ttraining's binary_logloss: 0.567717\n",
      "[305]\ttraining's binary_logloss: 0.567568\n",
      "[306]\ttraining's binary_logloss: 0.567407\n",
      "[307]\ttraining's binary_logloss: 0.567218\n",
      "[308]\ttraining's binary_logloss: 0.567089\n",
      "[309]\ttraining's binary_logloss: 0.566902\n",
      "[310]\ttraining's binary_logloss: 0.566756\n",
      "[311]\ttraining's binary_logloss: 0.566577\n",
      "[312]\ttraining's binary_logloss: 0.566499\n",
      "[313]\ttraining's binary_logloss: 0.56634\n",
      "[314]\ttraining's binary_logloss: 0.566136\n",
      "[315]\ttraining's binary_logloss: 0.565915\n",
      "[316]\ttraining's binary_logloss: 0.565683\n",
      "[317]\ttraining's binary_logloss: 0.565591\n",
      "[318]\ttraining's binary_logloss: 0.565477\n",
      "[319]\ttraining's binary_logloss: 0.565346\n",
      "[320]\ttraining's binary_logloss: 0.565174\n",
      "[321]\ttraining's binary_logloss: 0.565047\n",
      "[322]\ttraining's binary_logloss: 0.564891\n",
      "[323]\ttraining's binary_logloss: 0.564701\n",
      "[324]\ttraining's binary_logloss: 0.564528\n",
      "[325]\ttraining's binary_logloss: 0.56437\n",
      "[326]\ttraining's binary_logloss: 0.564235\n",
      "[327]\ttraining's binary_logloss: 0.56409\n",
      "[328]\ttraining's binary_logloss: 0.563948\n",
      "[329]\ttraining's binary_logloss: 0.563741\n",
      "[330]\ttraining's binary_logloss: 0.563566\n",
      "[331]\ttraining's binary_logloss: 0.563398\n",
      "[332]\ttraining's binary_logloss: 0.563245\n",
      "[333]\ttraining's binary_logloss: 0.56307\n",
      "[334]\ttraining's binary_logloss: 0.5629\n",
      "[335]\ttraining's binary_logloss: 0.562681\n",
      "[336]\ttraining's binary_logloss: 0.562469\n",
      "[337]\ttraining's binary_logloss: 0.562311\n",
      "[338]\ttraining's binary_logloss: 0.562193\n",
      "[339]\ttraining's binary_logloss: 0.561994\n",
      "[340]\ttraining's binary_logloss: 0.561824\n",
      "[341]\ttraining's binary_logloss: 0.561665\n",
      "[342]\ttraining's binary_logloss: 0.561473\n",
      "[343]\ttraining's binary_logloss: 0.561367\n",
      "[344]\ttraining's binary_logloss: 0.561246\n",
      "[345]\ttraining's binary_logloss: 0.561062\n",
      "[346]\ttraining's binary_logloss: 0.560937\n",
      "[347]\ttraining's binary_logloss: 0.560769\n",
      "[348]\ttraining's binary_logloss: 0.560547\n",
      "[349]\ttraining's binary_logloss: 0.560375\n",
      "[350]\ttraining's binary_logloss: 0.560227\n",
      "[351]\ttraining's binary_logloss: 0.560014\n",
      "[352]\ttraining's binary_logloss: 0.559838\n",
      "[353]\ttraining's binary_logloss: 0.559701\n",
      "[354]\ttraining's binary_logloss: 0.559549\n",
      "[355]\ttraining's binary_logloss: 0.559399\n",
      "[356]\ttraining's binary_logloss: 0.559254\n",
      "[357]\ttraining's binary_logloss: 0.559092\n",
      "[358]\ttraining's binary_logloss: 0.558952\n",
      "[359]\ttraining's binary_logloss: 0.558728\n",
      "[360]\ttraining's binary_logloss: 0.558554\n",
      "[361]\ttraining's binary_logloss: 0.558437\n",
      "[362]\ttraining's binary_logloss: 0.55827\n",
      "[363]\ttraining's binary_logloss: 0.558122\n",
      "[364]\ttraining's binary_logloss: 0.557914\n",
      "[365]\ttraining's binary_logloss: 0.557751\n",
      "[366]\ttraining's binary_logloss: 0.557656\n",
      "[367]\ttraining's binary_logloss: 0.557494\n",
      "[368]\ttraining's binary_logloss: 0.557267\n",
      "[369]\ttraining's binary_logloss: 0.557069\n",
      "[370]\ttraining's binary_logloss: 0.556969\n",
      "[371]\ttraining's binary_logloss: 0.556882\n",
      "[372]\ttraining's binary_logloss: 0.556743\n",
      "[373]\ttraining's binary_logloss: 0.556623\n",
      "[374]\ttraining's binary_logloss: 0.55646\n",
      "[375]\ttraining's binary_logloss: 0.556357\n",
      "[376]\ttraining's binary_logloss: 0.55623\n",
      "[377]\ttraining's binary_logloss: 0.55604\n",
      "[378]\ttraining's binary_logloss: 0.555951\n",
      "[379]\ttraining's binary_logloss: 0.55576\n",
      "[380]\ttraining's binary_logloss: 0.55557\n",
      "[381]\ttraining's binary_logloss: 0.555431\n",
      "[382]\ttraining's binary_logloss: 0.555238\n",
      "[383]\ttraining's binary_logloss: 0.555031\n",
      "[384]\ttraining's binary_logloss: 0.554881\n",
      "[385]\ttraining's binary_logloss: 0.554691\n",
      "[386]\ttraining's binary_logloss: 0.554523\n",
      "[387]\ttraining's binary_logloss: 0.554349\n",
      "[388]\ttraining's binary_logloss: 0.554223\n",
      "[389]\ttraining's binary_logloss: 0.554037\n",
      "[390]\ttraining's binary_logloss: 0.553855\n",
      "[391]\ttraining's binary_logloss: 0.553687\n",
      "[392]\ttraining's binary_logloss: 0.553536\n",
      "[393]\ttraining's binary_logloss: 0.553312\n",
      "[394]\ttraining's binary_logloss: 0.553106\n",
      "[395]\ttraining's binary_logloss: 0.552971\n",
      "[396]\ttraining's binary_logloss: 0.552824\n",
      "[397]\ttraining's binary_logloss: 0.55271\n",
      "[398]\ttraining's binary_logloss: 0.552585\n",
      "[399]\ttraining's binary_logloss: 0.552392\n",
      "[400]\ttraining's binary_logloss: 0.55222\n",
      "[401]\ttraining's binary_logloss: 0.552066\n",
      "[402]\ttraining's binary_logloss: 0.551998\n",
      "[403]\ttraining's binary_logloss: 0.551868\n",
      "[404]\ttraining's binary_logloss: 0.551677\n",
      "[405]\ttraining's binary_logloss: 0.551538\n",
      "[406]\ttraining's binary_logloss: 0.551431\n",
      "[407]\ttraining's binary_logloss: 0.551319\n",
      "[408]\ttraining's binary_logloss: 0.551158\n",
      "[409]\ttraining's binary_logloss: 0.551018\n",
      "[410]\ttraining's binary_logloss: 0.550902\n",
      "[411]\ttraining's binary_logloss: 0.550738\n",
      "[412]\ttraining's binary_logloss: 0.55052\n",
      "[413]\ttraining's binary_logloss: 0.550316\n",
      "[414]\ttraining's binary_logloss: 0.55014\n",
      "[415]\ttraining's binary_logloss: 0.550034\n",
      "[416]\ttraining's binary_logloss: 0.54993\n",
      "[417]\ttraining's binary_logloss: 0.549854\n",
      "[418]\ttraining's binary_logloss: 0.549729\n",
      "[419]\ttraining's binary_logloss: 0.549606\n",
      "[420]\ttraining's binary_logloss: 0.549431\n",
      "[421]\ttraining's binary_logloss: 0.549314\n",
      "[422]\ttraining's binary_logloss: 0.549091\n",
      "[423]\ttraining's binary_logloss: 0.548946\n",
      "[424]\ttraining's binary_logloss: 0.548788\n",
      "[425]\ttraining's binary_logloss: 0.548685\n",
      "[426]\ttraining's binary_logloss: 0.548577\n",
      "[427]\ttraining's binary_logloss: 0.548388\n",
      "[428]\ttraining's binary_logloss: 0.548275\n",
      "[429]\ttraining's binary_logloss: 0.548137\n",
      "[430]\ttraining's binary_logloss: 0.547983\n",
      "[431]\ttraining's binary_logloss: 0.547876\n",
      "[432]\ttraining's binary_logloss: 0.547765\n",
      "[433]\ttraining's binary_logloss: 0.547591\n",
      "[434]\ttraining's binary_logloss: 0.547404\n",
      "[435]\ttraining's binary_logloss: 0.547209\n",
      "[436]\ttraining's binary_logloss: 0.547033\n",
      "[437]\ttraining's binary_logloss: 0.546901\n",
      "[438]\ttraining's binary_logloss: 0.546805\n",
      "[439]\ttraining's binary_logloss: 0.546615\n",
      "[440]\ttraining's binary_logloss: 0.546429\n",
      "[441]\ttraining's binary_logloss: 0.546289\n",
      "[442]\ttraining's binary_logloss: 0.546124\n",
      "[443]\ttraining's binary_logloss: 0.545912\n",
      "[444]\ttraining's binary_logloss: 0.545735\n",
      "[445]\ttraining's binary_logloss: 0.545625\n",
      "[446]\ttraining's binary_logloss: 0.545466\n",
      "[447]\ttraining's binary_logloss: 0.545308\n",
      "[448]\ttraining's binary_logloss: 0.545164\n",
      "[449]\ttraining's binary_logloss: 0.545041\n",
      "[450]\ttraining's binary_logloss: 0.544936\n",
      "[451]\ttraining's binary_logloss: 0.544847\n",
      "[452]\ttraining's binary_logloss: 0.544697\n",
      "[453]\ttraining's binary_logloss: 0.544481\n",
      "[454]\ttraining's binary_logloss: 0.544345\n",
      "[455]\ttraining's binary_logloss: 0.544202\n",
      "[456]\ttraining's binary_logloss: 0.54406\n",
      "[457]\ttraining's binary_logloss: 0.544005\n",
      "[458]\ttraining's binary_logloss: 0.543853\n",
      "[459]\ttraining's binary_logloss: 0.543664\n",
      "[460]\ttraining's binary_logloss: 0.543486\n",
      "[461]\ttraining's binary_logloss: 0.543346\n",
      "[462]\ttraining's binary_logloss: 0.543116\n",
      "[463]\ttraining's binary_logloss: 0.54299\n",
      "[464]\ttraining's binary_logloss: 0.542785\n",
      "[465]\ttraining's binary_logloss: 0.54263\n",
      "[466]\ttraining's binary_logloss: 0.542451\n",
      "[467]\ttraining's binary_logloss: 0.542347\n",
      "[468]\ttraining's binary_logloss: 0.542232\n",
      "[469]\ttraining's binary_logloss: 0.542138\n",
      "[470]\ttraining's binary_logloss: 0.542053\n",
      "[471]\ttraining's binary_logloss: 0.541937\n",
      "[472]\ttraining's binary_logloss: 0.541762\n",
      "[473]\ttraining's binary_logloss: 0.541641\n",
      "[474]\ttraining's binary_logloss: 0.541561\n",
      "[475]\ttraining's binary_logloss: 0.541362\n",
      "[476]\ttraining's binary_logloss: 0.541211\n",
      "[477]\ttraining's binary_logloss: 0.541133\n",
      "[478]\ttraining's binary_logloss: 0.541053\n",
      "[479]\ttraining's binary_logloss: 0.540882\n",
      "[480]\ttraining's binary_logloss: 0.540698\n",
      "[481]\ttraining's binary_logloss: 0.540557\n",
      "[482]\ttraining's binary_logloss: 0.540429\n",
      "[483]\ttraining's binary_logloss: 0.54033\n",
      "[484]\ttraining's binary_logloss: 0.540198\n",
      "[485]\ttraining's binary_logloss: 0.540119\n",
      "[486]\ttraining's binary_logloss: 0.540006\n",
      "[487]\ttraining's binary_logloss: 0.539838\n",
      "[488]\ttraining's binary_logloss: 0.539695\n",
      "[489]\ttraining's binary_logloss: 0.539561\n",
      "[490]\ttraining's binary_logloss: 0.539421\n",
      "[491]\ttraining's binary_logloss: 0.5393\n",
      "[492]\ttraining's binary_logloss: 0.539163\n",
      "[493]\ttraining's binary_logloss: 0.538971\n",
      "[494]\ttraining's binary_logloss: 0.538888\n",
      "[495]\ttraining's binary_logloss: 0.538791\n",
      "[496]\ttraining's binary_logloss: 0.538721\n",
      "[497]\ttraining's binary_logloss: 0.538629\n",
      "[498]\ttraining's binary_logloss: 0.538465\n",
      "[499]\ttraining's binary_logloss: 0.538345\n",
      "[500]\ttraining's binary_logloss: 0.538209\n",
      "[501]\ttraining's binary_logloss: 0.538091\n",
      "[502]\ttraining's binary_logloss: 0.53801\n",
      "[503]\ttraining's binary_logloss: 0.537939\n",
      "[504]\ttraining's binary_logloss: 0.537834\n",
      "[505]\ttraining's binary_logloss: 0.537685\n",
      "[506]\ttraining's binary_logloss: 0.537461\n",
      "[507]\ttraining's binary_logloss: 0.537356\n",
      "[508]\ttraining's binary_logloss: 0.537173\n",
      "[509]\ttraining's binary_logloss: 0.536957\n",
      "[510]\ttraining's binary_logloss: 0.536826\n",
      "[511]\ttraining's binary_logloss: 0.536744\n",
      "[512]\ttraining's binary_logloss: 0.536606\n",
      "[513]\ttraining's binary_logloss: 0.536512\n",
      "[514]\ttraining's binary_logloss: 0.536384\n",
      "[515]\ttraining's binary_logloss: 0.536272\n",
      "[516]\ttraining's binary_logloss: 0.536097\n",
      "[517]\ttraining's binary_logloss: 0.535947\n",
      "[518]\ttraining's binary_logloss: 0.535826\n",
      "[519]\ttraining's binary_logloss: 0.535596\n",
      "[520]\ttraining's binary_logloss: 0.535473\n",
      "[521]\ttraining's binary_logloss: 0.535313\n",
      "[522]\ttraining's binary_logloss: 0.535139\n",
      "[523]\ttraining's binary_logloss: 0.534927\n",
      "[524]\ttraining's binary_logloss: 0.534831\n",
      "[525]\ttraining's binary_logloss: 0.534676\n",
      "[526]\ttraining's binary_logloss: 0.534549\n",
      "[527]\ttraining's binary_logloss: 0.534408\n",
      "[528]\ttraining's binary_logloss: 0.53429\n",
      "[529]\ttraining's binary_logloss: 0.534168\n",
      "[530]\ttraining's binary_logloss: 0.534077\n",
      "[531]\ttraining's binary_logloss: 0.53399\n",
      "[532]\ttraining's binary_logloss: 0.533895\n",
      "[533]\ttraining's binary_logloss: 0.533755\n",
      "[534]\ttraining's binary_logloss: 0.533652\n",
      "[535]\ttraining's binary_logloss: 0.533479\n",
      "[536]\ttraining's binary_logloss: 0.53333\n",
      "[537]\ttraining's binary_logloss: 0.533169\n",
      "[538]\ttraining's binary_logloss: 0.532995\n",
      "[539]\ttraining's binary_logloss: 0.53285\n",
      "[540]\ttraining's binary_logloss: 0.532707\n",
      "[541]\ttraining's binary_logloss: 0.532542\n",
      "[542]\ttraining's binary_logloss: 0.532388\n",
      "[543]\ttraining's binary_logloss: 0.532225\n",
      "[544]\ttraining's binary_logloss: 0.532097\n",
      "[545]\ttraining's binary_logloss: 0.531912\n",
      "[546]\ttraining's binary_logloss: 0.531814\n",
      "[547]\ttraining's binary_logloss: 0.531721\n",
      "[548]\ttraining's binary_logloss: 0.531612\n",
      "[549]\ttraining's binary_logloss: 0.531497\n",
      "[550]\ttraining's binary_logloss: 0.531411\n",
      "[551]\ttraining's binary_logloss: 0.531252\n",
      "[552]\ttraining's binary_logloss: 0.531129\n",
      "[553]\ttraining's binary_logloss: 0.530931\n",
      "[554]\ttraining's binary_logloss: 0.530782\n",
      "[555]\ttraining's binary_logloss: 0.530634\n",
      "[556]\ttraining's binary_logloss: 0.530463\n",
      "[557]\ttraining's binary_logloss: 0.530334\n",
      "[558]\ttraining's binary_logloss: 0.530188\n",
      "[559]\ttraining's binary_logloss: 0.530108\n",
      "[560]\ttraining's binary_logloss: 0.529943\n",
      "[561]\ttraining's binary_logloss: 0.529843\n",
      "[562]\ttraining's binary_logloss: 0.529642\n",
      "[563]\ttraining's binary_logloss: 0.529558\n",
      "[564]\ttraining's binary_logloss: 0.529456\n",
      "[565]\ttraining's binary_logloss: 0.529284\n",
      "[566]\ttraining's binary_logloss: 0.529156\n",
      "[567]\ttraining's binary_logloss: 0.528997\n",
      "[568]\ttraining's binary_logloss: 0.52885\n",
      "[569]\ttraining's binary_logloss: 0.528776\n",
      "[570]\ttraining's binary_logloss: 0.528662\n",
      "[571]\ttraining's binary_logloss: 0.528517\n",
      "[572]\ttraining's binary_logloss: 0.528347\n",
      "[573]\ttraining's binary_logloss: 0.528173\n",
      "[574]\ttraining's binary_logloss: 0.527992\n",
      "[575]\ttraining's binary_logloss: 0.527844\n",
      "[576]\ttraining's binary_logloss: 0.527714\n",
      "[577]\ttraining's binary_logloss: 0.527498\n",
      "[578]\ttraining's binary_logloss: 0.527314\n",
      "[579]\ttraining's binary_logloss: 0.527201\n",
      "[580]\ttraining's binary_logloss: 0.527131\n",
      "[581]\ttraining's binary_logloss: 0.526939\n",
      "[582]\ttraining's binary_logloss: 0.526768\n",
      "[583]\ttraining's binary_logloss: 0.526589\n",
      "[584]\ttraining's binary_logloss: 0.526426\n",
      "[585]\ttraining's binary_logloss: 0.526289\n",
      "[586]\ttraining's binary_logloss: 0.526127\n",
      "[587]\ttraining's binary_logloss: 0.526005\n",
      "[588]\ttraining's binary_logloss: 0.525842\n",
      "[589]\ttraining's binary_logloss: 0.525717\n",
      "[590]\ttraining's binary_logloss: 0.525558\n",
      "[591]\ttraining's binary_logloss: 0.525419\n",
      "[592]\ttraining's binary_logloss: 0.525303\n",
      "[593]\ttraining's binary_logloss: 0.525192\n",
      "[594]\ttraining's binary_logloss: 0.524974\n",
      "[595]\ttraining's binary_logloss: 0.52491\n",
      "[596]\ttraining's binary_logloss: 0.524714\n",
      "[597]\ttraining's binary_logloss: 0.524556\n",
      "[598]\ttraining's binary_logloss: 0.524424\n",
      "[599]\ttraining's binary_logloss: 0.524294\n",
      "[600]\ttraining's binary_logloss: 0.524163\n",
      "[601]\ttraining's binary_logloss: 0.524033\n",
      "[602]\ttraining's binary_logloss: 0.523855\n",
      "[603]\ttraining's binary_logloss: 0.523706\n",
      "[604]\ttraining's binary_logloss: 0.523595\n",
      "[605]\ttraining's binary_logloss: 0.523478\n",
      "[606]\ttraining's binary_logloss: 0.523345\n",
      "[607]\ttraining's binary_logloss: 0.523241\n",
      "[608]\ttraining's binary_logloss: 0.523097\n",
      "[609]\ttraining's binary_logloss: 0.522949\n",
      "[610]\ttraining's binary_logloss: 0.522815\n",
      "[611]\ttraining's binary_logloss: 0.522738\n",
      "[612]\ttraining's binary_logloss: 0.52262\n",
      "[613]\ttraining's binary_logloss: 0.522516\n",
      "[614]\ttraining's binary_logloss: 0.522325\n",
      "[615]\ttraining's binary_logloss: 0.522227\n",
      "[616]\ttraining's binary_logloss: 0.52215\n",
      "[617]\ttraining's binary_logloss: 0.521994\n",
      "[618]\ttraining's binary_logloss: 0.521851\n",
      "[619]\ttraining's binary_logloss: 0.521722\n",
      "[620]\ttraining's binary_logloss: 0.521507\n",
      "[621]\ttraining's binary_logloss: 0.521388\n",
      "[622]\ttraining's binary_logloss: 0.521261\n",
      "[623]\ttraining's binary_logloss: 0.521169\n",
      "[624]\ttraining's binary_logloss: 0.520996\n",
      "[625]\ttraining's binary_logloss: 0.52081\n",
      "[626]\ttraining's binary_logloss: 0.520679\n",
      "[627]\ttraining's binary_logloss: 0.52051\n",
      "[628]\ttraining's binary_logloss: 0.520413\n",
      "[629]\ttraining's binary_logloss: 0.5203\n",
      "[630]\ttraining's binary_logloss: 0.520163\n",
      "[631]\ttraining's binary_logloss: 0.520043\n",
      "[632]\ttraining's binary_logloss: 0.519976\n",
      "[633]\ttraining's binary_logloss: 0.519857\n",
      "[634]\ttraining's binary_logloss: 0.519709\n",
      "[635]\ttraining's binary_logloss: 0.519592\n",
      "[636]\ttraining's binary_logloss: 0.519391\n",
      "[637]\ttraining's binary_logloss: 0.519292\n",
      "[638]\ttraining's binary_logloss: 0.519209\n",
      "[639]\ttraining's binary_logloss: 0.519021\n",
      "[640]\ttraining's binary_logloss: 0.51891\n",
      "[641]\ttraining's binary_logloss: 0.518774\n",
      "[642]\ttraining's binary_logloss: 0.518672\n",
      "[643]\ttraining's binary_logloss: 0.518559\n",
      "[644]\ttraining's binary_logloss: 0.518411\n",
      "[645]\ttraining's binary_logloss: 0.518252\n",
      "[646]\ttraining's binary_logloss: 0.518161\n",
      "[647]\ttraining's binary_logloss: 0.517934\n",
      "[648]\ttraining's binary_logloss: 0.517829\n",
      "[649]\ttraining's binary_logloss: 0.517732\n",
      "[650]\ttraining's binary_logloss: 0.517629\n",
      "[651]\ttraining's binary_logloss: 0.517541\n",
      "[652]\ttraining's binary_logloss: 0.517364\n",
      "[653]\ttraining's binary_logloss: 0.517244\n",
      "[654]\ttraining's binary_logloss: 0.517127\n",
      "[655]\ttraining's binary_logloss: 0.516978\n",
      "[656]\ttraining's binary_logloss: 0.516868\n",
      "[657]\ttraining's binary_logloss: 0.516701\n",
      "[658]\ttraining's binary_logloss: 0.516565\n",
      "[659]\ttraining's binary_logloss: 0.516444\n",
      "[660]\ttraining's binary_logloss: 0.516364\n",
      "[661]\ttraining's binary_logloss: 0.51625\n",
      "[662]\ttraining's binary_logloss: 0.516135\n",
      "[663]\ttraining's binary_logloss: 0.516053\n",
      "[664]\ttraining's binary_logloss: 0.515894\n",
      "[665]\ttraining's binary_logloss: 0.515804\n",
      "[666]\ttraining's binary_logloss: 0.51568\n",
      "[667]\ttraining's binary_logloss: 0.515523\n",
      "[668]\ttraining's binary_logloss: 0.515389\n",
      "[669]\ttraining's binary_logloss: 0.515302\n",
      "[670]\ttraining's binary_logloss: 0.515214\n",
      "[671]\ttraining's binary_logloss: 0.515052\n",
      "[672]\ttraining's binary_logloss: 0.514966\n",
      "[673]\ttraining's binary_logloss: 0.514902\n",
      "[674]\ttraining's binary_logloss: 0.514803\n",
      "[675]\ttraining's binary_logloss: 0.514691\n",
      "[676]\ttraining's binary_logloss: 0.514491\n",
      "[677]\ttraining's binary_logloss: 0.514376\n",
      "[678]\ttraining's binary_logloss: 0.514172\n",
      "[679]\ttraining's binary_logloss: 0.5141\n",
      "[680]\ttraining's binary_logloss: 0.514008\n",
      "[681]\ttraining's binary_logloss: 0.513903\n",
      "[682]\ttraining's binary_logloss: 0.513771\n",
      "[683]\ttraining's binary_logloss: 0.513587\n",
      "[684]\ttraining's binary_logloss: 0.513457\n",
      "[685]\ttraining's binary_logloss: 0.513332\n",
      "[686]\ttraining's binary_logloss: 0.513245\n",
      "[687]\ttraining's binary_logloss: 0.513132\n",
      "[688]\ttraining's binary_logloss: 0.512979\n",
      "[689]\ttraining's binary_logloss: 0.512826\n",
      "[690]\ttraining's binary_logloss: 0.512692\n",
      "[691]\ttraining's binary_logloss: 0.51249\n",
      "[692]\ttraining's binary_logloss: 0.512386\n",
      "[693]\ttraining's binary_logloss: 0.512233\n",
      "[694]\ttraining's binary_logloss: 0.512113\n",
      "[695]\ttraining's binary_logloss: 0.511952\n",
      "[696]\ttraining's binary_logloss: 0.511796\n",
      "[697]\ttraining's binary_logloss: 0.511672\n",
      "[698]\ttraining's binary_logloss: 0.511572\n",
      "[699]\ttraining's binary_logloss: 0.511417\n",
      "[700]\ttraining's binary_logloss: 0.511316\n",
      "[701]\ttraining's binary_logloss: 0.511206\n",
      "[702]\ttraining's binary_logloss: 0.511102\n",
      "[703]\ttraining's binary_logloss: 0.511012\n",
      "[704]\ttraining's binary_logloss: 0.510834\n",
      "[705]\ttraining's binary_logloss: 0.510695\n",
      "[706]\ttraining's binary_logloss: 0.51058\n",
      "[707]\ttraining's binary_logloss: 0.510398\n",
      "[708]\ttraining's binary_logloss: 0.510256\n",
      "[709]\ttraining's binary_logloss: 0.510153\n",
      "[710]\ttraining's binary_logloss: 0.510012\n",
      "[711]\ttraining's binary_logloss: 0.509902\n",
      "[712]\ttraining's binary_logloss: 0.50971\n",
      "[713]\ttraining's binary_logloss: 0.509593\n",
      "[714]\ttraining's binary_logloss: 0.509456\n",
      "[715]\ttraining's binary_logloss: 0.509375\n",
      "[716]\ttraining's binary_logloss: 0.509289\n",
      "[717]\ttraining's binary_logloss: 0.509132\n",
      "[718]\ttraining's binary_logloss: 0.509001\n",
      "[719]\ttraining's binary_logloss: 0.508848\n",
      "[720]\ttraining's binary_logloss: 0.508742\n",
      "[721]\ttraining's binary_logloss: 0.508625\n",
      "[722]\ttraining's binary_logloss: 0.508504\n",
      "[723]\ttraining's binary_logloss: 0.50833\n",
      "[724]\ttraining's binary_logloss: 0.508212\n",
      "[725]\ttraining's binary_logloss: 0.508081\n",
      "[726]\ttraining's binary_logloss: 0.507975\n",
      "[727]\ttraining's binary_logloss: 0.50787\n",
      "[728]\ttraining's binary_logloss: 0.507743\n",
      "[729]\ttraining's binary_logloss: 0.50763\n",
      "[730]\ttraining's binary_logloss: 0.507543\n",
      "[731]\ttraining's binary_logloss: 0.507386\n",
      "[732]\ttraining's binary_logloss: 0.507253\n",
      "[733]\ttraining's binary_logloss: 0.507136\n",
      "[734]\ttraining's binary_logloss: 0.506953\n",
      "[735]\ttraining's binary_logloss: 0.506819\n",
      "[736]\ttraining's binary_logloss: 0.50674\n",
      "[737]\ttraining's binary_logloss: 0.506574\n",
      "[738]\ttraining's binary_logloss: 0.506454\n",
      "[739]\ttraining's binary_logloss: 0.5064\n",
      "[740]\ttraining's binary_logloss: 0.506296\n",
      "[741]\ttraining's binary_logloss: 0.506101\n",
      "[742]\ttraining's binary_logloss: 0.50599\n",
      "[743]\ttraining's binary_logloss: 0.505866\n",
      "[744]\ttraining's binary_logloss: 0.505711\n",
      "[745]\ttraining's binary_logloss: 0.505521\n",
      "[746]\ttraining's binary_logloss: 0.505358\n",
      "[747]\ttraining's binary_logloss: 0.505174\n",
      "[748]\ttraining's binary_logloss: 0.504993\n",
      "[749]\ttraining's binary_logloss: 0.504912\n",
      "[750]\ttraining's binary_logloss: 0.504767\n",
      "[751]\ttraining's binary_logloss: 0.504622\n",
      "[752]\ttraining's binary_logloss: 0.504529\n",
      "[753]\ttraining's binary_logloss: 0.504411\n",
      "[754]\ttraining's binary_logloss: 0.504278\n",
      "[755]\ttraining's binary_logloss: 0.50416\n",
      "[756]\ttraining's binary_logloss: 0.504031\n",
      "[757]\ttraining's binary_logloss: 0.503906\n",
      "[758]\ttraining's binary_logloss: 0.503703\n",
      "[759]\ttraining's binary_logloss: 0.503543\n",
      "[760]\ttraining's binary_logloss: 0.503414\n",
      "[761]\ttraining's binary_logloss: 0.503344\n",
      "[762]\ttraining's binary_logloss: 0.503279\n",
      "[763]\ttraining's binary_logloss: 0.503205\n",
      "[764]\ttraining's binary_logloss: 0.50313\n",
      "[765]\ttraining's binary_logloss: 0.502981\n",
      "[766]\ttraining's binary_logloss: 0.502886\n",
      "[767]\ttraining's binary_logloss: 0.502755\n",
      "[768]\ttraining's binary_logloss: 0.502651\n",
      "[769]\ttraining's binary_logloss: 0.502492\n",
      "[770]\ttraining's binary_logloss: 0.502344\n",
      "[771]\ttraining's binary_logloss: 0.502238\n",
      "[772]\ttraining's binary_logloss: 0.50208\n",
      "[773]\ttraining's binary_logloss: 0.501888\n",
      "[774]\ttraining's binary_logloss: 0.501674\n",
      "[775]\ttraining's binary_logloss: 0.501558\n",
      "[776]\ttraining's binary_logloss: 0.501433\n",
      "[777]\ttraining's binary_logloss: 0.501281\n",
      "[778]\ttraining's binary_logloss: 0.501188\n",
      "[779]\ttraining's binary_logloss: 0.501072\n",
      "[780]\ttraining's binary_logloss: 0.500946\n",
      "[781]\ttraining's binary_logloss: 0.500874\n",
      "[782]\ttraining's binary_logloss: 0.500791\n",
      "[783]\ttraining's binary_logloss: 0.500705\n",
      "[784]\ttraining's binary_logloss: 0.500544\n",
      "[785]\ttraining's binary_logloss: 0.500486\n",
      "[786]\ttraining's binary_logloss: 0.500338\n",
      "[787]\ttraining's binary_logloss: 0.50023\n",
      "[788]\ttraining's binary_logloss: 0.500038\n",
      "[789]\ttraining's binary_logloss: 0.499907\n",
      "[790]\ttraining's binary_logloss: 0.499801\n",
      "[791]\ttraining's binary_logloss: 0.499624\n",
      "[792]\ttraining's binary_logloss: 0.499515\n",
      "[793]\ttraining's binary_logloss: 0.499394\n",
      "[794]\ttraining's binary_logloss: 0.49926\n",
      "[795]\ttraining's binary_logloss: 0.499141\n",
      "[796]\ttraining's binary_logloss: 0.499051\n",
      "[797]\ttraining's binary_logloss: 0.498974\n",
      "[798]\ttraining's binary_logloss: 0.498791\n",
      "[799]\ttraining's binary_logloss: 0.498636\n",
      "[800]\ttraining's binary_logloss: 0.498569\n",
      "[801]\ttraining's binary_logloss: 0.498425\n",
      "[802]\ttraining's binary_logloss: 0.49831\n",
      "[803]\ttraining's binary_logloss: 0.498186\n",
      "[804]\ttraining's binary_logloss: 0.498071\n",
      "[805]\ttraining's binary_logloss: 0.497998\n",
      "[806]\ttraining's binary_logloss: 0.497918\n",
      "[807]\ttraining's binary_logloss: 0.497829\n",
      "[808]\ttraining's binary_logloss: 0.49775\n",
      "[809]\ttraining's binary_logloss: 0.497612\n",
      "[810]\ttraining's binary_logloss: 0.497526\n",
      "[811]\ttraining's binary_logloss: 0.497465\n",
      "[812]\ttraining's binary_logloss: 0.497417\n",
      "[813]\ttraining's binary_logloss: 0.497249\n",
      "[814]\ttraining's binary_logloss: 0.497136\n",
      "[815]\ttraining's binary_logloss: 0.49696\n",
      "[816]\ttraining's binary_logloss: 0.49678\n",
      "[817]\ttraining's binary_logloss: 0.4967\n",
      "[818]\ttraining's binary_logloss: 0.496625\n",
      "[819]\ttraining's binary_logloss: 0.496518\n",
      "[820]\ttraining's binary_logloss: 0.496405\n",
      "[821]\ttraining's binary_logloss: 0.496203\n",
      "[822]\ttraining's binary_logloss: 0.496104\n",
      "[823]\ttraining's binary_logloss: 0.495996\n",
      "[824]\ttraining's binary_logloss: 0.495924\n",
      "[825]\ttraining's binary_logloss: 0.495794\n",
      "[826]\ttraining's binary_logloss: 0.495679\n",
      "[827]\ttraining's binary_logloss: 0.495549\n",
      "[828]\ttraining's binary_logloss: 0.49547\n",
      "[829]\ttraining's binary_logloss: 0.495381\n",
      "[830]\ttraining's binary_logloss: 0.49531\n",
      "[831]\ttraining's binary_logloss: 0.495167\n",
      "[832]\ttraining's binary_logloss: 0.495109\n",
      "[833]\ttraining's binary_logloss: 0.495009\n",
      "[834]\ttraining's binary_logloss: 0.494894\n",
      "[835]\ttraining's binary_logloss: 0.494813\n",
      "[836]\ttraining's binary_logloss: 0.49471\n",
      "[837]\ttraining's binary_logloss: 0.494623\n",
      "[838]\ttraining's binary_logloss: 0.494516\n",
      "[839]\ttraining's binary_logloss: 0.494404\n",
      "[840]\ttraining's binary_logloss: 0.494239\n",
      "[841]\ttraining's binary_logloss: 0.494083\n",
      "[842]\ttraining's binary_logloss: 0.493968\n",
      "[843]\ttraining's binary_logloss: 0.493876\n",
      "[844]\ttraining's binary_logloss: 0.493734\n",
      "[845]\ttraining's binary_logloss: 0.493669\n",
      "[846]\ttraining's binary_logloss: 0.493607\n",
      "[847]\ttraining's binary_logloss: 0.493478\n",
      "[848]\ttraining's binary_logloss: 0.493291\n",
      "[849]\ttraining's binary_logloss: 0.49321\n",
      "[850]\ttraining's binary_logloss: 0.493147\n",
      "[851]\ttraining's binary_logloss: 0.493057\n",
      "[852]\ttraining's binary_logloss: 0.492897\n",
      "[853]\ttraining's binary_logloss: 0.492751\n",
      "[854]\ttraining's binary_logloss: 0.492667\n",
      "[855]\ttraining's binary_logloss: 0.492568\n",
      "[856]\ttraining's binary_logloss: 0.492456\n",
      "[857]\ttraining's binary_logloss: 0.492394\n",
      "[858]\ttraining's binary_logloss: 0.492255\n",
      "[859]\ttraining's binary_logloss: 0.492133\n",
      "[860]\ttraining's binary_logloss: 0.491974\n",
      "[861]\ttraining's binary_logloss: 0.491893\n",
      "[862]\ttraining's binary_logloss: 0.491788\n",
      "[863]\ttraining's binary_logloss: 0.491702\n",
      "[864]\ttraining's binary_logloss: 0.491617\n",
      "[865]\ttraining's binary_logloss: 0.491568\n",
      "[866]\ttraining's binary_logloss: 0.49146\n",
      "[867]\ttraining's binary_logloss: 0.49134\n",
      "[868]\ttraining's binary_logloss: 0.491276\n",
      "[869]\ttraining's binary_logloss: 0.491196\n",
      "[870]\ttraining's binary_logloss: 0.491008\n",
      "[871]\ttraining's binary_logloss: 0.490906\n",
      "[872]\ttraining's binary_logloss: 0.490829\n",
      "[873]\ttraining's binary_logloss: 0.490754\n",
      "[874]\ttraining's binary_logloss: 0.490611\n",
      "[875]\ttraining's binary_logloss: 0.490396\n",
      "[876]\ttraining's binary_logloss: 0.49028\n",
      "[877]\ttraining's binary_logloss: 0.490165\n",
      "[878]\ttraining's binary_logloss: 0.490001\n",
      "[879]\ttraining's binary_logloss: 0.489943\n",
      "[880]\ttraining's binary_logloss: 0.489845\n",
      "[881]\ttraining's binary_logloss: 0.489728\n",
      "[882]\ttraining's binary_logloss: 0.489654\n",
      "[883]\ttraining's binary_logloss: 0.489449\n",
      "[884]\ttraining's binary_logloss: 0.489271\n",
      "[885]\ttraining's binary_logloss: 0.489173\n",
      "[886]\ttraining's binary_logloss: 0.489054\n",
      "[887]\ttraining's binary_logloss: 0.48894\n",
      "[888]\ttraining's binary_logloss: 0.488879\n",
      "[889]\ttraining's binary_logloss: 0.488731\n",
      "[890]\ttraining's binary_logloss: 0.488673\n",
      "[891]\ttraining's binary_logloss: 0.488592\n",
      "[892]\ttraining's binary_logloss: 0.488539\n",
      "[893]\ttraining's binary_logloss: 0.488422\n",
      "[894]\ttraining's binary_logloss: 0.488262\n",
      "[895]\ttraining's binary_logloss: 0.488105\n",
      "[896]\ttraining's binary_logloss: 0.487983\n",
      "[897]\ttraining's binary_logloss: 0.487846\n",
      "[898]\ttraining's binary_logloss: 0.48772\n",
      "[899]\ttraining's binary_logloss: 0.487599\n",
      "[900]\ttraining's binary_logloss: 0.487495\n",
      "[901]\ttraining's binary_logloss: 0.487337\n",
      "[902]\ttraining's binary_logloss: 0.487174\n",
      "[903]\ttraining's binary_logloss: 0.487027\n",
      "[904]\ttraining's binary_logloss: 0.486967\n",
      "[905]\ttraining's binary_logloss: 0.486873\n",
      "[906]\ttraining's binary_logloss: 0.486747\n",
      "[907]\ttraining's binary_logloss: 0.486639\n",
      "[908]\ttraining's binary_logloss: 0.48653\n",
      "[909]\ttraining's binary_logloss: 0.486469\n",
      "[910]\ttraining's binary_logloss: 0.486372\n",
      "[911]\ttraining's binary_logloss: 0.486304\n",
      "[912]\ttraining's binary_logloss: 0.486185\n",
      "[913]\ttraining's binary_logloss: 0.486064\n",
      "[914]\ttraining's binary_logloss: 0.485956\n",
      "[915]\ttraining's binary_logloss: 0.485872\n",
      "[916]\ttraining's binary_logloss: 0.485779\n",
      "[917]\ttraining's binary_logloss: 0.485689\n",
      "[918]\ttraining's binary_logloss: 0.485586\n",
      "[919]\ttraining's binary_logloss: 0.48552\n",
      "[920]\ttraining's binary_logloss: 0.485393\n",
      "[921]\ttraining's binary_logloss: 0.485279\n",
      "[922]\ttraining's binary_logloss: 0.485202\n",
      "[923]\ttraining's binary_logloss: 0.485128\n",
      "[924]\ttraining's binary_logloss: 0.485015\n",
      "[925]\ttraining's binary_logloss: 0.484943\n",
      "[926]\ttraining's binary_logloss: 0.48489\n",
      "[927]\ttraining's binary_logloss: 0.484812\n",
      "[928]\ttraining's binary_logloss: 0.484638\n",
      "[929]\ttraining's binary_logloss: 0.484505\n",
      "[930]\ttraining's binary_logloss: 0.484455\n",
      "[931]\ttraining's binary_logloss: 0.48437\n",
      "[932]\ttraining's binary_logloss: 0.484229\n",
      "[933]\ttraining's binary_logloss: 0.484078\n",
      "[934]\ttraining's binary_logloss: 0.484006\n",
      "[935]\ttraining's binary_logloss: 0.483925\n",
      "[936]\ttraining's binary_logloss: 0.483885\n",
      "[937]\ttraining's binary_logloss: 0.48374\n",
      "[938]\ttraining's binary_logloss: 0.483647\n",
      "[939]\ttraining's binary_logloss: 0.483545\n",
      "[940]\ttraining's binary_logloss: 0.483468\n",
      "[941]\ttraining's binary_logloss: 0.483386\n",
      "[942]\ttraining's binary_logloss: 0.483311\n",
      "[943]\ttraining's binary_logloss: 0.483266\n",
      "[944]\ttraining's binary_logloss: 0.483201\n",
      "[945]\ttraining's binary_logloss: 0.483105\n",
      "[946]\ttraining's binary_logloss: 0.483042\n",
      "[947]\ttraining's binary_logloss: 0.482947\n",
      "[948]\ttraining's binary_logloss: 0.482782\n",
      "[949]\ttraining's binary_logloss: 0.482701\n",
      "[950]\ttraining's binary_logloss: 0.482583\n",
      "[951]\ttraining's binary_logloss: 0.482538\n",
      "[952]\ttraining's binary_logloss: 0.482467\n",
      "[953]\ttraining's binary_logloss: 0.482351\n",
      "[954]\ttraining's binary_logloss: 0.482187\n",
      "[955]\ttraining's binary_logloss: 0.482091\n",
      "[956]\ttraining's binary_logloss: 0.482004\n",
      "[957]\ttraining's binary_logloss: 0.481834\n",
      "[958]\ttraining's binary_logloss: 0.481714\n",
      "[959]\ttraining's binary_logloss: 0.481611\n",
      "[960]\ttraining's binary_logloss: 0.481528\n",
      "[961]\ttraining's binary_logloss: 0.481377\n",
      "[962]\ttraining's binary_logloss: 0.481267\n",
      "[963]\ttraining's binary_logloss: 0.481171\n",
      "[964]\ttraining's binary_logloss: 0.481092\n",
      "[965]\ttraining's binary_logloss: 0.481025\n",
      "[966]\ttraining's binary_logloss: 0.480949\n",
      "[967]\ttraining's binary_logloss: 0.480807\n",
      "[968]\ttraining's binary_logloss: 0.480667\n",
      "[969]\ttraining's binary_logloss: 0.480568\n",
      "[970]\ttraining's binary_logloss: 0.480437\n",
      "[971]\ttraining's binary_logloss: 0.480338\n",
      "[972]\ttraining's binary_logloss: 0.480195\n",
      "[973]\ttraining's binary_logloss: 0.480084\n",
      "[974]\ttraining's binary_logloss: 0.47997\n",
      "[975]\ttraining's binary_logloss: 0.479805\n",
      "[976]\ttraining's binary_logloss: 0.479709\n",
      "[977]\ttraining's binary_logloss: 0.479625\n",
      "[978]\ttraining's binary_logloss: 0.479536\n",
      "[979]\ttraining's binary_logloss: 0.479334\n",
      "[980]\ttraining's binary_logloss: 0.479242\n",
      "[981]\ttraining's binary_logloss: 0.479174\n",
      "[982]\ttraining's binary_logloss: 0.479028\n",
      "[983]\ttraining's binary_logloss: 0.478916\n",
      "[984]\ttraining's binary_logloss: 0.478773\n",
      "[985]\ttraining's binary_logloss: 0.478668\n",
      "[986]\ttraining's binary_logloss: 0.478632\n",
      "[987]\ttraining's binary_logloss: 0.478542\n",
      "[988]\ttraining's binary_logloss: 0.478409\n",
      "[989]\ttraining's binary_logloss: 0.478283\n",
      "[990]\ttraining's binary_logloss: 0.478207\n",
      "[991]\ttraining's binary_logloss: 0.478095\n",
      "[992]\ttraining's binary_logloss: 0.477992\n",
      "[993]\ttraining's binary_logloss: 0.477878\n",
      "[994]\ttraining's binary_logloss: 0.477749\n",
      "[995]\ttraining's binary_logloss: 0.477686\n",
      "[996]\ttraining's binary_logloss: 0.477598\n",
      "[997]\ttraining's binary_logloss: 0.477523\n",
      "[998]\ttraining's binary_logloss: 0.477343\n",
      "[999]\ttraining's binary_logloss: 0.477273\n",
      "[1000]\ttraining's binary_logloss: 0.477162\n",
      "[1001]\ttraining's binary_logloss: 0.477061\n",
      "[1002]\ttraining's binary_logloss: 0.477001\n",
      "[1003]\ttraining's binary_logloss: 0.47688\n",
      "[1004]\ttraining's binary_logloss: 0.476838\n",
      "[1005]\ttraining's binary_logloss: 0.476792\n",
      "[1006]\ttraining's binary_logloss: 0.476635\n",
      "[1007]\ttraining's binary_logloss: 0.476488\n",
      "[1008]\ttraining's binary_logloss: 0.476423\n",
      "[1009]\ttraining's binary_logloss: 0.476257\n",
      "[1010]\ttraining's binary_logloss: 0.476149\n",
      "[1011]\ttraining's binary_logloss: 0.476075\n",
      "[1012]\ttraining's binary_logloss: 0.475925\n",
      "[1013]\ttraining's binary_logloss: 0.475861\n",
      "[1014]\ttraining's binary_logloss: 0.475808\n",
      "[1015]\ttraining's binary_logloss: 0.475709\n",
      "[1016]\ttraining's binary_logloss: 0.475586\n",
      "[1017]\ttraining's binary_logloss: 0.475506\n",
      "[1018]\ttraining's binary_logloss: 0.475406\n",
      "[1019]\ttraining's binary_logloss: 0.475337\n",
      "[1020]\ttraining's binary_logloss: 0.475267\n",
      "[1021]\ttraining's binary_logloss: 0.475131\n",
      "[1022]\ttraining's binary_logloss: 0.475024\n",
      "[1023]\ttraining's binary_logloss: 0.474909\n",
      "[1024]\ttraining's binary_logloss: 0.4748\n",
      "[1025]\ttraining's binary_logloss: 0.474587\n",
      "[1026]\ttraining's binary_logloss: 0.474503\n",
      "[1027]\ttraining's binary_logloss: 0.47439\n",
      "[1028]\ttraining's binary_logloss: 0.474295\n",
      "[1029]\ttraining's binary_logloss: 0.474136\n",
      "[1030]\ttraining's binary_logloss: 0.474051\n",
      "[1031]\ttraining's binary_logloss: 0.473944\n",
      "[1032]\ttraining's binary_logloss: 0.473812\n",
      "[1033]\ttraining's binary_logloss: 0.473742\n",
      "[1034]\ttraining's binary_logloss: 0.473685\n",
      "[1035]\ttraining's binary_logloss: 0.473632\n",
      "[1036]\ttraining's binary_logloss: 0.473533\n",
      "[1037]\ttraining's binary_logloss: 0.473439\n",
      "[1038]\ttraining's binary_logloss: 0.473355\n",
      "[1039]\ttraining's binary_logloss: 0.473303\n",
      "[1040]\ttraining's binary_logloss: 0.473182\n",
      "[1041]\ttraining's binary_logloss: 0.47309\n",
      "[1042]\ttraining's binary_logloss: 0.473021\n",
      "[1043]\ttraining's binary_logloss: 0.47295\n",
      "[1044]\ttraining's binary_logloss: 0.472897\n",
      "[1045]\ttraining's binary_logloss: 0.472756\n",
      "[1046]\ttraining's binary_logloss: 0.47264\n",
      "[1047]\ttraining's binary_logloss: 0.472571\n",
      "[1048]\ttraining's binary_logloss: 0.472522\n",
      "[1049]\ttraining's binary_logloss: 0.472468\n",
      "[1050]\ttraining's binary_logloss: 0.472387\n",
      "[1051]\ttraining's binary_logloss: 0.472248\n",
      "[1052]\ttraining's binary_logloss: 0.472174\n",
      "[1053]\ttraining's binary_logloss: 0.47204\n",
      "[1054]\ttraining's binary_logloss: 0.471953\n",
      "[1055]\ttraining's binary_logloss: 0.471833\n",
      "[1056]\ttraining's binary_logloss: 0.471701\n",
      "[1057]\ttraining's binary_logloss: 0.4716\n",
      "[1058]\ttraining's binary_logloss: 0.471514\n",
      "[1059]\ttraining's binary_logloss: 0.471411\n",
      "[1060]\ttraining's binary_logloss: 0.471297\n",
      "[1061]\ttraining's binary_logloss: 0.471246\n",
      "[1062]\ttraining's binary_logloss: 0.471142\n",
      "[1063]\ttraining's binary_logloss: 0.471091\n",
      "[1064]\ttraining's binary_logloss: 0.470954\n",
      "[1065]\ttraining's binary_logloss: 0.470833\n",
      "[1066]\ttraining's binary_logloss: 0.470737\n",
      "[1067]\ttraining's binary_logloss: 0.470605\n",
      "[1068]\ttraining's binary_logloss: 0.470494\n",
      "[1069]\ttraining's binary_logloss: 0.470373\n",
      "[1070]\ttraining's binary_logloss: 0.470297\n",
      "[1071]\ttraining's binary_logloss: 0.47012\n",
      "[1072]\ttraining's binary_logloss: 0.470058\n",
      "[1073]\ttraining's binary_logloss: 0.469976\n",
      "[1074]\ttraining's binary_logloss: 0.469908\n",
      "[1075]\ttraining's binary_logloss: 0.469795\n",
      "[1076]\ttraining's binary_logloss: 0.469691\n",
      "[1077]\ttraining's binary_logloss: 0.469545\n",
      "[1078]\ttraining's binary_logloss: 0.469456\n",
      "[1079]\ttraining's binary_logloss: 0.469349\n",
      "[1080]\ttraining's binary_logloss: 0.469237\n",
      "[1081]\ttraining's binary_logloss: 0.469148\n",
      "[1082]\ttraining's binary_logloss: 0.469067\n",
      "[1083]\ttraining's binary_logloss: 0.46886\n",
      "[1084]\ttraining's binary_logloss: 0.468741\n",
      "[1085]\ttraining's binary_logloss: 0.468655\n",
      "[1086]\ttraining's binary_logloss: 0.468581\n",
      "[1087]\ttraining's binary_logloss: 0.468466\n",
      "[1088]\ttraining's binary_logloss: 0.46833\n",
      "[1089]\ttraining's binary_logloss: 0.468278\n",
      "[1090]\ttraining's binary_logloss: 0.468178\n",
      "[1091]\ttraining's binary_logloss: 0.468039\n",
      "[1092]\ttraining's binary_logloss: 0.46796\n",
      "[1093]\ttraining's binary_logloss: 0.467882\n",
      "[1094]\ttraining's binary_logloss: 0.467789\n",
      "[1095]\ttraining's binary_logloss: 0.46772\n",
      "[1096]\ttraining's binary_logloss: 0.46765\n",
      "[1097]\ttraining's binary_logloss: 0.467522\n",
      "[1098]\ttraining's binary_logloss: 0.467358\n",
      "[1099]\ttraining's binary_logloss: 0.467254\n",
      "[1100]\ttraining's binary_logloss: 0.467149\n",
      "[1101]\ttraining's binary_logloss: 0.466998\n",
      "[1102]\ttraining's binary_logloss: 0.466875\n",
      "[1103]\ttraining's binary_logloss: 0.466793\n",
      "[1104]\ttraining's binary_logloss: 0.466668\n",
      "[1105]\ttraining's binary_logloss: 0.46656\n",
      "[1106]\ttraining's binary_logloss: 0.466438\n",
      "[1107]\ttraining's binary_logloss: 0.466374\n",
      "[1108]\ttraining's binary_logloss: 0.466264\n",
      "[1109]\ttraining's binary_logloss: 0.466123\n",
      "[1110]\ttraining's binary_logloss: 0.466023\n",
      "[1111]\ttraining's binary_logloss: 0.465942\n",
      "[1112]\ttraining's binary_logloss: 0.465867\n",
      "[1113]\ttraining's binary_logloss: 0.465785\n",
      "[1114]\ttraining's binary_logloss: 0.465697\n",
      "[1115]\ttraining's binary_logloss: 0.465655\n",
      "[1116]\ttraining's binary_logloss: 0.465561\n",
      "[1117]\ttraining's binary_logloss: 0.465455\n",
      "[1118]\ttraining's binary_logloss: 0.465328\n",
      "[1119]\ttraining's binary_logloss: 0.465126\n",
      "[1120]\ttraining's binary_logloss: 0.464989\n",
      "[1121]\ttraining's binary_logloss: 0.46482\n",
      "[1122]\ttraining's binary_logloss: 0.464702\n",
      "[1123]\ttraining's binary_logloss: 0.464589\n",
      "[1124]\ttraining's binary_logloss: 0.464484\n",
      "[1125]\ttraining's binary_logloss: 0.464388\n",
      "[1126]\ttraining's binary_logloss: 0.464338\n",
      "[1127]\ttraining's binary_logloss: 0.464214\n",
      "[1128]\ttraining's binary_logloss: 0.464151\n",
      "[1129]\ttraining's binary_logloss: 0.464088\n",
      "[1130]\ttraining's binary_logloss: 0.463973\n",
      "[1131]\ttraining's binary_logloss: 0.463879\n",
      "[1132]\ttraining's binary_logloss: 0.463795\n",
      "[1133]\ttraining's binary_logloss: 0.463679\n",
      "[1134]\ttraining's binary_logloss: 0.463624\n",
      "[1135]\ttraining's binary_logloss: 0.463568\n",
      "[1136]\ttraining's binary_logloss: 0.463461\n",
      "[1137]\ttraining's binary_logloss: 0.463355\n",
      "[1138]\ttraining's binary_logloss: 0.463193\n",
      "[1139]\ttraining's binary_logloss: 0.463071\n",
      "[1140]\ttraining's binary_logloss: 0.462896\n",
      "[1141]\ttraining's binary_logloss: 0.462841\n",
      "[1142]\ttraining's binary_logloss: 0.462761\n",
      "[1143]\ttraining's binary_logloss: 0.462679\n",
      "[1144]\ttraining's binary_logloss: 0.462636\n",
      "[1145]\ttraining's binary_logloss: 0.462586\n",
      "[1146]\ttraining's binary_logloss: 0.462534\n",
      "[1147]\ttraining's binary_logloss: 0.462425\n",
      "[1148]\ttraining's binary_logloss: 0.462346\n",
      "[1149]\ttraining's binary_logloss: 0.462287\n",
      "[1150]\ttraining's binary_logloss: 0.462172\n",
      "[1151]\ttraining's binary_logloss: 0.462105\n",
      "[1152]\ttraining's binary_logloss: 0.461992\n",
      "[1153]\ttraining's binary_logloss: 0.461924\n",
      "[1154]\ttraining's binary_logloss: 0.461859\n",
      "[1155]\ttraining's binary_logloss: 0.461786\n",
      "[1156]\ttraining's binary_logloss: 0.46169\n",
      "[1157]\ttraining's binary_logloss: 0.461612\n",
      "[1158]\ttraining's binary_logloss: 0.46154\n",
      "[1159]\ttraining's binary_logloss: 0.461444\n",
      "[1160]\ttraining's binary_logloss: 0.461346\n",
      "[1161]\ttraining's binary_logloss: 0.461211\n",
      "[1162]\ttraining's binary_logloss: 0.461105\n",
      "[1163]\ttraining's binary_logloss: 0.460966\n",
      "[1164]\ttraining's binary_logloss: 0.460888\n",
      "[1165]\ttraining's binary_logloss: 0.460735\n",
      "[1166]\ttraining's binary_logloss: 0.460577\n",
      "[1167]\ttraining's binary_logloss: 0.460519\n",
      "[1168]\ttraining's binary_logloss: 0.460456\n",
      "[1169]\ttraining's binary_logloss: 0.460405\n",
      "[1170]\ttraining's binary_logloss: 0.460338\n",
      "[1171]\ttraining's binary_logloss: 0.460304\n",
      "[1172]\ttraining's binary_logloss: 0.46023\n",
      "[1173]\ttraining's binary_logloss: 0.460099\n",
      "[1174]\ttraining's binary_logloss: 0.460042\n",
      "[1175]\ttraining's binary_logloss: 0.459938\n",
      "[1176]\ttraining's binary_logloss: 0.459881\n",
      "[1177]\ttraining's binary_logloss: 0.459805\n",
      "[1178]\ttraining's binary_logloss: 0.459698\n",
      "[1179]\ttraining's binary_logloss: 0.459608\n",
      "[1180]\ttraining's binary_logloss: 0.459556\n",
      "[1181]\ttraining's binary_logloss: 0.459519\n",
      "[1182]\ttraining's binary_logloss: 0.459418\n",
      "[1183]\ttraining's binary_logloss: 0.459305\n",
      "[1184]\ttraining's binary_logloss: 0.459187\n",
      "[1185]\ttraining's binary_logloss: 0.459046\n",
      "[1186]\ttraining's binary_logloss: 0.458951\n",
      "[1187]\ttraining's binary_logloss: 0.4589\n",
      "[1188]\ttraining's binary_logloss: 0.458768\n",
      "[1189]\ttraining's binary_logloss: 0.45866\n",
      "[1190]\ttraining's binary_logloss: 0.458532\n",
      "[1191]\ttraining's binary_logloss: 0.458415\n",
      "[1192]\ttraining's binary_logloss: 0.458328\n",
      "[1193]\ttraining's binary_logloss: 0.45827\n",
      "[1194]\ttraining's binary_logloss: 0.458174\n",
      "[1195]\ttraining's binary_logloss: 0.458102\n",
      "[1196]\ttraining's binary_logloss: 0.458003\n",
      "[1197]\ttraining's binary_logloss: 0.45792\n",
      "[1198]\ttraining's binary_logloss: 0.4578\n",
      "[1199]\ttraining's binary_logloss: 0.457689\n",
      "[1200]\ttraining's binary_logloss: 0.45761\n",
      "[1201]\ttraining's binary_logloss: 0.45755\n",
      "[1202]\ttraining's binary_logloss: 0.457426\n",
      "[1203]\ttraining's binary_logloss: 0.457302\n",
      "[1204]\ttraining's binary_logloss: 0.45723\n",
      "[1205]\ttraining's binary_logloss: 0.457141\n",
      "[1206]\ttraining's binary_logloss: 0.45708\n",
      "[1207]\ttraining's binary_logloss: 0.457024\n",
      "[1208]\ttraining's binary_logloss: 0.45687\n",
      "[1209]\ttraining's binary_logloss: 0.456801\n",
      "[1210]\ttraining's binary_logloss: 0.456699\n",
      "[1211]\ttraining's binary_logloss: 0.456557\n",
      "[1212]\ttraining's binary_logloss: 0.456424\n",
      "[1213]\ttraining's binary_logloss: 0.456324\n",
      "[1214]\ttraining's binary_logloss: 0.456226\n",
      "[1215]\ttraining's binary_logloss: 0.456187\n",
      "[1216]\ttraining's binary_logloss: 0.456108\n",
      "[1217]\ttraining's binary_logloss: 0.456042\n",
      "[1218]\ttraining's binary_logloss: 0.455923\n",
      "[1219]\ttraining's binary_logloss: 0.455813\n",
      "[1220]\ttraining's binary_logloss: 0.455676\n",
      "[1221]\ttraining's binary_logloss: 0.455555\n",
      "[1222]\ttraining's binary_logloss: 0.455461\n",
      "[1223]\ttraining's binary_logloss: 0.455375\n",
      "[1224]\ttraining's binary_logloss: 0.455283\n",
      "[1225]\ttraining's binary_logloss: 0.455237\n",
      "[1226]\ttraining's binary_logloss: 0.455176\n",
      "[1227]\ttraining's binary_logloss: 0.455095\n",
      "[1228]\ttraining's binary_logloss: 0.45499\n",
      "[1229]\ttraining's binary_logloss: 0.454892\n",
      "[1230]\ttraining's binary_logloss: 0.45479\n",
      "[1231]\ttraining's binary_logloss: 0.454648\n",
      "[1232]\ttraining's binary_logloss: 0.454534\n",
      "[1233]\ttraining's binary_logloss: 0.454471\n",
      "[1234]\ttraining's binary_logloss: 0.454367\n",
      "[1235]\ttraining's binary_logloss: 0.454309\n",
      "[1236]\ttraining's binary_logloss: 0.45421\n",
      "[1237]\ttraining's binary_logloss: 0.454124\n",
      "[1238]\ttraining's binary_logloss: 0.454029\n",
      "[1239]\ttraining's binary_logloss: 0.453954\n",
      "[1240]\ttraining's binary_logloss: 0.453861\n",
      "[1241]\ttraining's binary_logloss: 0.453777\n",
      "[1242]\ttraining's binary_logloss: 0.453735\n",
      "[1243]\ttraining's binary_logloss: 0.453661\n",
      "[1244]\ttraining's binary_logloss: 0.453588\n",
      "[1245]\ttraining's binary_logloss: 0.4535\n",
      "[1246]\ttraining's binary_logloss: 0.453444\n",
      "[1247]\ttraining's binary_logloss: 0.453345\n",
      "[1248]\ttraining's binary_logloss: 0.453242\n",
      "[1249]\ttraining's binary_logloss: 0.45312\n",
      "[1250]\ttraining's binary_logloss: 0.453038\n",
      "[1251]\ttraining's binary_logloss: 0.452945\n",
      "[1252]\ttraining's binary_logloss: 0.452838\n",
      "[1253]\ttraining's binary_logloss: 0.452763\n",
      "[1254]\ttraining's binary_logloss: 0.452675\n",
      "[1255]\ttraining's binary_logloss: 0.452628\n",
      "[1256]\ttraining's binary_logloss: 0.452583\n",
      "[1257]\ttraining's binary_logloss: 0.452535\n",
      "[1258]\ttraining's binary_logloss: 0.452437\n",
      "[1259]\ttraining's binary_logloss: 0.452325\n",
      "[1260]\ttraining's binary_logloss: 0.452205\n",
      "[1261]\ttraining's binary_logloss: 0.452108\n",
      "[1262]\ttraining's binary_logloss: 0.452029\n",
      "[1263]\ttraining's binary_logloss: 0.451928\n",
      "[1264]\ttraining's binary_logloss: 0.451828\n",
      "[1265]\ttraining's binary_logloss: 0.451724\n",
      "[1266]\ttraining's binary_logloss: 0.45161\n",
      "[1267]\ttraining's binary_logloss: 0.45147\n",
      "[1268]\ttraining's binary_logloss: 0.451319\n",
      "[1269]\ttraining's binary_logloss: 0.451238\n",
      "[1270]\ttraining's binary_logloss: 0.451158\n",
      "[1271]\ttraining's binary_logloss: 0.451075\n",
      "[1272]\ttraining's binary_logloss: 0.450963\n",
      "[1273]\ttraining's binary_logloss: 0.450856\n",
      "[1274]\ttraining's binary_logloss: 0.450756\n",
      "[1275]\ttraining's binary_logloss: 0.450667\n",
      "[1276]\ttraining's binary_logloss: 0.450556\n",
      "[1277]\ttraining's binary_logloss: 0.450432\n",
      "[1278]\ttraining's binary_logloss: 0.450367\n",
      "[1279]\ttraining's binary_logloss: 0.450234\n",
      "[1280]\ttraining's binary_logloss: 0.450143\n",
      "[1281]\ttraining's binary_logloss: 0.450016\n",
      "[1282]\ttraining's binary_logloss: 0.449922\n",
      "[1283]\ttraining's binary_logloss: 0.449837\n",
      "[1284]\ttraining's binary_logloss: 0.449772\n",
      "[1285]\ttraining's binary_logloss: 0.449677\n",
      "[1286]\ttraining's binary_logloss: 0.449631\n",
      "[1287]\ttraining's binary_logloss: 0.449551\n",
      "[1288]\ttraining's binary_logloss: 0.449444\n",
      "[1289]\ttraining's binary_logloss: 0.449368\n",
      "[1290]\ttraining's binary_logloss: 0.449307\n",
      "[1291]\ttraining's binary_logloss: 0.449258\n",
      "[1292]\ttraining's binary_logloss: 0.449221\n",
      "[1293]\ttraining's binary_logloss: 0.449136\n",
      "[1294]\ttraining's binary_logloss: 0.449101\n",
      "[1295]\ttraining's binary_logloss: 0.449034\n",
      "[1296]\ttraining's binary_logloss: 0.448981\n",
      "[1297]\ttraining's binary_logloss: 0.448853\n",
      "[1298]\ttraining's binary_logloss: 0.448731\n",
      "[1299]\ttraining's binary_logloss: 0.448605\n",
      "[1300]\ttraining's binary_logloss: 0.448515\n",
      "[1301]\ttraining's binary_logloss: 0.448409\n",
      "[1302]\ttraining's binary_logloss: 0.448329\n",
      "[1303]\ttraining's binary_logloss: 0.448273\n",
      "[1304]\ttraining's binary_logloss: 0.44811\n",
      "[1305]\ttraining's binary_logloss: 0.448009\n",
      "[1306]\ttraining's binary_logloss: 0.447951\n",
      "[1307]\ttraining's binary_logloss: 0.447895\n",
      "[1308]\ttraining's binary_logloss: 0.447803\n",
      "[1309]\ttraining's binary_logloss: 0.447681\n",
      "[1310]\ttraining's binary_logloss: 0.447624\n",
      "[1311]\ttraining's binary_logloss: 0.447531\n",
      "[1312]\ttraining's binary_logloss: 0.44745\n",
      "[1313]\ttraining's binary_logloss: 0.447389\n",
      "[1314]\ttraining's binary_logloss: 0.447292\n",
      "[1315]\ttraining's binary_logloss: 0.447184\n",
      "[1316]\ttraining's binary_logloss: 0.447095\n",
      "[1317]\ttraining's binary_logloss: 0.447037\n",
      "[1318]\ttraining's binary_logloss: 0.446983\n",
      "[1319]\ttraining's binary_logloss: 0.446833\n",
      "[1320]\ttraining's binary_logloss: 0.446777\n",
      "[1321]\ttraining's binary_logloss: 0.446744\n",
      "[1322]\ttraining's binary_logloss: 0.446648\n",
      "[1323]\ttraining's binary_logloss: 0.446515\n",
      "[1324]\ttraining's binary_logloss: 0.446418\n",
      "[1325]\ttraining's binary_logloss: 0.446366\n",
      "[1326]\ttraining's binary_logloss: 0.446291\n",
      "[1327]\ttraining's binary_logloss: 0.446179\n",
      "[1328]\ttraining's binary_logloss: 0.446121\n",
      "[1329]\ttraining's binary_logloss: 0.446027\n",
      "[1330]\ttraining's binary_logloss: 0.445992\n",
      "[1331]\ttraining's binary_logloss: 0.445911\n",
      "[1332]\ttraining's binary_logloss: 0.44584\n",
      "[1333]\ttraining's binary_logloss: 0.445773\n",
      "[1334]\ttraining's binary_logloss: 0.445668\n",
      "[1335]\ttraining's binary_logloss: 0.445588\n",
      "[1336]\ttraining's binary_logloss: 0.445535\n",
      "[1337]\ttraining's binary_logloss: 0.445477\n",
      "[1338]\ttraining's binary_logloss: 0.445437\n",
      "[1339]\ttraining's binary_logloss: 0.445331\n",
      "[1340]\ttraining's binary_logloss: 0.445264\n",
      "[1341]\ttraining's binary_logloss: 0.445179\n",
      "[1342]\ttraining's binary_logloss: 0.445121\n",
      "[1343]\ttraining's binary_logloss: 0.445021\n",
      "[1344]\ttraining's binary_logloss: 0.444931\n",
      "[1345]\ttraining's binary_logloss: 0.444865\n",
      "[1346]\ttraining's binary_logloss: 0.444761\n",
      "[1347]\ttraining's binary_logloss: 0.444615\n",
      "[1348]\ttraining's binary_logloss: 0.444549\n",
      "[1349]\ttraining's binary_logloss: 0.444396\n",
      "[1350]\ttraining's binary_logloss: 0.444288\n",
      "[1351]\ttraining's binary_logloss: 0.444214\n",
      "[1352]\ttraining's binary_logloss: 0.444119\n",
      "[1353]\ttraining's binary_logloss: 0.444037\n",
      "[1354]\ttraining's binary_logloss: 0.443983\n",
      "[1355]\ttraining's binary_logloss: 0.443919\n",
      "[1356]\ttraining's binary_logloss: 0.443827\n",
      "[1357]\ttraining's binary_logloss: 0.443732\n",
      "[1358]\ttraining's binary_logloss: 0.443656\n",
      "[1359]\ttraining's binary_logloss: 0.443559\n",
      "[1360]\ttraining's binary_logloss: 0.443475\n",
      "[1361]\ttraining's binary_logloss: 0.443398\n",
      "[1362]\ttraining's binary_logloss: 0.443304\n",
      "[1363]\ttraining's binary_logloss: 0.443217\n",
      "[1364]\ttraining's binary_logloss: 0.443142\n",
      "[1365]\ttraining's binary_logloss: 0.44309\n",
      "[1366]\ttraining's binary_logloss: 0.443014\n",
      "[1367]\ttraining's binary_logloss: 0.442922\n",
      "[1368]\ttraining's binary_logloss: 0.44284\n",
      "[1369]\ttraining's binary_logloss: 0.442709\n",
      "[1370]\ttraining's binary_logloss: 0.442632\n",
      "[1371]\ttraining's binary_logloss: 0.44254\n",
      "[1372]\ttraining's binary_logloss: 0.442455\n",
      "[1373]\ttraining's binary_logloss: 0.442344\n",
      "[1374]\ttraining's binary_logloss: 0.442271\n",
      "[1375]\ttraining's binary_logloss: 0.442156\n",
      "[1376]\ttraining's binary_logloss: 0.442058\n",
      "[1377]\ttraining's binary_logloss: 0.441984\n",
      "[1378]\ttraining's binary_logloss: 0.441926\n",
      "[1379]\ttraining's binary_logloss: 0.441855\n",
      "[1380]\ttraining's binary_logloss: 0.441698\n",
      "[1381]\ttraining's binary_logloss: 0.44161\n",
      "[1382]\ttraining's binary_logloss: 0.441504\n",
      "[1383]\ttraining's binary_logloss: 0.44144\n",
      "[1384]\ttraining's binary_logloss: 0.441333\n",
      "[1385]\ttraining's binary_logloss: 0.441237\n",
      "[1386]\ttraining's binary_logloss: 0.441146\n",
      "[1387]\ttraining's binary_logloss: 0.441064\n",
      "[1388]\ttraining's binary_logloss: 0.44099\n",
      "[1389]\ttraining's binary_logloss: 0.44094\n",
      "[1390]\ttraining's binary_logloss: 0.440866\n",
      "[1391]\ttraining's binary_logloss: 0.440744\n",
      "[1392]\ttraining's binary_logloss: 0.44065\n",
      "[1393]\ttraining's binary_logloss: 0.440594\n",
      "[1394]\ttraining's binary_logloss: 0.440478\n",
      "[1395]\ttraining's binary_logloss: 0.440395\n",
      "[1396]\ttraining's binary_logloss: 0.440359\n",
      "[1397]\ttraining's binary_logloss: 0.440213\n",
      "[1398]\ttraining's binary_logloss: 0.440063\n",
      "[1399]\ttraining's binary_logloss: 0.439983\n",
      "[1400]\ttraining's binary_logloss: 0.439932\n",
      "[1401]\ttraining's binary_logloss: 0.439892\n",
      "[1402]\ttraining's binary_logloss: 0.439798\n",
      "[1403]\ttraining's binary_logloss: 0.439715\n",
      "[1404]\ttraining's binary_logloss: 0.439651\n",
      "[1405]\ttraining's binary_logloss: 0.439594\n",
      "[1406]\ttraining's binary_logloss: 0.439544\n",
      "[1407]\ttraining's binary_logloss: 0.439477\n",
      "[1408]\ttraining's binary_logloss: 0.439388\n",
      "[1409]\ttraining's binary_logloss: 0.439262\n",
      "[1410]\ttraining's binary_logloss: 0.439119\n",
      "[1411]\ttraining's binary_logloss: 0.439036\n",
      "[1412]\ttraining's binary_logloss: 0.438925\n",
      "[1413]\ttraining's binary_logloss: 0.43884\n",
      "[1414]\ttraining's binary_logloss: 0.438723\n",
      "[1415]\ttraining's binary_logloss: 0.438688\n",
      "[1416]\ttraining's binary_logloss: 0.438609\n",
      "[1417]\ttraining's binary_logloss: 0.438494\n",
      "[1418]\ttraining's binary_logloss: 0.438417\n",
      "[1419]\ttraining's binary_logloss: 0.438343\n",
      "[1420]\ttraining's binary_logloss: 0.438298\n",
      "[1421]\ttraining's binary_logloss: 0.438259\n",
      "[1422]\ttraining's binary_logloss: 0.438152\n",
      "[1423]\ttraining's binary_logloss: 0.438059\n",
      "[1424]\ttraining's binary_logloss: 0.437949\n",
      "[1425]\ttraining's binary_logloss: 0.437861\n",
      "[1426]\ttraining's binary_logloss: 0.437769\n",
      "[1427]\ttraining's binary_logloss: 0.437697\n",
      "[1428]\ttraining's binary_logloss: 0.437629\n",
      "[1429]\ttraining's binary_logloss: 0.437536\n",
      "[1430]\ttraining's binary_logloss: 0.437474\n",
      "[1431]\ttraining's binary_logloss: 0.437404\n",
      "[1432]\ttraining's binary_logloss: 0.4373\n",
      "[1433]\ttraining's binary_logloss: 0.437166\n",
      "[1434]\ttraining's binary_logloss: 0.437082\n",
      "[1435]\ttraining's binary_logloss: 0.436979\n",
      "[1436]\ttraining's binary_logloss: 0.436904\n",
      "[1437]\ttraining's binary_logloss: 0.436783\n",
      "[1438]\ttraining's binary_logloss: 0.436735\n",
      "[1439]\ttraining's binary_logloss: 0.436629\n",
      "[1440]\ttraining's binary_logloss: 0.436548\n",
      "[1441]\ttraining's binary_logloss: 0.436437\n",
      "[1442]\ttraining's binary_logloss: 0.436371\n",
      "[1443]\ttraining's binary_logloss: 0.436272\n",
      "[1444]\ttraining's binary_logloss: 0.436209\n",
      "[1445]\ttraining's binary_logloss: 0.436075\n",
      "[1446]\ttraining's binary_logloss: 0.435915\n",
      "[1447]\ttraining's binary_logloss: 0.435862\n",
      "[1448]\ttraining's binary_logloss: 0.435781\n",
      "[1449]\ttraining's binary_logloss: 0.435726\n",
      "[1450]\ttraining's binary_logloss: 0.435554\n",
      "[1451]\ttraining's binary_logloss: 0.43551\n",
      "[1452]\ttraining's binary_logloss: 0.435411\n",
      "[1453]\ttraining's binary_logloss: 0.43533\n",
      "[1454]\ttraining's binary_logloss: 0.435288\n",
      "[1455]\ttraining's binary_logloss: 0.435235\n",
      "[1456]\ttraining's binary_logloss: 0.435117\n",
      "[1457]\ttraining's binary_logloss: 0.435054\n",
      "[1458]\ttraining's binary_logloss: 0.434985\n",
      "[1459]\ttraining's binary_logloss: 0.43489\n",
      "[1460]\ttraining's binary_logloss: 0.434814\n",
      "[1461]\ttraining's binary_logloss: 0.434757\n",
      "[1462]\ttraining's binary_logloss: 0.434693\n",
      "[1463]\ttraining's binary_logloss: 0.434632\n",
      "[1464]\ttraining's binary_logloss: 0.434552\n",
      "[1465]\ttraining's binary_logloss: 0.434436\n",
      "[1466]\ttraining's binary_logloss: 0.434345\n",
      "[1467]\ttraining's binary_logloss: 0.434234\n",
      "[1468]\ttraining's binary_logloss: 0.434173\n",
      "[1469]\ttraining's binary_logloss: 0.434097\n",
      "[1470]\ttraining's binary_logloss: 0.434038\n",
      "[1471]\ttraining's binary_logloss: 0.43395\n",
      "[1472]\ttraining's binary_logloss: 0.433874\n",
      "[1473]\ttraining's binary_logloss: 0.433819\n",
      "[1474]\ttraining's binary_logloss: 0.433767\n",
      "[1475]\ttraining's binary_logloss: 0.433657\n",
      "[1476]\ttraining's binary_logloss: 0.433579\n",
      "[1477]\ttraining's binary_logloss: 0.433519\n",
      "[1478]\ttraining's binary_logloss: 0.433473\n",
      "[1479]\ttraining's binary_logloss: 0.433399\n",
      "[1480]\ttraining's binary_logloss: 0.433332\n",
      "[1481]\ttraining's binary_logloss: 0.433267\n",
      "[1482]\ttraining's binary_logloss: 0.433161\n",
      "[1483]\ttraining's binary_logloss: 0.433123\n",
      "[1484]\ttraining's binary_logloss: 0.433041\n",
      "[1485]\ttraining's binary_logloss: 0.432954\n",
      "[1486]\ttraining's binary_logloss: 0.432851\n",
      "[1487]\ttraining's binary_logloss: 0.43276\n",
      "[1488]\ttraining's binary_logloss: 0.432717\n",
      "[1489]\ttraining's binary_logloss: 0.432663\n",
      "[1490]\ttraining's binary_logloss: 0.4326\n",
      "[1491]\ttraining's binary_logloss: 0.432548\n",
      "[1492]\ttraining's binary_logloss: 0.432508\n",
      "[1493]\ttraining's binary_logloss: 0.43241\n",
      "[1494]\ttraining's binary_logloss: 0.432298\n",
      "[1495]\ttraining's binary_logloss: 0.432206\n",
      "[1496]\ttraining's binary_logloss: 0.43213\n",
      "[1497]\ttraining's binary_logloss: 0.432051\n",
      "[1498]\ttraining's binary_logloss: 0.432016\n",
      "[1499]\ttraining's binary_logloss: 0.431901\n",
      "[1500]\ttraining's binary_logloss: 0.431788\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as LightGBM\n",
    "\n",
    "lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=1500,\n",
    "                               max_depth = 50,\n",
    "                               min_data_in_leaf = 50,\n",
    "                               class_weight={True: 5, False: 1}\n",
    "                              ) \n",
    "\n",
    "evals = [(x_train_features, y_train_bool)]\n",
    "lgbm.fit(x_train_features, y_train_bool, eval_metric='logloss', eval_set=evals)\n",
    "y_pred = lgbm.predict(x_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.98      0.78      0.87     63391\n",
      "        risk       0.45      0.91      0.60     12724\n",
      "\n",
      "    accuracy                           0.80     76115\n",
      "   macro avg       0.71      0.84      0.73     76115\n",
      "weighted avg       0.89      0.80      0.82     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_train_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.87      0.70      0.77     21052\n",
      "        risk       0.25      0.48      0.33      4344\n",
      "\n",
      "    accuracy                           0.66     25396\n",
      "   macro avg       0.56      0.59      0.55     25396\n",
      "weighted avg       0.76      0.66      0.70     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tree Ensemble Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BPS',\n",
       " 'PER',\n",
       " 'PBR',\n",
       " 'EPS',\n",
       " 'DIV',\n",
       " 'DPS',\n",
       " '거래량',\n",
       " '시가총액',\n",
       " '금리',\n",
       " '유동자산',\n",
       " '비유동자산',\n",
       " '자산총계',\n",
       " '유동부채',\n",
       " '비유동부채',\n",
       " '부채총계',\n",
       " '이익잉여금',\n",
       " '자본총계',\n",
       " '매출액',\n",
       " '영업이익',\n",
       " '법인세차감전 순이익',\n",
       " '당기순이익',\n",
       " '자본금']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_feature_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "sfs_feature_list = ['BPS', 'DIV', '거래량', '금리', '비유동자산', '자산총계', '부채총계', '법인세차감전 순이익', '당기순이익']\n",
    "stock_info_list = ['BPS', 'PER', 'PBR', 'EPS', 'DIV', 'DPS', '거래량']\n",
    "financial_info_list = ['유동자산', '비유동자산', '자산총계', '유동부채', '비유동부채', '부채총계', '이익잉여금', '자본총계', '매출액', '영업이익', '법인세차감전 순이익', '당기순이익', '자본금']\n",
    "\n",
    "\n",
    "def make_feature_set(x) :\n",
    "    x_whole = x\n",
    "    x_rfecv = x[rfecv_feature_list]\n",
    "    x_sfs = x[sfs_feature_list]\n",
    "    x_f = x[financial_info_list]\n",
    "    x_s = x[stock_info_list]\n",
    "    return x_whole, x_rfecv, x_sfs, x_f, x_s\n",
    "\n",
    "\n",
    "x_whole, x_rfecv, x_sfs, x_f, x_s= make_feature_set(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=50, early_stopping_rounds=50 will be ignored. Current value: early_stopping_round=50\n",
      "[1]\ttraining's binary_logloss: 0.628963\n",
      "[2]\ttraining's binary_logloss: 0.623169\n",
      "[3]\ttraining's binary_logloss: 0.618132\n",
      "[4]\ttraining's binary_logloss: 0.613871\n",
      "[5]\ttraining's binary_logloss: 0.610405\n",
      "[6]\ttraining's binary_logloss: 0.607457\n",
      "[7]\ttraining's binary_logloss: 0.604673\n",
      "[8]\ttraining's binary_logloss: 0.602339\n",
      "[9]\ttraining's binary_logloss: 0.600031\n",
      "[10]\ttraining's binary_logloss: 0.598022\n",
      "[11]\ttraining's binary_logloss: 0.596078\n",
      "[12]\ttraining's binary_logloss: 0.594463\n",
      "[13]\ttraining's binary_logloss: 0.592965\n",
      "[14]\ttraining's binary_logloss: 0.591672\n",
      "[15]\ttraining's binary_logloss: 0.590438\n",
      "[16]\ttraining's binary_logloss: 0.589205\n",
      "[17]\ttraining's binary_logloss: 0.58799\n",
      "[18]\ttraining's binary_logloss: 0.586932\n",
      "[19]\ttraining's binary_logloss: 0.586027\n",
      "[20]\ttraining's binary_logloss: 0.585062\n",
      "[21]\ttraining's binary_logloss: 0.584205\n",
      "[22]\ttraining's binary_logloss: 0.583322\n",
      "[23]\ttraining's binary_logloss: 0.582515\n",
      "[24]\ttraining's binary_logloss: 0.581757\n",
      "[25]\ttraining's binary_logloss: 0.581065\n",
      "[26]\ttraining's binary_logloss: 0.580291\n",
      "[27]\ttraining's binary_logloss: 0.579554\n",
      "[28]\ttraining's binary_logloss: 0.578878\n",
      "[29]\ttraining's binary_logloss: 0.578191\n",
      "[30]\ttraining's binary_logloss: 0.577551\n",
      "[31]\ttraining's binary_logloss: 0.576949\n",
      "[32]\ttraining's binary_logloss: 0.576275\n",
      "[33]\ttraining's binary_logloss: 0.575626\n",
      "[34]\ttraining's binary_logloss: 0.575082\n",
      "[35]\ttraining's binary_logloss: 0.574525\n",
      "[36]\ttraining's binary_logloss: 0.573887\n",
      "[37]\ttraining's binary_logloss: 0.573376\n",
      "[38]\ttraining's binary_logloss: 0.572831\n",
      "[39]\ttraining's binary_logloss: 0.572289\n",
      "[40]\ttraining's binary_logloss: 0.57176\n",
      "[41]\ttraining's binary_logloss: 0.57127\n",
      "[42]\ttraining's binary_logloss: 0.570833\n",
      "[43]\ttraining's binary_logloss: 0.570326\n",
      "[44]\ttraining's binary_logloss: 0.569763\n",
      "[45]\ttraining's binary_logloss: 0.56932\n",
      "[46]\ttraining's binary_logloss: 0.568873\n",
      "[47]\ttraining's binary_logloss: 0.568429\n",
      "[48]\ttraining's binary_logloss: 0.567925\n",
      "[49]\ttraining's binary_logloss: 0.567528\n",
      "[50]\ttraining's binary_logloss: 0.567096\n",
      "[51]\ttraining's binary_logloss: 0.566672\n",
      "[52]\ttraining's binary_logloss: 0.566279\n",
      "[53]\ttraining's binary_logloss: 0.565862\n",
      "[54]\ttraining's binary_logloss: 0.565487\n",
      "[55]\ttraining's binary_logloss: 0.56503\n",
      "[56]\ttraining's binary_logloss: 0.564633\n",
      "[57]\ttraining's binary_logloss: 0.564211\n",
      "[58]\ttraining's binary_logloss: 0.563864\n",
      "[59]\ttraining's binary_logloss: 0.563461\n",
      "[60]\ttraining's binary_logloss: 0.563151\n",
      "[61]\ttraining's binary_logloss: 0.562823\n",
      "[62]\ttraining's binary_logloss: 0.562519\n",
      "[63]\ttraining's binary_logloss: 0.562131\n",
      "[64]\ttraining's binary_logloss: 0.561663\n",
      "[65]\ttraining's binary_logloss: 0.561196\n",
      "[66]\ttraining's binary_logloss: 0.560746\n",
      "[67]\ttraining's binary_logloss: 0.560411\n",
      "[68]\ttraining's binary_logloss: 0.560078\n",
      "[69]\ttraining's binary_logloss: 0.559679\n",
      "[70]\ttraining's binary_logloss: 0.5594\n",
      "[71]\ttraining's binary_logloss: 0.559048\n",
      "[72]\ttraining's binary_logloss: 0.55875\n",
      "[73]\ttraining's binary_logloss: 0.558479\n",
      "[74]\ttraining's binary_logloss: 0.558188\n",
      "[75]\ttraining's binary_logloss: 0.557852\n",
      "[76]\ttraining's binary_logloss: 0.557576\n",
      "[77]\ttraining's binary_logloss: 0.557132\n",
      "[78]\ttraining's binary_logloss: 0.55685\n",
      "[79]\ttraining's binary_logloss: 0.556548\n",
      "[80]\ttraining's binary_logloss: 0.556304\n",
      "[81]\ttraining's binary_logloss: 0.555917\n",
      "[82]\ttraining's binary_logloss: 0.555596\n",
      "[83]\ttraining's binary_logloss: 0.555257\n",
      "[84]\ttraining's binary_logloss: 0.554879\n",
      "[85]\ttraining's binary_logloss: 0.55459\n",
      "[86]\ttraining's binary_logloss: 0.554257\n",
      "[87]\ttraining's binary_logloss: 0.554022\n",
      "[88]\ttraining's binary_logloss: 0.553781\n",
      "[89]\ttraining's binary_logloss: 0.553501\n",
      "[90]\ttraining's binary_logloss: 0.553192\n",
      "[91]\ttraining's binary_logloss: 0.552838\n",
      "[92]\ttraining's binary_logloss: 0.552474\n",
      "[93]\ttraining's binary_logloss: 0.552238\n",
      "[94]\ttraining's binary_logloss: 0.551973\n",
      "[95]\ttraining's binary_logloss: 0.551712\n",
      "[96]\ttraining's binary_logloss: 0.551472\n",
      "[97]\ttraining's binary_logloss: 0.551184\n",
      "[98]\ttraining's binary_logloss: 0.550854\n",
      "[99]\ttraining's binary_logloss: 0.55057\n",
      "[100]\ttraining's binary_logloss: 0.55029\n",
      "[101]\ttraining's binary_logloss: 0.550033\n",
      "[102]\ttraining's binary_logloss: 0.549799\n",
      "[103]\ttraining's binary_logloss: 0.549484\n",
      "[104]\ttraining's binary_logloss: 0.549154\n",
      "[105]\ttraining's binary_logloss: 0.548913\n",
      "[106]\ttraining's binary_logloss: 0.548643\n",
      "[107]\ttraining's binary_logloss: 0.548331\n",
      "[108]\ttraining's binary_logloss: 0.548013\n",
      "[109]\ttraining's binary_logloss: 0.547777\n",
      "[110]\ttraining's binary_logloss: 0.547375\n",
      "[111]\ttraining's binary_logloss: 0.547132\n",
      "[112]\ttraining's binary_logloss: 0.546923\n",
      "[113]\ttraining's binary_logloss: 0.546686\n",
      "[114]\ttraining's binary_logloss: 0.546384\n",
      "[115]\ttraining's binary_logloss: 0.546143\n",
      "[116]\ttraining's binary_logloss: 0.545849\n",
      "[117]\ttraining's binary_logloss: 0.54567\n",
      "[118]\ttraining's binary_logloss: 0.545371\n",
      "[119]\ttraining's binary_logloss: 0.545069\n",
      "[120]\ttraining's binary_logloss: 0.544742\n",
      "[121]\ttraining's binary_logloss: 0.544384\n",
      "[122]\ttraining's binary_logloss: 0.544054\n",
      "[123]\ttraining's binary_logloss: 0.54374\n",
      "[124]\ttraining's binary_logloss: 0.543489\n",
      "[125]\ttraining's binary_logloss: 0.543209\n",
      "[126]\ttraining's binary_logloss: 0.542837\n",
      "[127]\ttraining's binary_logloss: 0.542633\n",
      "[128]\ttraining's binary_logloss: 0.542391\n",
      "[129]\ttraining's binary_logloss: 0.542102\n",
      "[130]\ttraining's binary_logloss: 0.54183\n",
      "[131]\ttraining's binary_logloss: 0.541624\n",
      "[132]\ttraining's binary_logloss: 0.541309\n",
      "[133]\ttraining's binary_logloss: 0.541043\n",
      "[134]\ttraining's binary_logloss: 0.540826\n",
      "[135]\ttraining's binary_logloss: 0.540511\n",
      "[136]\ttraining's binary_logloss: 0.540145\n",
      "[137]\ttraining's binary_logloss: 0.539931\n",
      "[138]\ttraining's binary_logloss: 0.539734\n",
      "[139]\ttraining's binary_logloss: 0.539565\n",
      "[140]\ttraining's binary_logloss: 0.539319\n",
      "[141]\ttraining's binary_logloss: 0.539047\n",
      "[142]\ttraining's binary_logloss: 0.538828\n",
      "[143]\ttraining's binary_logloss: 0.538638\n",
      "[144]\ttraining's binary_logloss: 0.538369\n",
      "[145]\ttraining's binary_logloss: 0.538118\n",
      "[146]\ttraining's binary_logloss: 0.537855\n",
      "[147]\ttraining's binary_logloss: 0.537596\n",
      "[148]\ttraining's binary_logloss: 0.537388\n",
      "[149]\ttraining's binary_logloss: 0.537133\n",
      "[150]\ttraining's binary_logloss: 0.536822\n",
      "[151]\ttraining's binary_logloss: 0.536527\n",
      "[152]\ttraining's binary_logloss: 0.536282\n",
      "[153]\ttraining's binary_logloss: 0.535975\n",
      "[154]\ttraining's binary_logloss: 0.535681\n",
      "[155]\ttraining's binary_logloss: 0.535497\n",
      "[156]\ttraining's binary_logloss: 0.53522\n",
      "[157]\ttraining's binary_logloss: 0.534839\n",
      "[158]\ttraining's binary_logloss: 0.534591\n",
      "[159]\ttraining's binary_logloss: 0.534298\n",
      "[160]\ttraining's binary_logloss: 0.534116\n",
      "[161]\ttraining's binary_logloss: 0.533928\n",
      "[162]\ttraining's binary_logloss: 0.533771\n",
      "[163]\ttraining's binary_logloss: 0.533576\n",
      "[164]\ttraining's binary_logloss: 0.533428\n",
      "[165]\ttraining's binary_logloss: 0.53321\n",
      "[166]\ttraining's binary_logloss: 0.533061\n",
      "[167]\ttraining's binary_logloss: 0.532792\n",
      "[168]\ttraining's binary_logloss: 0.532446\n",
      "[169]\ttraining's binary_logloss: 0.532181\n",
      "[170]\ttraining's binary_logloss: 0.531947\n",
      "[171]\ttraining's binary_logloss: 0.531703\n",
      "[172]\ttraining's binary_logloss: 0.531467\n",
      "[173]\ttraining's binary_logloss: 0.531269\n",
      "[174]\ttraining's binary_logloss: 0.53107\n",
      "[175]\ttraining's binary_logloss: 0.53081\n",
      "[176]\ttraining's binary_logloss: 0.530613\n",
      "[177]\ttraining's binary_logloss: 0.530322\n",
      "[178]\ttraining's binary_logloss: 0.530067\n",
      "[179]\ttraining's binary_logloss: 0.529826\n",
      "[180]\ttraining's binary_logloss: 0.529678\n",
      "[181]\ttraining's binary_logloss: 0.529545\n",
      "[182]\ttraining's binary_logloss: 0.52928\n",
      "[183]\ttraining's binary_logloss: 0.529072\n",
      "[184]\ttraining's binary_logloss: 0.528764\n",
      "[185]\ttraining's binary_logloss: 0.528605\n",
      "[186]\ttraining's binary_logloss: 0.528314\n",
      "[187]\ttraining's binary_logloss: 0.528015\n",
      "[188]\ttraining's binary_logloss: 0.527846\n",
      "[189]\ttraining's binary_logloss: 0.527646\n",
      "[190]\ttraining's binary_logloss: 0.527409\n",
      "[191]\ttraining's binary_logloss: 0.527284\n",
      "[192]\ttraining's binary_logloss: 0.52704\n",
      "[193]\ttraining's binary_logloss: 0.526782\n",
      "[194]\ttraining's binary_logloss: 0.526506\n",
      "[195]\ttraining's binary_logloss: 0.52622\n",
      "[196]\ttraining's binary_logloss: 0.525902\n",
      "[197]\ttraining's binary_logloss: 0.525704\n",
      "[198]\ttraining's binary_logloss: 0.525568\n",
      "[199]\ttraining's binary_logloss: 0.525439\n",
      "[200]\ttraining's binary_logloss: 0.525184\n",
      "[201]\ttraining's binary_logloss: 0.524963\n",
      "[202]\ttraining's binary_logloss: 0.524763\n",
      "[203]\ttraining's binary_logloss: 0.524554\n",
      "[204]\ttraining's binary_logloss: 0.524341\n",
      "[205]\ttraining's binary_logloss: 0.524146\n",
      "[206]\ttraining's binary_logloss: 0.523969\n",
      "[207]\ttraining's binary_logloss: 0.523749\n",
      "[208]\ttraining's binary_logloss: 0.523573\n",
      "[209]\ttraining's binary_logloss: 0.523306\n",
      "[210]\ttraining's binary_logloss: 0.523124\n",
      "[211]\ttraining's binary_logloss: 0.522977\n",
      "[212]\ttraining's binary_logloss: 0.522834\n",
      "[213]\ttraining's binary_logloss: 0.522621\n",
      "[214]\ttraining's binary_logloss: 0.522394\n",
      "[215]\ttraining's binary_logloss: 0.52209\n",
      "[216]\ttraining's binary_logloss: 0.521829\n",
      "[217]\ttraining's binary_logloss: 0.521638\n",
      "[218]\ttraining's binary_logloss: 0.521505\n",
      "[219]\ttraining's binary_logloss: 0.521299\n",
      "[220]\ttraining's binary_logloss: 0.521062\n",
      "[221]\ttraining's binary_logloss: 0.520805\n",
      "[222]\ttraining's binary_logloss: 0.520564\n",
      "[223]\ttraining's binary_logloss: 0.520301\n",
      "[224]\ttraining's binary_logloss: 0.520175\n",
      "[225]\ttraining's binary_logloss: 0.520006\n",
      "[226]\ttraining's binary_logloss: 0.519783\n",
      "[227]\ttraining's binary_logloss: 0.519583\n",
      "[228]\ttraining's binary_logloss: 0.519397\n",
      "[229]\ttraining's binary_logloss: 0.519158\n",
      "[230]\ttraining's binary_logloss: 0.518981\n",
      "[231]\ttraining's binary_logloss: 0.518752\n",
      "[232]\ttraining's binary_logloss: 0.518537\n",
      "[233]\ttraining's binary_logloss: 0.518417\n",
      "[234]\ttraining's binary_logloss: 0.518313\n",
      "[235]\ttraining's binary_logloss: 0.518119\n",
      "[236]\ttraining's binary_logloss: 0.517887\n",
      "[237]\ttraining's binary_logloss: 0.517676\n",
      "[238]\ttraining's binary_logloss: 0.517511\n",
      "[239]\ttraining's binary_logloss: 0.517324\n",
      "[240]\ttraining's binary_logloss: 0.51706\n",
      "[241]\ttraining's binary_logloss: 0.516854\n",
      "[242]\ttraining's binary_logloss: 0.516727\n",
      "[243]\ttraining's binary_logloss: 0.51652\n",
      "[244]\ttraining's binary_logloss: 0.516268\n",
      "[245]\ttraining's binary_logloss: 0.515985\n",
      "[246]\ttraining's binary_logloss: 0.515788\n",
      "[247]\ttraining's binary_logloss: 0.515488\n",
      "[248]\ttraining's binary_logloss: 0.515241\n",
      "[249]\ttraining's binary_logloss: 0.515049\n",
      "[250]\ttraining's binary_logloss: 0.514931\n",
      "[251]\ttraining's binary_logloss: 0.514815\n",
      "[252]\ttraining's binary_logloss: 0.514585\n",
      "[253]\ttraining's binary_logloss: 0.514342\n",
      "[254]\ttraining's binary_logloss: 0.514209\n",
      "[255]\ttraining's binary_logloss: 0.513976\n",
      "[256]\ttraining's binary_logloss: 0.513734\n",
      "[257]\ttraining's binary_logloss: 0.513576\n",
      "[258]\ttraining's binary_logloss: 0.513421\n",
      "[259]\ttraining's binary_logloss: 0.513241\n",
      "[260]\ttraining's binary_logloss: 0.51309\n",
      "[261]\ttraining's binary_logloss: 0.512969\n",
      "[262]\ttraining's binary_logloss: 0.512757\n",
      "[263]\ttraining's binary_logloss: 0.512501\n",
      "[264]\ttraining's binary_logloss: 0.512233\n",
      "[265]\ttraining's binary_logloss: 0.511984\n",
      "[266]\ttraining's binary_logloss: 0.511743\n",
      "[267]\ttraining's binary_logloss: 0.511542\n",
      "[268]\ttraining's binary_logloss: 0.511428\n",
      "[269]\ttraining's binary_logloss: 0.511197\n",
      "[270]\ttraining's binary_logloss: 0.510995\n",
      "[271]\ttraining's binary_logloss: 0.510851\n",
      "[272]\ttraining's binary_logloss: 0.510668\n",
      "[273]\ttraining's binary_logloss: 0.510424\n",
      "[274]\ttraining's binary_logloss: 0.510257\n",
      "[275]\ttraining's binary_logloss: 0.509956\n",
      "[276]\ttraining's binary_logloss: 0.509736\n",
      "[277]\ttraining's binary_logloss: 0.509524\n",
      "[278]\ttraining's binary_logloss: 0.509372\n",
      "[279]\ttraining's binary_logloss: 0.509159\n",
      "[280]\ttraining's binary_logloss: 0.509044\n",
      "[281]\ttraining's binary_logloss: 0.508841\n",
      "[282]\ttraining's binary_logloss: 0.5086\n",
      "[283]\ttraining's binary_logloss: 0.508433\n",
      "[284]\ttraining's binary_logloss: 0.508313\n",
      "[285]\ttraining's binary_logloss: 0.508061\n",
      "[286]\ttraining's binary_logloss: 0.507856\n",
      "[287]\ttraining's binary_logloss: 0.507655\n",
      "[288]\ttraining's binary_logloss: 0.507357\n",
      "[289]\ttraining's binary_logloss: 0.507135\n",
      "[290]\ttraining's binary_logloss: 0.506967\n",
      "[291]\ttraining's binary_logloss: 0.506698\n",
      "[292]\ttraining's binary_logloss: 0.506435\n",
      "[293]\ttraining's binary_logloss: 0.506289\n",
      "[294]\ttraining's binary_logloss: 0.506064\n",
      "[295]\ttraining's binary_logloss: 0.505848\n",
      "[296]\ttraining's binary_logloss: 0.505533\n",
      "[297]\ttraining's binary_logloss: 0.505319\n",
      "[298]\ttraining's binary_logloss: 0.505133\n",
      "[299]\ttraining's binary_logloss: 0.504953\n",
      "[300]\ttraining's binary_logloss: 0.504651\n",
      "[301]\ttraining's binary_logloss: 0.504473\n",
      "[302]\ttraining's binary_logloss: 0.50428\n",
      "[303]\ttraining's binary_logloss: 0.504172\n",
      "[304]\ttraining's binary_logloss: 0.504031\n",
      "[305]\ttraining's binary_logloss: 0.503866\n",
      "[306]\ttraining's binary_logloss: 0.503646\n",
      "[307]\ttraining's binary_logloss: 0.503457\n",
      "[308]\ttraining's binary_logloss: 0.503216\n",
      "[309]\ttraining's binary_logloss: 0.503005\n",
      "[310]\ttraining's binary_logloss: 0.502836\n",
      "[311]\ttraining's binary_logloss: 0.50264\n",
      "[312]\ttraining's binary_logloss: 0.502439\n",
      "[313]\ttraining's binary_logloss: 0.502245\n",
      "[314]\ttraining's binary_logloss: 0.502032\n",
      "[315]\ttraining's binary_logloss: 0.501869\n",
      "[316]\ttraining's binary_logloss: 0.501737\n",
      "[317]\ttraining's binary_logloss: 0.501593\n",
      "[318]\ttraining's binary_logloss: 0.501455\n",
      "[319]\ttraining's binary_logloss: 0.501214\n",
      "[320]\ttraining's binary_logloss: 0.501036\n",
      "[321]\ttraining's binary_logloss: 0.500769\n",
      "[322]\ttraining's binary_logloss: 0.500602\n",
      "[323]\ttraining's binary_logloss: 0.500264\n",
      "[324]\ttraining's binary_logloss: 0.500118\n",
      "[325]\ttraining's binary_logloss: 0.499936\n",
      "[326]\ttraining's binary_logloss: 0.499858\n",
      "[327]\ttraining's binary_logloss: 0.499676\n",
      "[328]\ttraining's binary_logloss: 0.499466\n",
      "[329]\ttraining's binary_logloss: 0.499334\n",
      "[330]\ttraining's binary_logloss: 0.499136\n",
      "[331]\ttraining's binary_logloss: 0.498983\n",
      "[332]\ttraining's binary_logloss: 0.498775\n",
      "[333]\ttraining's binary_logloss: 0.498609\n",
      "[334]\ttraining's binary_logloss: 0.498406\n",
      "[335]\ttraining's binary_logloss: 0.498241\n",
      "[336]\ttraining's binary_logloss: 0.498126\n",
      "[337]\ttraining's binary_logloss: 0.497983\n",
      "[338]\ttraining's binary_logloss: 0.497813\n",
      "[339]\ttraining's binary_logloss: 0.497574\n",
      "[340]\ttraining's binary_logloss: 0.497463\n",
      "[341]\ttraining's binary_logloss: 0.497246\n",
      "[342]\ttraining's binary_logloss: 0.497033\n",
      "[343]\ttraining's binary_logloss: 0.496933\n",
      "[344]\ttraining's binary_logloss: 0.496718\n",
      "[345]\ttraining's binary_logloss: 0.49653\n",
      "[346]\ttraining's binary_logloss: 0.496253\n",
      "[347]\ttraining's binary_logloss: 0.496105\n",
      "[348]\ttraining's binary_logloss: 0.495853\n",
      "[349]\ttraining's binary_logloss: 0.495633\n",
      "[350]\ttraining's binary_logloss: 0.495453\n",
      "[351]\ttraining's binary_logloss: 0.495376\n",
      "[352]\ttraining's binary_logloss: 0.495258\n",
      "[353]\ttraining's binary_logloss: 0.495034\n",
      "[354]\ttraining's binary_logloss: 0.494871\n",
      "[355]\ttraining's binary_logloss: 0.494659\n",
      "[356]\ttraining's binary_logloss: 0.494448\n",
      "[357]\ttraining's binary_logloss: 0.494341\n",
      "[358]\ttraining's binary_logloss: 0.494148\n",
      "[359]\ttraining's binary_logloss: 0.49405\n",
      "[360]\ttraining's binary_logloss: 0.493858\n",
      "[361]\ttraining's binary_logloss: 0.4937\n",
      "[362]\ttraining's binary_logloss: 0.493492\n",
      "[363]\ttraining's binary_logloss: 0.49338\n",
      "[364]\ttraining's binary_logloss: 0.493125\n",
      "[365]\ttraining's binary_logloss: 0.492962\n",
      "[366]\ttraining's binary_logloss: 0.492737\n",
      "[367]\ttraining's binary_logloss: 0.492438\n",
      "[368]\ttraining's binary_logloss: 0.492224\n",
      "[369]\ttraining's binary_logloss: 0.492001\n",
      "[370]\ttraining's binary_logloss: 0.491811\n",
      "[371]\ttraining's binary_logloss: 0.491696\n",
      "[372]\ttraining's binary_logloss: 0.491553\n",
      "[373]\ttraining's binary_logloss: 0.491436\n",
      "[374]\ttraining's binary_logloss: 0.491236\n",
      "[375]\ttraining's binary_logloss: 0.491145\n",
      "[376]\ttraining's binary_logloss: 0.490904\n",
      "[377]\ttraining's binary_logloss: 0.490739\n",
      "[378]\ttraining's binary_logloss: 0.490539\n",
      "[379]\ttraining's binary_logloss: 0.490312\n",
      "[380]\ttraining's binary_logloss: 0.490081\n",
      "[381]\ttraining's binary_logloss: 0.489937\n",
      "[382]\ttraining's binary_logloss: 0.48973\n",
      "[383]\ttraining's binary_logloss: 0.489605\n",
      "[384]\ttraining's binary_logloss: 0.489412\n",
      "[385]\ttraining's binary_logloss: 0.489313\n",
      "[386]\ttraining's binary_logloss: 0.48925\n",
      "[387]\ttraining's binary_logloss: 0.488976\n",
      "[388]\ttraining's binary_logloss: 0.488783\n",
      "[389]\ttraining's binary_logloss: 0.488631\n",
      "[390]\ttraining's binary_logloss: 0.488418\n",
      "[391]\ttraining's binary_logloss: 0.48824\n",
      "[392]\ttraining's binary_logloss: 0.488073\n",
      "[393]\ttraining's binary_logloss: 0.487853\n",
      "[394]\ttraining's binary_logloss: 0.48769\n",
      "[395]\ttraining's binary_logloss: 0.487534\n",
      "[396]\ttraining's binary_logloss: 0.487411\n",
      "[397]\ttraining's binary_logloss: 0.487266\n",
      "[398]\ttraining's binary_logloss: 0.487084\n",
      "[399]\ttraining's binary_logloss: 0.486854\n",
      "[400]\ttraining's binary_logloss: 0.486568\n",
      "[401]\ttraining's binary_logloss: 0.486376\n",
      "[402]\ttraining's binary_logloss: 0.486231\n",
      "[403]\ttraining's binary_logloss: 0.486077\n",
      "[404]\ttraining's binary_logloss: 0.485881\n",
      "[405]\ttraining's binary_logloss: 0.485688\n",
      "[406]\ttraining's binary_logloss: 0.485563\n",
      "[407]\ttraining's binary_logloss: 0.485447\n",
      "[408]\ttraining's binary_logloss: 0.485356\n",
      "[409]\ttraining's binary_logloss: 0.485249\n",
      "[410]\ttraining's binary_logloss: 0.485068\n",
      "[411]\ttraining's binary_logloss: 0.484849\n",
      "[412]\ttraining's binary_logloss: 0.484686\n",
      "[413]\ttraining's binary_logloss: 0.484494\n",
      "[414]\ttraining's binary_logloss: 0.484333\n",
      "[415]\ttraining's binary_logloss: 0.484214\n",
      "[416]\ttraining's binary_logloss: 0.484\n",
      "[417]\ttraining's binary_logloss: 0.483872\n",
      "[418]\ttraining's binary_logloss: 0.483718\n",
      "[419]\ttraining's binary_logloss: 0.483523\n",
      "[420]\ttraining's binary_logloss: 0.483186\n",
      "[421]\ttraining's binary_logloss: 0.482936\n",
      "[422]\ttraining's binary_logloss: 0.482771\n",
      "[423]\ttraining's binary_logloss: 0.482646\n",
      "[424]\ttraining's binary_logloss: 0.482517\n",
      "[425]\ttraining's binary_logloss: 0.482368\n",
      "[426]\ttraining's binary_logloss: 0.482195\n",
      "[427]\ttraining's binary_logloss: 0.482082\n",
      "[428]\ttraining's binary_logloss: 0.481847\n",
      "[429]\ttraining's binary_logloss: 0.481691\n",
      "[430]\ttraining's binary_logloss: 0.481534\n",
      "[431]\ttraining's binary_logloss: 0.481309\n",
      "[432]\ttraining's binary_logloss: 0.481115\n",
      "[433]\ttraining's binary_logloss: 0.480905\n",
      "[434]\ttraining's binary_logloss: 0.480809\n",
      "[435]\ttraining's binary_logloss: 0.480606\n",
      "[436]\ttraining's binary_logloss: 0.480391\n",
      "[437]\ttraining's binary_logloss: 0.480306\n",
      "[438]\ttraining's binary_logloss: 0.480102\n",
      "[439]\ttraining's binary_logloss: 0.479944\n",
      "[440]\ttraining's binary_logloss: 0.479826\n",
      "[441]\ttraining's binary_logloss: 0.479665\n",
      "[442]\ttraining's binary_logloss: 0.479483\n",
      "[443]\ttraining's binary_logloss: 0.479281\n",
      "[444]\ttraining's binary_logloss: 0.47905\n",
      "[445]\ttraining's binary_logloss: 0.478883\n",
      "[446]\ttraining's binary_logloss: 0.478725\n",
      "[447]\ttraining's binary_logloss: 0.478521\n",
      "[448]\ttraining's binary_logloss: 0.478379\n",
      "[449]\ttraining's binary_logloss: 0.478181\n",
      "[450]\ttraining's binary_logloss: 0.47794\n",
      "[451]\ttraining's binary_logloss: 0.477785\n",
      "[452]\ttraining's binary_logloss: 0.477612\n",
      "[453]\ttraining's binary_logloss: 0.477426\n",
      "[454]\ttraining's binary_logloss: 0.477348\n",
      "[455]\ttraining's binary_logloss: 0.477095\n",
      "[456]\ttraining's binary_logloss: 0.476935\n",
      "[457]\ttraining's binary_logloss: 0.476787\n",
      "[458]\ttraining's binary_logloss: 0.476592\n",
      "[459]\ttraining's binary_logloss: 0.476478\n",
      "[460]\ttraining's binary_logloss: 0.476164\n",
      "[461]\ttraining's binary_logloss: 0.47592\n",
      "[462]\ttraining's binary_logloss: 0.475767\n",
      "[463]\ttraining's binary_logloss: 0.47568\n",
      "[464]\ttraining's binary_logloss: 0.475434\n",
      "[465]\ttraining's binary_logloss: 0.475247\n",
      "[466]\ttraining's binary_logloss: 0.475108\n",
      "[467]\ttraining's binary_logloss: 0.474981\n",
      "[468]\ttraining's binary_logloss: 0.474711\n",
      "[469]\ttraining's binary_logloss: 0.474549\n",
      "[470]\ttraining's binary_logloss: 0.474432\n",
      "[471]\ttraining's binary_logloss: 0.474252\n",
      "[472]\ttraining's binary_logloss: 0.474114\n",
      "[473]\ttraining's binary_logloss: 0.473909\n",
      "[474]\ttraining's binary_logloss: 0.473744\n",
      "[475]\ttraining's binary_logloss: 0.473617\n",
      "[476]\ttraining's binary_logloss: 0.47342\n",
      "[477]\ttraining's binary_logloss: 0.473228\n",
      "[478]\ttraining's binary_logloss: 0.472896\n",
      "[479]\ttraining's binary_logloss: 0.472775\n",
      "[480]\ttraining's binary_logloss: 0.472636\n",
      "[481]\ttraining's binary_logloss: 0.472495\n",
      "[482]\ttraining's binary_logloss: 0.472153\n",
      "[483]\ttraining's binary_logloss: 0.471953\n",
      "[484]\ttraining's binary_logloss: 0.471806\n",
      "[485]\ttraining's binary_logloss: 0.471665\n",
      "[486]\ttraining's binary_logloss: 0.471517\n",
      "[487]\ttraining's binary_logloss: 0.47133\n",
      "[488]\ttraining's binary_logloss: 0.471176\n",
      "[489]\ttraining's binary_logloss: 0.470996\n",
      "[490]\ttraining's binary_logloss: 0.470837\n",
      "[491]\ttraining's binary_logloss: 0.470672\n",
      "[492]\ttraining's binary_logloss: 0.470514\n",
      "[493]\ttraining's binary_logloss: 0.470294\n",
      "[494]\ttraining's binary_logloss: 0.470209\n",
      "[495]\ttraining's binary_logloss: 0.470052\n",
      "[496]\ttraining's binary_logloss: 0.469856\n",
      "[497]\ttraining's binary_logloss: 0.469716\n",
      "[498]\ttraining's binary_logloss: 0.469499\n",
      "[499]\ttraining's binary_logloss: 0.469329\n",
      "[500]\ttraining's binary_logloss: 0.469195\n",
      "[1]\ttraining's binary_logloss: 0.628988\n",
      "[2]\ttraining's binary_logloss: 0.623424\n",
      "[3]\ttraining's binary_logloss: 0.618594\n",
      "[4]\ttraining's binary_logloss: 0.614541\n",
      "[5]\ttraining's binary_logloss: 0.611217\n",
      "[6]\ttraining's binary_logloss: 0.608271\n",
      "[7]\ttraining's binary_logloss: 0.605842\n",
      "[8]\ttraining's binary_logloss: 0.603481\n",
      "[9]\ttraining's binary_logloss: 0.6014\n",
      "[10]\ttraining's binary_logloss: 0.599418\n",
      "[11]\ttraining's binary_logloss: 0.597893\n",
      "[12]\ttraining's binary_logloss: 0.596375\n",
      "[13]\ttraining's binary_logloss: 0.594996\n",
      "[14]\ttraining's binary_logloss: 0.59367\n",
      "[15]\ttraining's binary_logloss: 0.592428\n",
      "[16]\ttraining's binary_logloss: 0.591239\n",
      "[17]\ttraining's binary_logloss: 0.590224\n",
      "[18]\ttraining's binary_logloss: 0.589195\n",
      "[19]\ttraining's binary_logloss: 0.588204\n",
      "[20]\ttraining's binary_logloss: 0.587215\n",
      "[21]\ttraining's binary_logloss: 0.586474\n",
      "[22]\ttraining's binary_logloss: 0.58577\n",
      "[23]\ttraining's binary_logloss: 0.584994\n",
      "[24]\ttraining's binary_logloss: 0.584274\n",
      "[25]\ttraining's binary_logloss: 0.583608\n",
      "[26]\ttraining's binary_logloss: 0.582952\n",
      "[27]\ttraining's binary_logloss: 0.582369\n",
      "[28]\ttraining's binary_logloss: 0.581682\n",
      "[29]\ttraining's binary_logloss: 0.581004\n",
      "[30]\ttraining's binary_logloss: 0.580331\n",
      "[31]\ttraining's binary_logloss: 0.579719\n",
      "[32]\ttraining's binary_logloss: 0.579187\n",
      "[33]\ttraining's binary_logloss: 0.578456\n",
      "[34]\ttraining's binary_logloss: 0.577923\n",
      "[35]\ttraining's binary_logloss: 0.577372\n",
      "[36]\ttraining's binary_logloss: 0.576778\n",
      "[37]\ttraining's binary_logloss: 0.57628\n",
      "[38]\ttraining's binary_logloss: 0.575791\n",
      "[39]\ttraining's binary_logloss: 0.575329\n",
      "[40]\ttraining's binary_logloss: 0.574874\n",
      "[41]\ttraining's binary_logloss: 0.574448\n",
      "[42]\ttraining's binary_logloss: 0.574\n",
      "[43]\ttraining's binary_logloss: 0.573592\n",
      "[44]\ttraining's binary_logloss: 0.573082\n",
      "[45]\ttraining's binary_logloss: 0.572633\n",
      "[46]\ttraining's binary_logloss: 0.572178\n",
      "[47]\ttraining's binary_logloss: 0.571777\n",
      "[48]\ttraining's binary_logloss: 0.571301\n",
      "[49]\ttraining's binary_logloss: 0.570935\n",
      "[50]\ttraining's binary_logloss: 0.570414\n",
      "[51]\ttraining's binary_logloss: 0.570016\n",
      "[52]\ttraining's binary_logloss: 0.569662\n",
      "[53]\ttraining's binary_logloss: 0.569274\n",
      "[54]\ttraining's binary_logloss: 0.568918\n",
      "[55]\ttraining's binary_logloss: 0.568571\n",
      "[56]\ttraining's binary_logloss: 0.568195\n",
      "[57]\ttraining's binary_logloss: 0.567834\n",
      "[58]\ttraining's binary_logloss: 0.567481\n",
      "[59]\ttraining's binary_logloss: 0.567138\n",
      "[60]\ttraining's binary_logloss: 0.566778\n",
      "[61]\ttraining's binary_logloss: 0.566468\n",
      "[62]\ttraining's binary_logloss: 0.566047\n",
      "[63]\ttraining's binary_logloss: 0.56572\n",
      "[64]\ttraining's binary_logloss: 0.56539\n",
      "[65]\ttraining's binary_logloss: 0.565053\n",
      "[66]\ttraining's binary_logloss: 0.564772\n",
      "[67]\ttraining's binary_logloss: 0.564447\n",
      "[68]\ttraining's binary_logloss: 0.564144\n",
      "[69]\ttraining's binary_logloss: 0.563821\n",
      "[70]\ttraining's binary_logloss: 0.563515\n",
      "[71]\ttraining's binary_logloss: 0.563229\n",
      "[72]\ttraining's binary_logloss: 0.562984\n",
      "[73]\ttraining's binary_logloss: 0.562686\n",
      "[74]\ttraining's binary_logloss: 0.562409\n",
      "[75]\ttraining's binary_logloss: 0.562027\n",
      "[76]\ttraining's binary_logloss: 0.56173\n",
      "[77]\ttraining's binary_logloss: 0.561475\n",
      "[78]\ttraining's binary_logloss: 0.561119\n",
      "[79]\ttraining's binary_logloss: 0.56076\n",
      "[80]\ttraining's binary_logloss: 0.560438\n",
      "[81]\ttraining's binary_logloss: 0.560075\n",
      "[82]\ttraining's binary_logloss: 0.559724\n",
      "[83]\ttraining's binary_logloss: 0.559428\n",
      "[84]\ttraining's binary_logloss: 0.559181\n",
      "[85]\ttraining's binary_logloss: 0.558868\n",
      "[86]\ttraining's binary_logloss: 0.558535\n",
      "[87]\ttraining's binary_logloss: 0.558205\n",
      "[88]\ttraining's binary_logloss: 0.557891\n",
      "[89]\ttraining's binary_logloss: 0.557587\n",
      "[90]\ttraining's binary_logloss: 0.557372\n",
      "[91]\ttraining's binary_logloss: 0.557102\n",
      "[92]\ttraining's binary_logloss: 0.556883\n",
      "[93]\ttraining's binary_logloss: 0.556548\n",
      "[94]\ttraining's binary_logloss: 0.556256\n",
      "[95]\ttraining's binary_logloss: 0.556053\n",
      "[96]\ttraining's binary_logloss: 0.555796\n",
      "[97]\ttraining's binary_logloss: 0.555554\n",
      "[98]\ttraining's binary_logloss: 0.555236\n",
      "[99]\ttraining's binary_logloss: 0.554936\n",
      "[100]\ttraining's binary_logloss: 0.554647\n",
      "[101]\ttraining's binary_logloss: 0.554332\n",
      "[102]\ttraining's binary_logloss: 0.554063\n",
      "[103]\ttraining's binary_logloss: 0.553904\n",
      "[104]\ttraining's binary_logloss: 0.553679\n",
      "[105]\ttraining's binary_logloss: 0.553354\n",
      "[106]\ttraining's binary_logloss: 0.553053\n",
      "[107]\ttraining's binary_logloss: 0.552677\n",
      "[108]\ttraining's binary_logloss: 0.552406\n",
      "[109]\ttraining's binary_logloss: 0.552143\n",
      "[110]\ttraining's binary_logloss: 0.551944\n",
      "[111]\ttraining's binary_logloss: 0.551635\n",
      "[112]\ttraining's binary_logloss: 0.551407\n",
      "[113]\ttraining's binary_logloss: 0.551119\n",
      "[114]\ttraining's binary_logloss: 0.550899\n",
      "[115]\ttraining's binary_logloss: 0.550603\n",
      "[116]\ttraining's binary_logloss: 0.550249\n",
      "[117]\ttraining's binary_logloss: 0.549906\n",
      "[118]\ttraining's binary_logloss: 0.549745\n",
      "[119]\ttraining's binary_logloss: 0.549544\n",
      "[120]\ttraining's binary_logloss: 0.549305\n",
      "[121]\ttraining's binary_logloss: 0.549042\n",
      "[122]\ttraining's binary_logloss: 0.548853\n",
      "[123]\ttraining's binary_logloss: 0.548619\n",
      "[124]\ttraining's binary_logloss: 0.548349\n",
      "[125]\ttraining's binary_logloss: 0.548055\n",
      "[126]\ttraining's binary_logloss: 0.547752\n",
      "[127]\ttraining's binary_logloss: 0.547485\n",
      "[128]\ttraining's binary_logloss: 0.547217\n",
      "[129]\ttraining's binary_logloss: 0.54692\n",
      "[130]\ttraining's binary_logloss: 0.546675\n",
      "[131]\ttraining's binary_logloss: 0.546358\n",
      "[132]\ttraining's binary_logloss: 0.546066\n",
      "[133]\ttraining's binary_logloss: 0.545817\n",
      "[134]\ttraining's binary_logloss: 0.545586\n",
      "[135]\ttraining's binary_logloss: 0.545295\n",
      "[136]\ttraining's binary_logloss: 0.545124\n",
      "[137]\ttraining's binary_logloss: 0.544895\n",
      "[138]\ttraining's binary_logloss: 0.544657\n",
      "[139]\ttraining's binary_logloss: 0.544478\n",
      "[140]\ttraining's binary_logloss: 0.544129\n",
      "[141]\ttraining's binary_logloss: 0.54382\n",
      "[142]\ttraining's binary_logloss: 0.543594\n",
      "[143]\ttraining's binary_logloss: 0.543341\n",
      "[144]\ttraining's binary_logloss: 0.543079\n",
      "[145]\ttraining's binary_logloss: 0.542839\n",
      "[146]\ttraining's binary_logloss: 0.542531\n",
      "[147]\ttraining's binary_logloss: 0.542197\n",
      "[148]\ttraining's binary_logloss: 0.541904\n",
      "[149]\ttraining's binary_logloss: 0.54162\n",
      "[150]\ttraining's binary_logloss: 0.541396\n",
      "[151]\ttraining's binary_logloss: 0.54111\n",
      "[152]\ttraining's binary_logloss: 0.540801\n",
      "[153]\ttraining's binary_logloss: 0.540597\n",
      "[154]\ttraining's binary_logloss: 0.540405\n",
      "[155]\ttraining's binary_logloss: 0.540123\n",
      "[156]\ttraining's binary_logloss: 0.53988\n",
      "[157]\ttraining's binary_logloss: 0.539573\n",
      "[158]\ttraining's binary_logloss: 0.5393\n",
      "[159]\ttraining's binary_logloss: 0.539074\n",
      "[160]\ttraining's binary_logloss: 0.538862\n",
      "[161]\ttraining's binary_logloss: 0.538622\n",
      "[162]\ttraining's binary_logloss: 0.538377\n",
      "[163]\ttraining's binary_logloss: 0.538179\n",
      "[164]\ttraining's binary_logloss: 0.537951\n",
      "[165]\ttraining's binary_logloss: 0.537754\n",
      "[166]\ttraining's binary_logloss: 0.53752\n",
      "[167]\ttraining's binary_logloss: 0.537216\n",
      "[168]\ttraining's binary_logloss: 0.536995\n",
      "[169]\ttraining's binary_logloss: 0.536817\n",
      "[170]\ttraining's binary_logloss: 0.536537\n",
      "[171]\ttraining's binary_logloss: 0.536343\n",
      "[172]\ttraining's binary_logloss: 0.536095\n",
      "[173]\ttraining's binary_logloss: 0.535962\n",
      "[174]\ttraining's binary_logloss: 0.535737\n",
      "[175]\ttraining's binary_logloss: 0.535541\n",
      "[176]\ttraining's binary_logloss: 0.535357\n",
      "[177]\ttraining's binary_logloss: 0.535153\n",
      "[178]\ttraining's binary_logloss: 0.534939\n",
      "[179]\ttraining's binary_logloss: 0.534788\n",
      "[180]\ttraining's binary_logloss: 0.534558\n",
      "[181]\ttraining's binary_logloss: 0.534319\n",
      "[182]\ttraining's binary_logloss: 0.534124\n",
      "[183]\ttraining's binary_logloss: 0.533842\n",
      "[184]\ttraining's binary_logloss: 0.533618\n",
      "[185]\ttraining's binary_logloss: 0.533486\n",
      "[186]\ttraining's binary_logloss: 0.533334\n",
      "[187]\ttraining's binary_logloss: 0.533199\n",
      "[188]\ttraining's binary_logloss: 0.532997\n",
      "[189]\ttraining's binary_logloss: 0.532803\n",
      "[190]\ttraining's binary_logloss: 0.532514\n",
      "[191]\ttraining's binary_logloss: 0.53227\n",
      "[192]\ttraining's binary_logloss: 0.531957\n",
      "[193]\ttraining's binary_logloss: 0.531782\n",
      "[194]\ttraining's binary_logloss: 0.531608\n",
      "[195]\ttraining's binary_logloss: 0.531337\n",
      "[196]\ttraining's binary_logloss: 0.531147\n",
      "[197]\ttraining's binary_logloss: 0.530816\n",
      "[198]\ttraining's binary_logloss: 0.530564\n",
      "[199]\ttraining's binary_logloss: 0.530316\n",
      "[200]\ttraining's binary_logloss: 0.530105\n",
      "[201]\ttraining's binary_logloss: 0.529863\n",
      "[202]\ttraining's binary_logloss: 0.529693\n",
      "[203]\ttraining's binary_logloss: 0.529545\n",
      "[204]\ttraining's binary_logloss: 0.529348\n",
      "[205]\ttraining's binary_logloss: 0.529203\n",
      "[206]\ttraining's binary_logloss: 0.529069\n",
      "[207]\ttraining's binary_logloss: 0.528844\n",
      "[208]\ttraining's binary_logloss: 0.528662\n",
      "[209]\ttraining's binary_logloss: 0.528463\n",
      "[210]\ttraining's binary_logloss: 0.528277\n",
      "[211]\ttraining's binary_logloss: 0.527926\n",
      "[212]\ttraining's binary_logloss: 0.527683\n",
      "[213]\ttraining's binary_logloss: 0.52743\n",
      "[214]\ttraining's binary_logloss: 0.52712\n",
      "[215]\ttraining's binary_logloss: 0.526964\n",
      "[216]\ttraining's binary_logloss: 0.526747\n",
      "[217]\ttraining's binary_logloss: 0.526572\n",
      "[218]\ttraining's binary_logloss: 0.526354\n",
      "[219]\ttraining's binary_logloss: 0.526146\n",
      "[220]\ttraining's binary_logloss: 0.526034\n",
      "[221]\ttraining's binary_logloss: 0.525743\n",
      "[222]\ttraining's binary_logloss: 0.52556\n",
      "[223]\ttraining's binary_logloss: 0.525384\n",
      "[224]\ttraining's binary_logloss: 0.525146\n",
      "[225]\ttraining's binary_logloss: 0.524942\n",
      "[226]\ttraining's binary_logloss: 0.524789\n",
      "[227]\ttraining's binary_logloss: 0.524539\n",
      "[228]\ttraining's binary_logloss: 0.524353\n",
      "[229]\ttraining's binary_logloss: 0.524101\n",
      "[230]\ttraining's binary_logloss: 0.523945\n",
      "[231]\ttraining's binary_logloss: 0.523672\n",
      "[232]\ttraining's binary_logloss: 0.523478\n",
      "[233]\ttraining's binary_logloss: 0.523262\n",
      "[234]\ttraining's binary_logloss: 0.522981\n",
      "[235]\ttraining's binary_logloss: 0.522748\n",
      "[236]\ttraining's binary_logloss: 0.522592\n",
      "[237]\ttraining's binary_logloss: 0.522476\n",
      "[238]\ttraining's binary_logloss: 0.522241\n",
      "[239]\ttraining's binary_logloss: 0.52209\n",
      "[240]\ttraining's binary_logloss: 0.521885\n",
      "[241]\ttraining's binary_logloss: 0.521765\n",
      "[242]\ttraining's binary_logloss: 0.521564\n",
      "[243]\ttraining's binary_logloss: 0.521463\n",
      "[244]\ttraining's binary_logloss: 0.521318\n",
      "[245]\ttraining's binary_logloss: 0.521144\n",
      "[246]\ttraining's binary_logloss: 0.520948\n",
      "[247]\ttraining's binary_logloss: 0.520702\n",
      "[248]\ttraining's binary_logloss: 0.520436\n",
      "[249]\ttraining's binary_logloss: 0.52025\n",
      "[250]\ttraining's binary_logloss: 0.520135\n",
      "[251]\ttraining's binary_logloss: 0.519902\n",
      "[252]\ttraining's binary_logloss: 0.51969\n",
      "[253]\ttraining's binary_logloss: 0.519571\n",
      "[254]\ttraining's binary_logloss: 0.519352\n",
      "[255]\ttraining's binary_logloss: 0.519152\n",
      "[256]\ttraining's binary_logloss: 0.518932\n",
      "[257]\ttraining's binary_logloss: 0.518706\n",
      "[258]\ttraining's binary_logloss: 0.518604\n",
      "[259]\ttraining's binary_logloss: 0.518403\n",
      "[260]\ttraining's binary_logloss: 0.518164\n",
      "[261]\ttraining's binary_logloss: 0.518036\n",
      "[262]\ttraining's binary_logloss: 0.517879\n",
      "[263]\ttraining's binary_logloss: 0.517709\n",
      "[264]\ttraining's binary_logloss: 0.517499\n",
      "[265]\ttraining's binary_logloss: 0.517288\n",
      "[266]\ttraining's binary_logloss: 0.517096\n",
      "[267]\ttraining's binary_logloss: 0.516873\n",
      "[268]\ttraining's binary_logloss: 0.516745\n",
      "[269]\ttraining's binary_logloss: 0.516503\n",
      "[270]\ttraining's binary_logloss: 0.516369\n",
      "[271]\ttraining's binary_logloss: 0.516202\n",
      "[272]\ttraining's binary_logloss: 0.515966\n",
      "[273]\ttraining's binary_logloss: 0.515714\n",
      "[274]\ttraining's binary_logloss: 0.515429\n",
      "[275]\ttraining's binary_logloss: 0.515198\n",
      "[276]\ttraining's binary_logloss: 0.515015\n",
      "[277]\ttraining's binary_logloss: 0.514844\n",
      "[278]\ttraining's binary_logloss: 0.514669\n",
      "[279]\ttraining's binary_logloss: 0.514481\n",
      "[280]\ttraining's binary_logloss: 0.514258\n",
      "[281]\ttraining's binary_logloss: 0.51406\n",
      "[282]\ttraining's binary_logloss: 0.513913\n",
      "[283]\ttraining's binary_logloss: 0.513807\n",
      "[284]\ttraining's binary_logloss: 0.513589\n",
      "[285]\ttraining's binary_logloss: 0.513395\n",
      "[286]\ttraining's binary_logloss: 0.513281\n",
      "[287]\ttraining's binary_logloss: 0.513051\n",
      "[288]\ttraining's binary_logloss: 0.512822\n",
      "[289]\ttraining's binary_logloss: 0.512678\n",
      "[290]\ttraining's binary_logloss: 0.512587\n",
      "[291]\ttraining's binary_logloss: 0.512391\n",
      "[292]\ttraining's binary_logloss: 0.512231\n",
      "[293]\ttraining's binary_logloss: 0.512067\n",
      "[294]\ttraining's binary_logloss: 0.511968\n",
      "[295]\ttraining's binary_logloss: 0.511777\n",
      "[296]\ttraining's binary_logloss: 0.511647\n",
      "[297]\ttraining's binary_logloss: 0.511399\n",
      "[298]\ttraining's binary_logloss: 0.51129\n",
      "[299]\ttraining's binary_logloss: 0.511057\n",
      "[300]\ttraining's binary_logloss: 0.510809\n",
      "[301]\ttraining's binary_logloss: 0.510619\n",
      "[302]\ttraining's binary_logloss: 0.510449\n",
      "[303]\ttraining's binary_logloss: 0.510245\n",
      "[304]\ttraining's binary_logloss: 0.510015\n",
      "[305]\ttraining's binary_logloss: 0.509915\n",
      "[306]\ttraining's binary_logloss: 0.509726\n",
      "[307]\ttraining's binary_logloss: 0.509536\n",
      "[308]\ttraining's binary_logloss: 0.509346\n",
      "[309]\ttraining's binary_logloss: 0.509119\n",
      "[310]\ttraining's binary_logloss: 0.508937\n",
      "[311]\ttraining's binary_logloss: 0.508772\n",
      "[312]\ttraining's binary_logloss: 0.508631\n",
      "[313]\ttraining's binary_logloss: 0.508455\n",
      "[314]\ttraining's binary_logloss: 0.508265\n",
      "[315]\ttraining's binary_logloss: 0.508057\n",
      "[316]\ttraining's binary_logloss: 0.507903\n",
      "[317]\ttraining's binary_logloss: 0.50769\n",
      "[318]\ttraining's binary_logloss: 0.507591\n",
      "[319]\ttraining's binary_logloss: 0.507448\n",
      "[320]\ttraining's binary_logloss: 0.507251\n",
      "[321]\ttraining's binary_logloss: 0.507123\n",
      "[322]\ttraining's binary_logloss: 0.506938\n",
      "[323]\ttraining's binary_logloss: 0.506776\n",
      "[324]\ttraining's binary_logloss: 0.506555\n",
      "[325]\ttraining's binary_logloss: 0.506423\n",
      "[326]\ttraining's binary_logloss: 0.506247\n",
      "[327]\ttraining's binary_logloss: 0.506167\n",
      "[328]\ttraining's binary_logloss: 0.505971\n",
      "[329]\ttraining's binary_logloss: 0.505734\n",
      "[330]\ttraining's binary_logloss: 0.505523\n",
      "[331]\ttraining's binary_logloss: 0.505273\n",
      "[332]\ttraining's binary_logloss: 0.505156\n",
      "[333]\ttraining's binary_logloss: 0.504971\n",
      "[334]\ttraining's binary_logloss: 0.504793\n",
      "[335]\ttraining's binary_logloss: 0.50464\n",
      "[336]\ttraining's binary_logloss: 0.504566\n",
      "[337]\ttraining's binary_logloss: 0.504352\n",
      "[338]\ttraining's binary_logloss: 0.504079\n",
      "[339]\ttraining's binary_logloss: 0.503949\n",
      "[340]\ttraining's binary_logloss: 0.503838\n",
      "[341]\ttraining's binary_logloss: 0.503746\n",
      "[342]\ttraining's binary_logloss: 0.503533\n",
      "[343]\ttraining's binary_logloss: 0.503283\n",
      "[344]\ttraining's binary_logloss: 0.503128\n",
      "[345]\ttraining's binary_logloss: 0.502933\n",
      "[346]\ttraining's binary_logloss: 0.502751\n",
      "[347]\ttraining's binary_logloss: 0.502633\n",
      "[348]\ttraining's binary_logloss: 0.502545\n",
      "[349]\ttraining's binary_logloss: 0.502369\n",
      "[350]\ttraining's binary_logloss: 0.502143\n",
      "[351]\ttraining's binary_logloss: 0.502032\n",
      "[352]\ttraining's binary_logloss: 0.501888\n",
      "[353]\ttraining's binary_logloss: 0.501715\n",
      "[354]\ttraining's binary_logloss: 0.501531\n",
      "[355]\ttraining's binary_logloss: 0.501333\n",
      "[356]\ttraining's binary_logloss: 0.501244\n",
      "[357]\ttraining's binary_logloss: 0.501034\n",
      "[358]\ttraining's binary_logloss: 0.500835\n",
      "[359]\ttraining's binary_logloss: 0.500669\n",
      "[360]\ttraining's binary_logloss: 0.500448\n",
      "[361]\ttraining's binary_logloss: 0.500244\n",
      "[362]\ttraining's binary_logloss: 0.500095\n",
      "[363]\ttraining's binary_logloss: 0.499964\n",
      "[364]\ttraining's binary_logloss: 0.499873\n",
      "[365]\ttraining's binary_logloss: 0.499631\n",
      "[366]\ttraining's binary_logloss: 0.499529\n",
      "[367]\ttraining's binary_logloss: 0.499292\n",
      "[368]\ttraining's binary_logloss: 0.499056\n",
      "[369]\ttraining's binary_logloss: 0.498836\n",
      "[370]\ttraining's binary_logloss: 0.498549\n",
      "[371]\ttraining's binary_logloss: 0.498452\n",
      "[372]\ttraining's binary_logloss: 0.498331\n",
      "[373]\ttraining's binary_logloss: 0.49819\n",
      "[374]\ttraining's binary_logloss: 0.498039\n",
      "[375]\ttraining's binary_logloss: 0.497861\n",
      "[376]\ttraining's binary_logloss: 0.497626\n",
      "[377]\ttraining's binary_logloss: 0.497467\n",
      "[378]\ttraining's binary_logloss: 0.497297\n",
      "[379]\ttraining's binary_logloss: 0.497232\n",
      "[380]\ttraining's binary_logloss: 0.497037\n",
      "[381]\ttraining's binary_logloss: 0.496898\n",
      "[382]\ttraining's binary_logloss: 0.496725\n",
      "[383]\ttraining's binary_logloss: 0.496548\n",
      "[384]\ttraining's binary_logloss: 0.496368\n",
      "[385]\ttraining's binary_logloss: 0.496233\n",
      "[386]\ttraining's binary_logloss: 0.496091\n",
      "[387]\ttraining's binary_logloss: 0.495815\n",
      "[388]\ttraining's binary_logloss: 0.495608\n",
      "[389]\ttraining's binary_logloss: 0.495416\n",
      "[390]\ttraining's binary_logloss: 0.495184\n",
      "[391]\ttraining's binary_logloss: 0.495069\n",
      "[392]\ttraining's binary_logloss: 0.49488\n",
      "[393]\ttraining's binary_logloss: 0.494729\n",
      "[394]\ttraining's binary_logloss: 0.49458\n",
      "[395]\ttraining's binary_logloss: 0.49446\n",
      "[396]\ttraining's binary_logloss: 0.49427\n",
      "[397]\ttraining's binary_logloss: 0.494026\n",
      "[398]\ttraining's binary_logloss: 0.493816\n",
      "[399]\ttraining's binary_logloss: 0.493688\n",
      "[400]\ttraining's binary_logloss: 0.493466\n",
      "[401]\ttraining's binary_logloss: 0.493272\n",
      "[402]\ttraining's binary_logloss: 0.493178\n",
      "[403]\ttraining's binary_logloss: 0.492987\n",
      "[404]\ttraining's binary_logloss: 0.492753\n",
      "[405]\ttraining's binary_logloss: 0.492563\n",
      "[406]\ttraining's binary_logloss: 0.492422\n",
      "[407]\ttraining's binary_logloss: 0.492305\n",
      "[408]\ttraining's binary_logloss: 0.492061\n",
      "[409]\ttraining's binary_logloss: 0.491983\n",
      "[410]\ttraining's binary_logloss: 0.491876\n",
      "[411]\ttraining's binary_logloss: 0.491714\n",
      "[412]\ttraining's binary_logloss: 0.491615\n",
      "[413]\ttraining's binary_logloss: 0.491444\n",
      "[414]\ttraining's binary_logloss: 0.491272\n",
      "[415]\ttraining's binary_logloss: 0.491091\n",
      "[416]\ttraining's binary_logloss: 0.491015\n",
      "[417]\ttraining's binary_logloss: 0.490954\n",
      "[418]\ttraining's binary_logloss: 0.490807\n",
      "[419]\ttraining's binary_logloss: 0.490602\n",
      "[420]\ttraining's binary_logloss: 0.490434\n",
      "[421]\ttraining's binary_logloss: 0.490351\n",
      "[422]\ttraining's binary_logloss: 0.490228\n",
      "[423]\ttraining's binary_logloss: 0.490101\n",
      "[424]\ttraining's binary_logloss: 0.490041\n",
      "[425]\ttraining's binary_logloss: 0.489915\n",
      "[426]\ttraining's binary_logloss: 0.489781\n",
      "[427]\ttraining's binary_logloss: 0.489623\n",
      "[428]\ttraining's binary_logloss: 0.489451\n",
      "[429]\ttraining's binary_logloss: 0.489348\n",
      "[430]\ttraining's binary_logloss: 0.489196\n",
      "[431]\ttraining's binary_logloss: 0.489014\n",
      "[432]\ttraining's binary_logloss: 0.488797\n",
      "[433]\ttraining's binary_logloss: 0.488591\n",
      "[434]\ttraining's binary_logloss: 0.488481\n",
      "[435]\ttraining's binary_logloss: 0.488255\n",
      "[436]\ttraining's binary_logloss: 0.488159\n",
      "[437]\ttraining's binary_logloss: 0.487951\n",
      "[438]\ttraining's binary_logloss: 0.487787\n",
      "[439]\ttraining's binary_logloss: 0.487633\n",
      "[440]\ttraining's binary_logloss: 0.487508\n",
      "[441]\ttraining's binary_logloss: 0.487419\n",
      "[442]\ttraining's binary_logloss: 0.48727\n",
      "[443]\ttraining's binary_logloss: 0.487091\n",
      "[444]\ttraining's binary_logloss: 0.48698\n",
      "[445]\ttraining's binary_logloss: 0.48683\n",
      "[446]\ttraining's binary_logloss: 0.486767\n",
      "[447]\ttraining's binary_logloss: 0.486532\n",
      "[448]\ttraining's binary_logloss: 0.486367\n",
      "[449]\ttraining's binary_logloss: 0.486227\n",
      "[450]\ttraining's binary_logloss: 0.486088\n",
      "[451]\ttraining's binary_logloss: 0.48593\n",
      "[452]\ttraining's binary_logloss: 0.485818\n",
      "[453]\ttraining's binary_logloss: 0.485641\n",
      "[454]\ttraining's binary_logloss: 0.485448\n",
      "[455]\ttraining's binary_logloss: 0.48534\n",
      "[456]\ttraining's binary_logloss: 0.485101\n",
      "[457]\ttraining's binary_logloss: 0.484941\n",
      "[458]\ttraining's binary_logloss: 0.484802\n",
      "[459]\ttraining's binary_logloss: 0.484651\n",
      "[460]\ttraining's binary_logloss: 0.484501\n",
      "[461]\ttraining's binary_logloss: 0.484385\n",
      "[462]\ttraining's binary_logloss: 0.484251\n",
      "[463]\ttraining's binary_logloss: 0.484037\n",
      "[464]\ttraining's binary_logloss: 0.483772\n",
      "[465]\ttraining's binary_logloss: 0.4836\n",
      "[466]\ttraining's binary_logloss: 0.48346\n",
      "[467]\ttraining's binary_logloss: 0.483329\n",
      "[468]\ttraining's binary_logloss: 0.48326\n",
      "[469]\ttraining's binary_logloss: 0.48319\n",
      "[470]\ttraining's binary_logloss: 0.483013\n",
      "[471]\ttraining's binary_logloss: 0.482962\n",
      "[472]\ttraining's binary_logloss: 0.482778\n",
      "[473]\ttraining's binary_logloss: 0.48257\n",
      "[474]\ttraining's binary_logloss: 0.482459\n",
      "[475]\ttraining's binary_logloss: 0.48233\n",
      "[476]\ttraining's binary_logloss: 0.482212\n",
      "[477]\ttraining's binary_logloss: 0.482066\n",
      "[478]\ttraining's binary_logloss: 0.481985\n",
      "[479]\ttraining's binary_logloss: 0.481913\n",
      "[480]\ttraining's binary_logloss: 0.481738\n",
      "[481]\ttraining's binary_logloss: 0.481526\n",
      "[482]\ttraining's binary_logloss: 0.48136\n",
      "[483]\ttraining's binary_logloss: 0.481099\n",
      "[484]\ttraining's binary_logloss: 0.480927\n",
      "[485]\ttraining's binary_logloss: 0.480693\n",
      "[486]\ttraining's binary_logloss: 0.480464\n",
      "[487]\ttraining's binary_logloss: 0.480302\n",
      "[488]\ttraining's binary_logloss: 0.480074\n",
      "[489]\ttraining's binary_logloss: 0.479898\n",
      "[490]\ttraining's binary_logloss: 0.479833\n",
      "[491]\ttraining's binary_logloss: 0.479707\n",
      "[492]\ttraining's binary_logloss: 0.479524\n",
      "[493]\ttraining's binary_logloss: 0.47934\n",
      "[494]\ttraining's binary_logloss: 0.479168\n",
      "[495]\ttraining's binary_logloss: 0.479\n",
      "[496]\ttraining's binary_logloss: 0.478816\n",
      "[497]\ttraining's binary_logloss: 0.478642\n",
      "[498]\ttraining's binary_logloss: 0.478509\n",
      "[499]\ttraining's binary_logloss: 0.47835\n",
      "[500]\ttraining's binary_logloss: 0.478151\n",
      "[1]\ttraining's binary_logloss: 0.62902\n",
      "[2]\ttraining's binary_logloss: 0.6234\n",
      "[3]\ttraining's binary_logloss: 0.618577\n",
      "[4]\ttraining's binary_logloss: 0.614496\n",
      "[5]\ttraining's binary_logloss: 0.611207\n",
      "[6]\ttraining's binary_logloss: 0.60842\n",
      "[7]\ttraining's binary_logloss: 0.605822\n",
      "[8]\ttraining's binary_logloss: 0.603557\n",
      "[9]\ttraining's binary_logloss: 0.601602\n",
      "[10]\ttraining's binary_logloss: 0.599746\n",
      "[11]\ttraining's binary_logloss: 0.597971\n",
      "[12]\ttraining's binary_logloss: 0.596528\n",
      "[13]\ttraining's binary_logloss: 0.595275\n",
      "[14]\ttraining's binary_logloss: 0.593861\n",
      "[15]\ttraining's binary_logloss: 0.592799\n",
      "[16]\ttraining's binary_logloss: 0.591743\n",
      "[17]\ttraining's binary_logloss: 0.590587\n",
      "[18]\ttraining's binary_logloss: 0.589689\n",
      "[19]\ttraining's binary_logloss: 0.588844\n",
      "[20]\ttraining's binary_logloss: 0.588053\n",
      "[21]\ttraining's binary_logloss: 0.587198\n",
      "[22]\ttraining's binary_logloss: 0.586425\n",
      "[23]\ttraining's binary_logloss: 0.585776\n",
      "[24]\ttraining's binary_logloss: 0.585081\n",
      "[25]\ttraining's binary_logloss: 0.584463\n",
      "[26]\ttraining's binary_logloss: 0.583796\n",
      "[27]\ttraining's binary_logloss: 0.583259\n",
      "[28]\ttraining's binary_logloss: 0.582591\n",
      "[29]\ttraining's binary_logloss: 0.581906\n",
      "[30]\ttraining's binary_logloss: 0.581372\n",
      "[31]\ttraining's binary_logloss: 0.580705\n",
      "[32]\ttraining's binary_logloss: 0.580158\n",
      "[33]\ttraining's binary_logloss: 0.57966\n",
      "[34]\ttraining's binary_logloss: 0.579152\n",
      "[35]\ttraining's binary_logloss: 0.578551\n",
      "[36]\ttraining's binary_logloss: 0.578109\n",
      "[37]\ttraining's binary_logloss: 0.577626\n",
      "[38]\ttraining's binary_logloss: 0.577189\n",
      "[39]\ttraining's binary_logloss: 0.576672\n",
      "[40]\ttraining's binary_logloss: 0.576237\n",
      "[41]\ttraining's binary_logloss: 0.5758\n",
      "[42]\ttraining's binary_logloss: 0.575335\n",
      "[43]\ttraining's binary_logloss: 0.574905\n",
      "[44]\ttraining's binary_logloss: 0.574524\n",
      "[45]\ttraining's binary_logloss: 0.574127\n",
      "[46]\ttraining's binary_logloss: 0.573639\n",
      "[47]\ttraining's binary_logloss: 0.573286\n",
      "[48]\ttraining's binary_logloss: 0.572906\n",
      "[49]\ttraining's binary_logloss: 0.572469\n",
      "[50]\ttraining's binary_logloss: 0.572038\n",
      "[51]\ttraining's binary_logloss: 0.571734\n",
      "[52]\ttraining's binary_logloss: 0.571378\n",
      "[53]\ttraining's binary_logloss: 0.570993\n",
      "[54]\ttraining's binary_logloss: 0.570645\n",
      "[55]\ttraining's binary_logloss: 0.570317\n",
      "[56]\ttraining's binary_logloss: 0.569945\n",
      "[57]\ttraining's binary_logloss: 0.569636\n",
      "[58]\ttraining's binary_logloss: 0.569214\n",
      "[59]\ttraining's binary_logloss: 0.56878\n",
      "[60]\ttraining's binary_logloss: 0.568508\n",
      "[61]\ttraining's binary_logloss: 0.568141\n",
      "[62]\ttraining's binary_logloss: 0.567782\n",
      "[63]\ttraining's binary_logloss: 0.567409\n",
      "[64]\ttraining's binary_logloss: 0.566988\n",
      "[65]\ttraining's binary_logloss: 0.56662\n",
      "[66]\ttraining's binary_logloss: 0.566291\n",
      "[67]\ttraining's binary_logloss: 0.565964\n",
      "[68]\ttraining's binary_logloss: 0.565653\n",
      "[69]\ttraining's binary_logloss: 0.565391\n",
      "[70]\ttraining's binary_logloss: 0.565042\n",
      "[71]\ttraining's binary_logloss: 0.564781\n",
      "[72]\ttraining's binary_logloss: 0.564392\n",
      "[73]\ttraining's binary_logloss: 0.564056\n",
      "[74]\ttraining's binary_logloss: 0.563739\n",
      "[75]\ttraining's binary_logloss: 0.563453\n",
      "[76]\ttraining's binary_logloss: 0.563177\n",
      "[77]\ttraining's binary_logloss: 0.562899\n",
      "[78]\ttraining's binary_logloss: 0.562661\n",
      "[79]\ttraining's binary_logloss: 0.562263\n",
      "[80]\ttraining's binary_logloss: 0.561942\n",
      "[81]\ttraining's binary_logloss: 0.561739\n",
      "[82]\ttraining's binary_logloss: 0.561522\n",
      "[83]\ttraining's binary_logloss: 0.561246\n",
      "[84]\ttraining's binary_logloss: 0.560948\n",
      "[85]\ttraining's binary_logloss: 0.56074\n",
      "[86]\ttraining's binary_logloss: 0.560414\n",
      "[87]\ttraining's binary_logloss: 0.560168\n",
      "[88]\ttraining's binary_logloss: 0.559769\n",
      "[89]\ttraining's binary_logloss: 0.55953\n",
      "[90]\ttraining's binary_logloss: 0.559248\n",
      "[91]\ttraining's binary_logloss: 0.558942\n",
      "[92]\ttraining's binary_logloss: 0.558632\n",
      "[93]\ttraining's binary_logloss: 0.558388\n",
      "[94]\ttraining's binary_logloss: 0.55815\n",
      "[95]\ttraining's binary_logloss: 0.557946\n",
      "[96]\ttraining's binary_logloss: 0.557735\n",
      "[97]\ttraining's binary_logloss: 0.557451\n",
      "[98]\ttraining's binary_logloss: 0.55715\n",
      "[99]\ttraining's binary_logloss: 0.556918\n",
      "[100]\ttraining's binary_logloss: 0.556634\n",
      "[101]\ttraining's binary_logloss: 0.556396\n",
      "[102]\ttraining's binary_logloss: 0.556104\n",
      "[103]\ttraining's binary_logloss: 0.555928\n",
      "[104]\ttraining's binary_logloss: 0.555719\n",
      "[105]\ttraining's binary_logloss: 0.555506\n",
      "[106]\ttraining's binary_logloss: 0.555297\n",
      "[107]\ttraining's binary_logloss: 0.555006\n",
      "[108]\ttraining's binary_logloss: 0.554812\n",
      "[109]\ttraining's binary_logloss: 0.554501\n",
      "[110]\ttraining's binary_logloss: 0.554256\n",
      "[111]\ttraining's binary_logloss: 0.554049\n",
      "[112]\ttraining's binary_logloss: 0.553787\n",
      "[113]\ttraining's binary_logloss: 0.553497\n",
      "[114]\ttraining's binary_logloss: 0.553267\n",
      "[115]\ttraining's binary_logloss: 0.553006\n",
      "[116]\ttraining's binary_logloss: 0.552848\n",
      "[117]\ttraining's binary_logloss: 0.552649\n",
      "[118]\ttraining's binary_logloss: 0.552384\n",
      "[119]\ttraining's binary_logloss: 0.552078\n",
      "[120]\ttraining's binary_logloss: 0.551904\n",
      "[121]\ttraining's binary_logloss: 0.55162\n",
      "[122]\ttraining's binary_logloss: 0.551377\n",
      "[123]\ttraining's binary_logloss: 0.551124\n",
      "[124]\ttraining's binary_logloss: 0.550847\n",
      "[125]\ttraining's binary_logloss: 0.550633\n",
      "[126]\ttraining's binary_logloss: 0.550474\n",
      "[127]\ttraining's binary_logloss: 0.550252\n",
      "[128]\ttraining's binary_logloss: 0.549993\n",
      "[129]\ttraining's binary_logloss: 0.549669\n",
      "[130]\ttraining's binary_logloss: 0.549424\n",
      "[131]\ttraining's binary_logloss: 0.549201\n",
      "[132]\ttraining's binary_logloss: 0.548981\n",
      "[133]\ttraining's binary_logloss: 0.548681\n",
      "[134]\ttraining's binary_logloss: 0.548443\n",
      "[135]\ttraining's binary_logloss: 0.548248\n",
      "[136]\ttraining's binary_logloss: 0.548025\n",
      "[137]\ttraining's binary_logloss: 0.547827\n",
      "[138]\ttraining's binary_logloss: 0.547552\n",
      "[139]\ttraining's binary_logloss: 0.547343\n",
      "[140]\ttraining's binary_logloss: 0.547068\n",
      "[141]\ttraining's binary_logloss: 0.546713\n",
      "[142]\ttraining's binary_logloss: 0.546455\n",
      "[143]\ttraining's binary_logloss: 0.546203\n",
      "[144]\ttraining's binary_logloss: 0.545935\n",
      "[145]\ttraining's binary_logloss: 0.545728\n",
      "[146]\ttraining's binary_logloss: 0.545462\n",
      "[147]\ttraining's binary_logloss: 0.545244\n",
      "[148]\ttraining's binary_logloss: 0.544999\n",
      "[149]\ttraining's binary_logloss: 0.544766\n",
      "[150]\ttraining's binary_logloss: 0.54454\n",
      "[151]\ttraining's binary_logloss: 0.544276\n",
      "[152]\ttraining's binary_logloss: 0.544088\n",
      "[153]\ttraining's binary_logloss: 0.543825\n",
      "[154]\ttraining's binary_logloss: 0.543513\n",
      "[155]\ttraining's binary_logloss: 0.54333\n",
      "[156]\ttraining's binary_logloss: 0.543135\n",
      "[157]\ttraining's binary_logloss: 0.542817\n",
      "[158]\ttraining's binary_logloss: 0.542517\n",
      "[159]\ttraining's binary_logloss: 0.542386\n",
      "[160]\ttraining's binary_logloss: 0.542175\n",
      "[161]\ttraining's binary_logloss: 0.541879\n",
      "[162]\ttraining's binary_logloss: 0.541638\n",
      "[163]\ttraining's binary_logloss: 0.541359\n",
      "[164]\ttraining's binary_logloss: 0.541197\n",
      "[165]\ttraining's binary_logloss: 0.540931\n",
      "[166]\ttraining's binary_logloss: 0.54068\n",
      "[167]\ttraining's binary_logloss: 0.540451\n",
      "[168]\ttraining's binary_logloss: 0.540264\n",
      "[169]\ttraining's binary_logloss: 0.540154\n",
      "[170]\ttraining's binary_logloss: 0.539912\n",
      "[171]\ttraining's binary_logloss: 0.539623\n",
      "[172]\ttraining's binary_logloss: 0.539482\n",
      "[173]\ttraining's binary_logloss: 0.539344\n",
      "[174]\ttraining's binary_logloss: 0.539178\n",
      "[175]\ttraining's binary_logloss: 0.539017\n",
      "[176]\ttraining's binary_logloss: 0.538795\n",
      "[177]\ttraining's binary_logloss: 0.538538\n",
      "[178]\ttraining's binary_logloss: 0.538308\n",
      "[179]\ttraining's binary_logloss: 0.538092\n",
      "[180]\ttraining's binary_logloss: 0.537811\n",
      "[181]\ttraining's binary_logloss: 0.537586\n",
      "[182]\ttraining's binary_logloss: 0.537393\n",
      "[183]\ttraining's binary_logloss: 0.537182\n",
      "[184]\ttraining's binary_logloss: 0.536976\n",
      "[185]\ttraining's binary_logloss: 0.536784\n",
      "[186]\ttraining's binary_logloss: 0.536553\n",
      "[187]\ttraining's binary_logloss: 0.536346\n",
      "[188]\ttraining's binary_logloss: 0.536084\n",
      "[189]\ttraining's binary_logloss: 0.535831\n",
      "[190]\ttraining's binary_logloss: 0.535547\n",
      "[191]\ttraining's binary_logloss: 0.535376\n",
      "[192]\ttraining's binary_logloss: 0.535225\n",
      "[193]\ttraining's binary_logloss: 0.535019\n",
      "[194]\ttraining's binary_logloss: 0.534783\n",
      "[195]\ttraining's binary_logloss: 0.534633\n",
      "[196]\ttraining's binary_logloss: 0.534485\n",
      "[197]\ttraining's binary_logloss: 0.53436\n",
      "[198]\ttraining's binary_logloss: 0.534162\n",
      "[199]\ttraining's binary_logloss: 0.533938\n",
      "[200]\ttraining's binary_logloss: 0.533631\n",
      "[201]\ttraining's binary_logloss: 0.533439\n",
      "[202]\ttraining's binary_logloss: 0.533304\n",
      "[203]\ttraining's binary_logloss: 0.533119\n",
      "[204]\ttraining's binary_logloss: 0.532915\n",
      "[205]\ttraining's binary_logloss: 0.532707\n",
      "[206]\ttraining's binary_logloss: 0.532509\n",
      "[207]\ttraining's binary_logloss: 0.53231\n",
      "[208]\ttraining's binary_logloss: 0.532073\n",
      "[209]\ttraining's binary_logloss: 0.531825\n",
      "[210]\ttraining's binary_logloss: 0.531623\n",
      "[211]\ttraining's binary_logloss: 0.531492\n",
      "[212]\ttraining's binary_logloss: 0.531205\n",
      "[213]\ttraining's binary_logloss: 0.531062\n",
      "[214]\ttraining's binary_logloss: 0.530958\n",
      "[215]\ttraining's binary_logloss: 0.530776\n",
      "[216]\ttraining's binary_logloss: 0.530579\n",
      "[217]\ttraining's binary_logloss: 0.530275\n",
      "[218]\ttraining's binary_logloss: 0.530081\n",
      "[219]\ttraining's binary_logloss: 0.529876\n",
      "[220]\ttraining's binary_logloss: 0.529652\n",
      "[221]\ttraining's binary_logloss: 0.529453\n",
      "[222]\ttraining's binary_logloss: 0.529301\n",
      "[223]\ttraining's binary_logloss: 0.529083\n",
      "[224]\ttraining's binary_logloss: 0.528904\n",
      "[225]\ttraining's binary_logloss: 0.528747\n",
      "[226]\ttraining's binary_logloss: 0.528553\n",
      "[227]\ttraining's binary_logloss: 0.528353\n",
      "[228]\ttraining's binary_logloss: 0.528236\n",
      "[229]\ttraining's binary_logloss: 0.528012\n",
      "[230]\ttraining's binary_logloss: 0.5278\n",
      "[231]\ttraining's binary_logloss: 0.527611\n",
      "[232]\ttraining's binary_logloss: 0.52745\n",
      "[233]\ttraining's binary_logloss: 0.527252\n",
      "[234]\ttraining's binary_logloss: 0.527129\n",
      "[235]\ttraining's binary_logloss: 0.526945\n",
      "[236]\ttraining's binary_logloss: 0.526713\n",
      "[237]\ttraining's binary_logloss: 0.526601\n",
      "[238]\ttraining's binary_logloss: 0.526489\n",
      "[239]\ttraining's binary_logloss: 0.526292\n",
      "[240]\ttraining's binary_logloss: 0.526022\n",
      "[241]\ttraining's binary_logloss: 0.525854\n",
      "[242]\ttraining's binary_logloss: 0.525646\n",
      "[243]\ttraining's binary_logloss: 0.52548\n",
      "[244]\ttraining's binary_logloss: 0.525272\n",
      "[245]\ttraining's binary_logloss: 0.524975\n",
      "[246]\ttraining's binary_logloss: 0.524766\n",
      "[247]\ttraining's binary_logloss: 0.524554\n",
      "[248]\ttraining's binary_logloss: 0.524367\n",
      "[249]\ttraining's binary_logloss: 0.524262\n",
      "[250]\ttraining's binary_logloss: 0.524129\n",
      "[251]\ttraining's binary_logloss: 0.524025\n",
      "[252]\ttraining's binary_logloss: 0.523892\n",
      "[253]\ttraining's binary_logloss: 0.523698\n",
      "[254]\ttraining's binary_logloss: 0.523568\n",
      "[255]\ttraining's binary_logloss: 0.523319\n",
      "[256]\ttraining's binary_logloss: 0.523129\n",
      "[257]\ttraining's binary_logloss: 0.523004\n",
      "[258]\ttraining's binary_logloss: 0.522805\n",
      "[259]\ttraining's binary_logloss: 0.522593\n",
      "[260]\ttraining's binary_logloss: 0.522386\n",
      "[261]\ttraining's binary_logloss: 0.522225\n",
      "[262]\ttraining's binary_logloss: 0.52204\n",
      "[263]\ttraining's binary_logloss: 0.521871\n",
      "[264]\ttraining's binary_logloss: 0.521717\n",
      "[265]\ttraining's binary_logloss: 0.521614\n",
      "[266]\ttraining's binary_logloss: 0.521455\n",
      "[267]\ttraining's binary_logloss: 0.521339\n",
      "[268]\ttraining's binary_logloss: 0.521101\n",
      "[269]\ttraining's binary_logloss: 0.520973\n",
      "[270]\ttraining's binary_logloss: 0.520801\n",
      "[271]\ttraining's binary_logloss: 0.520529\n",
      "[272]\ttraining's binary_logloss: 0.520364\n",
      "[273]\ttraining's binary_logloss: 0.520106\n",
      "[274]\ttraining's binary_logloss: 0.519917\n",
      "[275]\ttraining's binary_logloss: 0.519732\n",
      "[276]\ttraining's binary_logloss: 0.5195\n",
      "[277]\ttraining's binary_logloss: 0.519285\n",
      "[278]\ttraining's binary_logloss: 0.519118\n",
      "[279]\ttraining's binary_logloss: 0.51902\n",
      "[280]\ttraining's binary_logloss: 0.518861\n",
      "[281]\ttraining's binary_logloss: 0.518611\n",
      "[282]\ttraining's binary_logloss: 0.518485\n",
      "[283]\ttraining's binary_logloss: 0.518239\n",
      "[284]\ttraining's binary_logloss: 0.517977\n",
      "[285]\ttraining's binary_logloss: 0.51784\n",
      "[286]\ttraining's binary_logloss: 0.517708\n",
      "[287]\ttraining's binary_logloss: 0.51754\n",
      "[288]\ttraining's binary_logloss: 0.517421\n",
      "[289]\ttraining's binary_logloss: 0.517261\n",
      "[290]\ttraining's binary_logloss: 0.517146\n",
      "[291]\ttraining's binary_logloss: 0.51699\n",
      "[292]\ttraining's binary_logloss: 0.516785\n",
      "[293]\ttraining's binary_logloss: 0.516659\n",
      "[294]\ttraining's binary_logloss: 0.516498\n",
      "[295]\ttraining's binary_logloss: 0.516358\n",
      "[296]\ttraining's binary_logloss: 0.516136\n",
      "[297]\ttraining's binary_logloss: 0.515911\n",
      "[298]\ttraining's binary_logloss: 0.515813\n",
      "[299]\ttraining's binary_logloss: 0.515698\n",
      "[300]\ttraining's binary_logloss: 0.515532\n",
      "[301]\ttraining's binary_logloss: 0.515376\n",
      "[302]\ttraining's binary_logloss: 0.515245\n",
      "[303]\ttraining's binary_logloss: 0.515071\n",
      "[304]\ttraining's binary_logloss: 0.514929\n",
      "[305]\ttraining's binary_logloss: 0.514857\n",
      "[306]\ttraining's binary_logloss: 0.514764\n",
      "[307]\ttraining's binary_logloss: 0.51467\n",
      "[308]\ttraining's binary_logloss: 0.514562\n",
      "[309]\ttraining's binary_logloss: 0.514376\n",
      "[310]\ttraining's binary_logloss: 0.514188\n",
      "[311]\ttraining's binary_logloss: 0.51403\n",
      "[312]\ttraining's binary_logloss: 0.513825\n",
      "[313]\ttraining's binary_logloss: 0.513659\n",
      "[314]\ttraining's binary_logloss: 0.513456\n",
      "[315]\ttraining's binary_logloss: 0.513253\n",
      "[316]\ttraining's binary_logloss: 0.5131\n",
      "[317]\ttraining's binary_logloss: 0.512927\n",
      "[318]\ttraining's binary_logloss: 0.512792\n",
      "[319]\ttraining's binary_logloss: 0.512618\n",
      "[320]\ttraining's binary_logloss: 0.512442\n",
      "[321]\ttraining's binary_logloss: 0.512296\n",
      "[322]\ttraining's binary_logloss: 0.512114\n",
      "[323]\ttraining's binary_logloss: 0.511965\n",
      "[324]\ttraining's binary_logloss: 0.511853\n",
      "[325]\ttraining's binary_logloss: 0.511763\n",
      "[326]\ttraining's binary_logloss: 0.511619\n",
      "[327]\ttraining's binary_logloss: 0.511497\n",
      "[328]\ttraining's binary_logloss: 0.511359\n",
      "[329]\ttraining's binary_logloss: 0.511165\n",
      "[330]\ttraining's binary_logloss: 0.510996\n",
      "[331]\ttraining's binary_logloss: 0.510792\n",
      "[332]\ttraining's binary_logloss: 0.510726\n",
      "[333]\ttraining's binary_logloss: 0.510564\n",
      "[334]\ttraining's binary_logloss: 0.510367\n",
      "[335]\ttraining's binary_logloss: 0.510227\n",
      "[336]\ttraining's binary_logloss: 0.509968\n",
      "[337]\ttraining's binary_logloss: 0.509884\n",
      "[338]\ttraining's binary_logloss: 0.509755\n",
      "[339]\ttraining's binary_logloss: 0.509633\n",
      "[340]\ttraining's binary_logloss: 0.509484\n",
      "[341]\ttraining's binary_logloss: 0.509212\n",
      "[342]\ttraining's binary_logloss: 0.509032\n",
      "[343]\ttraining's binary_logloss: 0.50884\n",
      "[344]\ttraining's binary_logloss: 0.508759\n",
      "[345]\ttraining's binary_logloss: 0.508594\n",
      "[346]\ttraining's binary_logloss: 0.50849\n",
      "[347]\ttraining's binary_logloss: 0.508398\n",
      "[348]\ttraining's binary_logloss: 0.508314\n",
      "[349]\ttraining's binary_logloss: 0.508182\n",
      "[350]\ttraining's binary_logloss: 0.50807\n",
      "[351]\ttraining's binary_logloss: 0.507909\n",
      "[352]\ttraining's binary_logloss: 0.507743\n",
      "[353]\ttraining's binary_logloss: 0.507511\n",
      "[354]\ttraining's binary_logloss: 0.5073\n",
      "[355]\ttraining's binary_logloss: 0.507237\n",
      "[356]\ttraining's binary_logloss: 0.507126\n",
      "[357]\ttraining's binary_logloss: 0.506949\n",
      "[358]\ttraining's binary_logloss: 0.506794\n",
      "[359]\ttraining's binary_logloss: 0.506667\n",
      "[360]\ttraining's binary_logloss: 0.506536\n",
      "[361]\ttraining's binary_logloss: 0.506274\n",
      "[362]\ttraining's binary_logloss: 0.506073\n",
      "[363]\ttraining's binary_logloss: 0.505965\n",
      "[364]\ttraining's binary_logloss: 0.505857\n",
      "[365]\ttraining's binary_logloss: 0.505748\n",
      "[366]\ttraining's binary_logloss: 0.505632\n",
      "[367]\ttraining's binary_logloss: 0.505435\n",
      "[368]\ttraining's binary_logloss: 0.505284\n",
      "[369]\ttraining's binary_logloss: 0.50511\n",
      "[370]\ttraining's binary_logloss: 0.504863\n",
      "[371]\ttraining's binary_logloss: 0.50475\n",
      "[372]\ttraining's binary_logloss: 0.504624\n",
      "[373]\ttraining's binary_logloss: 0.504533\n",
      "[374]\ttraining's binary_logloss: 0.504418\n",
      "[375]\ttraining's binary_logloss: 0.504298\n",
      "[376]\ttraining's binary_logloss: 0.504105\n",
      "[377]\ttraining's binary_logloss: 0.503877\n",
      "[378]\ttraining's binary_logloss: 0.50372\n",
      "[379]\ttraining's binary_logloss: 0.503539\n",
      "[380]\ttraining's binary_logloss: 0.503315\n",
      "[381]\ttraining's binary_logloss: 0.503186\n",
      "[382]\ttraining's binary_logloss: 0.503083\n",
      "[383]\ttraining's binary_logloss: 0.502987\n",
      "[384]\ttraining's binary_logloss: 0.502896\n",
      "[385]\ttraining's binary_logloss: 0.50263\n",
      "[386]\ttraining's binary_logloss: 0.502345\n",
      "[387]\ttraining's binary_logloss: 0.502179\n",
      "[388]\ttraining's binary_logloss: 0.502062\n",
      "[389]\ttraining's binary_logloss: 0.501923\n",
      "[390]\ttraining's binary_logloss: 0.501845\n",
      "[391]\ttraining's binary_logloss: 0.501653\n",
      "[392]\ttraining's binary_logloss: 0.501453\n",
      "[393]\ttraining's binary_logloss: 0.501282\n",
      "[394]\ttraining's binary_logloss: 0.501143\n",
      "[395]\ttraining's binary_logloss: 0.501062\n",
      "[396]\ttraining's binary_logloss: 0.500964\n",
      "[397]\ttraining's binary_logloss: 0.500817\n",
      "[398]\ttraining's binary_logloss: 0.500664\n",
      "[399]\ttraining's binary_logloss: 0.500456\n",
      "[400]\ttraining's binary_logloss: 0.500331\n",
      "[401]\ttraining's binary_logloss: 0.500155\n",
      "[402]\ttraining's binary_logloss: 0.500069\n",
      "[403]\ttraining's binary_logloss: 0.499914\n",
      "[404]\ttraining's binary_logloss: 0.499793\n",
      "[405]\ttraining's binary_logloss: 0.499634\n",
      "[406]\ttraining's binary_logloss: 0.499394\n",
      "[407]\ttraining's binary_logloss: 0.499246\n",
      "[408]\ttraining's binary_logloss: 0.49912\n",
      "[409]\ttraining's binary_logloss: 0.499007\n",
      "[410]\ttraining's binary_logloss: 0.498849\n",
      "[411]\ttraining's binary_logloss: 0.49869\n",
      "[412]\ttraining's binary_logloss: 0.498536\n",
      "[413]\ttraining's binary_logloss: 0.498278\n",
      "[414]\ttraining's binary_logloss: 0.498117\n",
      "[415]\ttraining's binary_logloss: 0.498054\n",
      "[416]\ttraining's binary_logloss: 0.497719\n",
      "[417]\ttraining's binary_logloss: 0.497516\n",
      "[418]\ttraining's binary_logloss: 0.497339\n",
      "[419]\ttraining's binary_logloss: 0.49722\n",
      "[420]\ttraining's binary_logloss: 0.497065\n",
      "[421]\ttraining's binary_logloss: 0.496914\n",
      "[422]\ttraining's binary_logloss: 0.496795\n",
      "[423]\ttraining's binary_logloss: 0.496718\n",
      "[424]\ttraining's binary_logloss: 0.496522\n",
      "[425]\ttraining's binary_logloss: 0.496427\n",
      "[426]\ttraining's binary_logloss: 0.496274\n",
      "[427]\ttraining's binary_logloss: 0.496114\n",
      "[428]\ttraining's binary_logloss: 0.495949\n",
      "[429]\ttraining's binary_logloss: 0.495849\n",
      "[430]\ttraining's binary_logloss: 0.495759\n",
      "[431]\ttraining's binary_logloss: 0.495683\n",
      "[432]\ttraining's binary_logloss: 0.495484\n",
      "[433]\ttraining's binary_logloss: 0.495334\n",
      "[434]\ttraining's binary_logloss: 0.495138\n",
      "[435]\ttraining's binary_logloss: 0.49497\n",
      "[436]\ttraining's binary_logloss: 0.494834\n",
      "[437]\ttraining's binary_logloss: 0.494683\n",
      "[438]\ttraining's binary_logloss: 0.494532\n",
      "[439]\ttraining's binary_logloss: 0.494446\n",
      "[440]\ttraining's binary_logloss: 0.494264\n",
      "[441]\ttraining's binary_logloss: 0.494119\n",
      "[442]\ttraining's binary_logloss: 0.493987\n",
      "[443]\ttraining's binary_logloss: 0.493839\n",
      "[444]\ttraining's binary_logloss: 0.493718\n",
      "[445]\ttraining's binary_logloss: 0.493662\n",
      "[446]\ttraining's binary_logloss: 0.493603\n",
      "[447]\ttraining's binary_logloss: 0.493495\n",
      "[448]\ttraining's binary_logloss: 0.493337\n",
      "[449]\ttraining's binary_logloss: 0.493232\n",
      "[450]\ttraining's binary_logloss: 0.493019\n",
      "[451]\ttraining's binary_logloss: 0.492887\n",
      "[452]\ttraining's binary_logloss: 0.492698\n",
      "[453]\ttraining's binary_logloss: 0.492649\n",
      "[454]\ttraining's binary_logloss: 0.492497\n",
      "[455]\ttraining's binary_logloss: 0.49227\n",
      "[456]\ttraining's binary_logloss: 0.49212\n",
      "[457]\ttraining's binary_logloss: 0.491926\n",
      "[458]\ttraining's binary_logloss: 0.491702\n",
      "[459]\ttraining's binary_logloss: 0.491562\n",
      "[460]\ttraining's binary_logloss: 0.491377\n",
      "[461]\ttraining's binary_logloss: 0.49125\n",
      "[462]\ttraining's binary_logloss: 0.491175\n",
      "[463]\ttraining's binary_logloss: 0.491111\n",
      "[464]\ttraining's binary_logloss: 0.490977\n",
      "[465]\ttraining's binary_logloss: 0.490902\n",
      "[466]\ttraining's binary_logloss: 0.490734\n",
      "[467]\ttraining's binary_logloss: 0.490528\n",
      "[468]\ttraining's binary_logloss: 0.490422\n",
      "[469]\ttraining's binary_logloss: 0.490294\n",
      "[470]\ttraining's binary_logloss: 0.490181\n",
      "[471]\ttraining's binary_logloss: 0.49008\n",
      "[472]\ttraining's binary_logloss: 0.489982\n",
      "[473]\ttraining's binary_logloss: 0.489825\n",
      "[474]\ttraining's binary_logloss: 0.489685\n",
      "[475]\ttraining's binary_logloss: 0.489544\n",
      "[476]\ttraining's binary_logloss: 0.489492\n",
      "[477]\ttraining's binary_logloss: 0.489318\n",
      "[478]\ttraining's binary_logloss: 0.489184\n",
      "[479]\ttraining's binary_logloss: 0.48911\n",
      "[480]\ttraining's binary_logloss: 0.488986\n",
      "[481]\ttraining's binary_logloss: 0.488796\n",
      "[482]\ttraining's binary_logloss: 0.488658\n",
      "[483]\ttraining's binary_logloss: 0.488495\n",
      "[484]\ttraining's binary_logloss: 0.488334\n",
      "[485]\ttraining's binary_logloss: 0.488183\n",
      "[486]\ttraining's binary_logloss: 0.488128\n",
      "[487]\ttraining's binary_logloss: 0.487992\n",
      "[488]\ttraining's binary_logloss: 0.487846\n",
      "[489]\ttraining's binary_logloss: 0.487651\n",
      "[490]\ttraining's binary_logloss: 0.487408\n",
      "[491]\ttraining's binary_logloss: 0.487251\n",
      "[492]\ttraining's binary_logloss: 0.487077\n",
      "[493]\ttraining's binary_logloss: 0.486907\n",
      "[494]\ttraining's binary_logloss: 0.486738\n",
      "[495]\ttraining's binary_logloss: 0.486661\n",
      "[496]\ttraining's binary_logloss: 0.486456\n",
      "[497]\ttraining's binary_logloss: 0.486353\n",
      "[498]\ttraining's binary_logloss: 0.486137\n",
      "[499]\ttraining's binary_logloss: 0.485983\n",
      "[500]\ttraining's binary_logloss: 0.485838\n",
      "[1]\ttraining's binary_logloss: 0.632427\n",
      "[2]\ttraining's binary_logloss: 0.629498\n",
      "[3]\ttraining's binary_logloss: 0.626977\n",
      "[4]\ttraining's binary_logloss: 0.624687\n",
      "[5]\ttraining's binary_logloss: 0.622896\n",
      "[6]\ttraining's binary_logloss: 0.621149\n",
      "[7]\ttraining's binary_logloss: 0.619587\n",
      "[8]\ttraining's binary_logloss: 0.618054\n",
      "[9]\ttraining's binary_logloss: 0.616883\n",
      "[10]\ttraining's binary_logloss: 0.615668\n",
      "[11]\ttraining's binary_logloss: 0.614611\n",
      "[12]\ttraining's binary_logloss: 0.613594\n",
      "[13]\ttraining's binary_logloss: 0.612519\n",
      "[14]\ttraining's binary_logloss: 0.611686\n",
      "[15]\ttraining's binary_logloss: 0.610966\n",
      "[16]\ttraining's binary_logloss: 0.6102\n",
      "[17]\ttraining's binary_logloss: 0.609449\n",
      "[18]\ttraining's binary_logloss: 0.608778\n",
      "[19]\ttraining's binary_logloss: 0.608121\n",
      "[20]\ttraining's binary_logloss: 0.607433\n",
      "[21]\ttraining's binary_logloss: 0.606791\n",
      "[22]\ttraining's binary_logloss: 0.606177\n",
      "[23]\ttraining's binary_logloss: 0.605736\n",
      "[24]\ttraining's binary_logloss: 0.605225\n",
      "[25]\ttraining's binary_logloss: 0.60481\n",
      "[26]\ttraining's binary_logloss: 0.604377\n",
      "[27]\ttraining's binary_logloss: 0.603918\n",
      "[28]\ttraining's binary_logloss: 0.603447\n",
      "[29]\ttraining's binary_logloss: 0.602836\n",
      "[30]\ttraining's binary_logloss: 0.602445\n",
      "[31]\ttraining's binary_logloss: 0.601908\n",
      "[32]\ttraining's binary_logloss: 0.601459\n",
      "[33]\ttraining's binary_logloss: 0.601086\n",
      "[34]\ttraining's binary_logloss: 0.60079\n",
      "[35]\ttraining's binary_logloss: 0.600502\n",
      "[36]\ttraining's binary_logloss: 0.600031\n",
      "[37]\ttraining's binary_logloss: 0.599698\n",
      "[38]\ttraining's binary_logloss: 0.599189\n",
      "[39]\ttraining's binary_logloss: 0.598898\n",
      "[40]\ttraining's binary_logloss: 0.598565\n",
      "[41]\ttraining's binary_logloss: 0.59831\n",
      "[42]\ttraining's binary_logloss: 0.59813\n",
      "[43]\ttraining's binary_logloss: 0.597861\n",
      "[44]\ttraining's binary_logloss: 0.597597\n",
      "[45]\ttraining's binary_logloss: 0.597236\n",
      "[46]\ttraining's binary_logloss: 0.596917\n",
      "[47]\ttraining's binary_logloss: 0.596666\n",
      "[48]\ttraining's binary_logloss: 0.596475\n",
      "[49]\ttraining's binary_logloss: 0.596209\n",
      "[50]\ttraining's binary_logloss: 0.596003\n",
      "[51]\ttraining's binary_logloss: 0.595714\n",
      "[52]\ttraining's binary_logloss: 0.595563\n",
      "[53]\ttraining's binary_logloss: 0.595389\n",
      "[54]\ttraining's binary_logloss: 0.595234\n",
      "[55]\ttraining's binary_logloss: 0.595004\n",
      "[56]\ttraining's binary_logloss: 0.594726\n",
      "[57]\ttraining's binary_logloss: 0.594506\n",
      "[58]\ttraining's binary_logloss: 0.594384\n",
      "[59]\ttraining's binary_logloss: 0.594059\n",
      "[60]\ttraining's binary_logloss: 0.593819\n",
      "[61]\ttraining's binary_logloss: 0.593509\n",
      "[62]\ttraining's binary_logloss: 0.593351\n",
      "[63]\ttraining's binary_logloss: 0.593185\n",
      "[64]\ttraining's binary_logloss: 0.592917\n",
      "[65]\ttraining's binary_logloss: 0.592725\n",
      "[66]\ttraining's binary_logloss: 0.592477\n",
      "[67]\ttraining's binary_logloss: 0.592294\n",
      "[68]\ttraining's binary_logloss: 0.592057\n",
      "[69]\ttraining's binary_logloss: 0.591955\n",
      "[70]\ttraining's binary_logloss: 0.591687\n",
      "[71]\ttraining's binary_logloss: 0.591498\n",
      "[72]\ttraining's binary_logloss: 0.591363\n",
      "[73]\ttraining's binary_logloss: 0.591162\n",
      "[74]\ttraining's binary_logloss: 0.590964\n",
      "[75]\ttraining's binary_logloss: 0.590858\n",
      "[76]\ttraining's binary_logloss: 0.590666\n",
      "[77]\ttraining's binary_logloss: 0.59049\n",
      "[78]\ttraining's binary_logloss: 0.590259\n",
      "[79]\ttraining's binary_logloss: 0.590006\n",
      "[80]\ttraining's binary_logloss: 0.589819\n",
      "[81]\ttraining's binary_logloss: 0.589733\n",
      "[82]\ttraining's binary_logloss: 0.589507\n",
      "[83]\ttraining's binary_logloss: 0.589413\n",
      "[84]\ttraining's binary_logloss: 0.589241\n",
      "[85]\ttraining's binary_logloss: 0.589047\n",
      "[86]\ttraining's binary_logloss: 0.588886\n",
      "[87]\ttraining's binary_logloss: 0.588726\n",
      "[88]\ttraining's binary_logloss: 0.588607\n",
      "[89]\ttraining's binary_logloss: 0.58852\n",
      "[90]\ttraining's binary_logloss: 0.588448\n",
      "[91]\ttraining's binary_logloss: 0.588225\n",
      "[92]\ttraining's binary_logloss: 0.588035\n",
      "[93]\ttraining's binary_logloss: 0.587868\n",
      "[94]\ttraining's binary_logloss: 0.587767\n",
      "[95]\ttraining's binary_logloss: 0.587622\n",
      "[96]\ttraining's binary_logloss: 0.587463\n",
      "[97]\ttraining's binary_logloss: 0.587288\n",
      "[98]\ttraining's binary_logloss: 0.587107\n",
      "[99]\ttraining's binary_logloss: 0.586971\n",
      "[100]\ttraining's binary_logloss: 0.586808\n",
      "[101]\ttraining's binary_logloss: 0.586754\n",
      "[102]\ttraining's binary_logloss: 0.586684\n",
      "[103]\ttraining's binary_logloss: 0.586629\n",
      "[104]\ttraining's binary_logloss: 0.586519\n",
      "[105]\ttraining's binary_logloss: 0.586457\n",
      "[106]\ttraining's binary_logloss: 0.586316\n",
      "[107]\ttraining's binary_logloss: 0.586193\n",
      "[108]\ttraining's binary_logloss: 0.586028\n",
      "[109]\ttraining's binary_logloss: 0.585881\n",
      "[110]\ttraining's binary_logloss: 0.585739\n",
      "[111]\ttraining's binary_logloss: 0.585653\n",
      "[112]\ttraining's binary_logloss: 0.585605\n",
      "[113]\ttraining's binary_logloss: 0.585556\n",
      "[114]\ttraining's binary_logloss: 0.585386\n",
      "[115]\ttraining's binary_logloss: 0.585269\n",
      "[116]\ttraining's binary_logloss: 0.585151\n",
      "[117]\ttraining's binary_logloss: 0.585052\n",
      "[118]\ttraining's binary_logloss: 0.58492\n",
      "[119]\ttraining's binary_logloss: 0.584839\n",
      "[120]\ttraining's binary_logloss: 0.58475\n",
      "[121]\ttraining's binary_logloss: 0.584642\n",
      "[122]\ttraining's binary_logloss: 0.58452\n",
      "[123]\ttraining's binary_logloss: 0.584401\n",
      "[124]\ttraining's binary_logloss: 0.584318\n",
      "[125]\ttraining's binary_logloss: 0.584169\n",
      "[126]\ttraining's binary_logloss: 0.584026\n",
      "[127]\ttraining's binary_logloss: 0.583935\n",
      "[128]\ttraining's binary_logloss: 0.583837\n",
      "[129]\ttraining's binary_logloss: 0.583754\n",
      "[130]\ttraining's binary_logloss: 0.583683\n",
      "[131]\ttraining's binary_logloss: 0.583618\n",
      "[132]\ttraining's binary_logloss: 0.583531\n",
      "[133]\ttraining's binary_logloss: 0.583481\n",
      "[134]\ttraining's binary_logloss: 0.583372\n",
      "[135]\ttraining's binary_logloss: 0.5833\n",
      "[136]\ttraining's binary_logloss: 0.58326\n",
      "[137]\ttraining's binary_logloss: 0.583184\n",
      "[138]\ttraining's binary_logloss: 0.58307\n",
      "[139]\ttraining's binary_logloss: 0.58303\n",
      "[140]\ttraining's binary_logloss: 0.582932\n",
      "[141]\ttraining's binary_logloss: 0.58286\n",
      "[142]\ttraining's binary_logloss: 0.582809\n",
      "[143]\ttraining's binary_logloss: 0.582717\n",
      "[144]\ttraining's binary_logloss: 0.582672\n",
      "[145]\ttraining's binary_logloss: 0.582569\n",
      "[146]\ttraining's binary_logloss: 0.582497\n",
      "[147]\ttraining's binary_logloss: 0.582433\n",
      "[148]\ttraining's binary_logloss: 0.582345\n",
      "[149]\ttraining's binary_logloss: 0.582249\n",
      "[150]\ttraining's binary_logloss: 0.582167\n",
      "[151]\ttraining's binary_logloss: 0.582078\n",
      "[152]\ttraining's binary_logloss: 0.582019\n",
      "[153]\ttraining's binary_logloss: 0.581913\n",
      "[154]\ttraining's binary_logloss: 0.581854\n",
      "[155]\ttraining's binary_logloss: 0.581808\n",
      "[156]\ttraining's binary_logloss: 0.581733\n",
      "[157]\ttraining's binary_logloss: 0.581666\n",
      "[158]\ttraining's binary_logloss: 0.581632\n",
      "[159]\ttraining's binary_logloss: 0.581553\n",
      "[160]\ttraining's binary_logloss: 0.5815\n",
      "[161]\ttraining's binary_logloss: 0.581454\n",
      "[162]\ttraining's binary_logloss: 0.58137\n",
      "[163]\ttraining's binary_logloss: 0.581296\n",
      "[164]\ttraining's binary_logloss: 0.581243\n",
      "[165]\ttraining's binary_logloss: 0.581215\n",
      "[166]\ttraining's binary_logloss: 0.581123\n",
      "[167]\ttraining's binary_logloss: 0.581044\n",
      "[168]\ttraining's binary_logloss: 0.580978\n",
      "[169]\ttraining's binary_logloss: 0.580945\n",
      "[170]\ttraining's binary_logloss: 0.580872\n",
      "[171]\ttraining's binary_logloss: 0.580807\n",
      "[172]\ttraining's binary_logloss: 0.580734\n",
      "[173]\ttraining's binary_logloss: 0.580682\n",
      "[174]\ttraining's binary_logloss: 0.580642\n",
      "[175]\ttraining's binary_logloss: 0.580606\n",
      "[176]\ttraining's binary_logloss: 0.580558\n",
      "[177]\ttraining's binary_logloss: 0.580491\n",
      "[178]\ttraining's binary_logloss: 0.580462\n",
      "[179]\ttraining's binary_logloss: 0.580415\n",
      "[180]\ttraining's binary_logloss: 0.580336\n",
      "[181]\ttraining's binary_logloss: 0.580298\n",
      "[182]\ttraining's binary_logloss: 0.580243\n",
      "[183]\ttraining's binary_logloss: 0.58017\n",
      "[184]\ttraining's binary_logloss: 0.58012\n",
      "[185]\ttraining's binary_logloss: 0.580095\n",
      "[186]\ttraining's binary_logloss: 0.580026\n",
      "[187]\ttraining's binary_logloss: 0.579993\n",
      "[188]\ttraining's binary_logloss: 0.579945\n",
      "[189]\ttraining's binary_logloss: 0.579913\n",
      "[190]\ttraining's binary_logloss: 0.579861\n",
      "[191]\ttraining's binary_logloss: 0.57984\n",
      "[192]\ttraining's binary_logloss: 0.579818\n",
      "[193]\ttraining's binary_logloss: 0.57975\n",
      "[194]\ttraining's binary_logloss: 0.579714\n",
      "[195]\ttraining's binary_logloss: 0.579695\n",
      "[196]\ttraining's binary_logloss: 0.579623\n",
      "[197]\ttraining's binary_logloss: 0.579584\n",
      "[198]\ttraining's binary_logloss: 0.579546\n",
      "[199]\ttraining's binary_logloss: 0.579507\n",
      "[200]\ttraining's binary_logloss: 0.579445\n",
      "[201]\ttraining's binary_logloss: 0.579384\n",
      "[202]\ttraining's binary_logloss: 0.579338\n",
      "[203]\ttraining's binary_logloss: 0.579311\n",
      "[204]\ttraining's binary_logloss: 0.579246\n",
      "[205]\ttraining's binary_logloss: 0.579201\n",
      "[206]\ttraining's binary_logloss: 0.579147\n",
      "[207]\ttraining's binary_logloss: 0.579124\n",
      "[208]\ttraining's binary_logloss: 0.579088\n",
      "[209]\ttraining's binary_logloss: 0.579051\n",
      "[210]\ttraining's binary_logloss: 0.579013\n",
      "[211]\ttraining's binary_logloss: 0.578974\n",
      "[212]\ttraining's binary_logloss: 0.57891\n",
      "[213]\ttraining's binary_logloss: 0.57888\n",
      "[214]\ttraining's binary_logloss: 0.578835\n",
      "[215]\ttraining's binary_logloss: 0.578787\n",
      "[216]\ttraining's binary_logloss: 0.578745\n",
      "[217]\ttraining's binary_logloss: 0.578717\n",
      "[218]\ttraining's binary_logloss: 0.578672\n",
      "[219]\ttraining's binary_logloss: 0.57864\n",
      "[220]\ttraining's binary_logloss: 0.578617\n",
      "[221]\ttraining's binary_logloss: 0.578584\n",
      "[222]\ttraining's binary_logloss: 0.578538\n",
      "[223]\ttraining's binary_logloss: 0.578485\n",
      "[224]\ttraining's binary_logloss: 0.578463\n",
      "[225]\ttraining's binary_logloss: 0.578418\n",
      "[226]\ttraining's binary_logloss: 0.578392\n",
      "[227]\ttraining's binary_logloss: 0.578344\n",
      "[228]\ttraining's binary_logloss: 0.578305\n",
      "[229]\ttraining's binary_logloss: 0.578281\n",
      "[230]\ttraining's binary_logloss: 0.57826\n",
      "[231]\ttraining's binary_logloss: 0.578226\n",
      "[232]\ttraining's binary_logloss: 0.578187\n",
      "[233]\ttraining's binary_logloss: 0.578153\n",
      "[234]\ttraining's binary_logloss: 0.578124\n",
      "[235]\ttraining's binary_logloss: 0.578093\n",
      "[236]\ttraining's binary_logloss: 0.578066\n",
      "[237]\ttraining's binary_logloss: 0.578046\n",
      "[238]\ttraining's binary_logloss: 0.578023\n",
      "[239]\ttraining's binary_logloss: 0.577992\n",
      "[240]\ttraining's binary_logloss: 0.577979\n",
      "[241]\ttraining's binary_logloss: 0.577966\n",
      "[242]\ttraining's binary_logloss: 0.57794\n",
      "[243]\ttraining's binary_logloss: 0.577904\n",
      "[244]\ttraining's binary_logloss: 0.577876\n",
      "[245]\ttraining's binary_logloss: 0.577861\n",
      "[246]\ttraining's binary_logloss: 0.577835\n",
      "[247]\ttraining's binary_logloss: 0.577824\n",
      "[248]\ttraining's binary_logloss: 0.577784\n",
      "[249]\ttraining's binary_logloss: 0.577747\n",
      "[250]\ttraining's binary_logloss: 0.577709\n",
      "[251]\ttraining's binary_logloss: 0.577683\n",
      "[252]\ttraining's binary_logloss: 0.577646\n",
      "[253]\ttraining's binary_logloss: 0.577612\n",
      "[254]\ttraining's binary_logloss: 0.577583\n",
      "[255]\ttraining's binary_logloss: 0.577564\n",
      "[256]\ttraining's binary_logloss: 0.577536\n",
      "[257]\ttraining's binary_logloss: 0.57752\n",
      "[258]\ttraining's binary_logloss: 0.577489\n",
      "[259]\ttraining's binary_logloss: 0.577461\n",
      "[260]\ttraining's binary_logloss: 0.577435\n",
      "[261]\ttraining's binary_logloss: 0.577414\n",
      "[262]\ttraining's binary_logloss: 0.577389\n",
      "[263]\ttraining's binary_logloss: 0.577367\n",
      "[264]\ttraining's binary_logloss: 0.577337\n",
      "[265]\ttraining's binary_logloss: 0.577311\n",
      "[266]\ttraining's binary_logloss: 0.577281\n",
      "[267]\ttraining's binary_logloss: 0.577255\n",
      "[268]\ttraining's binary_logloss: 0.577241\n",
      "[269]\ttraining's binary_logloss: 0.577225\n",
      "[270]\ttraining's binary_logloss: 0.577214\n",
      "[271]\ttraining's binary_logloss: 0.577188\n",
      "[272]\ttraining's binary_logloss: 0.577166\n",
      "[273]\ttraining's binary_logloss: 0.577138\n",
      "[274]\ttraining's binary_logloss: 0.577116\n",
      "[275]\ttraining's binary_logloss: 0.577083\n",
      "[276]\ttraining's binary_logloss: 0.577056\n",
      "[277]\ttraining's binary_logloss: 0.577042\n",
      "[278]\ttraining's binary_logloss: 0.577012\n",
      "[279]\ttraining's binary_logloss: 0.576998\n",
      "[280]\ttraining's binary_logloss: 0.576984\n",
      "[281]\ttraining's binary_logloss: 0.576977\n",
      "[282]\ttraining's binary_logloss: 0.576951\n",
      "[283]\ttraining's binary_logloss: 0.57694\n",
      "[284]\ttraining's binary_logloss: 0.576919\n",
      "[285]\ttraining's binary_logloss: 0.576896\n",
      "[286]\ttraining's binary_logloss: 0.576879\n",
      "[287]\ttraining's binary_logloss: 0.576854\n",
      "[288]\ttraining's binary_logloss: 0.576837\n",
      "[289]\ttraining's binary_logloss: 0.576811\n",
      "[290]\ttraining's binary_logloss: 0.576777\n",
      "[291]\ttraining's binary_logloss: 0.576753\n",
      "[292]\ttraining's binary_logloss: 0.576743\n",
      "[293]\ttraining's binary_logloss: 0.57673\n",
      "[294]\ttraining's binary_logloss: 0.576718\n",
      "[295]\ttraining's binary_logloss: 0.57671\n",
      "[296]\ttraining's binary_logloss: 0.5767\n",
      "[297]\ttraining's binary_logloss: 0.57667\n",
      "[298]\ttraining's binary_logloss: 0.576663\n",
      "[299]\ttraining's binary_logloss: 0.576654\n",
      "[300]\ttraining's binary_logloss: 0.576642\n",
      "[301]\ttraining's binary_logloss: 0.576623\n",
      "[302]\ttraining's binary_logloss: 0.576615\n",
      "[303]\ttraining's binary_logloss: 0.576606\n",
      "[304]\ttraining's binary_logloss: 0.576579\n",
      "[305]\ttraining's binary_logloss: 0.57656\n",
      "[306]\ttraining's binary_logloss: 0.576542\n",
      "[307]\ttraining's binary_logloss: 0.576529\n",
      "[308]\ttraining's binary_logloss: 0.576519\n",
      "[309]\ttraining's binary_logloss: 0.5765\n",
      "[310]\ttraining's binary_logloss: 0.576481\n",
      "[311]\ttraining's binary_logloss: 0.576472\n",
      "[312]\ttraining's binary_logloss: 0.576455\n",
      "[313]\ttraining's binary_logloss: 0.576434\n",
      "[314]\ttraining's binary_logloss: 0.576425\n",
      "[315]\ttraining's binary_logloss: 0.576409\n",
      "[316]\ttraining's binary_logloss: 0.576384\n",
      "[317]\ttraining's binary_logloss: 0.576376\n",
      "[318]\ttraining's binary_logloss: 0.576365\n",
      "[319]\ttraining's binary_logloss: 0.576344\n",
      "[320]\ttraining's binary_logloss: 0.576335\n",
      "[321]\ttraining's binary_logloss: 0.57633\n",
      "[322]\ttraining's binary_logloss: 0.576317\n",
      "[323]\ttraining's binary_logloss: 0.576299\n",
      "[324]\ttraining's binary_logloss: 0.576282\n",
      "[325]\ttraining's binary_logloss: 0.576275\n",
      "[326]\ttraining's binary_logloss: 0.576261\n",
      "[327]\ttraining's binary_logloss: 0.576241\n",
      "[328]\ttraining's binary_logloss: 0.576235\n",
      "[329]\ttraining's binary_logloss: 0.576225\n",
      "[330]\ttraining's binary_logloss: 0.576219\n",
      "[331]\ttraining's binary_logloss: 0.5762\n",
      "[332]\ttraining's binary_logloss: 0.576189\n",
      "[333]\ttraining's binary_logloss: 0.576176\n",
      "[334]\ttraining's binary_logloss: 0.57616\n",
      "[335]\ttraining's binary_logloss: 0.576145\n",
      "[336]\ttraining's binary_logloss: 0.576138\n",
      "[337]\ttraining's binary_logloss: 0.576123\n",
      "[338]\ttraining's binary_logloss: 0.576115\n",
      "[339]\ttraining's binary_logloss: 0.576105\n",
      "[340]\ttraining's binary_logloss: 0.576093\n",
      "[341]\ttraining's binary_logloss: 0.576079\n",
      "[342]\ttraining's binary_logloss: 0.576064\n",
      "[343]\ttraining's binary_logloss: 0.576056\n",
      "[344]\ttraining's binary_logloss: 0.57605\n",
      "[345]\ttraining's binary_logloss: 0.57604\n",
      "[346]\ttraining's binary_logloss: 0.576033\n",
      "[347]\ttraining's binary_logloss: 0.576025\n",
      "[348]\ttraining's binary_logloss: 0.576011\n",
      "[349]\ttraining's binary_logloss: 0.576003\n",
      "[350]\ttraining's binary_logloss: 0.575996\n",
      "[351]\ttraining's binary_logloss: 0.575983\n",
      "[352]\ttraining's binary_logloss: 0.575963\n",
      "[353]\ttraining's binary_logloss: 0.57595\n",
      "[354]\ttraining's binary_logloss: 0.575941\n",
      "[355]\ttraining's binary_logloss: 0.575936\n",
      "[356]\ttraining's binary_logloss: 0.57592\n",
      "[357]\ttraining's binary_logloss: 0.575908\n",
      "[358]\ttraining's binary_logloss: 0.575891\n",
      "[359]\ttraining's binary_logloss: 0.575871\n",
      "[360]\ttraining's binary_logloss: 0.575857\n",
      "[361]\ttraining's binary_logloss: 0.57585\n",
      "[362]\ttraining's binary_logloss: 0.575842\n",
      "[363]\ttraining's binary_logloss: 0.575835\n",
      "[364]\ttraining's binary_logloss: 0.575822\n",
      "[365]\ttraining's binary_logloss: 0.575808\n",
      "[366]\ttraining's binary_logloss: 0.575793\n",
      "[367]\ttraining's binary_logloss: 0.575784\n",
      "[368]\ttraining's binary_logloss: 0.575773\n",
      "[369]\ttraining's binary_logloss: 0.575761\n",
      "[370]\ttraining's binary_logloss: 0.575754\n",
      "[371]\ttraining's binary_logloss: 0.575748\n",
      "[372]\ttraining's binary_logloss: 0.575736\n",
      "[373]\ttraining's binary_logloss: 0.575721\n",
      "[374]\ttraining's binary_logloss: 0.57571\n",
      "[375]\ttraining's binary_logloss: 0.575706\n",
      "[376]\ttraining's binary_logloss: 0.575694\n",
      "[377]\ttraining's binary_logloss: 0.575685\n",
      "[378]\ttraining's binary_logloss: 0.575674\n",
      "[379]\ttraining's binary_logloss: 0.575661\n",
      "[380]\ttraining's binary_logloss: 0.575646\n",
      "[381]\ttraining's binary_logloss: 0.575636\n",
      "[382]\ttraining's binary_logloss: 0.575625\n",
      "[383]\ttraining's binary_logloss: 0.575614\n",
      "[384]\ttraining's binary_logloss: 0.575599\n",
      "[385]\ttraining's binary_logloss: 0.575589\n",
      "[386]\ttraining's binary_logloss: 0.575583\n",
      "[387]\ttraining's binary_logloss: 0.575576\n",
      "[388]\ttraining's binary_logloss: 0.575569\n",
      "[389]\ttraining's binary_logloss: 0.575566\n",
      "[390]\ttraining's binary_logloss: 0.575562\n",
      "[391]\ttraining's binary_logloss: 0.57555\n",
      "[392]\ttraining's binary_logloss: 0.57554\n",
      "[393]\ttraining's binary_logloss: 0.575531\n",
      "[394]\ttraining's binary_logloss: 0.575521\n",
      "[395]\ttraining's binary_logloss: 0.575515\n",
      "[396]\ttraining's binary_logloss: 0.575505\n",
      "[397]\ttraining's binary_logloss: 0.575498\n",
      "[398]\ttraining's binary_logloss: 0.57549\n",
      "[399]\ttraining's binary_logloss: 0.575483\n",
      "[400]\ttraining's binary_logloss: 0.575474\n",
      "[401]\ttraining's binary_logloss: 0.575465\n",
      "[402]\ttraining's binary_logloss: 0.575458\n",
      "[403]\ttraining's binary_logloss: 0.575448\n",
      "[404]\ttraining's binary_logloss: 0.575443\n",
      "[405]\ttraining's binary_logloss: 0.575437\n",
      "[406]\ttraining's binary_logloss: 0.57543\n",
      "[407]\ttraining's binary_logloss: 0.575421\n",
      "[408]\ttraining's binary_logloss: 0.575415\n",
      "[409]\ttraining's binary_logloss: 0.575411\n",
      "[410]\ttraining's binary_logloss: 0.575401\n",
      "[411]\ttraining's binary_logloss: 0.575397\n",
      "[412]\ttraining's binary_logloss: 0.575392\n",
      "[413]\ttraining's binary_logloss: 0.575383\n",
      "[414]\ttraining's binary_logloss: 0.575378\n",
      "[415]\ttraining's binary_logloss: 0.575368\n",
      "[416]\ttraining's binary_logloss: 0.575362\n",
      "[417]\ttraining's binary_logloss: 0.575357\n",
      "[418]\ttraining's binary_logloss: 0.575352\n",
      "[419]\ttraining's binary_logloss: 0.575346\n",
      "[420]\ttraining's binary_logloss: 0.575343\n",
      "[421]\ttraining's binary_logloss: 0.575337\n",
      "[422]\ttraining's binary_logloss: 0.575331\n",
      "[423]\ttraining's binary_logloss: 0.575323\n",
      "[424]\ttraining's binary_logloss: 0.575317\n",
      "[425]\ttraining's binary_logloss: 0.575312\n",
      "[426]\ttraining's binary_logloss: 0.575306\n",
      "[427]\ttraining's binary_logloss: 0.575299\n",
      "[428]\ttraining's binary_logloss: 0.575291\n",
      "[429]\ttraining's binary_logloss: 0.575287\n",
      "[430]\ttraining's binary_logloss: 0.57528\n",
      "[431]\ttraining's binary_logloss: 0.575273\n",
      "[432]\ttraining's binary_logloss: 0.57527\n",
      "[433]\ttraining's binary_logloss: 0.575261\n",
      "[434]\ttraining's binary_logloss: 0.575252\n",
      "[435]\ttraining's binary_logloss: 0.575246\n",
      "[436]\ttraining's binary_logloss: 0.575238\n",
      "[437]\ttraining's binary_logloss: 0.575235\n",
      "[438]\ttraining's binary_logloss: 0.575228\n",
      "[439]\ttraining's binary_logloss: 0.575223\n",
      "[440]\ttraining's binary_logloss: 0.575218\n",
      "[441]\ttraining's binary_logloss: 0.575214\n",
      "[442]\ttraining's binary_logloss: 0.575208\n",
      "[443]\ttraining's binary_logloss: 0.575203\n",
      "[444]\ttraining's binary_logloss: 0.575199\n",
      "[445]\ttraining's binary_logloss: 0.575193\n",
      "[446]\ttraining's binary_logloss: 0.575186\n",
      "[447]\ttraining's binary_logloss: 0.575179\n",
      "[448]\ttraining's binary_logloss: 0.575175\n",
      "[449]\ttraining's binary_logloss: 0.575172\n",
      "[450]\ttraining's binary_logloss: 0.575168\n",
      "[451]\ttraining's binary_logloss: 0.575161\n",
      "[452]\ttraining's binary_logloss: 0.575156\n",
      "[453]\ttraining's binary_logloss: 0.575151\n",
      "[454]\ttraining's binary_logloss: 0.575146\n",
      "[455]\ttraining's binary_logloss: 0.57514\n",
      "[456]\ttraining's binary_logloss: 0.575138\n",
      "[457]\ttraining's binary_logloss: 0.575133\n",
      "[458]\ttraining's binary_logloss: 0.575128\n",
      "[459]\ttraining's binary_logloss: 0.575124\n",
      "[460]\ttraining's binary_logloss: 0.575119\n",
      "[461]\ttraining's binary_logloss: 0.575112\n",
      "[462]\ttraining's binary_logloss: 0.575108\n",
      "[463]\ttraining's binary_logloss: 0.575104\n",
      "[464]\ttraining's binary_logloss: 0.5751\n",
      "[465]\ttraining's binary_logloss: 0.575094\n",
      "[466]\ttraining's binary_logloss: 0.575091\n",
      "[467]\ttraining's binary_logloss: 0.575088\n",
      "[468]\ttraining's binary_logloss: 0.575084\n",
      "[469]\ttraining's binary_logloss: 0.575081\n",
      "[470]\ttraining's binary_logloss: 0.575076\n",
      "[471]\ttraining's binary_logloss: 0.575071\n",
      "[472]\ttraining's binary_logloss: 0.575064\n",
      "[473]\ttraining's binary_logloss: 0.575059\n",
      "[474]\ttraining's binary_logloss: 0.575056\n",
      "[475]\ttraining's binary_logloss: 0.575051\n",
      "[476]\ttraining's binary_logloss: 0.575044\n",
      "[477]\ttraining's binary_logloss: 0.575038\n",
      "[478]\ttraining's binary_logloss: 0.575029\n",
      "[479]\ttraining's binary_logloss: 0.575025\n",
      "[480]\ttraining's binary_logloss: 0.575021\n",
      "[481]\ttraining's binary_logloss: 0.575016\n",
      "[482]\ttraining's binary_logloss: 0.575013\n",
      "[483]\ttraining's binary_logloss: 0.575009\n",
      "[484]\ttraining's binary_logloss: 0.575007\n",
      "[485]\ttraining's binary_logloss: 0.575002\n",
      "[486]\ttraining's binary_logloss: 0.574999\n",
      "[487]\ttraining's binary_logloss: 0.574995\n",
      "[488]\ttraining's binary_logloss: 0.574991\n",
      "[489]\ttraining's binary_logloss: 0.574986\n",
      "[490]\ttraining's binary_logloss: 0.574982\n",
      "[491]\ttraining's binary_logloss: 0.574979\n",
      "[492]\ttraining's binary_logloss: 0.574976\n",
      "[493]\ttraining's binary_logloss: 0.574974\n",
      "[494]\ttraining's binary_logloss: 0.574971\n",
      "[495]\ttraining's binary_logloss: 0.574966\n",
      "[496]\ttraining's binary_logloss: 0.574964\n",
      "[497]\ttraining's binary_logloss: 0.574962\n",
      "[498]\ttraining's binary_logloss: 0.574958\n",
      "[499]\ttraining's binary_logloss: 0.574954\n",
      "[500]\ttraining's binary_logloss: 0.574949\n",
      "[1]\ttraining's binary_logloss: 0.630327\n",
      "[2]\ttraining's binary_logloss: 0.625707\n",
      "[3]\ttraining's binary_logloss: 0.621911\n",
      "[4]\ttraining's binary_logloss: 0.618738\n",
      "[5]\ttraining's binary_logloss: 0.616051\n",
      "[6]\ttraining's binary_logloss: 0.613677\n",
      "[7]\ttraining's binary_logloss: 0.611693\n",
      "[8]\ttraining's binary_logloss: 0.609794\n",
      "[9]\ttraining's binary_logloss: 0.608109\n",
      "[10]\ttraining's binary_logloss: 0.606728\n",
      "[11]\ttraining's binary_logloss: 0.605529\n",
      "[12]\ttraining's binary_logloss: 0.604376\n",
      "[13]\ttraining's binary_logloss: 0.603425\n",
      "[14]\ttraining's binary_logloss: 0.602421\n",
      "[15]\ttraining's binary_logloss: 0.601527\n",
      "[16]\ttraining's binary_logloss: 0.600659\n",
      "[17]\ttraining's binary_logloss: 0.599828\n",
      "[18]\ttraining's binary_logloss: 0.599041\n",
      "[19]\ttraining's binary_logloss: 0.598404\n",
      "[20]\ttraining's binary_logloss: 0.597661\n",
      "[21]\ttraining's binary_logloss: 0.597049\n",
      "[22]\ttraining's binary_logloss: 0.596525\n",
      "[23]\ttraining's binary_logloss: 0.595831\n",
      "[24]\ttraining's binary_logloss: 0.595349\n",
      "[25]\ttraining's binary_logloss: 0.594775\n",
      "[26]\ttraining's binary_logloss: 0.594279\n",
      "[27]\ttraining's binary_logloss: 0.593816\n",
      "[28]\ttraining's binary_logloss: 0.593364\n",
      "[29]\ttraining's binary_logloss: 0.592872\n",
      "[30]\ttraining's binary_logloss: 0.592345\n",
      "[31]\ttraining's binary_logloss: 0.591939\n",
      "[32]\ttraining's binary_logloss: 0.59154\n",
      "[33]\ttraining's binary_logloss: 0.59118\n",
      "[34]\ttraining's binary_logloss: 0.590774\n",
      "[35]\ttraining's binary_logloss: 0.590438\n",
      "[36]\ttraining's binary_logloss: 0.59013\n",
      "[37]\ttraining's binary_logloss: 0.589695\n",
      "[38]\ttraining's binary_logloss: 0.589303\n",
      "[39]\ttraining's binary_logloss: 0.588955\n",
      "[40]\ttraining's binary_logloss: 0.588451\n",
      "[41]\ttraining's binary_logloss: 0.588067\n",
      "[42]\ttraining's binary_logloss: 0.587588\n",
      "[43]\ttraining's binary_logloss: 0.587056\n",
      "[44]\ttraining's binary_logloss: 0.586676\n",
      "[45]\ttraining's binary_logloss: 0.586369\n",
      "[46]\ttraining's binary_logloss: 0.586076\n",
      "[47]\ttraining's binary_logloss: 0.585742\n",
      "[48]\ttraining's binary_logloss: 0.585411\n",
      "[49]\ttraining's binary_logloss: 0.585009\n",
      "[50]\ttraining's binary_logloss: 0.584546\n",
      "[51]\ttraining's binary_logloss: 0.584291\n",
      "[52]\ttraining's binary_logloss: 0.583972\n",
      "[53]\ttraining's binary_logloss: 0.583579\n",
      "[54]\ttraining's binary_logloss: 0.583332\n",
      "[55]\ttraining's binary_logloss: 0.583083\n",
      "[56]\ttraining's binary_logloss: 0.582767\n",
      "[57]\ttraining's binary_logloss: 0.582547\n",
      "[58]\ttraining's binary_logloss: 0.58221\n",
      "[59]\ttraining's binary_logloss: 0.581964\n",
      "[60]\ttraining's binary_logloss: 0.581585\n",
      "[61]\ttraining's binary_logloss: 0.581262\n",
      "[62]\ttraining's binary_logloss: 0.580884\n",
      "[63]\ttraining's binary_logloss: 0.580508\n",
      "[64]\ttraining's binary_logloss: 0.580253\n",
      "[65]\ttraining's binary_logloss: 0.580016\n",
      "[66]\ttraining's binary_logloss: 0.579601\n",
      "[67]\ttraining's binary_logloss: 0.579347\n",
      "[68]\ttraining's binary_logloss: 0.578986\n",
      "[69]\ttraining's binary_logloss: 0.578729\n",
      "[70]\ttraining's binary_logloss: 0.578347\n",
      "[71]\ttraining's binary_logloss: 0.578141\n",
      "[72]\ttraining's binary_logloss: 0.577833\n",
      "[73]\ttraining's binary_logloss: 0.577404\n",
      "[74]\ttraining's binary_logloss: 0.57703\n",
      "[75]\ttraining's binary_logloss: 0.576744\n",
      "[76]\ttraining's binary_logloss: 0.576494\n",
      "[77]\ttraining's binary_logloss: 0.57618\n",
      "[78]\ttraining's binary_logloss: 0.575909\n",
      "[79]\ttraining's binary_logloss: 0.575628\n",
      "[80]\ttraining's binary_logloss: 0.575327\n",
      "[81]\ttraining's binary_logloss: 0.574958\n",
      "[82]\ttraining's binary_logloss: 0.574736\n",
      "[83]\ttraining's binary_logloss: 0.57438\n",
      "[84]\ttraining's binary_logloss: 0.574077\n",
      "[85]\ttraining's binary_logloss: 0.573679\n",
      "[86]\ttraining's binary_logloss: 0.573478\n",
      "[87]\ttraining's binary_logloss: 0.573257\n",
      "[88]\ttraining's binary_logloss: 0.573007\n",
      "[89]\ttraining's binary_logloss: 0.572797\n",
      "[90]\ttraining's binary_logloss: 0.57265\n",
      "[91]\ttraining's binary_logloss: 0.572387\n",
      "[92]\ttraining's binary_logloss: 0.572114\n",
      "[93]\ttraining's binary_logloss: 0.571809\n",
      "[94]\ttraining's binary_logloss: 0.571456\n",
      "[95]\ttraining's binary_logloss: 0.571196\n",
      "[96]\ttraining's binary_logloss: 0.570876\n",
      "[97]\ttraining's binary_logloss: 0.570689\n",
      "[98]\ttraining's binary_logloss: 0.570455\n",
      "[99]\ttraining's binary_logloss: 0.570148\n",
      "[100]\ttraining's binary_logloss: 0.56986\n",
      "[101]\ttraining's binary_logloss: 0.56955\n",
      "[102]\ttraining's binary_logloss: 0.569291\n",
      "[103]\ttraining's binary_logloss: 0.569101\n",
      "[104]\ttraining's binary_logloss: 0.568701\n",
      "[105]\ttraining's binary_logloss: 0.568501\n",
      "[106]\ttraining's binary_logloss: 0.568221\n",
      "[107]\ttraining's binary_logloss: 0.567885\n",
      "[108]\ttraining's binary_logloss: 0.567686\n",
      "[109]\ttraining's binary_logloss: 0.567401\n",
      "[110]\ttraining's binary_logloss: 0.567178\n",
      "[111]\ttraining's binary_logloss: 0.56704\n",
      "[112]\ttraining's binary_logloss: 0.566868\n",
      "[113]\ttraining's binary_logloss: 0.566731\n",
      "[114]\ttraining's binary_logloss: 0.56656\n",
      "[115]\ttraining's binary_logloss: 0.566391\n",
      "[116]\ttraining's binary_logloss: 0.566133\n",
      "[117]\ttraining's binary_logloss: 0.565783\n",
      "[118]\ttraining's binary_logloss: 0.565467\n",
      "[119]\ttraining's binary_logloss: 0.565195\n",
      "[120]\ttraining's binary_logloss: 0.564877\n",
      "[121]\ttraining's binary_logloss: 0.564522\n",
      "[122]\ttraining's binary_logloss: 0.564389\n",
      "[123]\ttraining's binary_logloss: 0.564129\n",
      "[124]\ttraining's binary_logloss: 0.56386\n",
      "[125]\ttraining's binary_logloss: 0.563539\n",
      "[126]\ttraining's binary_logloss: 0.563364\n",
      "[127]\ttraining's binary_logloss: 0.563158\n",
      "[128]\ttraining's binary_logloss: 0.562902\n",
      "[129]\ttraining's binary_logloss: 0.562667\n",
      "[130]\ttraining's binary_logloss: 0.562528\n",
      "[131]\ttraining's binary_logloss: 0.562254\n",
      "[132]\ttraining's binary_logloss: 0.562036\n",
      "[133]\ttraining's binary_logloss: 0.561805\n",
      "[134]\ttraining's binary_logloss: 0.561568\n",
      "[135]\ttraining's binary_logloss: 0.561399\n",
      "[136]\ttraining's binary_logloss: 0.561082\n",
      "[137]\ttraining's binary_logloss: 0.560829\n",
      "[138]\ttraining's binary_logloss: 0.560569\n",
      "[139]\ttraining's binary_logloss: 0.560291\n",
      "[140]\ttraining's binary_logloss: 0.56004\n",
      "[141]\ttraining's binary_logloss: 0.559884\n",
      "[142]\ttraining's binary_logloss: 0.559667\n",
      "[143]\ttraining's binary_logloss: 0.559379\n",
      "[144]\ttraining's binary_logloss: 0.55911\n",
      "[145]\ttraining's binary_logloss: 0.558853\n",
      "[146]\ttraining's binary_logloss: 0.558531\n",
      "[147]\ttraining's binary_logloss: 0.558302\n",
      "[148]\ttraining's binary_logloss: 0.558065\n",
      "[149]\ttraining's binary_logloss: 0.557854\n",
      "[150]\ttraining's binary_logloss: 0.557704\n",
      "[151]\ttraining's binary_logloss: 0.557435\n",
      "[152]\ttraining's binary_logloss: 0.557159\n",
      "[153]\ttraining's binary_logloss: 0.556869\n",
      "[154]\ttraining's binary_logloss: 0.556544\n",
      "[155]\ttraining's binary_logloss: 0.556369\n",
      "[156]\ttraining's binary_logloss: 0.556156\n",
      "[157]\ttraining's binary_logloss: 0.555792\n",
      "[158]\ttraining's binary_logloss: 0.555509\n",
      "[159]\ttraining's binary_logloss: 0.555315\n",
      "[160]\ttraining's binary_logloss: 0.555198\n",
      "[161]\ttraining's binary_logloss: 0.555006\n",
      "[162]\ttraining's binary_logloss: 0.554759\n",
      "[163]\ttraining's binary_logloss: 0.554435\n",
      "[164]\ttraining's binary_logloss: 0.554162\n",
      "[165]\ttraining's binary_logloss: 0.55393\n",
      "[166]\ttraining's binary_logloss: 0.553733\n",
      "[167]\ttraining's binary_logloss: 0.553448\n",
      "[168]\ttraining's binary_logloss: 0.553218\n",
      "[169]\ttraining's binary_logloss: 0.552963\n",
      "[170]\ttraining's binary_logloss: 0.552657\n",
      "[171]\ttraining's binary_logloss: 0.55251\n",
      "[172]\ttraining's binary_logloss: 0.552388\n",
      "[173]\ttraining's binary_logloss: 0.552206\n",
      "[174]\ttraining's binary_logloss: 0.551963\n",
      "[175]\ttraining's binary_logloss: 0.551694\n",
      "[176]\ttraining's binary_logloss: 0.551493\n",
      "[177]\ttraining's binary_logloss: 0.551253\n",
      "[178]\ttraining's binary_logloss: 0.551061\n",
      "[179]\ttraining's binary_logloss: 0.550897\n",
      "[180]\ttraining's binary_logloss: 0.550651\n",
      "[181]\ttraining's binary_logloss: 0.550388\n",
      "[182]\ttraining's binary_logloss: 0.550103\n",
      "[183]\ttraining's binary_logloss: 0.549881\n",
      "[184]\ttraining's binary_logloss: 0.54976\n",
      "[185]\ttraining's binary_logloss: 0.54947\n",
      "[186]\ttraining's binary_logloss: 0.549259\n",
      "[187]\ttraining's binary_logloss: 0.549083\n",
      "[188]\ttraining's binary_logloss: 0.548827\n",
      "[189]\ttraining's binary_logloss: 0.548585\n",
      "[190]\ttraining's binary_logloss: 0.548397\n",
      "[191]\ttraining's binary_logloss: 0.548207\n",
      "[192]\ttraining's binary_logloss: 0.548015\n",
      "[193]\ttraining's binary_logloss: 0.547784\n",
      "[194]\ttraining's binary_logloss: 0.547568\n",
      "[195]\ttraining's binary_logloss: 0.547363\n",
      "[196]\ttraining's binary_logloss: 0.547115\n",
      "[197]\ttraining's binary_logloss: 0.546982\n",
      "[198]\ttraining's binary_logloss: 0.54687\n",
      "[199]\ttraining's binary_logloss: 0.546678\n",
      "[200]\ttraining's binary_logloss: 0.546531\n",
      "[201]\ttraining's binary_logloss: 0.546439\n",
      "[202]\ttraining's binary_logloss: 0.546241\n",
      "[203]\ttraining's binary_logloss: 0.546144\n",
      "[204]\ttraining's binary_logloss: 0.546017\n",
      "[205]\ttraining's binary_logloss: 0.545768\n",
      "[206]\ttraining's binary_logloss: 0.545531\n",
      "[207]\ttraining's binary_logloss: 0.545337\n",
      "[208]\ttraining's binary_logloss: 0.545237\n",
      "[209]\ttraining's binary_logloss: 0.545065\n",
      "[210]\ttraining's binary_logloss: 0.544862\n",
      "[211]\ttraining's binary_logloss: 0.544644\n",
      "[212]\ttraining's binary_logloss: 0.544451\n",
      "[213]\ttraining's binary_logloss: 0.544274\n",
      "[214]\ttraining's binary_logloss: 0.544197\n",
      "[215]\ttraining's binary_logloss: 0.544042\n",
      "[216]\ttraining's binary_logloss: 0.543867\n",
      "[217]\ttraining's binary_logloss: 0.543702\n",
      "[218]\ttraining's binary_logloss: 0.543589\n",
      "[219]\ttraining's binary_logloss: 0.543438\n",
      "[220]\ttraining's binary_logloss: 0.543253\n",
      "[221]\ttraining's binary_logloss: 0.543032\n",
      "[222]\ttraining's binary_logloss: 0.542853\n",
      "[223]\ttraining's binary_logloss: 0.542698\n",
      "[224]\ttraining's binary_logloss: 0.542407\n",
      "[225]\ttraining's binary_logloss: 0.542327\n",
      "[226]\ttraining's binary_logloss: 0.542137\n",
      "[227]\ttraining's binary_logloss: 0.541886\n",
      "[228]\ttraining's binary_logloss: 0.541666\n",
      "[229]\ttraining's binary_logloss: 0.541478\n",
      "[230]\ttraining's binary_logloss: 0.541259\n",
      "[231]\ttraining's binary_logloss: 0.541106\n",
      "[232]\ttraining's binary_logloss: 0.540824\n",
      "[233]\ttraining's binary_logloss: 0.540651\n",
      "[234]\ttraining's binary_logloss: 0.540568\n",
      "[235]\ttraining's binary_logloss: 0.540267\n",
      "[236]\ttraining's binary_logloss: 0.540118\n",
      "[237]\ttraining's binary_logloss: 0.539921\n",
      "[238]\ttraining's binary_logloss: 0.539676\n",
      "[239]\ttraining's binary_logloss: 0.53946\n",
      "[240]\ttraining's binary_logloss: 0.539282\n",
      "[241]\ttraining's binary_logloss: 0.539177\n",
      "[242]\ttraining's binary_logloss: 0.538975\n",
      "[243]\ttraining's binary_logloss: 0.538822\n",
      "[244]\ttraining's binary_logloss: 0.538664\n",
      "[245]\ttraining's binary_logloss: 0.538445\n",
      "[246]\ttraining's binary_logloss: 0.538262\n",
      "[247]\ttraining's binary_logloss: 0.538007\n",
      "[248]\ttraining's binary_logloss: 0.537785\n",
      "[249]\ttraining's binary_logloss: 0.537615\n",
      "[250]\ttraining's binary_logloss: 0.53746\n",
      "[251]\ttraining's binary_logloss: 0.53717\n",
      "[252]\ttraining's binary_logloss: 0.536964\n",
      "[253]\ttraining's binary_logloss: 0.536759\n",
      "[254]\ttraining's binary_logloss: 0.536652\n",
      "[255]\ttraining's binary_logloss: 0.536485\n",
      "[256]\ttraining's binary_logloss: 0.536174\n",
      "[257]\ttraining's binary_logloss: 0.535971\n",
      "[258]\ttraining's binary_logloss: 0.535746\n",
      "[259]\ttraining's binary_logloss: 0.535651\n",
      "[260]\ttraining's binary_logloss: 0.535487\n",
      "[261]\ttraining's binary_logloss: 0.535276\n",
      "[262]\ttraining's binary_logloss: 0.535038\n",
      "[263]\ttraining's binary_logloss: 0.534837\n",
      "[264]\ttraining's binary_logloss: 0.534617\n",
      "[265]\ttraining's binary_logloss: 0.534416\n",
      "[266]\ttraining's binary_logloss: 0.534256\n",
      "[267]\ttraining's binary_logloss: 0.534162\n",
      "[268]\ttraining's binary_logloss: 0.533978\n",
      "[269]\ttraining's binary_logloss: 0.533776\n",
      "[270]\ttraining's binary_logloss: 0.533582\n",
      "[271]\ttraining's binary_logloss: 0.53332\n",
      "[272]\ttraining's binary_logloss: 0.533078\n",
      "[273]\ttraining's binary_logloss: 0.532921\n",
      "[274]\ttraining's binary_logloss: 0.532752\n",
      "[275]\ttraining's binary_logloss: 0.532566\n",
      "[276]\ttraining's binary_logloss: 0.532314\n",
      "[277]\ttraining's binary_logloss: 0.532118\n",
      "[278]\ttraining's binary_logloss: 0.531963\n",
      "[279]\ttraining's binary_logloss: 0.531776\n",
      "[280]\ttraining's binary_logloss: 0.531554\n",
      "[281]\ttraining's binary_logloss: 0.53142\n",
      "[282]\ttraining's binary_logloss: 0.531349\n",
      "[283]\ttraining's binary_logloss: 0.531258\n",
      "[284]\ttraining's binary_logloss: 0.53104\n",
      "[285]\ttraining's binary_logloss: 0.530896\n",
      "[286]\ttraining's binary_logloss: 0.530735\n",
      "[287]\ttraining's binary_logloss: 0.530654\n",
      "[288]\ttraining's binary_logloss: 0.53046\n",
      "[289]\ttraining's binary_logloss: 0.530261\n",
      "[290]\ttraining's binary_logloss: 0.530101\n",
      "[291]\ttraining's binary_logloss: 0.529957\n",
      "[292]\ttraining's binary_logloss: 0.529811\n",
      "[293]\ttraining's binary_logloss: 0.529646\n",
      "[294]\ttraining's binary_logloss: 0.529468\n",
      "[295]\ttraining's binary_logloss: 0.52923\n",
      "[296]\ttraining's binary_logloss: 0.529028\n",
      "[297]\ttraining's binary_logloss: 0.528847\n",
      "[298]\ttraining's binary_logloss: 0.528628\n",
      "[299]\ttraining's binary_logloss: 0.52839\n",
      "[300]\ttraining's binary_logloss: 0.528246\n",
      "[301]\ttraining's binary_logloss: 0.528064\n",
      "[302]\ttraining's binary_logloss: 0.527919\n",
      "[303]\ttraining's binary_logloss: 0.527714\n",
      "[304]\ttraining's binary_logloss: 0.527501\n",
      "[305]\ttraining's binary_logloss: 0.527305\n",
      "[306]\ttraining's binary_logloss: 0.527134\n",
      "[307]\ttraining's binary_logloss: 0.527057\n",
      "[308]\ttraining's binary_logloss: 0.526877\n",
      "[309]\ttraining's binary_logloss: 0.52675\n",
      "[310]\ttraining's binary_logloss: 0.526446\n",
      "[311]\ttraining's binary_logloss: 0.526319\n",
      "[312]\ttraining's binary_logloss: 0.526218\n",
      "[313]\ttraining's binary_logloss: 0.526024\n",
      "[314]\ttraining's binary_logloss: 0.525858\n",
      "[315]\ttraining's binary_logloss: 0.525624\n",
      "[316]\ttraining's binary_logloss: 0.525477\n",
      "[317]\ttraining's binary_logloss: 0.525291\n",
      "[318]\ttraining's binary_logloss: 0.525096\n",
      "[319]\ttraining's binary_logloss: 0.524981\n",
      "[320]\ttraining's binary_logloss: 0.524802\n",
      "[321]\ttraining's binary_logloss: 0.524574\n",
      "[322]\ttraining's binary_logloss: 0.524402\n",
      "[323]\ttraining's binary_logloss: 0.524323\n",
      "[324]\ttraining's binary_logloss: 0.52415\n",
      "[325]\ttraining's binary_logloss: 0.523992\n",
      "[326]\ttraining's binary_logloss: 0.523835\n",
      "[327]\ttraining's binary_logloss: 0.52352\n",
      "[328]\ttraining's binary_logloss: 0.523393\n",
      "[329]\ttraining's binary_logloss: 0.523215\n",
      "[330]\ttraining's binary_logloss: 0.523066\n",
      "[331]\ttraining's binary_logloss: 0.522952\n",
      "[332]\ttraining's binary_logloss: 0.522743\n",
      "[333]\ttraining's binary_logloss: 0.522563\n",
      "[334]\ttraining's binary_logloss: 0.522332\n",
      "[335]\ttraining's binary_logloss: 0.522177\n",
      "[336]\ttraining's binary_logloss: 0.522087\n",
      "[337]\ttraining's binary_logloss: 0.521939\n",
      "[338]\ttraining's binary_logloss: 0.521846\n",
      "[339]\ttraining's binary_logloss: 0.521777\n",
      "[340]\ttraining's binary_logloss: 0.521706\n",
      "[341]\ttraining's binary_logloss: 0.521522\n",
      "[342]\ttraining's binary_logloss: 0.521333\n",
      "[343]\ttraining's binary_logloss: 0.521133\n",
      "[344]\ttraining's binary_logloss: 0.520924\n",
      "[345]\ttraining's binary_logloss: 0.520713\n",
      "[346]\ttraining's binary_logloss: 0.520576\n",
      "[347]\ttraining's binary_logloss: 0.520374\n",
      "[348]\ttraining's binary_logloss: 0.520146\n",
      "[349]\ttraining's binary_logloss: 0.520019\n",
      "[350]\ttraining's binary_logloss: 0.519826\n",
      "[351]\ttraining's binary_logloss: 0.51963\n",
      "[352]\ttraining's binary_logloss: 0.519441\n",
      "[353]\ttraining's binary_logloss: 0.519212\n",
      "[354]\ttraining's binary_logloss: 0.519025\n",
      "[355]\ttraining's binary_logloss: 0.518811\n",
      "[356]\ttraining's binary_logloss: 0.518606\n",
      "[357]\ttraining's binary_logloss: 0.518431\n",
      "[358]\ttraining's binary_logloss: 0.518269\n",
      "[359]\ttraining's binary_logloss: 0.518127\n",
      "[360]\ttraining's binary_logloss: 0.518028\n",
      "[361]\ttraining's binary_logloss: 0.517884\n",
      "[362]\ttraining's binary_logloss: 0.517706\n",
      "[363]\ttraining's binary_logloss: 0.517568\n",
      "[364]\ttraining's binary_logloss: 0.51739\n",
      "[365]\ttraining's binary_logloss: 0.517213\n",
      "[366]\ttraining's binary_logloss: 0.517062\n",
      "[367]\ttraining's binary_logloss: 0.516903\n",
      "[368]\ttraining's binary_logloss: 0.51675\n",
      "[369]\ttraining's binary_logloss: 0.516583\n",
      "[370]\ttraining's binary_logloss: 0.516435\n",
      "[371]\ttraining's binary_logloss: 0.516287\n",
      "[372]\ttraining's binary_logloss: 0.51616\n",
      "[373]\ttraining's binary_logloss: 0.516012\n",
      "[374]\ttraining's binary_logloss: 0.515836\n",
      "[375]\ttraining's binary_logloss: 0.515629\n",
      "[376]\ttraining's binary_logloss: 0.515425\n",
      "[377]\ttraining's binary_logloss: 0.515242\n",
      "[378]\ttraining's binary_logloss: 0.515131\n",
      "[379]\ttraining's binary_logloss: 0.514941\n",
      "[380]\ttraining's binary_logloss: 0.514841\n",
      "[381]\ttraining's binary_logloss: 0.514751\n",
      "[382]\ttraining's binary_logloss: 0.514567\n",
      "[383]\ttraining's binary_logloss: 0.514365\n",
      "[384]\ttraining's binary_logloss: 0.514196\n",
      "[385]\ttraining's binary_logloss: 0.514065\n",
      "[386]\ttraining's binary_logloss: 0.513883\n",
      "[387]\ttraining's binary_logloss: 0.513706\n",
      "[388]\ttraining's binary_logloss: 0.513518\n",
      "[389]\ttraining's binary_logloss: 0.513268\n",
      "[390]\ttraining's binary_logloss: 0.51307\n",
      "[391]\ttraining's binary_logloss: 0.512812\n",
      "[392]\ttraining's binary_logloss: 0.512618\n",
      "[393]\ttraining's binary_logloss: 0.512378\n",
      "[394]\ttraining's binary_logloss: 0.51217\n",
      "[395]\ttraining's binary_logloss: 0.511899\n",
      "[396]\ttraining's binary_logloss: 0.511712\n",
      "[397]\ttraining's binary_logloss: 0.511648\n",
      "[398]\ttraining's binary_logloss: 0.51148\n",
      "[399]\ttraining's binary_logloss: 0.511349\n",
      "[400]\ttraining's binary_logloss: 0.511176\n",
      "[401]\ttraining's binary_logloss: 0.511042\n",
      "[402]\ttraining's binary_logloss: 0.510873\n",
      "[403]\ttraining's binary_logloss: 0.510707\n",
      "[404]\ttraining's binary_logloss: 0.510631\n",
      "[405]\ttraining's binary_logloss: 0.510548\n",
      "[406]\ttraining's binary_logloss: 0.51041\n",
      "[407]\ttraining's binary_logloss: 0.510174\n",
      "[408]\ttraining's binary_logloss: 0.510065\n",
      "[409]\ttraining's binary_logloss: 0.50997\n",
      "[410]\ttraining's binary_logloss: 0.50991\n",
      "[411]\ttraining's binary_logloss: 0.509856\n",
      "[412]\ttraining's binary_logloss: 0.509724\n",
      "[413]\ttraining's binary_logloss: 0.509613\n",
      "[414]\ttraining's binary_logloss: 0.509517\n",
      "[415]\ttraining's binary_logloss: 0.509374\n",
      "[416]\ttraining's binary_logloss: 0.509221\n",
      "[417]\ttraining's binary_logloss: 0.50914\n",
      "[418]\ttraining's binary_logloss: 0.508988\n",
      "[419]\ttraining's binary_logloss: 0.508813\n",
      "[420]\ttraining's binary_logloss: 0.508715\n",
      "[421]\ttraining's binary_logloss: 0.508575\n",
      "[422]\ttraining's binary_logloss: 0.508347\n",
      "[423]\ttraining's binary_logloss: 0.50816\n",
      "[424]\ttraining's binary_logloss: 0.508039\n",
      "[425]\ttraining's binary_logloss: 0.507839\n",
      "[426]\ttraining's binary_logloss: 0.507744\n",
      "[427]\ttraining's binary_logloss: 0.50763\n",
      "[428]\ttraining's binary_logloss: 0.507515\n",
      "[429]\ttraining's binary_logloss: 0.507435\n",
      "[430]\ttraining's binary_logloss: 0.507376\n",
      "[431]\ttraining's binary_logloss: 0.507216\n",
      "[432]\ttraining's binary_logloss: 0.507107\n",
      "[433]\ttraining's binary_logloss: 0.506951\n",
      "[434]\ttraining's binary_logloss: 0.506815\n",
      "[435]\ttraining's binary_logloss: 0.506624\n",
      "[436]\ttraining's binary_logloss: 0.506423\n",
      "[437]\ttraining's binary_logloss: 0.506233\n",
      "[438]\ttraining's binary_logloss: 0.506082\n",
      "[439]\ttraining's binary_logloss: 0.505985\n",
      "[440]\ttraining's binary_logloss: 0.505862\n",
      "[441]\ttraining's binary_logloss: 0.505676\n",
      "[442]\ttraining's binary_logloss: 0.505499\n",
      "[443]\ttraining's binary_logloss: 0.505394\n",
      "[444]\ttraining's binary_logloss: 0.505223\n",
      "[445]\ttraining's binary_logloss: 0.505047\n",
      "[446]\ttraining's binary_logloss: 0.504784\n",
      "[447]\ttraining's binary_logloss: 0.504602\n",
      "[448]\ttraining's binary_logloss: 0.504485\n",
      "[449]\ttraining's binary_logloss: 0.504265\n",
      "[450]\ttraining's binary_logloss: 0.50408\n",
      "[451]\ttraining's binary_logloss: 0.503969\n",
      "[452]\ttraining's binary_logloss: 0.50381\n",
      "[453]\ttraining's binary_logloss: 0.503641\n",
      "[454]\ttraining's binary_logloss: 0.503423\n",
      "[455]\ttraining's binary_logloss: 0.503261\n",
      "[456]\ttraining's binary_logloss: 0.503096\n",
      "[457]\ttraining's binary_logloss: 0.502956\n",
      "[458]\ttraining's binary_logloss: 0.502837\n",
      "[459]\ttraining's binary_logloss: 0.502757\n",
      "[460]\ttraining's binary_logloss: 0.502555\n",
      "[461]\ttraining's binary_logloss: 0.50234\n",
      "[462]\ttraining's binary_logloss: 0.502155\n",
      "[463]\ttraining's binary_logloss: 0.501988\n",
      "[464]\ttraining's binary_logloss: 0.501925\n",
      "[465]\ttraining's binary_logloss: 0.501826\n",
      "[466]\ttraining's binary_logloss: 0.501713\n",
      "[467]\ttraining's binary_logloss: 0.5016\n",
      "[468]\ttraining's binary_logloss: 0.501354\n",
      "[469]\ttraining's binary_logloss: 0.501202\n",
      "[470]\ttraining's binary_logloss: 0.501045\n",
      "[471]\ttraining's binary_logloss: 0.500892\n",
      "[472]\ttraining's binary_logloss: 0.500693\n",
      "[473]\ttraining's binary_logloss: 0.500617\n",
      "[474]\ttraining's binary_logloss: 0.500385\n",
      "[475]\ttraining's binary_logloss: 0.50024\n",
      "[476]\ttraining's binary_logloss: 0.500095\n",
      "[477]\ttraining's binary_logloss: 0.499889\n",
      "[478]\ttraining's binary_logloss: 0.499696\n",
      "[479]\ttraining's binary_logloss: 0.499511\n",
      "[480]\ttraining's binary_logloss: 0.49926\n",
      "[481]\ttraining's binary_logloss: 0.499109\n",
      "[482]\ttraining's binary_logloss: 0.498964\n",
      "[483]\ttraining's binary_logloss: 0.498754\n",
      "[484]\ttraining's binary_logloss: 0.498604\n",
      "[485]\ttraining's binary_logloss: 0.498537\n",
      "[486]\ttraining's binary_logloss: 0.498348\n",
      "[487]\ttraining's binary_logloss: 0.498155\n",
      "[488]\ttraining's binary_logloss: 0.497941\n",
      "[489]\ttraining's binary_logloss: 0.497711\n",
      "[490]\ttraining's binary_logloss: 0.497441\n",
      "[491]\ttraining's binary_logloss: 0.497277\n",
      "[492]\ttraining's binary_logloss: 0.497171\n",
      "[493]\ttraining's binary_logloss: 0.497104\n",
      "[494]\ttraining's binary_logloss: 0.496872\n",
      "[495]\ttraining's binary_logloss: 0.496716\n",
      "[496]\ttraining's binary_logloss: 0.496637\n",
      "[497]\ttraining's binary_logloss: 0.49648\n",
      "[498]\ttraining's binary_logloss: 0.496414\n",
      "[499]\ttraining's binary_logloss: 0.496182\n",
      "[500]\ttraining's binary_logloss: 0.496\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as LightGBM\n",
    "\n",
    "feature_set = []\n",
    "feature_set = make_feature_set(x_train)\n",
    "model = []\n",
    "\n",
    "## train\n",
    "for x in feature_set :\n",
    "    lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=50,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=500,\n",
    "                               max_depth = 30,\n",
    "                               class_weight={True: 10, False: 1}\n",
    "                              ) \n",
    "    evals = [(x, y_train_bool)]\n",
    "    lgbm.fit(x, y_train_bool, eval_metric='logloss', eval_set=evals)\n",
    "    model.append(lgbm)\n",
    "\n",
    "## prediction\n",
    "def predict_ensemble_model(x_) :\n",
    "    feature_set = make_feature_set(x_)\n",
    "    y_pred = []\n",
    "    i = 0\n",
    "    for x in feature_set :\n",
    "        pred = model[i].predict(x)\n",
    "        y_pred.append(pred)\n",
    "        i = i+1\n",
    "\n",
    "    vote = lambda t : 1 if t > 2 else 0\n",
    "    y_pred_sum = y_pred[0] & (y_pred[1] | y_pred[2] | y_pred[3] | y_pred[4])\n",
    "    return y_pred_sum\n",
    "#y_pred = np.array([vote(xi) for xi in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.99      0.44      0.61     63391\n",
      "        risk       0.26      0.98      0.41     12724\n",
      "\n",
      "    accuracy                           0.53     76115\n",
      "   macro avg       0.63      0.71      0.51     76115\n",
      "weighted avg       0.87      0.53      0.57     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = predict_ensemble_model(x_train)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_train_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.40      0.55     21052\n",
      "        risk       0.22      0.81      0.34      4344\n",
      "\n",
      "    accuracy                           0.47     25396\n",
      "   macro avg       0.56      0.61      0.45     25396\n",
      "weighted avg       0.79      0.47      0.52     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = predict_ensemble_model(x_valid)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dc4a390a9b2827902a4747c64ce7fc589728c7a1b50ebf341cedb4beb3b25a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
