{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = ['?', '??', 'N/A', 'NA', 'nan', 'NaN', '-nan', '-NaN', 'null', '-']\n",
    "x_train = pd.read_csv('./data/track1/features/x_train_normal.csv', na_values = null_values)\n",
    "x_valid = pd.read_csv('./data/track1/features/x_valid_normal.csv', na_values = null_values)\n",
    "x_test = pd.read_csv('./data/track1/features/x_test_normal.csv', na_values = null_values)\n",
    "y_train = pd.read_csv('./data/track1/features/y_train_normal.csv', na_values = null_values)\n",
    "y_valid = pd.read_csv('./data/track1/features/y_valid_normal.csv', na_values = null_values)\n",
    "y_test = pd.read_csv('./data/track1/features/y_test_normal.csv', na_values = null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_valid = x_valid.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_test = x_test.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "y_train_bool = y_train['Y'] <-2.0\n",
    "y_valid_bool = y_valid['Y'] <-2.0\n",
    "y_test_bool = y_test['Y'] <-2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tree Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_feature_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "rfe_features_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "x_train_features = x_train[sfs_feature_list]\n",
    "x_valid_features = x_valid[sfs_feature_list]\n",
    "x_test_features = x_test[sfs_feature_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[1]\ttraining's binary_logloss: 0.628988\n",
      "[2]\ttraining's binary_logloss: 0.623421\n",
      "[3]\ttraining's binary_logloss: 0.618595\n",
      "[4]\ttraining's binary_logloss: 0.614736\n",
      "[5]\ttraining's binary_logloss: 0.611145\n",
      "[6]\ttraining's binary_logloss: 0.608083\n",
      "[7]\ttraining's binary_logloss: 0.605599\n",
      "[8]\ttraining's binary_logloss: 0.60318\n",
      "[9]\ttraining's binary_logloss: 0.601156\n",
      "[10]\ttraining's binary_logloss: 0.599245\n",
      "[11]\ttraining's binary_logloss: 0.597674\n",
      "[12]\ttraining's binary_logloss: 0.596315\n",
      "[13]\ttraining's binary_logloss: 0.594907\n",
      "[14]\ttraining's binary_logloss: 0.593697\n",
      "[15]\ttraining's binary_logloss: 0.592459\n",
      "[16]\ttraining's binary_logloss: 0.591341\n",
      "[17]\ttraining's binary_logloss: 0.590368\n",
      "[18]\ttraining's binary_logloss: 0.589415\n",
      "[19]\ttraining's binary_logloss: 0.588542\n",
      "[20]\ttraining's binary_logloss: 0.587626\n",
      "[21]\ttraining's binary_logloss: 0.586785\n",
      "[22]\ttraining's binary_logloss: 0.585927\n",
      "[23]\ttraining's binary_logloss: 0.585217\n",
      "[24]\ttraining's binary_logloss: 0.584518\n",
      "[25]\ttraining's binary_logloss: 0.583815\n",
      "[26]\ttraining's binary_logloss: 0.583215\n",
      "[27]\ttraining's binary_logloss: 0.582612\n",
      "[28]\ttraining's binary_logloss: 0.582014\n",
      "[29]\ttraining's binary_logloss: 0.581421\n",
      "[30]\ttraining's binary_logloss: 0.580827\n",
      "[31]\ttraining's binary_logloss: 0.580219\n",
      "[32]\ttraining's binary_logloss: 0.579627\n",
      "[33]\ttraining's binary_logloss: 0.57907\n",
      "[34]\ttraining's binary_logloss: 0.578461\n",
      "[35]\ttraining's binary_logloss: 0.57794\n",
      "[36]\ttraining's binary_logloss: 0.577413\n",
      "[37]\ttraining's binary_logloss: 0.576984\n",
      "[38]\ttraining's binary_logloss: 0.576446\n",
      "[39]\ttraining's binary_logloss: 0.576013\n",
      "[40]\ttraining's binary_logloss: 0.575554\n",
      "[41]\ttraining's binary_logloss: 0.575091\n",
      "[42]\ttraining's binary_logloss: 0.574702\n",
      "[43]\ttraining's binary_logloss: 0.574224\n",
      "[44]\ttraining's binary_logloss: 0.573776\n",
      "[45]\ttraining's binary_logloss: 0.573357\n",
      "[46]\ttraining's binary_logloss: 0.572885\n",
      "[47]\ttraining's binary_logloss: 0.572478\n",
      "[48]\ttraining's binary_logloss: 0.572078\n",
      "[49]\ttraining's binary_logloss: 0.571684\n",
      "[50]\ttraining's binary_logloss: 0.571342\n",
      "[51]\ttraining's binary_logloss: 0.570987\n",
      "[52]\ttraining's binary_logloss: 0.570604\n",
      "[53]\ttraining's binary_logloss: 0.570223\n",
      "[54]\ttraining's binary_logloss: 0.569866\n",
      "[55]\ttraining's binary_logloss: 0.569547\n",
      "[56]\ttraining's binary_logloss: 0.569147\n",
      "[57]\ttraining's binary_logloss: 0.568802\n",
      "[58]\ttraining's binary_logloss: 0.568329\n",
      "[59]\ttraining's binary_logloss: 0.567903\n",
      "[60]\ttraining's binary_logloss: 0.567521\n",
      "[61]\ttraining's binary_logloss: 0.567223\n",
      "[62]\ttraining's binary_logloss: 0.566953\n",
      "[63]\ttraining's binary_logloss: 0.566651\n",
      "[64]\ttraining's binary_logloss: 0.566302\n",
      "[65]\ttraining's binary_logloss: 0.565949\n",
      "[66]\ttraining's binary_logloss: 0.565623\n",
      "[67]\ttraining's binary_logloss: 0.565279\n",
      "[68]\ttraining's binary_logloss: 0.564954\n",
      "[69]\ttraining's binary_logloss: 0.564539\n",
      "[70]\ttraining's binary_logloss: 0.564261\n",
      "[71]\ttraining's binary_logloss: 0.563952\n",
      "[72]\ttraining's binary_logloss: 0.563678\n",
      "[73]\ttraining's binary_logloss: 0.563482\n",
      "[74]\ttraining's binary_logloss: 0.563098\n",
      "[75]\ttraining's binary_logloss: 0.562868\n",
      "[76]\ttraining's binary_logloss: 0.562527\n",
      "[77]\ttraining's binary_logloss: 0.562208\n",
      "[78]\ttraining's binary_logloss: 0.561945\n",
      "[79]\ttraining's binary_logloss: 0.561602\n",
      "[80]\ttraining's binary_logloss: 0.561302\n",
      "[81]\ttraining's binary_logloss: 0.561\n",
      "[82]\ttraining's binary_logloss: 0.560753\n",
      "[83]\ttraining's binary_logloss: 0.560448\n",
      "[84]\ttraining's binary_logloss: 0.560219\n",
      "[85]\ttraining's binary_logloss: 0.559976\n",
      "[86]\ttraining's binary_logloss: 0.559661\n",
      "[87]\ttraining's binary_logloss: 0.559427\n",
      "[88]\ttraining's binary_logloss: 0.559171\n",
      "[89]\ttraining's binary_logloss: 0.558935\n",
      "[90]\ttraining's binary_logloss: 0.558638\n",
      "[91]\ttraining's binary_logloss: 0.558326\n",
      "[92]\ttraining's binary_logloss: 0.557987\n",
      "[93]\ttraining's binary_logloss: 0.557749\n",
      "[94]\ttraining's binary_logloss: 0.557557\n",
      "[95]\ttraining's binary_logloss: 0.557268\n",
      "[96]\ttraining's binary_logloss: 0.556977\n",
      "[97]\ttraining's binary_logloss: 0.556674\n",
      "[98]\ttraining's binary_logloss: 0.556364\n",
      "[99]\ttraining's binary_logloss: 0.556105\n",
      "[100]\ttraining's binary_logloss: 0.555852\n",
      "[101]\ttraining's binary_logloss: 0.555561\n",
      "[102]\ttraining's binary_logloss: 0.555314\n",
      "[103]\ttraining's binary_logloss: 0.554996\n",
      "[104]\ttraining's binary_logloss: 0.554814\n",
      "[105]\ttraining's binary_logloss: 0.554589\n",
      "[106]\ttraining's binary_logloss: 0.554296\n",
      "[107]\ttraining's binary_logloss: 0.554059\n",
      "[108]\ttraining's binary_logloss: 0.553875\n",
      "[109]\ttraining's binary_logloss: 0.553637\n",
      "[110]\ttraining's binary_logloss: 0.553318\n",
      "[111]\ttraining's binary_logloss: 0.553091\n",
      "[112]\ttraining's binary_logloss: 0.552917\n",
      "[113]\ttraining's binary_logloss: 0.552749\n",
      "[114]\ttraining's binary_logloss: 0.552485\n",
      "[115]\ttraining's binary_logloss: 0.552213\n",
      "[116]\ttraining's binary_logloss: 0.55188\n",
      "[117]\ttraining's binary_logloss: 0.551536\n",
      "[118]\ttraining's binary_logloss: 0.551318\n",
      "[119]\ttraining's binary_logloss: 0.551114\n",
      "[120]\ttraining's binary_logloss: 0.550837\n",
      "[121]\ttraining's binary_logloss: 0.550632\n",
      "[122]\ttraining's binary_logloss: 0.550362\n",
      "[123]\ttraining's binary_logloss: 0.550071\n",
      "[124]\ttraining's binary_logloss: 0.549858\n",
      "[125]\ttraining's binary_logloss: 0.549697\n",
      "[126]\ttraining's binary_logloss: 0.54944\n",
      "[127]\ttraining's binary_logloss: 0.549271\n",
      "[128]\ttraining's binary_logloss: 0.548994\n",
      "[129]\ttraining's binary_logloss: 0.548805\n",
      "[130]\ttraining's binary_logloss: 0.548558\n",
      "[131]\ttraining's binary_logloss: 0.548351\n",
      "[132]\ttraining's binary_logloss: 0.548144\n",
      "[133]\ttraining's binary_logloss: 0.547895\n",
      "[134]\ttraining's binary_logloss: 0.547648\n",
      "[135]\ttraining's binary_logloss: 0.547381\n",
      "[136]\ttraining's binary_logloss: 0.547155\n",
      "[137]\ttraining's binary_logloss: 0.546884\n",
      "[138]\ttraining's binary_logloss: 0.546658\n",
      "[139]\ttraining's binary_logloss: 0.546403\n",
      "[140]\ttraining's binary_logloss: 0.546274\n",
      "[141]\ttraining's binary_logloss: 0.546016\n",
      "[142]\ttraining's binary_logloss: 0.545819\n",
      "[143]\ttraining's binary_logloss: 0.545659\n",
      "[144]\ttraining's binary_logloss: 0.545511\n",
      "[145]\ttraining's binary_logloss: 0.545351\n",
      "[146]\ttraining's binary_logloss: 0.545155\n",
      "[147]\ttraining's binary_logloss: 0.544932\n",
      "[148]\ttraining's binary_logloss: 0.544764\n",
      "[149]\ttraining's binary_logloss: 0.544435\n",
      "[150]\ttraining's binary_logloss: 0.544229\n",
      "[151]\ttraining's binary_logloss: 0.543983\n",
      "[152]\ttraining's binary_logloss: 0.543728\n",
      "[153]\ttraining's binary_logloss: 0.543585\n",
      "[154]\ttraining's binary_logloss: 0.543335\n",
      "[155]\ttraining's binary_logloss: 0.543105\n",
      "[156]\ttraining's binary_logloss: 0.542911\n",
      "[157]\ttraining's binary_logloss: 0.542675\n",
      "[158]\ttraining's binary_logloss: 0.542439\n",
      "[159]\ttraining's binary_logloss: 0.5423\n",
      "[160]\ttraining's binary_logloss: 0.542053\n",
      "[161]\ttraining's binary_logloss: 0.541883\n",
      "[162]\ttraining's binary_logloss: 0.541685\n",
      "[163]\ttraining's binary_logloss: 0.541495\n",
      "[164]\ttraining's binary_logloss: 0.541345\n",
      "[165]\ttraining's binary_logloss: 0.541225\n",
      "[166]\ttraining's binary_logloss: 0.541012\n",
      "[167]\ttraining's binary_logloss: 0.540823\n",
      "[168]\ttraining's binary_logloss: 0.540631\n",
      "[169]\ttraining's binary_logloss: 0.540449\n",
      "[170]\ttraining's binary_logloss: 0.540231\n",
      "[171]\ttraining's binary_logloss: 0.539952\n",
      "[172]\ttraining's binary_logloss: 0.539802\n",
      "[173]\ttraining's binary_logloss: 0.539524\n",
      "[174]\ttraining's binary_logloss: 0.53938\n",
      "[175]\ttraining's binary_logloss: 0.53917\n",
      "[176]\ttraining's binary_logloss: 0.538953\n",
      "[177]\ttraining's binary_logloss: 0.538717\n",
      "[178]\ttraining's binary_logloss: 0.538486\n",
      "[179]\ttraining's binary_logloss: 0.538317\n",
      "[180]\ttraining's binary_logloss: 0.538166\n",
      "[181]\ttraining's binary_logloss: 0.537896\n",
      "[182]\ttraining's binary_logloss: 0.537605\n",
      "[183]\ttraining's binary_logloss: 0.537406\n",
      "[184]\ttraining's binary_logloss: 0.537169\n",
      "[185]\ttraining's binary_logloss: 0.536928\n",
      "[186]\ttraining's binary_logloss: 0.536683\n",
      "[187]\ttraining's binary_logloss: 0.536511\n",
      "[188]\ttraining's binary_logloss: 0.536364\n",
      "[189]\ttraining's binary_logloss: 0.536229\n",
      "[190]\ttraining's binary_logloss: 0.535961\n",
      "[191]\ttraining's binary_logloss: 0.535815\n",
      "[192]\ttraining's binary_logloss: 0.535647\n",
      "[193]\ttraining's binary_logloss: 0.535393\n",
      "[194]\ttraining's binary_logloss: 0.535139\n",
      "[195]\ttraining's binary_logloss: 0.534887\n",
      "[196]\ttraining's binary_logloss: 0.534687\n",
      "[197]\ttraining's binary_logloss: 0.534507\n",
      "[198]\ttraining's binary_logloss: 0.534299\n",
      "[199]\ttraining's binary_logloss: 0.534101\n",
      "[200]\ttraining's binary_logloss: 0.533881\n",
      "[201]\ttraining's binary_logloss: 0.533707\n",
      "[202]\ttraining's binary_logloss: 0.533481\n",
      "[203]\ttraining's binary_logloss: 0.533228\n",
      "[204]\ttraining's binary_logloss: 0.533083\n",
      "[205]\ttraining's binary_logloss: 0.532924\n",
      "[206]\ttraining's binary_logloss: 0.532758\n",
      "[207]\ttraining's binary_logloss: 0.532574\n",
      "[208]\ttraining's binary_logloss: 0.532385\n",
      "[209]\ttraining's binary_logloss: 0.532069\n",
      "[210]\ttraining's binary_logloss: 0.53196\n",
      "[211]\ttraining's binary_logloss: 0.531817\n",
      "[212]\ttraining's binary_logloss: 0.531615\n",
      "[213]\ttraining's binary_logloss: 0.531348\n",
      "[214]\ttraining's binary_logloss: 0.531182\n",
      "[215]\ttraining's binary_logloss: 0.531006\n",
      "[216]\ttraining's binary_logloss: 0.53084\n",
      "[217]\ttraining's binary_logloss: 0.530633\n",
      "[218]\ttraining's binary_logloss: 0.530433\n",
      "[219]\ttraining's binary_logloss: 0.530198\n",
      "[220]\ttraining's binary_logloss: 0.53008\n",
      "[221]\ttraining's binary_logloss: 0.529862\n",
      "[222]\ttraining's binary_logloss: 0.529695\n",
      "[223]\ttraining's binary_logloss: 0.529583\n",
      "[224]\ttraining's binary_logloss: 0.529468\n",
      "[225]\ttraining's binary_logloss: 0.529249\n",
      "[226]\ttraining's binary_logloss: 0.529043\n",
      "[227]\ttraining's binary_logloss: 0.528809\n",
      "[228]\ttraining's binary_logloss: 0.528607\n",
      "[229]\ttraining's binary_logloss: 0.528413\n",
      "[230]\ttraining's binary_logloss: 0.528164\n",
      "[231]\ttraining's binary_logloss: 0.527949\n",
      "[232]\ttraining's binary_logloss: 0.527773\n",
      "[233]\ttraining's binary_logloss: 0.52744\n",
      "[234]\ttraining's binary_logloss: 0.527209\n",
      "[235]\ttraining's binary_logloss: 0.526984\n",
      "[236]\ttraining's binary_logloss: 0.526783\n",
      "[237]\ttraining's binary_logloss: 0.526549\n",
      "[238]\ttraining's binary_logloss: 0.526364\n",
      "[239]\ttraining's binary_logloss: 0.526233\n",
      "[240]\ttraining's binary_logloss: 0.526064\n",
      "[241]\ttraining's binary_logloss: 0.525812\n",
      "[242]\ttraining's binary_logloss: 0.525578\n",
      "[243]\ttraining's binary_logloss: 0.525466\n",
      "[244]\ttraining's binary_logloss: 0.525309\n",
      "[245]\ttraining's binary_logloss: 0.525075\n",
      "[246]\ttraining's binary_logloss: 0.524932\n",
      "[247]\ttraining's binary_logloss: 0.524748\n",
      "[248]\ttraining's binary_logloss: 0.524624\n",
      "[249]\ttraining's binary_logloss: 0.524385\n",
      "[250]\ttraining's binary_logloss: 0.524211\n",
      "[251]\ttraining's binary_logloss: 0.523941\n",
      "[252]\ttraining's binary_logloss: 0.523787\n",
      "[253]\ttraining's binary_logloss: 0.523612\n",
      "[254]\ttraining's binary_logloss: 0.523421\n",
      "[255]\ttraining's binary_logloss: 0.523211\n",
      "[256]\ttraining's binary_logloss: 0.523096\n",
      "[257]\ttraining's binary_logloss: 0.522911\n",
      "[258]\ttraining's binary_logloss: 0.522737\n",
      "[259]\ttraining's binary_logloss: 0.522592\n",
      "[260]\ttraining's binary_logloss: 0.522367\n",
      "[261]\ttraining's binary_logloss: 0.522237\n",
      "[262]\ttraining's binary_logloss: 0.522139\n",
      "[263]\ttraining's binary_logloss: 0.521994\n",
      "[264]\ttraining's binary_logloss: 0.521802\n",
      "[265]\ttraining's binary_logloss: 0.521594\n",
      "[266]\ttraining's binary_logloss: 0.521445\n",
      "[267]\ttraining's binary_logloss: 0.521302\n",
      "[268]\ttraining's binary_logloss: 0.521156\n",
      "[269]\ttraining's binary_logloss: 0.520924\n",
      "[270]\ttraining's binary_logloss: 0.520802\n",
      "[271]\ttraining's binary_logloss: 0.520695\n",
      "[272]\ttraining's binary_logloss: 0.52045\n",
      "[273]\ttraining's binary_logloss: 0.520258\n",
      "[274]\ttraining's binary_logloss: 0.520119\n",
      "[275]\ttraining's binary_logloss: 0.519904\n",
      "[276]\ttraining's binary_logloss: 0.519718\n",
      "[277]\ttraining's binary_logloss: 0.519517\n",
      "[278]\ttraining's binary_logloss: 0.519358\n",
      "[279]\ttraining's binary_logloss: 0.519137\n",
      "[280]\ttraining's binary_logloss: 0.518928\n",
      "[281]\ttraining's binary_logloss: 0.518758\n",
      "[282]\ttraining's binary_logloss: 0.518617\n",
      "[283]\ttraining's binary_logloss: 0.518432\n",
      "[284]\ttraining's binary_logloss: 0.518235\n",
      "[285]\ttraining's binary_logloss: 0.518076\n",
      "[286]\ttraining's binary_logloss: 0.517902\n",
      "[287]\ttraining's binary_logloss: 0.517765\n",
      "[288]\ttraining's binary_logloss: 0.517566\n",
      "[289]\ttraining's binary_logloss: 0.517363\n",
      "[290]\ttraining's binary_logloss: 0.517185\n",
      "[291]\ttraining's binary_logloss: 0.517063\n",
      "[292]\ttraining's binary_logloss: 0.516949\n",
      "[293]\ttraining's binary_logloss: 0.516724\n",
      "[294]\ttraining's binary_logloss: 0.516606\n",
      "[295]\ttraining's binary_logloss: 0.516478\n",
      "[296]\ttraining's binary_logloss: 0.516358\n",
      "[297]\ttraining's binary_logloss: 0.516134\n",
      "[298]\ttraining's binary_logloss: 0.515972\n",
      "[299]\ttraining's binary_logloss: 0.515855\n",
      "[300]\ttraining's binary_logloss: 0.515758\n",
      "[301]\ttraining's binary_logloss: 0.515549\n",
      "[302]\ttraining's binary_logloss: 0.515324\n",
      "[303]\ttraining's binary_logloss: 0.515135\n",
      "[304]\ttraining's binary_logloss: 0.515002\n",
      "[305]\ttraining's binary_logloss: 0.514869\n",
      "[306]\ttraining's binary_logloss: 0.514722\n",
      "[307]\ttraining's binary_logloss: 0.514604\n",
      "[308]\ttraining's binary_logloss: 0.514416\n",
      "[309]\ttraining's binary_logloss: 0.514274\n",
      "[310]\ttraining's binary_logloss: 0.514173\n",
      "[311]\ttraining's binary_logloss: 0.514027\n",
      "[312]\ttraining's binary_logloss: 0.513902\n",
      "[313]\ttraining's binary_logloss: 0.513697\n",
      "[314]\ttraining's binary_logloss: 0.513577\n",
      "[315]\ttraining's binary_logloss: 0.513477\n",
      "[316]\ttraining's binary_logloss: 0.513385\n",
      "[317]\ttraining's binary_logloss: 0.513213\n",
      "[318]\ttraining's binary_logloss: 0.513084\n",
      "[319]\ttraining's binary_logloss: 0.512925\n",
      "[320]\ttraining's binary_logloss: 0.512728\n",
      "[321]\ttraining's binary_logloss: 0.512534\n",
      "[322]\ttraining's binary_logloss: 0.512377\n",
      "[323]\ttraining's binary_logloss: 0.512206\n",
      "[324]\ttraining's binary_logloss: 0.511964\n",
      "[325]\ttraining's binary_logloss: 0.511767\n",
      "[326]\ttraining's binary_logloss: 0.511654\n",
      "[327]\ttraining's binary_logloss: 0.511566\n",
      "[328]\ttraining's binary_logloss: 0.511438\n",
      "[329]\ttraining's binary_logloss: 0.511314\n",
      "[330]\ttraining's binary_logloss: 0.511167\n",
      "[331]\ttraining's binary_logloss: 0.511005\n",
      "[332]\ttraining's binary_logloss: 0.510866\n",
      "[333]\ttraining's binary_logloss: 0.510709\n",
      "[334]\ttraining's binary_logloss: 0.510605\n",
      "[335]\ttraining's binary_logloss: 0.510477\n",
      "[336]\ttraining's binary_logloss: 0.510289\n",
      "[337]\ttraining's binary_logloss: 0.510155\n",
      "[338]\ttraining's binary_logloss: 0.509992\n",
      "[339]\ttraining's binary_logloss: 0.509879\n",
      "[340]\ttraining's binary_logloss: 0.509773\n",
      "[341]\ttraining's binary_logloss: 0.509652\n",
      "[342]\ttraining's binary_logloss: 0.509516\n",
      "[343]\ttraining's binary_logloss: 0.509373\n",
      "[344]\ttraining's binary_logloss: 0.509185\n",
      "[345]\ttraining's binary_logloss: 0.509024\n",
      "[346]\ttraining's binary_logloss: 0.508872\n",
      "[347]\ttraining's binary_logloss: 0.508713\n",
      "[348]\ttraining's binary_logloss: 0.508581\n",
      "[349]\ttraining's binary_logloss: 0.508506\n",
      "[350]\ttraining's binary_logloss: 0.508319\n",
      "[351]\ttraining's binary_logloss: 0.508188\n",
      "[352]\ttraining's binary_logloss: 0.507979\n",
      "[353]\ttraining's binary_logloss: 0.507885\n",
      "[354]\ttraining's binary_logloss: 0.507774\n",
      "[355]\ttraining's binary_logloss: 0.507569\n",
      "[356]\ttraining's binary_logloss: 0.507382\n",
      "[357]\ttraining's binary_logloss: 0.507216\n",
      "[358]\ttraining's binary_logloss: 0.507041\n",
      "[359]\ttraining's binary_logloss: 0.506883\n",
      "[360]\ttraining's binary_logloss: 0.506773\n",
      "[361]\ttraining's binary_logloss: 0.506625\n",
      "[362]\ttraining's binary_logloss: 0.506412\n",
      "[363]\ttraining's binary_logloss: 0.506283\n",
      "[364]\ttraining's binary_logloss: 0.506137\n",
      "[365]\ttraining's binary_logloss: 0.506006\n",
      "[366]\ttraining's binary_logloss: 0.505889\n",
      "[367]\ttraining's binary_logloss: 0.505767\n",
      "[368]\ttraining's binary_logloss: 0.505686\n",
      "[369]\ttraining's binary_logloss: 0.505584\n",
      "[370]\ttraining's binary_logloss: 0.505375\n",
      "[371]\ttraining's binary_logloss: 0.505292\n",
      "[372]\ttraining's binary_logloss: 0.505186\n",
      "[373]\ttraining's binary_logloss: 0.504896\n",
      "[374]\ttraining's binary_logloss: 0.504785\n",
      "[375]\ttraining's binary_logloss: 0.504651\n",
      "[376]\ttraining's binary_logloss: 0.504527\n",
      "[377]\ttraining's binary_logloss: 0.504437\n",
      "[378]\ttraining's binary_logloss: 0.504246\n",
      "[379]\ttraining's binary_logloss: 0.504133\n",
      "[380]\ttraining's binary_logloss: 0.503959\n",
      "[381]\ttraining's binary_logloss: 0.503819\n",
      "[382]\ttraining's binary_logloss: 0.503661\n",
      "[383]\ttraining's binary_logloss: 0.503573\n",
      "[384]\ttraining's binary_logloss: 0.503427\n",
      "[385]\ttraining's binary_logloss: 0.503211\n",
      "[386]\ttraining's binary_logloss: 0.503052\n",
      "[387]\ttraining's binary_logloss: 0.502924\n",
      "[388]\ttraining's binary_logloss: 0.502776\n",
      "[389]\ttraining's binary_logloss: 0.502645\n",
      "[390]\ttraining's binary_logloss: 0.502505\n",
      "[391]\ttraining's binary_logloss: 0.502316\n",
      "[392]\ttraining's binary_logloss: 0.502187\n",
      "[393]\ttraining's binary_logloss: 0.502058\n",
      "[394]\ttraining's binary_logloss: 0.501894\n",
      "[395]\ttraining's binary_logloss: 0.501759\n",
      "[396]\ttraining's binary_logloss: 0.501558\n",
      "[397]\ttraining's binary_logloss: 0.501413\n",
      "[398]\ttraining's binary_logloss: 0.501316\n",
      "[399]\ttraining's binary_logloss: 0.501176\n",
      "[400]\ttraining's binary_logloss: 0.501055\n",
      "[401]\ttraining's binary_logloss: 0.500928\n",
      "[402]\ttraining's binary_logloss: 0.500761\n",
      "[403]\ttraining's binary_logloss: 0.500668\n",
      "[404]\ttraining's binary_logloss: 0.500543\n",
      "[405]\ttraining's binary_logloss: 0.500402\n",
      "[406]\ttraining's binary_logloss: 0.500267\n",
      "[407]\ttraining's binary_logloss: 0.500161\n",
      "[408]\ttraining's binary_logloss: 0.500062\n",
      "[409]\ttraining's binary_logloss: 0.499921\n",
      "[410]\ttraining's binary_logloss: 0.499807\n",
      "[411]\ttraining's binary_logloss: 0.499607\n",
      "[412]\ttraining's binary_logloss: 0.499425\n",
      "[413]\ttraining's binary_logloss: 0.49924\n",
      "[414]\ttraining's binary_logloss: 0.499064\n",
      "[415]\ttraining's binary_logloss: 0.498891\n",
      "[416]\ttraining's binary_logloss: 0.498716\n",
      "[417]\ttraining's binary_logloss: 0.498522\n",
      "[418]\ttraining's binary_logloss: 0.498374\n",
      "[419]\ttraining's binary_logloss: 0.498228\n",
      "[420]\ttraining's binary_logloss: 0.498068\n",
      "[421]\ttraining's binary_logloss: 0.497929\n",
      "[422]\ttraining's binary_logloss: 0.497798\n",
      "[423]\ttraining's binary_logloss: 0.497711\n",
      "[424]\ttraining's binary_logloss: 0.49759\n",
      "[425]\ttraining's binary_logloss: 0.497517\n",
      "[426]\ttraining's binary_logloss: 0.497441\n",
      "[427]\ttraining's binary_logloss: 0.497311\n",
      "[428]\ttraining's binary_logloss: 0.497187\n",
      "[429]\ttraining's binary_logloss: 0.497078\n",
      "[430]\ttraining's binary_logloss: 0.496973\n",
      "[431]\ttraining's binary_logloss: 0.496839\n",
      "[432]\ttraining's binary_logloss: 0.496677\n",
      "[433]\ttraining's binary_logloss: 0.496414\n",
      "[434]\ttraining's binary_logloss: 0.496302\n",
      "[435]\ttraining's binary_logloss: 0.4961\n",
      "[436]\ttraining's binary_logloss: 0.495954\n",
      "[437]\ttraining's binary_logloss: 0.495792\n",
      "[438]\ttraining's binary_logloss: 0.495639\n",
      "[439]\ttraining's binary_logloss: 0.495489\n",
      "[440]\ttraining's binary_logloss: 0.495268\n",
      "[441]\ttraining's binary_logloss: 0.495081\n",
      "[442]\ttraining's binary_logloss: 0.494945\n",
      "[443]\ttraining's binary_logloss: 0.494788\n",
      "[444]\ttraining's binary_logloss: 0.494641\n",
      "[445]\ttraining's binary_logloss: 0.494469\n",
      "[446]\ttraining's binary_logloss: 0.494293\n",
      "[447]\ttraining's binary_logloss: 0.494149\n",
      "[448]\ttraining's binary_logloss: 0.49404\n",
      "[449]\ttraining's binary_logloss: 0.493929\n",
      "[450]\ttraining's binary_logloss: 0.493802\n",
      "[451]\ttraining's binary_logloss: 0.493594\n",
      "[452]\ttraining's binary_logloss: 0.493461\n",
      "[453]\ttraining's binary_logloss: 0.493257\n",
      "[454]\ttraining's binary_logloss: 0.493138\n",
      "[455]\ttraining's binary_logloss: 0.4929\n",
      "[456]\ttraining's binary_logloss: 0.492708\n",
      "[457]\ttraining's binary_logloss: 0.492523\n",
      "[458]\ttraining's binary_logloss: 0.492346\n",
      "[459]\ttraining's binary_logloss: 0.492225\n",
      "[460]\ttraining's binary_logloss: 0.492074\n",
      "[461]\ttraining's binary_logloss: 0.491995\n",
      "[462]\ttraining's binary_logloss: 0.491841\n",
      "[463]\ttraining's binary_logloss: 0.491669\n",
      "[464]\ttraining's binary_logloss: 0.491585\n",
      "[465]\ttraining's binary_logloss: 0.49137\n",
      "[466]\ttraining's binary_logloss: 0.491247\n",
      "[467]\ttraining's binary_logloss: 0.491023\n",
      "[468]\ttraining's binary_logloss: 0.490787\n",
      "[469]\ttraining's binary_logloss: 0.490642\n",
      "[470]\ttraining's binary_logloss: 0.49047\n",
      "[471]\ttraining's binary_logloss: 0.49032\n",
      "[472]\ttraining's binary_logloss: 0.49025\n",
      "[473]\ttraining's binary_logloss: 0.490111\n",
      "[474]\ttraining's binary_logloss: 0.490028\n",
      "[475]\ttraining's binary_logloss: 0.489879\n",
      "[476]\ttraining's binary_logloss: 0.489743\n",
      "[477]\ttraining's binary_logloss: 0.489652\n",
      "[478]\ttraining's binary_logloss: 0.489449\n",
      "[479]\ttraining's binary_logloss: 0.489248\n",
      "[480]\ttraining's binary_logloss: 0.489098\n",
      "[481]\ttraining's binary_logloss: 0.488954\n",
      "[482]\ttraining's binary_logloss: 0.48875\n",
      "[483]\ttraining's binary_logloss: 0.488541\n",
      "[484]\ttraining's binary_logloss: 0.488423\n",
      "[485]\ttraining's binary_logloss: 0.488296\n",
      "[486]\ttraining's binary_logloss: 0.488137\n",
      "[487]\ttraining's binary_logloss: 0.487985\n",
      "[488]\ttraining's binary_logloss: 0.487906\n",
      "[489]\ttraining's binary_logloss: 0.487767\n",
      "[490]\ttraining's binary_logloss: 0.487648\n",
      "[491]\ttraining's binary_logloss: 0.487574\n",
      "[492]\ttraining's binary_logloss: 0.487436\n",
      "[493]\ttraining's binary_logloss: 0.487338\n",
      "[494]\ttraining's binary_logloss: 0.48721\n",
      "[495]\ttraining's binary_logloss: 0.487064\n",
      "[496]\ttraining's binary_logloss: 0.486942\n",
      "[497]\ttraining's binary_logloss: 0.486792\n",
      "[498]\ttraining's binary_logloss: 0.486698\n",
      "[499]\ttraining's binary_logloss: 0.48659\n",
      "[500]\ttraining's binary_logloss: 0.486445\n",
      "[501]\ttraining's binary_logloss: 0.486385\n",
      "[502]\ttraining's binary_logloss: 0.486237\n",
      "[503]\ttraining's binary_logloss: 0.486115\n",
      "[504]\ttraining's binary_logloss: 0.485927\n",
      "[505]\ttraining's binary_logloss: 0.485731\n",
      "[506]\ttraining's binary_logloss: 0.485578\n",
      "[507]\ttraining's binary_logloss: 0.485432\n",
      "[508]\ttraining's binary_logloss: 0.485369\n",
      "[509]\ttraining's binary_logloss: 0.485264\n",
      "[510]\ttraining's binary_logloss: 0.485204\n",
      "[511]\ttraining's binary_logloss: 0.485053\n",
      "[512]\ttraining's binary_logloss: 0.484837\n",
      "[513]\ttraining's binary_logloss: 0.484697\n",
      "[514]\ttraining's binary_logloss: 0.484614\n",
      "[515]\ttraining's binary_logloss: 0.484471\n",
      "[516]\ttraining's binary_logloss: 0.484355\n",
      "[517]\ttraining's binary_logloss: 0.484278\n",
      "[518]\ttraining's binary_logloss: 0.484161\n",
      "[519]\ttraining's binary_logloss: 0.484017\n",
      "[520]\ttraining's binary_logloss: 0.483871\n",
      "[521]\ttraining's binary_logloss: 0.483718\n",
      "[522]\ttraining's binary_logloss: 0.483561\n",
      "[523]\ttraining's binary_logloss: 0.483437\n",
      "[524]\ttraining's binary_logloss: 0.48327\n",
      "[525]\ttraining's binary_logloss: 0.483139\n",
      "[526]\ttraining's binary_logloss: 0.483041\n",
      "[527]\ttraining's binary_logloss: 0.482976\n",
      "[528]\ttraining's binary_logloss: 0.482896\n",
      "[529]\ttraining's binary_logloss: 0.482774\n",
      "[530]\ttraining's binary_logloss: 0.482655\n",
      "[531]\ttraining's binary_logloss: 0.482452\n",
      "[532]\ttraining's binary_logloss: 0.482208\n",
      "[533]\ttraining's binary_logloss: 0.482015\n",
      "[534]\ttraining's binary_logloss: 0.481875\n",
      "[535]\ttraining's binary_logloss: 0.48182\n",
      "[536]\ttraining's binary_logloss: 0.481705\n",
      "[537]\ttraining's binary_logloss: 0.481619\n",
      "[538]\ttraining's binary_logloss: 0.481566\n",
      "[539]\ttraining's binary_logloss: 0.481444\n",
      "[540]\ttraining's binary_logloss: 0.481317\n",
      "[541]\ttraining's binary_logloss: 0.481087\n",
      "[542]\ttraining's binary_logloss: 0.4809\n",
      "[543]\ttraining's binary_logloss: 0.480727\n",
      "[544]\ttraining's binary_logloss: 0.48066\n",
      "[545]\ttraining's binary_logloss: 0.480512\n",
      "[546]\ttraining's binary_logloss: 0.480356\n",
      "[547]\ttraining's binary_logloss: 0.480271\n",
      "[548]\ttraining's binary_logloss: 0.480147\n",
      "[549]\ttraining's binary_logloss: 0.480047\n",
      "[550]\ttraining's binary_logloss: 0.479899\n",
      "[551]\ttraining's binary_logloss: 0.47976\n",
      "[552]\ttraining's binary_logloss: 0.479687\n",
      "[553]\ttraining's binary_logloss: 0.479503\n",
      "[554]\ttraining's binary_logloss: 0.479271\n",
      "[555]\ttraining's binary_logloss: 0.47911\n",
      "[556]\ttraining's binary_logloss: 0.478939\n",
      "[557]\ttraining's binary_logloss: 0.478815\n",
      "[558]\ttraining's binary_logloss: 0.478681\n",
      "[559]\ttraining's binary_logloss: 0.478574\n",
      "[560]\ttraining's binary_logloss: 0.478449\n",
      "[561]\ttraining's binary_logloss: 0.478283\n",
      "[562]\ttraining's binary_logloss: 0.478128\n",
      "[563]\ttraining's binary_logloss: 0.477999\n",
      "[564]\ttraining's binary_logloss: 0.477863\n",
      "[565]\ttraining's binary_logloss: 0.477737\n",
      "[566]\ttraining's binary_logloss: 0.477555\n",
      "[567]\ttraining's binary_logloss: 0.477435\n",
      "[568]\ttraining's binary_logloss: 0.477335\n",
      "[569]\ttraining's binary_logloss: 0.477182\n",
      "[570]\ttraining's binary_logloss: 0.476987\n",
      "[571]\ttraining's binary_logloss: 0.476769\n",
      "[572]\ttraining's binary_logloss: 0.47669\n",
      "[573]\ttraining's binary_logloss: 0.476541\n",
      "[574]\ttraining's binary_logloss: 0.476399\n",
      "[575]\ttraining's binary_logloss: 0.476295\n",
      "[576]\ttraining's binary_logloss: 0.476219\n",
      "[577]\ttraining's binary_logloss: 0.476067\n",
      "[578]\ttraining's binary_logloss: 0.475861\n",
      "[579]\ttraining's binary_logloss: 0.475688\n",
      "[580]\ttraining's binary_logloss: 0.475621\n",
      "[581]\ttraining's binary_logloss: 0.47549\n",
      "[582]\ttraining's binary_logloss: 0.475425\n",
      "[583]\ttraining's binary_logloss: 0.475299\n",
      "[584]\ttraining's binary_logloss: 0.475218\n",
      "[585]\ttraining's binary_logloss: 0.475103\n",
      "[586]\ttraining's binary_logloss: 0.475023\n",
      "[587]\ttraining's binary_logloss: 0.474877\n",
      "[588]\ttraining's binary_logloss: 0.474806\n",
      "[589]\ttraining's binary_logloss: 0.474642\n",
      "[590]\ttraining's binary_logloss: 0.47448\n",
      "[591]\ttraining's binary_logloss: 0.474361\n",
      "[592]\ttraining's binary_logloss: 0.4742\n",
      "[593]\ttraining's binary_logloss: 0.474037\n",
      "[594]\ttraining's binary_logloss: 0.473923\n",
      "[595]\ttraining's binary_logloss: 0.47379\n",
      "[596]\ttraining's binary_logloss: 0.473665\n",
      "[597]\ttraining's binary_logloss: 0.473546\n",
      "[598]\ttraining's binary_logloss: 0.473405\n",
      "[599]\ttraining's binary_logloss: 0.473277\n",
      "[600]\ttraining's binary_logloss: 0.473123\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as LightGBM\n",
    "\n",
    "lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=600,\n",
    "                               max_depth = 50,\n",
    "                               min_data_in_leaf = 50,\n",
    "                               class_weight={True: 10, False: 1},\n",
    "                               learning_rate= 0.1\n",
    "                              ) \n",
    "\n",
    "evals = [(x_train_features, y_train_bool)]\n",
    "lgbm.fit(x_train_features, y_train_bool, eval_metric='logloss', eval_set=evals)\n",
    "y_pred = lgbm.predict(x_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.99      0.42      0.59     63391\n",
      "        risk       0.26      0.98      0.41     12724\n",
      "\n",
      "    accuracy                           0.52     76115\n",
      "   macro avg       0.62      0.70      0.50     76115\n",
      "weighted avg       0.87      0.52      0.56     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_train_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.39      0.55     21052\n",
      "        risk       0.22      0.82      0.34      4344\n",
      "\n",
      "    accuracy                           0.46     25396\n",
      "   macro avg       0.56      0.60      0.45     25396\n",
      "weighted avg       0.79      0.46      0.51     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tree Ensemble Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_feature_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "sfs_feature_list = ['BPS', 'DIV', '거래량', '금리', '비유동자산', '자산총계', '부채총계', '법인세차감전 순이익', '당기순이익']\n",
    "stock_info_list = ['BPS', 'PER', 'PBR', 'EPS', 'DIV', 'DPS', '거래량']\n",
    "financial_info_list = ['유동자산', '비유동자산', '자산총계', '유동부채', '비유동부채', '부채총계', '이익잉여금', '자본총계', '매출액', '영업이익', '법인세차감전 순이익', '당기순이익', '자본금']\n",
    "\n",
    "\n",
    "def make_feature_set(x) :\n",
    "    x_whole = x\n",
    "    x_rfecv = x[rfecv_feature_list]\n",
    "    x_sfs = x[sfs_feature_list]\n",
    "    x_f = x[financial_info_list]\n",
    "    x_s = x[stock_info_list]\n",
    "    return x_whole, x_rfecv, x_sfs, x_f, x_s\n",
    "\n",
    "\n",
    "x_whole, x_rfecv, x_sfs, x_f, x_s= make_feature_set(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[1]\ttraining's binary_logloss: 0.628963\n",
      "[2]\ttraining's binary_logloss: 0.623169\n",
      "[3]\ttraining's binary_logloss: 0.618132\n",
      "[4]\ttraining's binary_logloss: 0.613868\n",
      "[5]\ttraining's binary_logloss: 0.610407\n",
      "[6]\ttraining's binary_logloss: 0.607459\n",
      "[7]\ttraining's binary_logloss: 0.604669\n",
      "[8]\ttraining's binary_logloss: 0.602451\n",
      "[9]\ttraining's binary_logloss: 0.600126\n",
      "[10]\ttraining's binary_logloss: 0.598127\n",
      "[11]\ttraining's binary_logloss: 0.596193\n",
      "[12]\ttraining's binary_logloss: 0.594546\n",
      "[13]\ttraining's binary_logloss: 0.593102\n",
      "[14]\ttraining's binary_logloss: 0.591732\n",
      "[15]\ttraining's binary_logloss: 0.590413\n",
      "[16]\ttraining's binary_logloss: 0.589211\n",
      "[17]\ttraining's binary_logloss: 0.588091\n",
      "[18]\ttraining's binary_logloss: 0.587018\n",
      "[19]\ttraining's binary_logloss: 0.585988\n",
      "[20]\ttraining's binary_logloss: 0.585031\n",
      "[21]\ttraining's binary_logloss: 0.584051\n",
      "[22]\ttraining's binary_logloss: 0.58326\n",
      "[23]\ttraining's binary_logloss: 0.582493\n",
      "[24]\ttraining's binary_logloss: 0.581635\n",
      "[25]\ttraining's binary_logloss: 0.580872\n",
      "[26]\ttraining's binary_logloss: 0.580154\n",
      "[27]\ttraining's binary_logloss: 0.579441\n",
      "[28]\ttraining's binary_logloss: 0.578824\n",
      "[29]\ttraining's binary_logloss: 0.57811\n",
      "[30]\ttraining's binary_logloss: 0.577437\n",
      "[31]\ttraining's binary_logloss: 0.576835\n",
      "[32]\ttraining's binary_logloss: 0.576283\n",
      "[33]\ttraining's binary_logloss: 0.575666\n",
      "[34]\ttraining's binary_logloss: 0.575064\n",
      "[35]\ttraining's binary_logloss: 0.57452\n",
      "[36]\ttraining's binary_logloss: 0.574011\n",
      "[37]\ttraining's binary_logloss: 0.573532\n",
      "[38]\ttraining's binary_logloss: 0.572951\n",
      "[39]\ttraining's binary_logloss: 0.572392\n",
      "[40]\ttraining's binary_logloss: 0.571964\n",
      "[41]\ttraining's binary_logloss: 0.571437\n",
      "[42]\ttraining's binary_logloss: 0.570946\n",
      "[43]\ttraining's binary_logloss: 0.570487\n",
      "[44]\ttraining's binary_logloss: 0.570051\n",
      "[45]\ttraining's binary_logloss: 0.569539\n",
      "[46]\ttraining's binary_logloss: 0.569159\n",
      "[47]\ttraining's binary_logloss: 0.568708\n",
      "[48]\ttraining's binary_logloss: 0.568197\n",
      "[49]\ttraining's binary_logloss: 0.567752\n",
      "[50]\ttraining's binary_logloss: 0.56734\n",
      "[51]\ttraining's binary_logloss: 0.566889\n",
      "[52]\ttraining's binary_logloss: 0.56649\n",
      "[53]\ttraining's binary_logloss: 0.56608\n",
      "[54]\ttraining's binary_logloss: 0.565764\n",
      "[55]\ttraining's binary_logloss: 0.565339\n",
      "[56]\ttraining's binary_logloss: 0.564958\n",
      "[57]\ttraining's binary_logloss: 0.564519\n",
      "[58]\ttraining's binary_logloss: 0.564238\n",
      "[59]\ttraining's binary_logloss: 0.563877\n",
      "[60]\ttraining's binary_logloss: 0.563488\n",
      "[61]\ttraining's binary_logloss: 0.563102\n",
      "[62]\ttraining's binary_logloss: 0.562772\n",
      "[63]\ttraining's binary_logloss: 0.562396\n",
      "[64]\ttraining's binary_logloss: 0.562085\n",
      "[65]\ttraining's binary_logloss: 0.561705\n",
      "[66]\ttraining's binary_logloss: 0.561431\n",
      "[67]\ttraining's binary_logloss: 0.561093\n",
      "[68]\ttraining's binary_logloss: 0.560786\n",
      "[69]\ttraining's binary_logloss: 0.560433\n",
      "[70]\ttraining's binary_logloss: 0.560096\n",
      "[71]\ttraining's binary_logloss: 0.559874\n",
      "[72]\ttraining's binary_logloss: 0.559599\n",
      "[73]\ttraining's binary_logloss: 0.559252\n",
      "[74]\ttraining's binary_logloss: 0.558956\n",
      "[75]\ttraining's binary_logloss: 0.558668\n",
      "[76]\ttraining's binary_logloss: 0.558314\n",
      "[77]\ttraining's binary_logloss: 0.558076\n",
      "[78]\ttraining's binary_logloss: 0.557822\n",
      "[79]\ttraining's binary_logloss: 0.557471\n",
      "[80]\ttraining's binary_logloss: 0.557253\n",
      "[81]\ttraining's binary_logloss: 0.557024\n",
      "[82]\ttraining's binary_logloss: 0.556733\n",
      "[83]\ttraining's binary_logloss: 0.556431\n",
      "[84]\ttraining's binary_logloss: 0.556088\n",
      "[85]\ttraining's binary_logloss: 0.555829\n",
      "[86]\ttraining's binary_logloss: 0.555629\n",
      "[87]\ttraining's binary_logloss: 0.555208\n",
      "[88]\ttraining's binary_logloss: 0.554933\n",
      "[89]\ttraining's binary_logloss: 0.554658\n",
      "[90]\ttraining's binary_logloss: 0.554377\n",
      "[91]\ttraining's binary_logloss: 0.554078\n",
      "[92]\ttraining's binary_logloss: 0.553841\n",
      "[93]\ttraining's binary_logloss: 0.553642\n",
      "[94]\ttraining's binary_logloss: 0.553311\n",
      "[95]\ttraining's binary_logloss: 0.553111\n",
      "[96]\ttraining's binary_logloss: 0.552854\n",
      "[97]\ttraining's binary_logloss: 0.552546\n",
      "[98]\ttraining's binary_logloss: 0.552333\n",
      "[99]\ttraining's binary_logloss: 0.552051\n",
      "[100]\ttraining's binary_logloss: 0.551763\n",
      "[101]\ttraining's binary_logloss: 0.551425\n",
      "[102]\ttraining's binary_logloss: 0.551169\n",
      "[103]\ttraining's binary_logloss: 0.550913\n",
      "[104]\ttraining's binary_logloss: 0.55065\n",
      "[105]\ttraining's binary_logloss: 0.55043\n",
      "[106]\ttraining's binary_logloss: 0.550183\n",
      "[107]\ttraining's binary_logloss: 0.549872\n",
      "[108]\ttraining's binary_logloss: 0.549713\n",
      "[109]\ttraining's binary_logloss: 0.549458\n",
      "[110]\ttraining's binary_logloss: 0.549188\n",
      "[111]\ttraining's binary_logloss: 0.54897\n",
      "[112]\ttraining's binary_logloss: 0.548748\n",
      "[113]\ttraining's binary_logloss: 0.548499\n",
      "[114]\ttraining's binary_logloss: 0.548164\n",
      "[115]\ttraining's binary_logloss: 0.547928\n",
      "[116]\ttraining's binary_logloss: 0.54767\n",
      "[117]\ttraining's binary_logloss: 0.547412\n",
      "[118]\ttraining's binary_logloss: 0.547176\n",
      "[119]\ttraining's binary_logloss: 0.546962\n",
      "[120]\ttraining's binary_logloss: 0.546642\n",
      "[121]\ttraining's binary_logloss: 0.546368\n",
      "[122]\ttraining's binary_logloss: 0.546221\n",
      "[123]\ttraining's binary_logloss: 0.545972\n",
      "[124]\ttraining's binary_logloss: 0.545736\n",
      "[125]\ttraining's binary_logloss: 0.545469\n",
      "[126]\ttraining's binary_logloss: 0.54529\n",
      "[127]\ttraining's binary_logloss: 0.545093\n",
      "[128]\ttraining's binary_logloss: 0.544786\n",
      "[129]\ttraining's binary_logloss: 0.54451\n",
      "[130]\ttraining's binary_logloss: 0.544338\n",
      "[131]\ttraining's binary_logloss: 0.544032\n",
      "[132]\ttraining's binary_logloss: 0.543758\n",
      "[133]\ttraining's binary_logloss: 0.543488\n",
      "[134]\ttraining's binary_logloss: 0.543303\n",
      "[135]\ttraining's binary_logloss: 0.542975\n",
      "[136]\ttraining's binary_logloss: 0.542708\n",
      "[137]\ttraining's binary_logloss: 0.542454\n",
      "[138]\ttraining's binary_logloss: 0.542089\n",
      "[139]\ttraining's binary_logloss: 0.541921\n",
      "[140]\ttraining's binary_logloss: 0.541694\n",
      "[141]\ttraining's binary_logloss: 0.541396\n",
      "[142]\ttraining's binary_logloss: 0.541202\n",
      "[143]\ttraining's binary_logloss: 0.541023\n",
      "[144]\ttraining's binary_logloss: 0.540866\n",
      "[145]\ttraining's binary_logloss: 0.540646\n",
      "[146]\ttraining's binary_logloss: 0.540402\n",
      "[147]\ttraining's binary_logloss: 0.540107\n",
      "[148]\ttraining's binary_logloss: 0.539891\n",
      "[149]\ttraining's binary_logloss: 0.53965\n",
      "[150]\ttraining's binary_logloss: 0.539406\n",
      "[151]\ttraining's binary_logloss: 0.53918\n",
      "[152]\ttraining's binary_logloss: 0.538933\n",
      "[153]\ttraining's binary_logloss: 0.538629\n",
      "[154]\ttraining's binary_logloss: 0.53841\n",
      "[155]\ttraining's binary_logloss: 0.53821\n",
      "[156]\ttraining's binary_logloss: 0.537919\n",
      "[157]\ttraining's binary_logloss: 0.537743\n",
      "[158]\ttraining's binary_logloss: 0.537606\n",
      "[159]\ttraining's binary_logloss: 0.537413\n",
      "[160]\ttraining's binary_logloss: 0.537169\n",
      "[161]\ttraining's binary_logloss: 0.536936\n",
      "[162]\ttraining's binary_logloss: 0.536653\n",
      "[163]\ttraining's binary_logloss: 0.536487\n",
      "[164]\ttraining's binary_logloss: 0.536316\n",
      "[165]\ttraining's binary_logloss: 0.536026\n",
      "[166]\ttraining's binary_logloss: 0.535833\n",
      "[167]\ttraining's binary_logloss: 0.535549\n",
      "[168]\ttraining's binary_logloss: 0.53532\n",
      "[169]\ttraining's binary_logloss: 0.535192\n",
      "[170]\ttraining's binary_logloss: 0.534994\n",
      "[171]\ttraining's binary_logloss: 0.534719\n",
      "[172]\ttraining's binary_logloss: 0.534529\n",
      "[173]\ttraining's binary_logloss: 0.534327\n",
      "[174]\ttraining's binary_logloss: 0.534118\n",
      "[175]\ttraining's binary_logloss: 0.533936\n",
      "[176]\ttraining's binary_logloss: 0.533752\n",
      "[177]\ttraining's binary_logloss: 0.533587\n",
      "[178]\ttraining's binary_logloss: 0.533286\n",
      "[179]\ttraining's binary_logloss: 0.533048\n",
      "[180]\ttraining's binary_logloss: 0.532822\n",
      "[181]\ttraining's binary_logloss: 0.532626\n",
      "[182]\ttraining's binary_logloss: 0.532497\n",
      "[183]\ttraining's binary_logloss: 0.532311\n",
      "[184]\ttraining's binary_logloss: 0.532081\n",
      "[185]\ttraining's binary_logloss: 0.531808\n",
      "[186]\ttraining's binary_logloss: 0.531524\n",
      "[187]\ttraining's binary_logloss: 0.531321\n",
      "[188]\ttraining's binary_logloss: 0.531119\n",
      "[189]\ttraining's binary_logloss: 0.530978\n",
      "[190]\ttraining's binary_logloss: 0.530783\n",
      "[191]\ttraining's binary_logloss: 0.530557\n",
      "[192]\ttraining's binary_logloss: 0.530346\n",
      "[193]\ttraining's binary_logloss: 0.5302\n",
      "[194]\ttraining's binary_logloss: 0.529961\n",
      "[195]\ttraining's binary_logloss: 0.529692\n",
      "[196]\ttraining's binary_logloss: 0.529504\n",
      "[197]\ttraining's binary_logloss: 0.529278\n",
      "[198]\ttraining's binary_logloss: 0.529107\n",
      "[199]\ttraining's binary_logloss: 0.528869\n",
      "[200]\ttraining's binary_logloss: 0.528637\n",
      "[201]\ttraining's binary_logloss: 0.52835\n",
      "[202]\ttraining's binary_logloss: 0.528146\n",
      "[203]\ttraining's binary_logloss: 0.527961\n",
      "[204]\ttraining's binary_logloss: 0.527748\n",
      "[205]\ttraining's binary_logloss: 0.527561\n",
      "[206]\ttraining's binary_logloss: 0.527285\n",
      "[207]\ttraining's binary_logloss: 0.527118\n",
      "[208]\ttraining's binary_logloss: 0.526915\n",
      "[209]\ttraining's binary_logloss: 0.526719\n",
      "[210]\ttraining's binary_logloss: 0.526545\n",
      "[211]\ttraining's binary_logloss: 0.526362\n",
      "[212]\ttraining's binary_logloss: 0.526227\n",
      "[213]\ttraining's binary_logloss: 0.526104\n",
      "[214]\ttraining's binary_logloss: 0.525888\n",
      "[215]\ttraining's binary_logloss: 0.525649\n",
      "[216]\ttraining's binary_logloss: 0.525397\n",
      "[217]\ttraining's binary_logloss: 0.525249\n",
      "[218]\ttraining's binary_logloss: 0.525113\n",
      "[219]\ttraining's binary_logloss: 0.524808\n",
      "[220]\ttraining's binary_logloss: 0.524572\n",
      "[221]\ttraining's binary_logloss: 0.524386\n",
      "[222]\ttraining's binary_logloss: 0.524177\n",
      "[223]\ttraining's binary_logloss: 0.524064\n",
      "[224]\ttraining's binary_logloss: 0.523976\n",
      "[225]\ttraining's binary_logloss: 0.523782\n",
      "[226]\ttraining's binary_logloss: 0.523664\n",
      "[227]\ttraining's binary_logloss: 0.523469\n",
      "[228]\ttraining's binary_logloss: 0.523261\n",
      "[229]\ttraining's binary_logloss: 0.523089\n",
      "[230]\ttraining's binary_logloss: 0.522814\n",
      "[231]\ttraining's binary_logloss: 0.522642\n",
      "[232]\ttraining's binary_logloss: 0.522444\n",
      "[233]\ttraining's binary_logloss: 0.522336\n",
      "[234]\ttraining's binary_logloss: 0.522145\n",
      "[235]\ttraining's binary_logloss: 0.521952\n",
      "[236]\ttraining's binary_logloss: 0.521791\n",
      "[237]\ttraining's binary_logloss: 0.521519\n",
      "[238]\ttraining's binary_logloss: 0.521284\n",
      "[239]\ttraining's binary_logloss: 0.521173\n",
      "[240]\ttraining's binary_logloss: 0.521047\n",
      "[241]\ttraining's binary_logloss: 0.520912\n",
      "[242]\ttraining's binary_logloss: 0.520719\n",
      "[243]\ttraining's binary_logloss: 0.520576\n",
      "[244]\ttraining's binary_logloss: 0.520355\n",
      "[245]\ttraining's binary_logloss: 0.520102\n",
      "[246]\ttraining's binary_logloss: 0.519953\n",
      "[247]\ttraining's binary_logloss: 0.519811\n",
      "[248]\ttraining's binary_logloss: 0.519668\n",
      "[249]\ttraining's binary_logloss: 0.519487\n",
      "[250]\ttraining's binary_logloss: 0.519278\n",
      "[251]\ttraining's binary_logloss: 0.519018\n",
      "[252]\ttraining's binary_logloss: 0.518886\n",
      "[253]\ttraining's binary_logloss: 0.518701\n",
      "[254]\ttraining's binary_logloss: 0.518482\n",
      "[255]\ttraining's binary_logloss: 0.518367\n",
      "[256]\ttraining's binary_logloss: 0.518133\n",
      "[257]\ttraining's binary_logloss: 0.51803\n",
      "[258]\ttraining's binary_logloss: 0.517828\n",
      "[259]\ttraining's binary_logloss: 0.517612\n",
      "[260]\ttraining's binary_logloss: 0.517405\n",
      "[261]\ttraining's binary_logloss: 0.517199\n",
      "[262]\ttraining's binary_logloss: 0.517035\n",
      "[263]\ttraining's binary_logloss: 0.516878\n",
      "[264]\ttraining's binary_logloss: 0.516717\n",
      "[265]\ttraining's binary_logloss: 0.516588\n",
      "[266]\ttraining's binary_logloss: 0.516337\n",
      "[267]\ttraining's binary_logloss: 0.516185\n",
      "[268]\ttraining's binary_logloss: 0.515968\n",
      "[269]\ttraining's binary_logloss: 0.515692\n",
      "[270]\ttraining's binary_logloss: 0.515519\n",
      "[271]\ttraining's binary_logloss: 0.515204\n",
      "[272]\ttraining's binary_logloss: 0.515073\n",
      "[273]\ttraining's binary_logloss: 0.514892\n",
      "[274]\ttraining's binary_logloss: 0.514688\n",
      "[275]\ttraining's binary_logloss: 0.514448\n",
      "[276]\ttraining's binary_logloss: 0.514294\n",
      "[277]\ttraining's binary_logloss: 0.514108\n",
      "[278]\ttraining's binary_logloss: 0.513991\n",
      "[279]\ttraining's binary_logloss: 0.513746\n",
      "[280]\ttraining's binary_logloss: 0.513496\n",
      "[281]\ttraining's binary_logloss: 0.513302\n",
      "[282]\ttraining's binary_logloss: 0.513158\n",
      "[283]\ttraining's binary_logloss: 0.512831\n",
      "[284]\ttraining's binary_logloss: 0.512672\n",
      "[285]\ttraining's binary_logloss: 0.512494\n",
      "[286]\ttraining's binary_logloss: 0.512292\n",
      "[287]\ttraining's binary_logloss: 0.512111\n",
      "[288]\ttraining's binary_logloss: 0.511889\n",
      "[289]\ttraining's binary_logloss: 0.511692\n",
      "[290]\ttraining's binary_logloss: 0.511464\n",
      "[291]\ttraining's binary_logloss: 0.511172\n",
      "[292]\ttraining's binary_logloss: 0.51093\n",
      "[293]\ttraining's binary_logloss: 0.510798\n",
      "[294]\ttraining's binary_logloss: 0.510695\n",
      "[295]\ttraining's binary_logloss: 0.510527\n",
      "[296]\ttraining's binary_logloss: 0.51041\n",
      "[297]\ttraining's binary_logloss: 0.510314\n",
      "[298]\ttraining's binary_logloss: 0.510186\n",
      "[299]\ttraining's binary_logloss: 0.509989\n",
      "[300]\ttraining's binary_logloss: 0.509745\n",
      "[301]\ttraining's binary_logloss: 0.509635\n",
      "[302]\ttraining's binary_logloss: 0.509511\n",
      "[303]\ttraining's binary_logloss: 0.509292\n",
      "[304]\ttraining's binary_logloss: 0.509114\n",
      "[305]\ttraining's binary_logloss: 0.508991\n",
      "[306]\ttraining's binary_logloss: 0.508901\n",
      "[307]\ttraining's binary_logloss: 0.508667\n",
      "[308]\ttraining's binary_logloss: 0.508367\n",
      "[309]\ttraining's binary_logloss: 0.508164\n",
      "[310]\ttraining's binary_logloss: 0.508044\n",
      "[311]\ttraining's binary_logloss: 0.507893\n",
      "[312]\ttraining's binary_logloss: 0.507704\n",
      "[313]\ttraining's binary_logloss: 0.507546\n",
      "[314]\ttraining's binary_logloss: 0.507384\n",
      "[315]\ttraining's binary_logloss: 0.507197\n",
      "[316]\ttraining's binary_logloss: 0.507062\n",
      "[317]\ttraining's binary_logloss: 0.506856\n",
      "[318]\ttraining's binary_logloss: 0.506741\n",
      "[319]\ttraining's binary_logloss: 0.50665\n",
      "[320]\ttraining's binary_logloss: 0.506501\n",
      "[321]\ttraining's binary_logloss: 0.506332\n",
      "[322]\ttraining's binary_logloss: 0.506158\n",
      "[323]\ttraining's binary_logloss: 0.505978\n",
      "[324]\ttraining's binary_logloss: 0.505731\n",
      "[325]\ttraining's binary_logloss: 0.50557\n",
      "[326]\ttraining's binary_logloss: 0.505449\n",
      "[327]\ttraining's binary_logloss: 0.505225\n",
      "[328]\ttraining's binary_logloss: 0.505094\n",
      "[329]\ttraining's binary_logloss: 0.504949\n",
      "[330]\ttraining's binary_logloss: 0.504764\n",
      "[331]\ttraining's binary_logloss: 0.504538\n",
      "[332]\ttraining's binary_logloss: 0.504404\n",
      "[333]\ttraining's binary_logloss: 0.504289\n",
      "[334]\ttraining's binary_logloss: 0.504131\n",
      "[335]\ttraining's binary_logloss: 0.504032\n",
      "[336]\ttraining's binary_logloss: 0.503835\n",
      "[337]\ttraining's binary_logloss: 0.503614\n",
      "[338]\ttraining's binary_logloss: 0.503388\n",
      "[339]\ttraining's binary_logloss: 0.503211\n",
      "[340]\ttraining's binary_logloss: 0.503037\n",
      "[341]\ttraining's binary_logloss: 0.50295\n",
      "[342]\ttraining's binary_logloss: 0.502754\n",
      "[343]\ttraining's binary_logloss: 0.502552\n",
      "[344]\ttraining's binary_logloss: 0.50243\n",
      "[345]\ttraining's binary_logloss: 0.502327\n",
      "[346]\ttraining's binary_logloss: 0.502166\n",
      "[347]\ttraining's binary_logloss: 0.501959\n",
      "[348]\ttraining's binary_logloss: 0.50174\n",
      "[349]\ttraining's binary_logloss: 0.501642\n",
      "[350]\ttraining's binary_logloss: 0.501499\n",
      "[351]\ttraining's binary_logloss: 0.501405\n",
      "[352]\ttraining's binary_logloss: 0.501314\n",
      "[353]\ttraining's binary_logloss: 0.501153\n",
      "[354]\ttraining's binary_logloss: 0.50102\n",
      "[355]\ttraining's binary_logloss: 0.50088\n",
      "[356]\ttraining's binary_logloss: 0.50074\n",
      "[357]\ttraining's binary_logloss: 0.50062\n",
      "[358]\ttraining's binary_logloss: 0.500518\n",
      "[359]\ttraining's binary_logloss: 0.500343\n",
      "[360]\ttraining's binary_logloss: 0.500258\n",
      "[361]\ttraining's binary_logloss: 0.500166\n",
      "[362]\ttraining's binary_logloss: 0.499967\n",
      "[363]\ttraining's binary_logloss: 0.499782\n",
      "[364]\ttraining's binary_logloss: 0.499592\n",
      "[365]\ttraining's binary_logloss: 0.499376\n",
      "[366]\ttraining's binary_logloss: 0.49921\n",
      "[367]\ttraining's binary_logloss: 0.499135\n",
      "[368]\ttraining's binary_logloss: 0.498984\n",
      "[369]\ttraining's binary_logloss: 0.498841\n",
      "[370]\ttraining's binary_logloss: 0.498646\n",
      "[371]\ttraining's binary_logloss: 0.498474\n",
      "[372]\ttraining's binary_logloss: 0.498284\n",
      "[373]\ttraining's binary_logloss: 0.498098\n",
      "[374]\ttraining's binary_logloss: 0.497951\n",
      "[375]\ttraining's binary_logloss: 0.49783\n",
      "[376]\ttraining's binary_logloss: 0.497534\n",
      "[377]\ttraining's binary_logloss: 0.497322\n",
      "[378]\ttraining's binary_logloss: 0.497167\n",
      "[379]\ttraining's binary_logloss: 0.497058\n",
      "[380]\ttraining's binary_logloss: 0.496861\n",
      "[381]\ttraining's binary_logloss: 0.496741\n",
      "[382]\ttraining's binary_logloss: 0.496597\n",
      "[383]\ttraining's binary_logloss: 0.496449\n",
      "[384]\ttraining's binary_logloss: 0.496341\n",
      "[385]\ttraining's binary_logloss: 0.496181\n",
      "[386]\ttraining's binary_logloss: 0.496054\n",
      "[387]\ttraining's binary_logloss: 0.495881\n",
      "[388]\ttraining's binary_logloss: 0.495717\n",
      "[389]\ttraining's binary_logloss: 0.495503\n",
      "[390]\ttraining's binary_logloss: 0.495371\n",
      "[391]\ttraining's binary_logloss: 0.495277\n",
      "[392]\ttraining's binary_logloss: 0.495139\n",
      "[393]\ttraining's binary_logloss: 0.49506\n",
      "[394]\ttraining's binary_logloss: 0.494994\n",
      "[395]\ttraining's binary_logloss: 0.494764\n",
      "[396]\ttraining's binary_logloss: 0.494592\n",
      "[397]\ttraining's binary_logloss: 0.494502\n",
      "[398]\ttraining's binary_logloss: 0.494305\n",
      "[399]\ttraining's binary_logloss: 0.494221\n",
      "[400]\ttraining's binary_logloss: 0.494103\n",
      "[401]\ttraining's binary_logloss: 0.493948\n",
      "[402]\ttraining's binary_logloss: 0.493731\n",
      "[403]\ttraining's binary_logloss: 0.493633\n",
      "[404]\ttraining's binary_logloss: 0.493503\n",
      "[405]\ttraining's binary_logloss: 0.493387\n",
      "[406]\ttraining's binary_logloss: 0.493278\n",
      "[407]\ttraining's binary_logloss: 0.49309\n",
      "[408]\ttraining's binary_logloss: 0.4929\n",
      "[409]\ttraining's binary_logloss: 0.492772\n",
      "[410]\ttraining's binary_logloss: 0.492607\n",
      "[411]\ttraining's binary_logloss: 0.492454\n",
      "[412]\ttraining's binary_logloss: 0.492254\n",
      "[413]\ttraining's binary_logloss: 0.492026\n",
      "[414]\ttraining's binary_logloss: 0.491872\n",
      "[415]\ttraining's binary_logloss: 0.491686\n",
      "[416]\ttraining's binary_logloss: 0.491526\n",
      "[417]\ttraining's binary_logloss: 0.491369\n",
      "[418]\ttraining's binary_logloss: 0.491156\n",
      "[419]\ttraining's binary_logloss: 0.49102\n",
      "[420]\ttraining's binary_logloss: 0.490799\n",
      "[421]\ttraining's binary_logloss: 0.490684\n",
      "[422]\ttraining's binary_logloss: 0.490483\n",
      "[423]\ttraining's binary_logloss: 0.490284\n",
      "[424]\ttraining's binary_logloss: 0.490122\n",
      "[425]\ttraining's binary_logloss: 0.489985\n",
      "[426]\ttraining's binary_logloss: 0.489819\n",
      "[427]\ttraining's binary_logloss: 0.48961\n",
      "[428]\ttraining's binary_logloss: 0.489448\n",
      "[429]\ttraining's binary_logloss: 0.489314\n",
      "[430]\ttraining's binary_logloss: 0.489149\n",
      "[431]\ttraining's binary_logloss: 0.489017\n",
      "[432]\ttraining's binary_logloss: 0.488903\n",
      "[433]\ttraining's binary_logloss: 0.488755\n",
      "[434]\ttraining's binary_logloss: 0.488679\n",
      "[435]\ttraining's binary_logloss: 0.488583\n",
      "[436]\ttraining's binary_logloss: 0.488471\n",
      "[437]\ttraining's binary_logloss: 0.488335\n",
      "[438]\ttraining's binary_logloss: 0.488173\n",
      "[439]\ttraining's binary_logloss: 0.488063\n",
      "[440]\ttraining's binary_logloss: 0.487934\n",
      "[441]\ttraining's binary_logloss: 0.48779\n",
      "[442]\ttraining's binary_logloss: 0.487718\n",
      "[443]\ttraining's binary_logloss: 0.48765\n",
      "[444]\ttraining's binary_logloss: 0.487507\n",
      "[445]\ttraining's binary_logloss: 0.48738\n",
      "[446]\ttraining's binary_logloss: 0.487264\n",
      "[447]\ttraining's binary_logloss: 0.487094\n",
      "[448]\ttraining's binary_logloss: 0.486952\n",
      "[449]\ttraining's binary_logloss: 0.486816\n",
      "[450]\ttraining's binary_logloss: 0.486654\n",
      "[451]\ttraining's binary_logloss: 0.486544\n",
      "[452]\ttraining's binary_logloss: 0.486435\n",
      "[453]\ttraining's binary_logloss: 0.486215\n",
      "[454]\ttraining's binary_logloss: 0.486037\n",
      "[455]\ttraining's binary_logloss: 0.48585\n",
      "[456]\ttraining's binary_logloss: 0.48572\n",
      "[457]\ttraining's binary_logloss: 0.485635\n",
      "[458]\ttraining's binary_logloss: 0.485496\n",
      "[459]\ttraining's binary_logloss: 0.485301\n",
      "[460]\ttraining's binary_logloss: 0.485147\n",
      "[461]\ttraining's binary_logloss: 0.484997\n",
      "[462]\ttraining's binary_logloss: 0.484863\n",
      "[463]\ttraining's binary_logloss: 0.484703\n",
      "[464]\ttraining's binary_logloss: 0.484537\n",
      "[465]\ttraining's binary_logloss: 0.484333\n",
      "[466]\ttraining's binary_logloss: 0.484162\n",
      "[467]\ttraining's binary_logloss: 0.484026\n",
      "[468]\ttraining's binary_logloss: 0.483888\n",
      "[469]\ttraining's binary_logloss: 0.483826\n",
      "[470]\ttraining's binary_logloss: 0.483737\n",
      "[471]\ttraining's binary_logloss: 0.483616\n",
      "[472]\ttraining's binary_logloss: 0.483554\n",
      "[473]\ttraining's binary_logloss: 0.48344\n",
      "[474]\ttraining's binary_logloss: 0.483367\n",
      "[475]\ttraining's binary_logloss: 0.483258\n",
      "[476]\ttraining's binary_logloss: 0.483075\n",
      "[477]\ttraining's binary_logloss: 0.48292\n",
      "[478]\ttraining's binary_logloss: 0.482771\n",
      "[479]\ttraining's binary_logloss: 0.48262\n",
      "[480]\ttraining's binary_logloss: 0.482448\n",
      "[481]\ttraining's binary_logloss: 0.482272\n",
      "[482]\ttraining's binary_logloss: 0.482111\n",
      "[483]\ttraining's binary_logloss: 0.481985\n",
      "[484]\ttraining's binary_logloss: 0.481815\n",
      "[485]\ttraining's binary_logloss: 0.481725\n",
      "[486]\ttraining's binary_logloss: 0.48162\n",
      "[487]\ttraining's binary_logloss: 0.481492\n",
      "[488]\ttraining's binary_logloss: 0.481331\n",
      "[489]\ttraining's binary_logloss: 0.481201\n",
      "[490]\ttraining's binary_logloss: 0.481136\n",
      "[491]\ttraining's binary_logloss: 0.481032\n",
      "[492]\ttraining's binary_logloss: 0.480885\n",
      "[493]\ttraining's binary_logloss: 0.480762\n",
      "[494]\ttraining's binary_logloss: 0.480692\n",
      "[495]\ttraining's binary_logloss: 0.480563\n",
      "[496]\ttraining's binary_logloss: 0.480399\n",
      "[497]\ttraining's binary_logloss: 0.480186\n",
      "[498]\ttraining's binary_logloss: 0.480066\n",
      "[499]\ttraining's binary_logloss: 0.479982\n",
      "[500]\ttraining's binary_logloss: 0.479761\n",
      "[501]\ttraining's binary_logloss: 0.479581\n",
      "[502]\ttraining's binary_logloss: 0.479387\n",
      "[503]\ttraining's binary_logloss: 0.479279\n",
      "[504]\ttraining's binary_logloss: 0.479193\n",
      "[505]\ttraining's binary_logloss: 0.478995\n",
      "[506]\ttraining's binary_logloss: 0.478767\n",
      "[507]\ttraining's binary_logloss: 0.478631\n",
      "[508]\ttraining's binary_logloss: 0.478505\n",
      "[509]\ttraining's binary_logloss: 0.47838\n",
      "[510]\ttraining's binary_logloss: 0.478168\n",
      "[511]\ttraining's binary_logloss: 0.478009\n",
      "[512]\ttraining's binary_logloss: 0.47791\n",
      "[513]\ttraining's binary_logloss: 0.477835\n",
      "[514]\ttraining's binary_logloss: 0.477716\n",
      "[515]\ttraining's binary_logloss: 0.47762\n",
      "[516]\ttraining's binary_logloss: 0.477475\n",
      "[517]\ttraining's binary_logloss: 0.477348\n",
      "[518]\ttraining's binary_logloss: 0.477177\n",
      "[519]\ttraining's binary_logloss: 0.477073\n",
      "[520]\ttraining's binary_logloss: 0.476862\n",
      "[521]\ttraining's binary_logloss: 0.476692\n",
      "[522]\ttraining's binary_logloss: 0.476576\n",
      "[523]\ttraining's binary_logloss: 0.476419\n",
      "[524]\ttraining's binary_logloss: 0.476318\n",
      "[525]\ttraining's binary_logloss: 0.476224\n",
      "[526]\ttraining's binary_logloss: 0.476126\n",
      "[527]\ttraining's binary_logloss: 0.475943\n",
      "[528]\ttraining's binary_logloss: 0.47589\n",
      "[529]\ttraining's binary_logloss: 0.475805\n",
      "[530]\ttraining's binary_logloss: 0.475638\n",
      "[531]\ttraining's binary_logloss: 0.475502\n",
      "[532]\ttraining's binary_logloss: 0.47541\n",
      "[533]\ttraining's binary_logloss: 0.475277\n",
      "[534]\ttraining's binary_logloss: 0.475232\n",
      "[535]\ttraining's binary_logloss: 0.475085\n",
      "[536]\ttraining's binary_logloss: 0.474991\n",
      "[537]\ttraining's binary_logloss: 0.474771\n",
      "[538]\ttraining's binary_logloss: 0.474699\n",
      "[539]\ttraining's binary_logloss: 0.474572\n",
      "[540]\ttraining's binary_logloss: 0.474442\n",
      "[541]\ttraining's binary_logloss: 0.474373\n",
      "[542]\ttraining's binary_logloss: 0.474241\n",
      "[543]\ttraining's binary_logloss: 0.474164\n",
      "[544]\ttraining's binary_logloss: 0.47398\n",
      "[545]\ttraining's binary_logloss: 0.473862\n",
      "[546]\ttraining's binary_logloss: 0.473504\n",
      "[547]\ttraining's binary_logloss: 0.47334\n",
      "[548]\ttraining's binary_logloss: 0.473258\n",
      "[549]\ttraining's binary_logloss: 0.47316\n",
      "[550]\ttraining's binary_logloss: 0.473052\n",
      "[551]\ttraining's binary_logloss: 0.472875\n",
      "[552]\ttraining's binary_logloss: 0.472728\n",
      "[553]\ttraining's binary_logloss: 0.472631\n",
      "[554]\ttraining's binary_logloss: 0.472494\n",
      "[555]\ttraining's binary_logloss: 0.472395\n",
      "[556]\ttraining's binary_logloss: 0.472122\n",
      "[557]\ttraining's binary_logloss: 0.472003\n",
      "[558]\ttraining's binary_logloss: 0.471867\n",
      "[559]\ttraining's binary_logloss: 0.471736\n",
      "[560]\ttraining's binary_logloss: 0.471607\n",
      "[561]\ttraining's binary_logloss: 0.471419\n",
      "[562]\ttraining's binary_logloss: 0.471248\n",
      "[563]\ttraining's binary_logloss: 0.471153\n",
      "[564]\ttraining's binary_logloss: 0.471032\n",
      "[565]\ttraining's binary_logloss: 0.470869\n",
      "[566]\ttraining's binary_logloss: 0.470742\n",
      "[567]\ttraining's binary_logloss: 0.47054\n",
      "[568]\ttraining's binary_logloss: 0.47038\n",
      "[569]\ttraining's binary_logloss: 0.470244\n",
      "[570]\ttraining's binary_logloss: 0.470083\n",
      "[571]\ttraining's binary_logloss: 0.469976\n",
      "[572]\ttraining's binary_logloss: 0.469779\n",
      "[573]\ttraining's binary_logloss: 0.469666\n",
      "[574]\ttraining's binary_logloss: 0.469504\n",
      "[575]\ttraining's binary_logloss: 0.469384\n",
      "[576]\ttraining's binary_logloss: 0.469214\n",
      "[577]\ttraining's binary_logloss: 0.468921\n",
      "[578]\ttraining's binary_logloss: 0.46878\n",
      "[579]\ttraining's binary_logloss: 0.468612\n",
      "[580]\ttraining's binary_logloss: 0.46852\n",
      "[581]\ttraining's binary_logloss: 0.468378\n",
      "[582]\ttraining's binary_logloss: 0.468214\n",
      "[583]\ttraining's binary_logloss: 0.468023\n",
      "[584]\ttraining's binary_logloss: 0.467852\n",
      "[585]\ttraining's binary_logloss: 0.467737\n",
      "[586]\ttraining's binary_logloss: 0.467614\n",
      "[587]\ttraining's binary_logloss: 0.467507\n",
      "[588]\ttraining's binary_logloss: 0.467342\n",
      "[589]\ttraining's binary_logloss: 0.467104\n",
      "[590]\ttraining's binary_logloss: 0.46702\n",
      "[591]\ttraining's binary_logloss: 0.466902\n",
      "[592]\ttraining's binary_logloss: 0.466784\n",
      "[593]\ttraining's binary_logloss: 0.466684\n",
      "[594]\ttraining's binary_logloss: 0.466494\n",
      "[595]\ttraining's binary_logloss: 0.466348\n",
      "[596]\ttraining's binary_logloss: 0.466235\n",
      "[597]\ttraining's binary_logloss: 0.466055\n",
      "[598]\ttraining's binary_logloss: 0.465919\n",
      "[599]\ttraining's binary_logloss: 0.465723\n",
      "[600]\ttraining's binary_logloss: 0.465571\n",
      "[1]\ttraining's binary_logloss: 0.628988\n",
      "[2]\ttraining's binary_logloss: 0.623421\n",
      "[3]\ttraining's binary_logloss: 0.618595\n",
      "[4]\ttraining's binary_logloss: 0.614736\n",
      "[5]\ttraining's binary_logloss: 0.611145\n",
      "[6]\ttraining's binary_logloss: 0.608083\n",
      "[7]\ttraining's binary_logloss: 0.605599\n",
      "[8]\ttraining's binary_logloss: 0.60318\n",
      "[9]\ttraining's binary_logloss: 0.601156\n",
      "[10]\ttraining's binary_logloss: 0.599245\n",
      "[11]\ttraining's binary_logloss: 0.597674\n",
      "[12]\ttraining's binary_logloss: 0.596315\n",
      "[13]\ttraining's binary_logloss: 0.594907\n",
      "[14]\ttraining's binary_logloss: 0.593697\n",
      "[15]\ttraining's binary_logloss: 0.592459\n",
      "[16]\ttraining's binary_logloss: 0.591341\n",
      "[17]\ttraining's binary_logloss: 0.590368\n",
      "[18]\ttraining's binary_logloss: 0.589415\n",
      "[19]\ttraining's binary_logloss: 0.588542\n",
      "[20]\ttraining's binary_logloss: 0.587626\n",
      "[21]\ttraining's binary_logloss: 0.586785\n",
      "[22]\ttraining's binary_logloss: 0.585927\n",
      "[23]\ttraining's binary_logloss: 0.585217\n",
      "[24]\ttraining's binary_logloss: 0.584518\n",
      "[25]\ttraining's binary_logloss: 0.583815\n",
      "[26]\ttraining's binary_logloss: 0.583215\n",
      "[27]\ttraining's binary_logloss: 0.582612\n",
      "[28]\ttraining's binary_logloss: 0.582014\n",
      "[29]\ttraining's binary_logloss: 0.581421\n",
      "[30]\ttraining's binary_logloss: 0.580827\n",
      "[31]\ttraining's binary_logloss: 0.580219\n",
      "[32]\ttraining's binary_logloss: 0.579627\n",
      "[33]\ttraining's binary_logloss: 0.57907\n",
      "[34]\ttraining's binary_logloss: 0.578461\n",
      "[35]\ttraining's binary_logloss: 0.57794\n",
      "[36]\ttraining's binary_logloss: 0.577413\n",
      "[37]\ttraining's binary_logloss: 0.576984\n",
      "[38]\ttraining's binary_logloss: 0.576446\n",
      "[39]\ttraining's binary_logloss: 0.576013\n",
      "[40]\ttraining's binary_logloss: 0.575554\n",
      "[41]\ttraining's binary_logloss: 0.575091\n",
      "[42]\ttraining's binary_logloss: 0.574702\n",
      "[43]\ttraining's binary_logloss: 0.574224\n",
      "[44]\ttraining's binary_logloss: 0.573776\n",
      "[45]\ttraining's binary_logloss: 0.573357\n",
      "[46]\ttraining's binary_logloss: 0.572885\n",
      "[47]\ttraining's binary_logloss: 0.572478\n",
      "[48]\ttraining's binary_logloss: 0.572078\n",
      "[49]\ttraining's binary_logloss: 0.571684\n",
      "[50]\ttraining's binary_logloss: 0.571342\n",
      "[51]\ttraining's binary_logloss: 0.570987\n",
      "[52]\ttraining's binary_logloss: 0.570604\n",
      "[53]\ttraining's binary_logloss: 0.570223\n",
      "[54]\ttraining's binary_logloss: 0.569866\n",
      "[55]\ttraining's binary_logloss: 0.569547\n",
      "[56]\ttraining's binary_logloss: 0.569147\n",
      "[57]\ttraining's binary_logloss: 0.568802\n",
      "[58]\ttraining's binary_logloss: 0.568329\n",
      "[59]\ttraining's binary_logloss: 0.567903\n",
      "[60]\ttraining's binary_logloss: 0.567521\n",
      "[61]\ttraining's binary_logloss: 0.567223\n",
      "[62]\ttraining's binary_logloss: 0.566953\n",
      "[63]\ttraining's binary_logloss: 0.566651\n",
      "[64]\ttraining's binary_logloss: 0.566302\n",
      "[65]\ttraining's binary_logloss: 0.565949\n",
      "[66]\ttraining's binary_logloss: 0.565623\n",
      "[67]\ttraining's binary_logloss: 0.565279\n",
      "[68]\ttraining's binary_logloss: 0.564954\n",
      "[69]\ttraining's binary_logloss: 0.564539\n",
      "[70]\ttraining's binary_logloss: 0.564261\n",
      "[71]\ttraining's binary_logloss: 0.563952\n",
      "[72]\ttraining's binary_logloss: 0.563678\n",
      "[73]\ttraining's binary_logloss: 0.563482\n",
      "[74]\ttraining's binary_logloss: 0.563098\n",
      "[75]\ttraining's binary_logloss: 0.562868\n",
      "[76]\ttraining's binary_logloss: 0.562527\n",
      "[77]\ttraining's binary_logloss: 0.562208\n",
      "[78]\ttraining's binary_logloss: 0.561945\n",
      "[79]\ttraining's binary_logloss: 0.561602\n",
      "[80]\ttraining's binary_logloss: 0.561302\n",
      "[81]\ttraining's binary_logloss: 0.561\n",
      "[82]\ttraining's binary_logloss: 0.560753\n",
      "[83]\ttraining's binary_logloss: 0.560448\n",
      "[84]\ttraining's binary_logloss: 0.560219\n",
      "[85]\ttraining's binary_logloss: 0.559976\n",
      "[86]\ttraining's binary_logloss: 0.559661\n",
      "[87]\ttraining's binary_logloss: 0.559427\n",
      "[88]\ttraining's binary_logloss: 0.559171\n",
      "[89]\ttraining's binary_logloss: 0.558935\n",
      "[90]\ttraining's binary_logloss: 0.558638\n",
      "[91]\ttraining's binary_logloss: 0.558326\n",
      "[92]\ttraining's binary_logloss: 0.557987\n",
      "[93]\ttraining's binary_logloss: 0.557749\n",
      "[94]\ttraining's binary_logloss: 0.557557\n",
      "[95]\ttraining's binary_logloss: 0.557268\n",
      "[96]\ttraining's binary_logloss: 0.556977\n",
      "[97]\ttraining's binary_logloss: 0.556674\n",
      "[98]\ttraining's binary_logloss: 0.556364\n",
      "[99]\ttraining's binary_logloss: 0.556105\n",
      "[100]\ttraining's binary_logloss: 0.555852\n",
      "[101]\ttraining's binary_logloss: 0.555561\n",
      "[102]\ttraining's binary_logloss: 0.555314\n",
      "[103]\ttraining's binary_logloss: 0.554996\n",
      "[104]\ttraining's binary_logloss: 0.554814\n",
      "[105]\ttraining's binary_logloss: 0.554589\n",
      "[106]\ttraining's binary_logloss: 0.554296\n",
      "[107]\ttraining's binary_logloss: 0.554059\n",
      "[108]\ttraining's binary_logloss: 0.553875\n",
      "[109]\ttraining's binary_logloss: 0.553637\n",
      "[110]\ttraining's binary_logloss: 0.553318\n",
      "[111]\ttraining's binary_logloss: 0.553091\n",
      "[112]\ttraining's binary_logloss: 0.552917\n",
      "[113]\ttraining's binary_logloss: 0.552749\n",
      "[114]\ttraining's binary_logloss: 0.552485\n",
      "[115]\ttraining's binary_logloss: 0.552213\n",
      "[116]\ttraining's binary_logloss: 0.55188\n",
      "[117]\ttraining's binary_logloss: 0.551536\n",
      "[118]\ttraining's binary_logloss: 0.551318\n",
      "[119]\ttraining's binary_logloss: 0.551114\n",
      "[120]\ttraining's binary_logloss: 0.550837\n",
      "[121]\ttraining's binary_logloss: 0.550632\n",
      "[122]\ttraining's binary_logloss: 0.550362\n",
      "[123]\ttraining's binary_logloss: 0.550071\n",
      "[124]\ttraining's binary_logloss: 0.549858\n",
      "[125]\ttraining's binary_logloss: 0.549697\n",
      "[126]\ttraining's binary_logloss: 0.54944\n",
      "[127]\ttraining's binary_logloss: 0.549271\n",
      "[128]\ttraining's binary_logloss: 0.548994\n",
      "[129]\ttraining's binary_logloss: 0.548805\n",
      "[130]\ttraining's binary_logloss: 0.548558\n",
      "[131]\ttraining's binary_logloss: 0.548351\n",
      "[132]\ttraining's binary_logloss: 0.548144\n",
      "[133]\ttraining's binary_logloss: 0.547895\n",
      "[134]\ttraining's binary_logloss: 0.547648\n",
      "[135]\ttraining's binary_logloss: 0.547381\n",
      "[136]\ttraining's binary_logloss: 0.547155\n",
      "[137]\ttraining's binary_logloss: 0.546884\n",
      "[138]\ttraining's binary_logloss: 0.546658\n",
      "[139]\ttraining's binary_logloss: 0.546403\n",
      "[140]\ttraining's binary_logloss: 0.546274\n",
      "[141]\ttraining's binary_logloss: 0.546016\n",
      "[142]\ttraining's binary_logloss: 0.545819\n",
      "[143]\ttraining's binary_logloss: 0.545659\n",
      "[144]\ttraining's binary_logloss: 0.545511\n",
      "[145]\ttraining's binary_logloss: 0.545351\n",
      "[146]\ttraining's binary_logloss: 0.545155\n",
      "[147]\ttraining's binary_logloss: 0.544932\n",
      "[148]\ttraining's binary_logloss: 0.544764\n",
      "[149]\ttraining's binary_logloss: 0.544435\n",
      "[150]\ttraining's binary_logloss: 0.544229\n",
      "[151]\ttraining's binary_logloss: 0.543983\n",
      "[152]\ttraining's binary_logloss: 0.543728\n",
      "[153]\ttraining's binary_logloss: 0.543585\n",
      "[154]\ttraining's binary_logloss: 0.543335\n",
      "[155]\ttraining's binary_logloss: 0.543105\n",
      "[156]\ttraining's binary_logloss: 0.542911\n",
      "[157]\ttraining's binary_logloss: 0.542675\n",
      "[158]\ttraining's binary_logloss: 0.542439\n",
      "[159]\ttraining's binary_logloss: 0.5423\n",
      "[160]\ttraining's binary_logloss: 0.542053\n",
      "[161]\ttraining's binary_logloss: 0.541883\n",
      "[162]\ttraining's binary_logloss: 0.541685\n",
      "[163]\ttraining's binary_logloss: 0.541495\n",
      "[164]\ttraining's binary_logloss: 0.541345\n",
      "[165]\ttraining's binary_logloss: 0.541225\n",
      "[166]\ttraining's binary_logloss: 0.541012\n",
      "[167]\ttraining's binary_logloss: 0.540823\n",
      "[168]\ttraining's binary_logloss: 0.540631\n",
      "[169]\ttraining's binary_logloss: 0.540449\n",
      "[170]\ttraining's binary_logloss: 0.540231\n",
      "[171]\ttraining's binary_logloss: 0.539952\n",
      "[172]\ttraining's binary_logloss: 0.539802\n",
      "[173]\ttraining's binary_logloss: 0.539524\n",
      "[174]\ttraining's binary_logloss: 0.53938\n",
      "[175]\ttraining's binary_logloss: 0.53917\n",
      "[176]\ttraining's binary_logloss: 0.538953\n",
      "[177]\ttraining's binary_logloss: 0.538717\n",
      "[178]\ttraining's binary_logloss: 0.538486\n",
      "[179]\ttraining's binary_logloss: 0.538317\n",
      "[180]\ttraining's binary_logloss: 0.538166\n",
      "[181]\ttraining's binary_logloss: 0.537896\n",
      "[182]\ttraining's binary_logloss: 0.537605\n",
      "[183]\ttraining's binary_logloss: 0.537406\n",
      "[184]\ttraining's binary_logloss: 0.537169\n",
      "[185]\ttraining's binary_logloss: 0.536928\n",
      "[186]\ttraining's binary_logloss: 0.536683\n",
      "[187]\ttraining's binary_logloss: 0.536511\n",
      "[188]\ttraining's binary_logloss: 0.536364\n",
      "[189]\ttraining's binary_logloss: 0.536229\n",
      "[190]\ttraining's binary_logloss: 0.535961\n",
      "[191]\ttraining's binary_logloss: 0.535815\n",
      "[192]\ttraining's binary_logloss: 0.535647\n",
      "[193]\ttraining's binary_logloss: 0.535393\n",
      "[194]\ttraining's binary_logloss: 0.535139\n",
      "[195]\ttraining's binary_logloss: 0.534887\n",
      "[196]\ttraining's binary_logloss: 0.534687\n",
      "[197]\ttraining's binary_logloss: 0.534507\n",
      "[198]\ttraining's binary_logloss: 0.534299\n",
      "[199]\ttraining's binary_logloss: 0.534101\n",
      "[200]\ttraining's binary_logloss: 0.533881\n",
      "[201]\ttraining's binary_logloss: 0.533707\n",
      "[202]\ttraining's binary_logloss: 0.533481\n",
      "[203]\ttraining's binary_logloss: 0.533228\n",
      "[204]\ttraining's binary_logloss: 0.533083\n",
      "[205]\ttraining's binary_logloss: 0.532924\n",
      "[206]\ttraining's binary_logloss: 0.532758\n",
      "[207]\ttraining's binary_logloss: 0.532574\n",
      "[208]\ttraining's binary_logloss: 0.532385\n",
      "[209]\ttraining's binary_logloss: 0.532069\n",
      "[210]\ttraining's binary_logloss: 0.53196\n",
      "[211]\ttraining's binary_logloss: 0.531817\n",
      "[212]\ttraining's binary_logloss: 0.531615\n",
      "[213]\ttraining's binary_logloss: 0.531348\n",
      "[214]\ttraining's binary_logloss: 0.531182\n",
      "[215]\ttraining's binary_logloss: 0.531006\n",
      "[216]\ttraining's binary_logloss: 0.53084\n",
      "[217]\ttraining's binary_logloss: 0.530633\n",
      "[218]\ttraining's binary_logloss: 0.530433\n",
      "[219]\ttraining's binary_logloss: 0.530198\n",
      "[220]\ttraining's binary_logloss: 0.53008\n",
      "[221]\ttraining's binary_logloss: 0.529862\n",
      "[222]\ttraining's binary_logloss: 0.529695\n",
      "[223]\ttraining's binary_logloss: 0.529583\n",
      "[224]\ttraining's binary_logloss: 0.529468\n",
      "[225]\ttraining's binary_logloss: 0.529249\n",
      "[226]\ttraining's binary_logloss: 0.529043\n",
      "[227]\ttraining's binary_logloss: 0.528809\n",
      "[228]\ttraining's binary_logloss: 0.528607\n",
      "[229]\ttraining's binary_logloss: 0.528413\n",
      "[230]\ttraining's binary_logloss: 0.528164\n",
      "[231]\ttraining's binary_logloss: 0.527949\n",
      "[232]\ttraining's binary_logloss: 0.527773\n",
      "[233]\ttraining's binary_logloss: 0.52744\n",
      "[234]\ttraining's binary_logloss: 0.527209\n",
      "[235]\ttraining's binary_logloss: 0.526984\n",
      "[236]\ttraining's binary_logloss: 0.526783\n",
      "[237]\ttraining's binary_logloss: 0.526549\n",
      "[238]\ttraining's binary_logloss: 0.526364\n",
      "[239]\ttraining's binary_logloss: 0.526233\n",
      "[240]\ttraining's binary_logloss: 0.526064\n",
      "[241]\ttraining's binary_logloss: 0.525812\n",
      "[242]\ttraining's binary_logloss: 0.525578\n",
      "[243]\ttraining's binary_logloss: 0.525466\n",
      "[244]\ttraining's binary_logloss: 0.525309\n",
      "[245]\ttraining's binary_logloss: 0.525075\n",
      "[246]\ttraining's binary_logloss: 0.524932\n",
      "[247]\ttraining's binary_logloss: 0.524748\n",
      "[248]\ttraining's binary_logloss: 0.524624\n",
      "[249]\ttraining's binary_logloss: 0.524385\n",
      "[250]\ttraining's binary_logloss: 0.524211\n",
      "[251]\ttraining's binary_logloss: 0.523941\n",
      "[252]\ttraining's binary_logloss: 0.523787\n",
      "[253]\ttraining's binary_logloss: 0.523612\n",
      "[254]\ttraining's binary_logloss: 0.523421\n",
      "[255]\ttraining's binary_logloss: 0.523211\n",
      "[256]\ttraining's binary_logloss: 0.523096\n",
      "[257]\ttraining's binary_logloss: 0.522911\n",
      "[258]\ttraining's binary_logloss: 0.522737\n",
      "[259]\ttraining's binary_logloss: 0.522592\n",
      "[260]\ttraining's binary_logloss: 0.522367\n",
      "[261]\ttraining's binary_logloss: 0.522237\n",
      "[262]\ttraining's binary_logloss: 0.522139\n",
      "[263]\ttraining's binary_logloss: 0.521994\n",
      "[264]\ttraining's binary_logloss: 0.521802\n",
      "[265]\ttraining's binary_logloss: 0.521594\n",
      "[266]\ttraining's binary_logloss: 0.521445\n",
      "[267]\ttraining's binary_logloss: 0.521302\n",
      "[268]\ttraining's binary_logloss: 0.521156\n",
      "[269]\ttraining's binary_logloss: 0.520924\n",
      "[270]\ttraining's binary_logloss: 0.520802\n",
      "[271]\ttraining's binary_logloss: 0.520695\n",
      "[272]\ttraining's binary_logloss: 0.52045\n",
      "[273]\ttraining's binary_logloss: 0.520258\n",
      "[274]\ttraining's binary_logloss: 0.520119\n",
      "[275]\ttraining's binary_logloss: 0.519904\n",
      "[276]\ttraining's binary_logloss: 0.519718\n",
      "[277]\ttraining's binary_logloss: 0.519517\n",
      "[278]\ttraining's binary_logloss: 0.519358\n",
      "[279]\ttraining's binary_logloss: 0.519137\n",
      "[280]\ttraining's binary_logloss: 0.518928\n",
      "[281]\ttraining's binary_logloss: 0.518758\n",
      "[282]\ttraining's binary_logloss: 0.518617\n",
      "[283]\ttraining's binary_logloss: 0.518432\n",
      "[284]\ttraining's binary_logloss: 0.518235\n",
      "[285]\ttraining's binary_logloss: 0.518076\n",
      "[286]\ttraining's binary_logloss: 0.517902\n",
      "[287]\ttraining's binary_logloss: 0.517765\n",
      "[288]\ttraining's binary_logloss: 0.517566\n",
      "[289]\ttraining's binary_logloss: 0.517363\n",
      "[290]\ttraining's binary_logloss: 0.517185\n",
      "[291]\ttraining's binary_logloss: 0.517063\n",
      "[292]\ttraining's binary_logloss: 0.516949\n",
      "[293]\ttraining's binary_logloss: 0.516724\n",
      "[294]\ttraining's binary_logloss: 0.516606\n",
      "[295]\ttraining's binary_logloss: 0.516478\n",
      "[296]\ttraining's binary_logloss: 0.516358\n",
      "[297]\ttraining's binary_logloss: 0.516134\n",
      "[298]\ttraining's binary_logloss: 0.515972\n",
      "[299]\ttraining's binary_logloss: 0.515855\n",
      "[300]\ttraining's binary_logloss: 0.515758\n",
      "[301]\ttraining's binary_logloss: 0.515549\n",
      "[302]\ttraining's binary_logloss: 0.515324\n",
      "[303]\ttraining's binary_logloss: 0.515135\n",
      "[304]\ttraining's binary_logloss: 0.515002\n",
      "[305]\ttraining's binary_logloss: 0.514869\n",
      "[306]\ttraining's binary_logloss: 0.514722\n",
      "[307]\ttraining's binary_logloss: 0.514604\n",
      "[308]\ttraining's binary_logloss: 0.514416\n",
      "[309]\ttraining's binary_logloss: 0.514274\n",
      "[310]\ttraining's binary_logloss: 0.514173\n",
      "[311]\ttraining's binary_logloss: 0.514027\n",
      "[312]\ttraining's binary_logloss: 0.513902\n",
      "[313]\ttraining's binary_logloss: 0.513697\n",
      "[314]\ttraining's binary_logloss: 0.513577\n",
      "[315]\ttraining's binary_logloss: 0.513477\n",
      "[316]\ttraining's binary_logloss: 0.513385\n",
      "[317]\ttraining's binary_logloss: 0.513213\n",
      "[318]\ttraining's binary_logloss: 0.513084\n",
      "[319]\ttraining's binary_logloss: 0.512925\n",
      "[320]\ttraining's binary_logloss: 0.512728\n",
      "[321]\ttraining's binary_logloss: 0.512534\n",
      "[322]\ttraining's binary_logloss: 0.512377\n",
      "[323]\ttraining's binary_logloss: 0.512206\n",
      "[324]\ttraining's binary_logloss: 0.511964\n",
      "[325]\ttraining's binary_logloss: 0.511767\n",
      "[326]\ttraining's binary_logloss: 0.511654\n",
      "[327]\ttraining's binary_logloss: 0.511566\n",
      "[328]\ttraining's binary_logloss: 0.511438\n",
      "[329]\ttraining's binary_logloss: 0.511314\n",
      "[330]\ttraining's binary_logloss: 0.511167\n",
      "[331]\ttraining's binary_logloss: 0.511005\n",
      "[332]\ttraining's binary_logloss: 0.510866\n",
      "[333]\ttraining's binary_logloss: 0.510709\n",
      "[334]\ttraining's binary_logloss: 0.510605\n",
      "[335]\ttraining's binary_logloss: 0.510477\n",
      "[336]\ttraining's binary_logloss: 0.510289\n",
      "[337]\ttraining's binary_logloss: 0.510155\n",
      "[338]\ttraining's binary_logloss: 0.509992\n",
      "[339]\ttraining's binary_logloss: 0.509879\n",
      "[340]\ttraining's binary_logloss: 0.509773\n",
      "[341]\ttraining's binary_logloss: 0.509652\n",
      "[342]\ttraining's binary_logloss: 0.509516\n",
      "[343]\ttraining's binary_logloss: 0.509373\n",
      "[344]\ttraining's binary_logloss: 0.509185\n",
      "[345]\ttraining's binary_logloss: 0.509024\n",
      "[346]\ttraining's binary_logloss: 0.508872\n",
      "[347]\ttraining's binary_logloss: 0.508713\n",
      "[348]\ttraining's binary_logloss: 0.508581\n",
      "[349]\ttraining's binary_logloss: 0.508506\n",
      "[350]\ttraining's binary_logloss: 0.508319\n",
      "[351]\ttraining's binary_logloss: 0.508188\n",
      "[352]\ttraining's binary_logloss: 0.507979\n",
      "[353]\ttraining's binary_logloss: 0.507885\n",
      "[354]\ttraining's binary_logloss: 0.507774\n",
      "[355]\ttraining's binary_logloss: 0.507569\n",
      "[356]\ttraining's binary_logloss: 0.507382\n",
      "[357]\ttraining's binary_logloss: 0.507216\n",
      "[358]\ttraining's binary_logloss: 0.507041\n",
      "[359]\ttraining's binary_logloss: 0.506883\n",
      "[360]\ttraining's binary_logloss: 0.506773\n",
      "[361]\ttraining's binary_logloss: 0.506625\n",
      "[362]\ttraining's binary_logloss: 0.506412\n",
      "[363]\ttraining's binary_logloss: 0.506283\n",
      "[364]\ttraining's binary_logloss: 0.506137\n",
      "[365]\ttraining's binary_logloss: 0.506006\n",
      "[366]\ttraining's binary_logloss: 0.505889\n",
      "[367]\ttraining's binary_logloss: 0.505767\n",
      "[368]\ttraining's binary_logloss: 0.505686\n",
      "[369]\ttraining's binary_logloss: 0.505584\n",
      "[370]\ttraining's binary_logloss: 0.505375\n",
      "[371]\ttraining's binary_logloss: 0.505292\n",
      "[372]\ttraining's binary_logloss: 0.505186\n",
      "[373]\ttraining's binary_logloss: 0.504896\n",
      "[374]\ttraining's binary_logloss: 0.504785\n",
      "[375]\ttraining's binary_logloss: 0.504651\n",
      "[376]\ttraining's binary_logloss: 0.504527\n",
      "[377]\ttraining's binary_logloss: 0.504437\n",
      "[378]\ttraining's binary_logloss: 0.504246\n",
      "[379]\ttraining's binary_logloss: 0.504133\n",
      "[380]\ttraining's binary_logloss: 0.503959\n",
      "[381]\ttraining's binary_logloss: 0.503819\n",
      "[382]\ttraining's binary_logloss: 0.503661\n",
      "[383]\ttraining's binary_logloss: 0.503573\n",
      "[384]\ttraining's binary_logloss: 0.503427\n",
      "[385]\ttraining's binary_logloss: 0.503211\n",
      "[386]\ttraining's binary_logloss: 0.503052\n",
      "[387]\ttraining's binary_logloss: 0.502924\n",
      "[388]\ttraining's binary_logloss: 0.502776\n",
      "[389]\ttraining's binary_logloss: 0.502645\n",
      "[390]\ttraining's binary_logloss: 0.502505\n",
      "[391]\ttraining's binary_logloss: 0.502316\n",
      "[392]\ttraining's binary_logloss: 0.502187\n",
      "[393]\ttraining's binary_logloss: 0.502058\n",
      "[394]\ttraining's binary_logloss: 0.501894\n",
      "[395]\ttraining's binary_logloss: 0.501759\n",
      "[396]\ttraining's binary_logloss: 0.501558\n",
      "[397]\ttraining's binary_logloss: 0.501413\n",
      "[398]\ttraining's binary_logloss: 0.501316\n",
      "[399]\ttraining's binary_logloss: 0.501176\n",
      "[400]\ttraining's binary_logloss: 0.501055\n",
      "[401]\ttraining's binary_logloss: 0.500928\n",
      "[402]\ttraining's binary_logloss: 0.500761\n",
      "[403]\ttraining's binary_logloss: 0.500668\n",
      "[404]\ttraining's binary_logloss: 0.500543\n",
      "[405]\ttraining's binary_logloss: 0.500402\n",
      "[406]\ttraining's binary_logloss: 0.500267\n",
      "[407]\ttraining's binary_logloss: 0.500161\n",
      "[408]\ttraining's binary_logloss: 0.500062\n",
      "[409]\ttraining's binary_logloss: 0.499921\n",
      "[410]\ttraining's binary_logloss: 0.499807\n",
      "[411]\ttraining's binary_logloss: 0.499607\n",
      "[412]\ttraining's binary_logloss: 0.499425\n",
      "[413]\ttraining's binary_logloss: 0.49924\n",
      "[414]\ttraining's binary_logloss: 0.499064\n",
      "[415]\ttraining's binary_logloss: 0.498891\n",
      "[416]\ttraining's binary_logloss: 0.498716\n",
      "[417]\ttraining's binary_logloss: 0.498522\n",
      "[418]\ttraining's binary_logloss: 0.498374\n",
      "[419]\ttraining's binary_logloss: 0.498228\n",
      "[420]\ttraining's binary_logloss: 0.498068\n",
      "[421]\ttraining's binary_logloss: 0.497929\n",
      "[422]\ttraining's binary_logloss: 0.497798\n",
      "[423]\ttraining's binary_logloss: 0.497711\n",
      "[424]\ttraining's binary_logloss: 0.49759\n",
      "[425]\ttraining's binary_logloss: 0.497517\n",
      "[426]\ttraining's binary_logloss: 0.497441\n",
      "[427]\ttraining's binary_logloss: 0.497311\n",
      "[428]\ttraining's binary_logloss: 0.497187\n",
      "[429]\ttraining's binary_logloss: 0.497078\n",
      "[430]\ttraining's binary_logloss: 0.496973\n",
      "[431]\ttraining's binary_logloss: 0.496839\n",
      "[432]\ttraining's binary_logloss: 0.496677\n",
      "[433]\ttraining's binary_logloss: 0.496414\n",
      "[434]\ttraining's binary_logloss: 0.496302\n",
      "[435]\ttraining's binary_logloss: 0.4961\n",
      "[436]\ttraining's binary_logloss: 0.495954\n",
      "[437]\ttraining's binary_logloss: 0.495792\n",
      "[438]\ttraining's binary_logloss: 0.495639\n",
      "[439]\ttraining's binary_logloss: 0.495489\n",
      "[440]\ttraining's binary_logloss: 0.495268\n",
      "[441]\ttraining's binary_logloss: 0.495081\n",
      "[442]\ttraining's binary_logloss: 0.494945\n",
      "[443]\ttraining's binary_logloss: 0.494788\n",
      "[444]\ttraining's binary_logloss: 0.494641\n",
      "[445]\ttraining's binary_logloss: 0.494469\n",
      "[446]\ttraining's binary_logloss: 0.494293\n",
      "[447]\ttraining's binary_logloss: 0.494149\n",
      "[448]\ttraining's binary_logloss: 0.49404\n",
      "[449]\ttraining's binary_logloss: 0.493929\n",
      "[450]\ttraining's binary_logloss: 0.493802\n",
      "[451]\ttraining's binary_logloss: 0.493594\n",
      "[452]\ttraining's binary_logloss: 0.493461\n",
      "[453]\ttraining's binary_logloss: 0.493257\n",
      "[454]\ttraining's binary_logloss: 0.493138\n",
      "[455]\ttraining's binary_logloss: 0.4929\n",
      "[456]\ttraining's binary_logloss: 0.492708\n",
      "[457]\ttraining's binary_logloss: 0.492523\n",
      "[458]\ttraining's binary_logloss: 0.492346\n",
      "[459]\ttraining's binary_logloss: 0.492225\n",
      "[460]\ttraining's binary_logloss: 0.492074\n",
      "[461]\ttraining's binary_logloss: 0.491995\n",
      "[462]\ttraining's binary_logloss: 0.491841\n",
      "[463]\ttraining's binary_logloss: 0.491669\n",
      "[464]\ttraining's binary_logloss: 0.491585\n",
      "[465]\ttraining's binary_logloss: 0.49137\n",
      "[466]\ttraining's binary_logloss: 0.491247\n",
      "[467]\ttraining's binary_logloss: 0.491023\n",
      "[468]\ttraining's binary_logloss: 0.490787\n",
      "[469]\ttraining's binary_logloss: 0.490642\n",
      "[470]\ttraining's binary_logloss: 0.49047\n",
      "[471]\ttraining's binary_logloss: 0.49032\n",
      "[472]\ttraining's binary_logloss: 0.49025\n",
      "[473]\ttraining's binary_logloss: 0.490111\n",
      "[474]\ttraining's binary_logloss: 0.490028\n",
      "[475]\ttraining's binary_logloss: 0.489879\n",
      "[476]\ttraining's binary_logloss: 0.489743\n",
      "[477]\ttraining's binary_logloss: 0.489652\n",
      "[478]\ttraining's binary_logloss: 0.489449\n",
      "[479]\ttraining's binary_logloss: 0.489248\n",
      "[480]\ttraining's binary_logloss: 0.489098\n",
      "[481]\ttraining's binary_logloss: 0.488954\n",
      "[482]\ttraining's binary_logloss: 0.48875\n",
      "[483]\ttraining's binary_logloss: 0.488541\n",
      "[484]\ttraining's binary_logloss: 0.488423\n",
      "[485]\ttraining's binary_logloss: 0.488296\n",
      "[486]\ttraining's binary_logloss: 0.488137\n",
      "[487]\ttraining's binary_logloss: 0.487985\n",
      "[488]\ttraining's binary_logloss: 0.487906\n",
      "[489]\ttraining's binary_logloss: 0.487767\n",
      "[490]\ttraining's binary_logloss: 0.487648\n",
      "[491]\ttraining's binary_logloss: 0.487574\n",
      "[492]\ttraining's binary_logloss: 0.487436\n",
      "[493]\ttraining's binary_logloss: 0.487338\n",
      "[494]\ttraining's binary_logloss: 0.48721\n",
      "[495]\ttraining's binary_logloss: 0.487064\n",
      "[496]\ttraining's binary_logloss: 0.486942\n",
      "[497]\ttraining's binary_logloss: 0.486792\n",
      "[498]\ttraining's binary_logloss: 0.486698\n",
      "[499]\ttraining's binary_logloss: 0.48659\n",
      "[500]\ttraining's binary_logloss: 0.486445\n",
      "[501]\ttraining's binary_logloss: 0.486385\n",
      "[502]\ttraining's binary_logloss: 0.486237\n",
      "[503]\ttraining's binary_logloss: 0.486115\n",
      "[504]\ttraining's binary_logloss: 0.485927\n",
      "[505]\ttraining's binary_logloss: 0.485731\n",
      "[506]\ttraining's binary_logloss: 0.485578\n",
      "[507]\ttraining's binary_logloss: 0.485432\n",
      "[508]\ttraining's binary_logloss: 0.485369\n",
      "[509]\ttraining's binary_logloss: 0.485264\n",
      "[510]\ttraining's binary_logloss: 0.485204\n",
      "[511]\ttraining's binary_logloss: 0.485053\n",
      "[512]\ttraining's binary_logloss: 0.484837\n",
      "[513]\ttraining's binary_logloss: 0.484697\n",
      "[514]\ttraining's binary_logloss: 0.484614\n",
      "[515]\ttraining's binary_logloss: 0.484471\n",
      "[516]\ttraining's binary_logloss: 0.484355\n",
      "[517]\ttraining's binary_logloss: 0.484278\n",
      "[518]\ttraining's binary_logloss: 0.484161\n",
      "[519]\ttraining's binary_logloss: 0.484017\n",
      "[520]\ttraining's binary_logloss: 0.483871\n",
      "[521]\ttraining's binary_logloss: 0.483718\n",
      "[522]\ttraining's binary_logloss: 0.483561\n",
      "[523]\ttraining's binary_logloss: 0.483437\n",
      "[524]\ttraining's binary_logloss: 0.48327\n",
      "[525]\ttraining's binary_logloss: 0.483139\n",
      "[526]\ttraining's binary_logloss: 0.483041\n",
      "[527]\ttraining's binary_logloss: 0.482976\n",
      "[528]\ttraining's binary_logloss: 0.482896\n",
      "[529]\ttraining's binary_logloss: 0.482774\n",
      "[530]\ttraining's binary_logloss: 0.482655\n",
      "[531]\ttraining's binary_logloss: 0.482452\n",
      "[532]\ttraining's binary_logloss: 0.482208\n",
      "[533]\ttraining's binary_logloss: 0.482015\n",
      "[534]\ttraining's binary_logloss: 0.481875\n",
      "[535]\ttraining's binary_logloss: 0.48182\n",
      "[536]\ttraining's binary_logloss: 0.481705\n",
      "[537]\ttraining's binary_logloss: 0.481619\n",
      "[538]\ttraining's binary_logloss: 0.481566\n",
      "[539]\ttraining's binary_logloss: 0.481444\n",
      "[540]\ttraining's binary_logloss: 0.481317\n",
      "[541]\ttraining's binary_logloss: 0.481087\n",
      "[542]\ttraining's binary_logloss: 0.4809\n",
      "[543]\ttraining's binary_logloss: 0.480727\n",
      "[544]\ttraining's binary_logloss: 0.48066\n",
      "[545]\ttraining's binary_logloss: 0.480512\n",
      "[546]\ttraining's binary_logloss: 0.480356\n",
      "[547]\ttraining's binary_logloss: 0.480271\n",
      "[548]\ttraining's binary_logloss: 0.480147\n",
      "[549]\ttraining's binary_logloss: 0.480047\n",
      "[550]\ttraining's binary_logloss: 0.479899\n",
      "[551]\ttraining's binary_logloss: 0.47976\n",
      "[552]\ttraining's binary_logloss: 0.479687\n",
      "[553]\ttraining's binary_logloss: 0.479503\n",
      "[554]\ttraining's binary_logloss: 0.479271\n",
      "[555]\ttraining's binary_logloss: 0.47911\n",
      "[556]\ttraining's binary_logloss: 0.478939\n",
      "[557]\ttraining's binary_logloss: 0.478815\n",
      "[558]\ttraining's binary_logloss: 0.478681\n",
      "[559]\ttraining's binary_logloss: 0.478574\n",
      "[560]\ttraining's binary_logloss: 0.478449\n",
      "[561]\ttraining's binary_logloss: 0.478283\n",
      "[562]\ttraining's binary_logloss: 0.478128\n",
      "[563]\ttraining's binary_logloss: 0.477999\n",
      "[564]\ttraining's binary_logloss: 0.477863\n",
      "[565]\ttraining's binary_logloss: 0.477737\n",
      "[566]\ttraining's binary_logloss: 0.477555\n",
      "[567]\ttraining's binary_logloss: 0.477435\n",
      "[568]\ttraining's binary_logloss: 0.477335\n",
      "[569]\ttraining's binary_logloss: 0.477182\n",
      "[570]\ttraining's binary_logloss: 0.476987\n",
      "[571]\ttraining's binary_logloss: 0.476769\n",
      "[572]\ttraining's binary_logloss: 0.47669\n",
      "[573]\ttraining's binary_logloss: 0.476541\n",
      "[574]\ttraining's binary_logloss: 0.476399\n",
      "[575]\ttraining's binary_logloss: 0.476295\n",
      "[576]\ttraining's binary_logloss: 0.476219\n",
      "[577]\ttraining's binary_logloss: 0.476067\n",
      "[578]\ttraining's binary_logloss: 0.475861\n",
      "[579]\ttraining's binary_logloss: 0.475688\n",
      "[580]\ttraining's binary_logloss: 0.475621\n",
      "[581]\ttraining's binary_logloss: 0.47549\n",
      "[582]\ttraining's binary_logloss: 0.475425\n",
      "[583]\ttraining's binary_logloss: 0.475299\n",
      "[584]\ttraining's binary_logloss: 0.475218\n",
      "[585]\ttraining's binary_logloss: 0.475103\n",
      "[586]\ttraining's binary_logloss: 0.475023\n",
      "[587]\ttraining's binary_logloss: 0.474877\n",
      "[588]\ttraining's binary_logloss: 0.474806\n",
      "[589]\ttraining's binary_logloss: 0.474642\n",
      "[590]\ttraining's binary_logloss: 0.47448\n",
      "[591]\ttraining's binary_logloss: 0.474361\n",
      "[592]\ttraining's binary_logloss: 0.4742\n",
      "[593]\ttraining's binary_logloss: 0.474037\n",
      "[594]\ttraining's binary_logloss: 0.473923\n",
      "[595]\ttraining's binary_logloss: 0.47379\n",
      "[596]\ttraining's binary_logloss: 0.473665\n",
      "[597]\ttraining's binary_logloss: 0.473546\n",
      "[598]\ttraining's binary_logloss: 0.473405\n",
      "[599]\ttraining's binary_logloss: 0.473277\n",
      "[600]\ttraining's binary_logloss: 0.473123\n",
      "[1]\ttraining's binary_logloss: 0.62902\n",
      "[2]\ttraining's binary_logloss: 0.6234\n",
      "[3]\ttraining's binary_logloss: 0.618577\n",
      "[4]\ttraining's binary_logloss: 0.614496\n",
      "[5]\ttraining's binary_logloss: 0.611207\n",
      "[6]\ttraining's binary_logloss: 0.60842\n",
      "[7]\ttraining's binary_logloss: 0.605822\n",
      "[8]\ttraining's binary_logloss: 0.60356\n",
      "[9]\ttraining's binary_logloss: 0.601606\n",
      "[10]\ttraining's binary_logloss: 0.599688\n",
      "[11]\ttraining's binary_logloss: 0.597967\n",
      "[12]\ttraining's binary_logloss: 0.596567\n",
      "[13]\ttraining's binary_logloss: 0.595091\n",
      "[14]\ttraining's binary_logloss: 0.593747\n",
      "[15]\ttraining's binary_logloss: 0.592669\n",
      "[16]\ttraining's binary_logloss: 0.591589\n",
      "[17]\ttraining's binary_logloss: 0.590556\n",
      "[18]\ttraining's binary_logloss: 0.589746\n",
      "[19]\ttraining's binary_logloss: 0.588872\n",
      "[20]\ttraining's binary_logloss: 0.588017\n",
      "[21]\ttraining's binary_logloss: 0.587175\n",
      "[22]\ttraining's binary_logloss: 0.586436\n",
      "[23]\ttraining's binary_logloss: 0.585739\n",
      "[24]\ttraining's binary_logloss: 0.584979\n",
      "[25]\ttraining's binary_logloss: 0.584267\n",
      "[26]\ttraining's binary_logloss: 0.583645\n",
      "[27]\ttraining's binary_logloss: 0.583049\n",
      "[28]\ttraining's binary_logloss: 0.582465\n",
      "[29]\ttraining's binary_logloss: 0.581815\n",
      "[30]\ttraining's binary_logloss: 0.581266\n",
      "[31]\ttraining's binary_logloss: 0.580749\n",
      "[32]\ttraining's binary_logloss: 0.580222\n",
      "[33]\ttraining's binary_logloss: 0.579616\n",
      "[34]\ttraining's binary_logloss: 0.579029\n",
      "[35]\ttraining's binary_logloss: 0.578498\n",
      "[36]\ttraining's binary_logloss: 0.578025\n",
      "[37]\ttraining's binary_logloss: 0.57757\n",
      "[38]\ttraining's binary_logloss: 0.577018\n",
      "[39]\ttraining's binary_logloss: 0.57663\n",
      "[40]\ttraining's binary_logloss: 0.576198\n",
      "[41]\ttraining's binary_logloss: 0.575726\n",
      "[42]\ttraining's binary_logloss: 0.575231\n",
      "[43]\ttraining's binary_logloss: 0.574813\n",
      "[44]\ttraining's binary_logloss: 0.574326\n",
      "[45]\ttraining's binary_logloss: 0.573923\n",
      "[46]\ttraining's binary_logloss: 0.573571\n",
      "[47]\ttraining's binary_logloss: 0.573124\n",
      "[48]\ttraining's binary_logloss: 0.572715\n",
      "[49]\ttraining's binary_logloss: 0.572362\n",
      "[50]\ttraining's binary_logloss: 0.571973\n",
      "[51]\ttraining's binary_logloss: 0.571657\n",
      "[52]\ttraining's binary_logloss: 0.571248\n",
      "[53]\ttraining's binary_logloss: 0.570899\n",
      "[54]\ttraining's binary_logloss: 0.570596\n",
      "[55]\ttraining's binary_logloss: 0.570286\n",
      "[56]\ttraining's binary_logloss: 0.569993\n",
      "[57]\ttraining's binary_logloss: 0.569618\n",
      "[58]\ttraining's binary_logloss: 0.569217\n",
      "[59]\ttraining's binary_logloss: 0.568946\n",
      "[60]\ttraining's binary_logloss: 0.568611\n",
      "[61]\ttraining's binary_logloss: 0.568327\n",
      "[62]\ttraining's binary_logloss: 0.567982\n",
      "[63]\ttraining's binary_logloss: 0.567609\n",
      "[64]\ttraining's binary_logloss: 0.567316\n",
      "[65]\ttraining's binary_logloss: 0.567069\n",
      "[66]\ttraining's binary_logloss: 0.566753\n",
      "[67]\ttraining's binary_logloss: 0.566477\n",
      "[68]\ttraining's binary_logloss: 0.566274\n",
      "[69]\ttraining's binary_logloss: 0.56599\n",
      "[70]\ttraining's binary_logloss: 0.565699\n",
      "[71]\ttraining's binary_logloss: 0.565355\n",
      "[72]\ttraining's binary_logloss: 0.565077\n",
      "[73]\ttraining's binary_logloss: 0.564866\n",
      "[74]\ttraining's binary_logloss: 0.564603\n",
      "[75]\ttraining's binary_logloss: 0.564309\n",
      "[76]\ttraining's binary_logloss: 0.564114\n",
      "[77]\ttraining's binary_logloss: 0.56374\n",
      "[78]\ttraining's binary_logloss: 0.563463\n",
      "[79]\ttraining's binary_logloss: 0.563196\n",
      "[80]\ttraining's binary_logloss: 0.562946\n",
      "[81]\ttraining's binary_logloss: 0.562724\n",
      "[82]\ttraining's binary_logloss: 0.562472\n",
      "[83]\ttraining's binary_logloss: 0.562188\n",
      "[84]\ttraining's binary_logloss: 0.562035\n",
      "[85]\ttraining's binary_logloss: 0.561802\n",
      "[86]\ttraining's binary_logloss: 0.561515\n",
      "[87]\ttraining's binary_logloss: 0.561322\n",
      "[88]\ttraining's binary_logloss: 0.561039\n",
      "[89]\ttraining's binary_logloss: 0.560828\n",
      "[90]\ttraining's binary_logloss: 0.560637\n",
      "[91]\ttraining's binary_logloss: 0.560489\n",
      "[92]\ttraining's binary_logloss: 0.560295\n",
      "[93]\ttraining's binary_logloss: 0.560094\n",
      "[94]\ttraining's binary_logloss: 0.559781\n",
      "[95]\ttraining's binary_logloss: 0.559491\n",
      "[96]\ttraining's binary_logloss: 0.559243\n",
      "[97]\ttraining's binary_logloss: 0.559034\n",
      "[98]\ttraining's binary_logloss: 0.558807\n",
      "[99]\ttraining's binary_logloss: 0.558456\n",
      "[100]\ttraining's binary_logloss: 0.558107\n",
      "[101]\ttraining's binary_logloss: 0.55792\n",
      "[102]\ttraining's binary_logloss: 0.557615\n",
      "[103]\ttraining's binary_logloss: 0.557366\n",
      "[104]\ttraining's binary_logloss: 0.55706\n",
      "[105]\ttraining's binary_logloss: 0.556762\n",
      "[106]\ttraining's binary_logloss: 0.556548\n",
      "[107]\ttraining's binary_logloss: 0.55636\n",
      "[108]\ttraining's binary_logloss: 0.556117\n",
      "[109]\ttraining's binary_logloss: 0.555889\n",
      "[110]\ttraining's binary_logloss: 0.555599\n",
      "[111]\ttraining's binary_logloss: 0.555453\n",
      "[112]\ttraining's binary_logloss: 0.555264\n",
      "[113]\ttraining's binary_logloss: 0.555055\n",
      "[114]\ttraining's binary_logloss: 0.554874\n",
      "[115]\ttraining's binary_logloss: 0.554548\n",
      "[116]\ttraining's binary_logloss: 0.554312\n",
      "[117]\ttraining's binary_logloss: 0.554038\n",
      "[118]\ttraining's binary_logloss: 0.553781\n",
      "[119]\ttraining's binary_logloss: 0.553576\n",
      "[120]\ttraining's binary_logloss: 0.553368\n",
      "[121]\ttraining's binary_logloss: 0.553219\n",
      "[122]\ttraining's binary_logloss: 0.553059\n",
      "[123]\ttraining's binary_logloss: 0.552736\n",
      "[124]\ttraining's binary_logloss: 0.552407\n",
      "[125]\ttraining's binary_logloss: 0.552057\n",
      "[126]\ttraining's binary_logloss: 0.551885\n",
      "[127]\ttraining's binary_logloss: 0.551618\n",
      "[128]\ttraining's binary_logloss: 0.551394\n",
      "[129]\ttraining's binary_logloss: 0.551128\n",
      "[130]\ttraining's binary_logloss: 0.550991\n",
      "[131]\ttraining's binary_logloss: 0.550749\n",
      "[132]\ttraining's binary_logloss: 0.550588\n",
      "[133]\ttraining's binary_logloss: 0.550433\n",
      "[134]\ttraining's binary_logloss: 0.55016\n",
      "[135]\ttraining's binary_logloss: 0.549909\n",
      "[136]\ttraining's binary_logloss: 0.549702\n",
      "[137]\ttraining's binary_logloss: 0.549511\n",
      "[138]\ttraining's binary_logloss: 0.549267\n",
      "[139]\ttraining's binary_logloss: 0.54899\n",
      "[140]\ttraining's binary_logloss: 0.54884\n",
      "[141]\ttraining's binary_logloss: 0.548576\n",
      "[142]\ttraining's binary_logloss: 0.548332\n",
      "[143]\ttraining's binary_logloss: 0.548132\n",
      "[144]\ttraining's binary_logloss: 0.547832\n",
      "[145]\ttraining's binary_logloss: 0.547618\n",
      "[146]\ttraining's binary_logloss: 0.547414\n",
      "[147]\ttraining's binary_logloss: 0.547258\n",
      "[148]\ttraining's binary_logloss: 0.547043\n",
      "[149]\ttraining's binary_logloss: 0.546848\n",
      "[150]\ttraining's binary_logloss: 0.546669\n",
      "[151]\ttraining's binary_logloss: 0.546543\n",
      "[152]\ttraining's binary_logloss: 0.546398\n",
      "[153]\ttraining's binary_logloss: 0.546194\n",
      "[154]\ttraining's binary_logloss: 0.546021\n",
      "[155]\ttraining's binary_logloss: 0.545793\n",
      "[156]\ttraining's binary_logloss: 0.54565\n",
      "[157]\ttraining's binary_logloss: 0.545465\n",
      "[158]\ttraining's binary_logloss: 0.545259\n",
      "[159]\ttraining's binary_logloss: 0.54505\n",
      "[160]\ttraining's binary_logloss: 0.544863\n",
      "[161]\ttraining's binary_logloss: 0.544582\n",
      "[162]\ttraining's binary_logloss: 0.544371\n",
      "[163]\ttraining's binary_logloss: 0.544225\n",
      "[164]\ttraining's binary_logloss: 0.544002\n",
      "[165]\ttraining's binary_logloss: 0.543868\n",
      "[166]\ttraining's binary_logloss: 0.543608\n",
      "[167]\ttraining's binary_logloss: 0.543415\n",
      "[168]\ttraining's binary_logloss: 0.543287\n",
      "[169]\ttraining's binary_logloss: 0.543092\n",
      "[170]\ttraining's binary_logloss: 0.542949\n",
      "[171]\ttraining's binary_logloss: 0.542765\n",
      "[172]\ttraining's binary_logloss: 0.54256\n",
      "[173]\ttraining's binary_logloss: 0.5423\n",
      "[174]\ttraining's binary_logloss: 0.542097\n",
      "[175]\ttraining's binary_logloss: 0.541871\n",
      "[176]\ttraining's binary_logloss: 0.541625\n",
      "[177]\ttraining's binary_logloss: 0.541456\n",
      "[178]\ttraining's binary_logloss: 0.541275\n",
      "[179]\ttraining's binary_logloss: 0.541133\n",
      "[180]\ttraining's binary_logloss: 0.540857\n",
      "[181]\ttraining's binary_logloss: 0.540649\n",
      "[182]\ttraining's binary_logloss: 0.540477\n",
      "[183]\ttraining's binary_logloss: 0.540284\n",
      "[184]\ttraining's binary_logloss: 0.540031\n",
      "[185]\ttraining's binary_logloss: 0.53987\n",
      "[186]\ttraining's binary_logloss: 0.539659\n",
      "[187]\ttraining's binary_logloss: 0.539432\n",
      "[188]\ttraining's binary_logloss: 0.539272\n",
      "[189]\ttraining's binary_logloss: 0.539083\n",
      "[190]\ttraining's binary_logloss: 0.538897\n",
      "[191]\ttraining's binary_logloss: 0.538785\n",
      "[192]\ttraining's binary_logloss: 0.538549\n",
      "[193]\ttraining's binary_logloss: 0.538331\n",
      "[194]\ttraining's binary_logloss: 0.538231\n",
      "[195]\ttraining's binary_logloss: 0.537988\n",
      "[196]\ttraining's binary_logloss: 0.537876\n",
      "[197]\ttraining's binary_logloss: 0.537685\n",
      "[198]\ttraining's binary_logloss: 0.537563\n",
      "[199]\ttraining's binary_logloss: 0.537409\n",
      "[200]\ttraining's binary_logloss: 0.53724\n",
      "[201]\ttraining's binary_logloss: 0.537066\n",
      "[202]\ttraining's binary_logloss: 0.536855\n",
      "[203]\ttraining's binary_logloss: 0.536688\n",
      "[204]\ttraining's binary_logloss: 0.536488\n",
      "[205]\ttraining's binary_logloss: 0.536288\n",
      "[206]\ttraining's binary_logloss: 0.536194\n",
      "[207]\ttraining's binary_logloss: 0.535991\n",
      "[208]\ttraining's binary_logloss: 0.535832\n",
      "[209]\ttraining's binary_logloss: 0.535643\n",
      "[210]\ttraining's binary_logloss: 0.535389\n",
      "[211]\ttraining's binary_logloss: 0.535207\n",
      "[212]\ttraining's binary_logloss: 0.534977\n",
      "[213]\ttraining's binary_logloss: 0.534787\n",
      "[214]\ttraining's binary_logloss: 0.534595\n",
      "[215]\ttraining's binary_logloss: 0.534499\n",
      "[216]\ttraining's binary_logloss: 0.534405\n",
      "[217]\ttraining's binary_logloss: 0.534291\n",
      "[218]\ttraining's binary_logloss: 0.534097\n",
      "[219]\ttraining's binary_logloss: 0.534004\n",
      "[220]\ttraining's binary_logloss: 0.533878\n",
      "[221]\ttraining's binary_logloss: 0.533719\n",
      "[222]\ttraining's binary_logloss: 0.5336\n",
      "[223]\ttraining's binary_logloss: 0.53347\n",
      "[224]\ttraining's binary_logloss: 0.533379\n",
      "[225]\ttraining's binary_logloss: 0.533244\n",
      "[226]\ttraining's binary_logloss: 0.533117\n",
      "[227]\ttraining's binary_logloss: 0.532926\n",
      "[228]\ttraining's binary_logloss: 0.532736\n",
      "[229]\ttraining's binary_logloss: 0.532503\n",
      "[230]\ttraining's binary_logloss: 0.532413\n",
      "[231]\ttraining's binary_logloss: 0.532278\n",
      "[232]\ttraining's binary_logloss: 0.532116\n",
      "[233]\ttraining's binary_logloss: 0.531987\n",
      "[234]\ttraining's binary_logloss: 0.531779\n",
      "[235]\ttraining's binary_logloss: 0.531552\n",
      "[236]\ttraining's binary_logloss: 0.531346\n",
      "[237]\ttraining's binary_logloss: 0.531243\n",
      "[238]\ttraining's binary_logloss: 0.531032\n",
      "[239]\ttraining's binary_logloss: 0.53088\n",
      "[240]\ttraining's binary_logloss: 0.530762\n",
      "[241]\ttraining's binary_logloss: 0.530514\n",
      "[242]\ttraining's binary_logloss: 0.530389\n",
      "[243]\ttraining's binary_logloss: 0.530193\n",
      "[244]\ttraining's binary_logloss: 0.529989\n",
      "[245]\ttraining's binary_logloss: 0.529795\n",
      "[246]\ttraining's binary_logloss: 0.52961\n",
      "[247]\ttraining's binary_logloss: 0.5294\n",
      "[248]\ttraining's binary_logloss: 0.52912\n",
      "[249]\ttraining's binary_logloss: 0.529011\n",
      "[250]\ttraining's binary_logloss: 0.528856\n",
      "[251]\ttraining's binary_logloss: 0.528667\n",
      "[252]\ttraining's binary_logloss: 0.528479\n",
      "[253]\ttraining's binary_logloss: 0.528303\n",
      "[254]\ttraining's binary_logloss: 0.528221\n",
      "[255]\ttraining's binary_logloss: 0.528077\n",
      "[256]\ttraining's binary_logloss: 0.527853\n",
      "[257]\ttraining's binary_logloss: 0.527786\n",
      "[258]\ttraining's binary_logloss: 0.527586\n",
      "[259]\ttraining's binary_logloss: 0.527443\n",
      "[260]\ttraining's binary_logloss: 0.527319\n",
      "[261]\ttraining's binary_logloss: 0.527069\n",
      "[262]\ttraining's binary_logloss: 0.526921\n",
      "[263]\ttraining's binary_logloss: 0.526808\n",
      "[264]\ttraining's binary_logloss: 0.526669\n",
      "[265]\ttraining's binary_logloss: 0.526466\n",
      "[266]\ttraining's binary_logloss: 0.526302\n",
      "[267]\ttraining's binary_logloss: 0.526144\n",
      "[268]\ttraining's binary_logloss: 0.526034\n",
      "[269]\ttraining's binary_logloss: 0.525855\n",
      "[270]\ttraining's binary_logloss: 0.525671\n",
      "[271]\ttraining's binary_logloss: 0.525482\n",
      "[272]\ttraining's binary_logloss: 0.525298\n",
      "[273]\ttraining's binary_logloss: 0.525117\n",
      "[274]\ttraining's binary_logloss: 0.524943\n",
      "[275]\ttraining's binary_logloss: 0.524761\n",
      "[276]\ttraining's binary_logloss: 0.524673\n",
      "[277]\ttraining's binary_logloss: 0.524503\n",
      "[278]\ttraining's binary_logloss: 0.524265\n",
      "[279]\ttraining's binary_logloss: 0.524057\n",
      "[280]\ttraining's binary_logloss: 0.523914\n",
      "[281]\ttraining's binary_logloss: 0.523791\n",
      "[282]\ttraining's binary_logloss: 0.523644\n",
      "[283]\ttraining's binary_logloss: 0.523525\n",
      "[284]\ttraining's binary_logloss: 0.523371\n",
      "[285]\ttraining's binary_logloss: 0.523202\n",
      "[286]\ttraining's binary_logloss: 0.523083\n",
      "[287]\ttraining's binary_logloss: 0.522977\n",
      "[288]\ttraining's binary_logloss: 0.522871\n",
      "[289]\ttraining's binary_logloss: 0.522763\n",
      "[290]\ttraining's binary_logloss: 0.522564\n",
      "[291]\ttraining's binary_logloss: 0.522355\n",
      "[292]\ttraining's binary_logloss: 0.52221\n",
      "[293]\ttraining's binary_logloss: 0.522072\n",
      "[294]\ttraining's binary_logloss: 0.521901\n",
      "[295]\ttraining's binary_logloss: 0.521751\n",
      "[296]\ttraining's binary_logloss: 0.521576\n",
      "[297]\ttraining's binary_logloss: 0.521401\n",
      "[298]\ttraining's binary_logloss: 0.521221\n",
      "[299]\ttraining's binary_logloss: 0.521095\n",
      "[300]\ttraining's binary_logloss: 0.520937\n",
      "[301]\ttraining's binary_logloss: 0.520784\n",
      "[302]\ttraining's binary_logloss: 0.520655\n",
      "[303]\ttraining's binary_logloss: 0.520557\n",
      "[304]\ttraining's binary_logloss: 0.520481\n",
      "[305]\ttraining's binary_logloss: 0.520412\n",
      "[306]\ttraining's binary_logloss: 0.520227\n",
      "[307]\ttraining's binary_logloss: 0.520058\n",
      "[308]\ttraining's binary_logloss: 0.519934\n",
      "[309]\ttraining's binary_logloss: 0.519835\n",
      "[310]\ttraining's binary_logloss: 0.519718\n",
      "[311]\ttraining's binary_logloss: 0.519525\n",
      "[312]\ttraining's binary_logloss: 0.519229\n",
      "[313]\ttraining's binary_logloss: 0.519013\n",
      "[314]\ttraining's binary_logloss: 0.5189\n",
      "[315]\ttraining's binary_logloss: 0.518793\n",
      "[316]\ttraining's binary_logloss: 0.518626\n",
      "[317]\ttraining's binary_logloss: 0.518445\n",
      "[318]\ttraining's binary_logloss: 0.518348\n",
      "[319]\ttraining's binary_logloss: 0.518174\n",
      "[320]\ttraining's binary_logloss: 0.517933\n",
      "[321]\ttraining's binary_logloss: 0.517862\n",
      "[322]\ttraining's binary_logloss: 0.51773\n",
      "[323]\ttraining's binary_logloss: 0.517529\n",
      "[324]\ttraining's binary_logloss: 0.517377\n",
      "[325]\ttraining's binary_logloss: 0.51728\n",
      "[326]\ttraining's binary_logloss: 0.51709\n",
      "[327]\ttraining's binary_logloss: 0.516934\n",
      "[328]\ttraining's binary_logloss: 0.516854\n",
      "[329]\ttraining's binary_logloss: 0.51672\n",
      "[330]\ttraining's binary_logloss: 0.51658\n",
      "[331]\ttraining's binary_logloss: 0.516441\n",
      "[332]\ttraining's binary_logloss: 0.516308\n",
      "[333]\ttraining's binary_logloss: 0.516223\n",
      "[334]\ttraining's binary_logloss: 0.516134\n",
      "[335]\ttraining's binary_logloss: 0.515982\n",
      "[336]\ttraining's binary_logloss: 0.515836\n",
      "[337]\ttraining's binary_logloss: 0.515669\n",
      "[338]\ttraining's binary_logloss: 0.51553\n",
      "[339]\ttraining's binary_logloss: 0.515406\n",
      "[340]\ttraining's binary_logloss: 0.515258\n",
      "[341]\ttraining's binary_logloss: 0.51515\n",
      "[342]\ttraining's binary_logloss: 0.514993\n",
      "[343]\ttraining's binary_logloss: 0.514847\n",
      "[344]\ttraining's binary_logloss: 0.514781\n",
      "[345]\ttraining's binary_logloss: 0.514686\n",
      "[346]\ttraining's binary_logloss: 0.514527\n",
      "[347]\ttraining's binary_logloss: 0.514412\n",
      "[348]\ttraining's binary_logloss: 0.514179\n",
      "[349]\ttraining's binary_logloss: 0.514028\n",
      "[350]\ttraining's binary_logloss: 0.513931\n",
      "[351]\ttraining's binary_logloss: 0.513786\n",
      "[352]\ttraining's binary_logloss: 0.513621\n",
      "[353]\ttraining's binary_logloss: 0.513488\n",
      "[354]\ttraining's binary_logloss: 0.513327\n",
      "[355]\ttraining's binary_logloss: 0.513201\n",
      "[356]\ttraining's binary_logloss: 0.513108\n",
      "[357]\ttraining's binary_logloss: 0.512865\n",
      "[358]\ttraining's binary_logloss: 0.512746\n",
      "[359]\ttraining's binary_logloss: 0.512634\n",
      "[360]\ttraining's binary_logloss: 0.512495\n",
      "[361]\ttraining's binary_logloss: 0.512328\n",
      "[362]\ttraining's binary_logloss: 0.512158\n",
      "[363]\ttraining's binary_logloss: 0.51206\n",
      "[364]\ttraining's binary_logloss: 0.511903\n",
      "[365]\ttraining's binary_logloss: 0.51179\n",
      "[366]\ttraining's binary_logloss: 0.511678\n",
      "[367]\ttraining's binary_logloss: 0.51156\n",
      "[368]\ttraining's binary_logloss: 0.511395\n",
      "[369]\ttraining's binary_logloss: 0.511238\n",
      "[370]\ttraining's binary_logloss: 0.511114\n",
      "[371]\ttraining's binary_logloss: 0.510987\n",
      "[372]\ttraining's binary_logloss: 0.510908\n",
      "[373]\ttraining's binary_logloss: 0.510835\n",
      "[374]\ttraining's binary_logloss: 0.510694\n",
      "[375]\ttraining's binary_logloss: 0.510543\n",
      "[376]\ttraining's binary_logloss: 0.510425\n",
      "[377]\ttraining's binary_logloss: 0.510277\n",
      "[378]\ttraining's binary_logloss: 0.510152\n",
      "[379]\ttraining's binary_logloss: 0.510013\n",
      "[380]\ttraining's binary_logloss: 0.509897\n",
      "[381]\ttraining's binary_logloss: 0.509793\n",
      "[382]\ttraining's binary_logloss: 0.509642\n",
      "[383]\ttraining's binary_logloss: 0.509524\n",
      "[384]\ttraining's binary_logloss: 0.509392\n",
      "[385]\ttraining's binary_logloss: 0.509279\n",
      "[386]\ttraining's binary_logloss: 0.509159\n",
      "[387]\ttraining's binary_logloss: 0.508986\n",
      "[388]\ttraining's binary_logloss: 0.508871\n",
      "[389]\ttraining's binary_logloss: 0.508686\n",
      "[390]\ttraining's binary_logloss: 0.508488\n",
      "[391]\ttraining's binary_logloss: 0.508367\n",
      "[392]\ttraining's binary_logloss: 0.508187\n",
      "[393]\ttraining's binary_logloss: 0.50805\n",
      "[394]\ttraining's binary_logloss: 0.507983\n",
      "[395]\ttraining's binary_logloss: 0.507867\n",
      "[396]\ttraining's binary_logloss: 0.507762\n",
      "[397]\ttraining's binary_logloss: 0.50756\n",
      "[398]\ttraining's binary_logloss: 0.50745\n",
      "[399]\ttraining's binary_logloss: 0.5073\n",
      "[400]\ttraining's binary_logloss: 0.507134\n",
      "[401]\ttraining's binary_logloss: 0.507007\n",
      "[402]\ttraining's binary_logloss: 0.506868\n",
      "[403]\ttraining's binary_logloss: 0.506757\n",
      "[404]\ttraining's binary_logloss: 0.506563\n",
      "[405]\ttraining's binary_logloss: 0.506447\n",
      "[406]\ttraining's binary_logloss: 0.506314\n",
      "[407]\ttraining's binary_logloss: 0.506249\n",
      "[408]\ttraining's binary_logloss: 0.506142\n",
      "[409]\ttraining's binary_logloss: 0.505933\n",
      "[410]\ttraining's binary_logloss: 0.505864\n",
      "[411]\ttraining's binary_logloss: 0.505722\n",
      "[412]\ttraining's binary_logloss: 0.50553\n",
      "[413]\ttraining's binary_logloss: 0.505417\n",
      "[414]\ttraining's binary_logloss: 0.505301\n",
      "[415]\ttraining's binary_logloss: 0.505142\n",
      "[416]\ttraining's binary_logloss: 0.504981\n",
      "[417]\ttraining's binary_logloss: 0.504905\n",
      "[418]\ttraining's binary_logloss: 0.504745\n",
      "[419]\ttraining's binary_logloss: 0.504637\n",
      "[420]\ttraining's binary_logloss: 0.504562\n",
      "[421]\ttraining's binary_logloss: 0.504445\n",
      "[422]\ttraining's binary_logloss: 0.504372\n",
      "[423]\ttraining's binary_logloss: 0.504293\n",
      "[424]\ttraining's binary_logloss: 0.504158\n",
      "[425]\ttraining's binary_logloss: 0.504059\n",
      "[426]\ttraining's binary_logloss: 0.503909\n",
      "[427]\ttraining's binary_logloss: 0.503748\n",
      "[428]\ttraining's binary_logloss: 0.503543\n",
      "[429]\ttraining's binary_logloss: 0.503356\n",
      "[430]\ttraining's binary_logloss: 0.503216\n",
      "[431]\ttraining's binary_logloss: 0.503093\n",
      "[432]\ttraining's binary_logloss: 0.502927\n",
      "[433]\ttraining's binary_logloss: 0.502802\n",
      "[434]\ttraining's binary_logloss: 0.502673\n",
      "[435]\ttraining's binary_logloss: 0.502565\n",
      "[436]\ttraining's binary_logloss: 0.502373\n",
      "[437]\ttraining's binary_logloss: 0.502306\n",
      "[438]\ttraining's binary_logloss: 0.502191\n",
      "[439]\ttraining's binary_logloss: 0.502061\n",
      "[440]\ttraining's binary_logloss: 0.501928\n",
      "[441]\ttraining's binary_logloss: 0.501795\n",
      "[442]\ttraining's binary_logloss: 0.501647\n",
      "[443]\ttraining's binary_logloss: 0.501575\n",
      "[444]\ttraining's binary_logloss: 0.501402\n",
      "[445]\ttraining's binary_logloss: 0.501311\n",
      "[446]\ttraining's binary_logloss: 0.501197\n",
      "[447]\ttraining's binary_logloss: 0.501066\n",
      "[448]\ttraining's binary_logloss: 0.500913\n",
      "[449]\ttraining's binary_logloss: 0.500782\n",
      "[450]\ttraining's binary_logloss: 0.500614\n",
      "[451]\ttraining's binary_logloss: 0.500482\n",
      "[452]\ttraining's binary_logloss: 0.500353\n",
      "[453]\ttraining's binary_logloss: 0.500188\n",
      "[454]\ttraining's binary_logloss: 0.500018\n",
      "[455]\ttraining's binary_logloss: 0.499897\n",
      "[456]\ttraining's binary_logloss: 0.499728\n",
      "[457]\ttraining's binary_logloss: 0.49961\n",
      "[458]\ttraining's binary_logloss: 0.499476\n",
      "[459]\ttraining's binary_logloss: 0.499295\n",
      "[460]\ttraining's binary_logloss: 0.499146\n",
      "[461]\ttraining's binary_logloss: 0.499032\n",
      "[462]\ttraining's binary_logloss: 0.498827\n",
      "[463]\ttraining's binary_logloss: 0.498666\n",
      "[464]\ttraining's binary_logloss: 0.49858\n",
      "[465]\ttraining's binary_logloss: 0.498474\n",
      "[466]\ttraining's binary_logloss: 0.498352\n",
      "[467]\ttraining's binary_logloss: 0.498237\n",
      "[468]\ttraining's binary_logloss: 0.498078\n",
      "[469]\ttraining's binary_logloss: 0.498003\n",
      "[470]\ttraining's binary_logloss: 0.497932\n",
      "[471]\ttraining's binary_logloss: 0.497833\n",
      "[472]\ttraining's binary_logloss: 0.497678\n",
      "[473]\ttraining's binary_logloss: 0.497566\n",
      "[474]\ttraining's binary_logloss: 0.497468\n",
      "[475]\ttraining's binary_logloss: 0.497346\n",
      "[476]\ttraining's binary_logloss: 0.497238\n",
      "[477]\ttraining's binary_logloss: 0.497077\n",
      "[478]\ttraining's binary_logloss: 0.496935\n",
      "[479]\ttraining's binary_logloss: 0.496836\n",
      "[480]\ttraining's binary_logloss: 0.49673\n",
      "[481]\ttraining's binary_logloss: 0.496599\n",
      "[482]\ttraining's binary_logloss: 0.496546\n",
      "[483]\ttraining's binary_logloss: 0.496401\n",
      "[484]\ttraining's binary_logloss: 0.496267\n",
      "[485]\ttraining's binary_logloss: 0.496097\n",
      "[486]\ttraining's binary_logloss: 0.496039\n",
      "[487]\ttraining's binary_logloss: 0.495903\n",
      "[488]\ttraining's binary_logloss: 0.495799\n",
      "[489]\ttraining's binary_logloss: 0.495656\n",
      "[490]\ttraining's binary_logloss: 0.495517\n",
      "[491]\ttraining's binary_logloss: 0.495438\n",
      "[492]\ttraining's binary_logloss: 0.495292\n",
      "[493]\ttraining's binary_logloss: 0.495179\n",
      "[494]\ttraining's binary_logloss: 0.49507\n",
      "[495]\ttraining's binary_logloss: 0.494979\n",
      "[496]\ttraining's binary_logloss: 0.494858\n",
      "[497]\ttraining's binary_logloss: 0.49478\n",
      "[498]\ttraining's binary_logloss: 0.494704\n",
      "[499]\ttraining's binary_logloss: 0.494584\n",
      "[500]\ttraining's binary_logloss: 0.494476\n",
      "[501]\ttraining's binary_logloss: 0.494306\n",
      "[502]\ttraining's binary_logloss: 0.494177\n",
      "[503]\ttraining's binary_logloss: 0.494072\n",
      "[504]\ttraining's binary_logloss: 0.493959\n",
      "[505]\ttraining's binary_logloss: 0.493801\n",
      "[506]\ttraining's binary_logloss: 0.49371\n",
      "[507]\ttraining's binary_logloss: 0.493573\n",
      "[508]\ttraining's binary_logloss: 0.493465\n",
      "[509]\ttraining's binary_logloss: 0.493341\n",
      "[510]\ttraining's binary_logloss: 0.493241\n",
      "[511]\ttraining's binary_logloss: 0.493192\n",
      "[512]\ttraining's binary_logloss: 0.493097\n",
      "[513]\ttraining's binary_logloss: 0.493043\n",
      "[514]\ttraining's binary_logloss: 0.492924\n",
      "[515]\ttraining's binary_logloss: 0.492717\n",
      "[516]\ttraining's binary_logloss: 0.492651\n",
      "[517]\ttraining's binary_logloss: 0.492605\n",
      "[518]\ttraining's binary_logloss: 0.492507\n",
      "[519]\ttraining's binary_logloss: 0.492377\n",
      "[520]\ttraining's binary_logloss: 0.492269\n",
      "[521]\ttraining's binary_logloss: 0.492169\n",
      "[522]\ttraining's binary_logloss: 0.492001\n",
      "[523]\ttraining's binary_logloss: 0.491866\n",
      "[524]\ttraining's binary_logloss: 0.491747\n",
      "[525]\ttraining's binary_logloss: 0.491603\n",
      "[526]\ttraining's binary_logloss: 0.491433\n",
      "[527]\ttraining's binary_logloss: 0.491316\n",
      "[528]\ttraining's binary_logloss: 0.49118\n",
      "[529]\ttraining's binary_logloss: 0.491105\n",
      "[530]\ttraining's binary_logloss: 0.490932\n",
      "[531]\ttraining's binary_logloss: 0.490838\n",
      "[532]\ttraining's binary_logloss: 0.490646\n",
      "[533]\ttraining's binary_logloss: 0.490458\n",
      "[534]\ttraining's binary_logloss: 0.490349\n",
      "[535]\ttraining's binary_logloss: 0.490248\n",
      "[536]\ttraining's binary_logloss: 0.490073\n",
      "[537]\ttraining's binary_logloss: 0.489914\n",
      "[538]\ttraining's binary_logloss: 0.489823\n",
      "[539]\ttraining's binary_logloss: 0.489745\n",
      "[540]\ttraining's binary_logloss: 0.489606\n",
      "[541]\ttraining's binary_logloss: 0.489513\n",
      "[542]\ttraining's binary_logloss: 0.489403\n",
      "[543]\ttraining's binary_logloss: 0.489303\n",
      "[544]\ttraining's binary_logloss: 0.489153\n",
      "[545]\ttraining's binary_logloss: 0.489077\n",
      "[546]\ttraining's binary_logloss: 0.488923\n",
      "[547]\ttraining's binary_logloss: 0.488741\n",
      "[548]\ttraining's binary_logloss: 0.488655\n",
      "[549]\ttraining's binary_logloss: 0.488573\n",
      "[550]\ttraining's binary_logloss: 0.488407\n",
      "[551]\ttraining's binary_logloss: 0.488261\n",
      "[552]\ttraining's binary_logloss: 0.488129\n",
      "[553]\ttraining's binary_logloss: 0.487942\n",
      "[554]\ttraining's binary_logloss: 0.48786\n",
      "[555]\ttraining's binary_logloss: 0.487747\n",
      "[556]\ttraining's binary_logloss: 0.487647\n",
      "[557]\ttraining's binary_logloss: 0.487498\n",
      "[558]\ttraining's binary_logloss: 0.487383\n",
      "[559]\ttraining's binary_logloss: 0.487254\n",
      "[560]\ttraining's binary_logloss: 0.486981\n",
      "[561]\ttraining's binary_logloss: 0.48685\n",
      "[562]\ttraining's binary_logloss: 0.486624\n",
      "[563]\ttraining's binary_logloss: 0.486508\n",
      "[564]\ttraining's binary_logloss: 0.486365\n",
      "[565]\ttraining's binary_logloss: 0.486237\n",
      "[566]\ttraining's binary_logloss: 0.486136\n",
      "[567]\ttraining's binary_logloss: 0.485947\n",
      "[568]\ttraining's binary_logloss: 0.485824\n",
      "[569]\ttraining's binary_logloss: 0.485753\n",
      "[570]\ttraining's binary_logloss: 0.48564\n",
      "[571]\ttraining's binary_logloss: 0.485565\n",
      "[572]\ttraining's binary_logloss: 0.485465\n",
      "[573]\ttraining's binary_logloss: 0.48532\n",
      "[574]\ttraining's binary_logloss: 0.485174\n",
      "[575]\ttraining's binary_logloss: 0.485032\n",
      "[576]\ttraining's binary_logloss: 0.484962\n",
      "[577]\ttraining's binary_logloss: 0.484863\n",
      "[578]\ttraining's binary_logloss: 0.484665\n",
      "[579]\ttraining's binary_logloss: 0.4845\n",
      "[580]\ttraining's binary_logloss: 0.484392\n",
      "[581]\ttraining's binary_logloss: 0.484326\n",
      "[582]\ttraining's binary_logloss: 0.484243\n",
      "[583]\ttraining's binary_logloss: 0.484137\n",
      "[584]\ttraining's binary_logloss: 0.483989\n",
      "[585]\ttraining's binary_logloss: 0.483835\n",
      "[586]\ttraining's binary_logloss: 0.483721\n",
      "[587]\ttraining's binary_logloss: 0.483571\n",
      "[588]\ttraining's binary_logloss: 0.483473\n",
      "[589]\ttraining's binary_logloss: 0.483333\n",
      "[590]\ttraining's binary_logloss: 0.483222\n",
      "[591]\ttraining's binary_logloss: 0.483136\n",
      "[592]\ttraining's binary_logloss: 0.483011\n",
      "[593]\ttraining's binary_logloss: 0.482885\n",
      "[594]\ttraining's binary_logloss: 0.482754\n",
      "[595]\ttraining's binary_logloss: 0.482637\n",
      "[596]\ttraining's binary_logloss: 0.482559\n",
      "[597]\ttraining's binary_logloss: 0.482444\n",
      "[598]\ttraining's binary_logloss: 0.482242\n",
      "[599]\ttraining's binary_logloss: 0.482134\n",
      "[600]\ttraining's binary_logloss: 0.482029\n",
      "[1]\ttraining's binary_logloss: 0.632479\n",
      "[2]\ttraining's binary_logloss: 0.629569\n",
      "[3]\ttraining's binary_logloss: 0.627056\n",
      "[4]\ttraining's binary_logloss: 0.624724\n",
      "[5]\ttraining's binary_logloss: 0.622953\n",
      "[6]\ttraining's binary_logloss: 0.621122\n",
      "[7]\ttraining's binary_logloss: 0.619594\n",
      "[8]\ttraining's binary_logloss: 0.618353\n",
      "[9]\ttraining's binary_logloss: 0.617018\n",
      "[10]\ttraining's binary_logloss: 0.615812\n",
      "[11]\ttraining's binary_logloss: 0.614872\n",
      "[12]\ttraining's binary_logloss: 0.613893\n",
      "[13]\ttraining's binary_logloss: 0.613036\n",
      "[14]\ttraining's binary_logloss: 0.612103\n",
      "[15]\ttraining's binary_logloss: 0.611177\n",
      "[16]\ttraining's binary_logloss: 0.610404\n",
      "[17]\ttraining's binary_logloss: 0.60965\n",
      "[18]\ttraining's binary_logloss: 0.608894\n",
      "[19]\ttraining's binary_logloss: 0.608213\n",
      "[20]\ttraining's binary_logloss: 0.607629\n",
      "[21]\ttraining's binary_logloss: 0.60702\n",
      "[22]\ttraining's binary_logloss: 0.606481\n",
      "[23]\ttraining's binary_logloss: 0.605942\n",
      "[24]\ttraining's binary_logloss: 0.605344\n",
      "[25]\ttraining's binary_logloss: 0.604805\n",
      "[26]\ttraining's binary_logloss: 0.604349\n",
      "[27]\ttraining's binary_logloss: 0.603706\n",
      "[28]\ttraining's binary_logloss: 0.603243\n",
      "[29]\ttraining's binary_logloss: 0.602739\n",
      "[30]\ttraining's binary_logloss: 0.602398\n",
      "[31]\ttraining's binary_logloss: 0.601922\n",
      "[32]\ttraining's binary_logloss: 0.60151\n",
      "[33]\ttraining's binary_logloss: 0.601145\n",
      "[34]\ttraining's binary_logloss: 0.600781\n",
      "[35]\ttraining's binary_logloss: 0.600368\n",
      "[36]\ttraining's binary_logloss: 0.60003\n",
      "[37]\ttraining's binary_logloss: 0.599713\n",
      "[38]\ttraining's binary_logloss: 0.599472\n",
      "[39]\ttraining's binary_logloss: 0.599271\n",
      "[40]\ttraining's binary_logloss: 0.598884\n",
      "[41]\ttraining's binary_logloss: 0.598613\n",
      "[42]\ttraining's binary_logloss: 0.598344\n",
      "[43]\ttraining's binary_logloss: 0.59811\n",
      "[44]\ttraining's binary_logloss: 0.597875\n",
      "[45]\ttraining's binary_logloss: 0.597565\n",
      "[46]\ttraining's binary_logloss: 0.597326\n",
      "[47]\ttraining's binary_logloss: 0.596977\n",
      "[48]\ttraining's binary_logloss: 0.596694\n",
      "[49]\ttraining's binary_logloss: 0.596445\n",
      "[50]\ttraining's binary_logloss: 0.59609\n",
      "[51]\ttraining's binary_logloss: 0.595955\n",
      "[52]\ttraining's binary_logloss: 0.595756\n",
      "[53]\ttraining's binary_logloss: 0.595456\n",
      "[54]\ttraining's binary_logloss: 0.595275\n",
      "[55]\ttraining's binary_logloss: 0.595062\n",
      "[56]\ttraining's binary_logloss: 0.594927\n",
      "[57]\ttraining's binary_logloss: 0.594645\n",
      "[58]\ttraining's binary_logloss: 0.5945\n",
      "[59]\ttraining's binary_logloss: 0.594334\n",
      "[60]\ttraining's binary_logloss: 0.59417\n",
      "[61]\ttraining's binary_logloss: 0.593932\n",
      "[62]\ttraining's binary_logloss: 0.593666\n",
      "[63]\ttraining's binary_logloss: 0.593422\n",
      "[64]\ttraining's binary_logloss: 0.593296\n",
      "[65]\ttraining's binary_logloss: 0.593124\n",
      "[66]\ttraining's binary_logloss: 0.592934\n",
      "[67]\ttraining's binary_logloss: 0.592725\n",
      "[68]\ttraining's binary_logloss: 0.592476\n",
      "[69]\ttraining's binary_logloss: 0.59236\n",
      "[70]\ttraining's binary_logloss: 0.592097\n",
      "[71]\ttraining's binary_logloss: 0.591865\n",
      "[72]\ttraining's binary_logloss: 0.591677\n",
      "[73]\ttraining's binary_logloss: 0.591532\n",
      "[74]\ttraining's binary_logloss: 0.591287\n",
      "[75]\ttraining's binary_logloss: 0.59113\n",
      "[76]\ttraining's binary_logloss: 0.591016\n",
      "[77]\ttraining's binary_logloss: 0.590819\n",
      "[78]\ttraining's binary_logloss: 0.590582\n",
      "[79]\ttraining's binary_logloss: 0.590396\n",
      "[80]\ttraining's binary_logloss: 0.590269\n",
      "[81]\ttraining's binary_logloss: 0.590124\n",
      "[82]\ttraining's binary_logloss: 0.589894\n",
      "[83]\ttraining's binary_logloss: 0.589631\n",
      "[84]\ttraining's binary_logloss: 0.589455\n",
      "[85]\ttraining's binary_logloss: 0.589261\n",
      "[86]\ttraining's binary_logloss: 0.589107\n",
      "[87]\ttraining's binary_logloss: 0.589034\n",
      "[88]\ttraining's binary_logloss: 0.58892\n",
      "[89]\ttraining's binary_logloss: 0.588748\n",
      "[90]\ttraining's binary_logloss: 0.588633\n",
      "[91]\ttraining's binary_logloss: 0.588528\n",
      "[92]\ttraining's binary_logloss: 0.588407\n",
      "[93]\ttraining's binary_logloss: 0.588291\n",
      "[94]\ttraining's binary_logloss: 0.588119\n",
      "[95]\ttraining's binary_logloss: 0.587972\n",
      "[96]\ttraining's binary_logloss: 0.587739\n",
      "[97]\ttraining's binary_logloss: 0.587576\n",
      "[98]\ttraining's binary_logloss: 0.587455\n",
      "[99]\ttraining's binary_logloss: 0.587365\n",
      "[100]\ttraining's binary_logloss: 0.587272\n",
      "[101]\ttraining's binary_logloss: 0.587174\n",
      "[102]\ttraining's binary_logloss: 0.587011\n",
      "[103]\ttraining's binary_logloss: 0.586926\n",
      "[104]\ttraining's binary_logloss: 0.586746\n",
      "[105]\ttraining's binary_logloss: 0.586645\n",
      "[106]\ttraining's binary_logloss: 0.586493\n",
      "[107]\ttraining's binary_logloss: 0.586434\n",
      "[108]\ttraining's binary_logloss: 0.586332\n",
      "[109]\ttraining's binary_logloss: 0.586199\n",
      "[110]\ttraining's binary_logloss: 0.58604\n",
      "[111]\ttraining's binary_logloss: 0.58598\n",
      "[112]\ttraining's binary_logloss: 0.585917\n",
      "[113]\ttraining's binary_logloss: 0.585804\n",
      "[114]\ttraining's binary_logloss: 0.585661\n",
      "[115]\ttraining's binary_logloss: 0.585566\n",
      "[116]\ttraining's binary_logloss: 0.585483\n",
      "[117]\ttraining's binary_logloss: 0.585373\n",
      "[118]\ttraining's binary_logloss: 0.585312\n",
      "[119]\ttraining's binary_logloss: 0.585192\n",
      "[120]\ttraining's binary_logloss: 0.58506\n",
      "[121]\ttraining's binary_logloss: 0.584968\n",
      "[122]\ttraining's binary_logloss: 0.584859\n",
      "[123]\ttraining's binary_logloss: 0.584757\n",
      "[124]\ttraining's binary_logloss: 0.584698\n",
      "[125]\ttraining's binary_logloss: 0.58462\n",
      "[126]\ttraining's binary_logloss: 0.584486\n",
      "[127]\ttraining's binary_logloss: 0.584385\n",
      "[128]\ttraining's binary_logloss: 0.584302\n",
      "[129]\ttraining's binary_logloss: 0.584206\n",
      "[130]\ttraining's binary_logloss: 0.584153\n",
      "[131]\ttraining's binary_logloss: 0.584023\n",
      "[132]\ttraining's binary_logloss: 0.583923\n",
      "[133]\ttraining's binary_logloss: 0.583822\n",
      "[134]\ttraining's binary_logloss: 0.583769\n",
      "[135]\ttraining's binary_logloss: 0.583667\n",
      "[136]\ttraining's binary_logloss: 0.583576\n",
      "[137]\ttraining's binary_logloss: 0.583501\n",
      "[138]\ttraining's binary_logloss: 0.583401\n",
      "[139]\ttraining's binary_logloss: 0.583326\n",
      "[140]\ttraining's binary_logloss: 0.583221\n",
      "[141]\ttraining's binary_logloss: 0.583163\n",
      "[142]\ttraining's binary_logloss: 0.583091\n",
      "[143]\ttraining's binary_logloss: 0.583012\n",
      "[144]\ttraining's binary_logloss: 0.582957\n",
      "[145]\ttraining's binary_logloss: 0.582805\n",
      "[146]\ttraining's binary_logloss: 0.582734\n",
      "[147]\ttraining's binary_logloss: 0.582692\n",
      "[148]\ttraining's binary_logloss: 0.582618\n",
      "[149]\ttraining's binary_logloss: 0.582503\n",
      "[150]\ttraining's binary_logloss: 0.582449\n",
      "[151]\ttraining's binary_logloss: 0.58238\n",
      "[152]\ttraining's binary_logloss: 0.582302\n",
      "[153]\ttraining's binary_logloss: 0.582209\n",
      "[154]\ttraining's binary_logloss: 0.582165\n",
      "[155]\ttraining's binary_logloss: 0.582061\n",
      "[156]\ttraining's binary_logloss: 0.582033\n",
      "[157]\ttraining's binary_logloss: 0.581958\n",
      "[158]\ttraining's binary_logloss: 0.581893\n",
      "[159]\ttraining's binary_logloss: 0.581854\n",
      "[160]\ttraining's binary_logloss: 0.581791\n",
      "[161]\ttraining's binary_logloss: 0.581711\n",
      "[162]\ttraining's binary_logloss: 0.581604\n",
      "[163]\ttraining's binary_logloss: 0.581527\n",
      "[164]\ttraining's binary_logloss: 0.581471\n",
      "[165]\ttraining's binary_logloss: 0.581423\n",
      "[166]\ttraining's binary_logloss: 0.581376\n",
      "[167]\ttraining's binary_logloss: 0.581336\n",
      "[168]\ttraining's binary_logloss: 0.581262\n",
      "[169]\ttraining's binary_logloss: 0.58122\n",
      "[170]\ttraining's binary_logloss: 0.581161\n",
      "[171]\ttraining's binary_logloss: 0.58113\n",
      "[172]\ttraining's binary_logloss: 0.581074\n",
      "[173]\ttraining's binary_logloss: 0.581009\n",
      "[174]\ttraining's binary_logloss: 0.580946\n",
      "[175]\ttraining's binary_logloss: 0.5809\n",
      "[176]\ttraining's binary_logloss: 0.580848\n",
      "[177]\ttraining's binary_logloss: 0.580771\n",
      "[178]\ttraining's binary_logloss: 0.580734\n",
      "[179]\ttraining's binary_logloss: 0.580676\n",
      "[180]\ttraining's binary_logloss: 0.580637\n",
      "[181]\ttraining's binary_logloss: 0.580579\n",
      "[182]\ttraining's binary_logloss: 0.580527\n",
      "[183]\ttraining's binary_logloss: 0.580494\n",
      "[184]\ttraining's binary_logloss: 0.580453\n",
      "[185]\ttraining's binary_logloss: 0.580404\n",
      "[186]\ttraining's binary_logloss: 0.580373\n",
      "[187]\ttraining's binary_logloss: 0.580344\n",
      "[188]\ttraining's binary_logloss: 0.580287\n",
      "[189]\ttraining's binary_logloss: 0.580218\n",
      "[190]\ttraining's binary_logloss: 0.580158\n",
      "[191]\ttraining's binary_logloss: 0.580128\n",
      "[192]\ttraining's binary_logloss: 0.580066\n",
      "[193]\ttraining's binary_logloss: 0.580015\n",
      "[194]\ttraining's binary_logloss: 0.579959\n",
      "[195]\ttraining's binary_logloss: 0.579924\n",
      "[196]\ttraining's binary_logloss: 0.579881\n",
      "[197]\ttraining's binary_logloss: 0.579853\n",
      "[198]\ttraining's binary_logloss: 0.579789\n",
      "[199]\ttraining's binary_logloss: 0.579737\n",
      "[200]\ttraining's binary_logloss: 0.57968\n",
      "[201]\ttraining's binary_logloss: 0.579628\n",
      "[202]\ttraining's binary_logloss: 0.579589\n",
      "[203]\ttraining's binary_logloss: 0.579549\n",
      "[204]\ttraining's binary_logloss: 0.579516\n",
      "[205]\ttraining's binary_logloss: 0.579464\n",
      "[206]\ttraining's binary_logloss: 0.579418\n",
      "[207]\ttraining's binary_logloss: 0.579391\n",
      "[208]\ttraining's binary_logloss: 0.579343\n",
      "[209]\ttraining's binary_logloss: 0.579303\n",
      "[210]\ttraining's binary_logloss: 0.579261\n",
      "[211]\ttraining's binary_logloss: 0.579211\n",
      "[212]\ttraining's binary_logloss: 0.579186\n",
      "[213]\ttraining's binary_logloss: 0.579163\n",
      "[214]\ttraining's binary_logloss: 0.57914\n",
      "[215]\ttraining's binary_logloss: 0.579106\n",
      "[216]\ttraining's binary_logloss: 0.579065\n",
      "[217]\ttraining's binary_logloss: 0.579038\n",
      "[218]\ttraining's binary_logloss: 0.57902\n",
      "[219]\ttraining's binary_logloss: 0.578974\n",
      "[220]\ttraining's binary_logloss: 0.578934\n",
      "[221]\ttraining's binary_logloss: 0.57889\n",
      "[222]\ttraining's binary_logloss: 0.578862\n",
      "[223]\ttraining's binary_logloss: 0.578818\n",
      "[224]\ttraining's binary_logloss: 0.578764\n",
      "[225]\ttraining's binary_logloss: 0.57873\n",
      "[226]\ttraining's binary_logloss: 0.578701\n",
      "[227]\ttraining's binary_logloss: 0.57867\n",
      "[228]\ttraining's binary_logloss: 0.578634\n",
      "[229]\ttraining's binary_logloss: 0.578596\n",
      "[230]\ttraining's binary_logloss: 0.578572\n",
      "[231]\ttraining's binary_logloss: 0.578531\n",
      "[232]\ttraining's binary_logloss: 0.578517\n",
      "[233]\ttraining's binary_logloss: 0.578488\n",
      "[234]\ttraining's binary_logloss: 0.578459\n",
      "[235]\ttraining's binary_logloss: 0.578441\n",
      "[236]\ttraining's binary_logloss: 0.578428\n",
      "[237]\ttraining's binary_logloss: 0.578389\n",
      "[238]\ttraining's binary_logloss: 0.57837\n",
      "[239]\ttraining's binary_logloss: 0.578323\n",
      "[240]\ttraining's binary_logloss: 0.578291\n",
      "[241]\ttraining's binary_logloss: 0.57824\n",
      "[242]\ttraining's binary_logloss: 0.578224\n",
      "[243]\ttraining's binary_logloss: 0.578191\n",
      "[244]\ttraining's binary_logloss: 0.578159\n",
      "[245]\ttraining's binary_logloss: 0.578124\n",
      "[246]\ttraining's binary_logloss: 0.578109\n",
      "[247]\ttraining's binary_logloss: 0.578083\n",
      "[248]\ttraining's binary_logloss: 0.578059\n",
      "[249]\ttraining's binary_logloss: 0.578033\n",
      "[250]\ttraining's binary_logloss: 0.577997\n",
      "[251]\ttraining's binary_logloss: 0.577966\n",
      "[252]\ttraining's binary_logloss: 0.577943\n",
      "[253]\ttraining's binary_logloss: 0.577929\n",
      "[254]\ttraining's binary_logloss: 0.577909\n",
      "[255]\ttraining's binary_logloss: 0.57788\n",
      "[256]\ttraining's binary_logloss: 0.577861\n",
      "[257]\ttraining's binary_logloss: 0.577839\n",
      "[258]\ttraining's binary_logloss: 0.577814\n",
      "[259]\ttraining's binary_logloss: 0.577783\n",
      "[260]\ttraining's binary_logloss: 0.577754\n",
      "[261]\ttraining's binary_logloss: 0.577736\n",
      "[262]\ttraining's binary_logloss: 0.577695\n",
      "[263]\ttraining's binary_logloss: 0.577661\n",
      "[264]\ttraining's binary_logloss: 0.577647\n",
      "[265]\ttraining's binary_logloss: 0.57763\n",
      "[266]\ttraining's binary_logloss: 0.577598\n",
      "[267]\ttraining's binary_logloss: 0.577588\n",
      "[268]\ttraining's binary_logloss: 0.577564\n",
      "[269]\ttraining's binary_logloss: 0.57754\n",
      "[270]\ttraining's binary_logloss: 0.577506\n",
      "[271]\ttraining's binary_logloss: 0.577488\n",
      "[272]\ttraining's binary_logloss: 0.577464\n",
      "[273]\ttraining's binary_logloss: 0.57745\n",
      "[274]\ttraining's binary_logloss: 0.577425\n",
      "[275]\ttraining's binary_logloss: 0.577407\n",
      "[276]\ttraining's binary_logloss: 0.57738\n",
      "[277]\ttraining's binary_logloss: 0.577356\n",
      "[278]\ttraining's binary_logloss: 0.577326\n",
      "[279]\ttraining's binary_logloss: 0.577315\n",
      "[280]\ttraining's binary_logloss: 0.577303\n",
      "[281]\ttraining's binary_logloss: 0.577284\n",
      "[282]\ttraining's binary_logloss: 0.577267\n",
      "[283]\ttraining's binary_logloss: 0.577246\n",
      "[284]\ttraining's binary_logloss: 0.577236\n",
      "[285]\ttraining's binary_logloss: 0.577215\n",
      "[286]\ttraining's binary_logloss: 0.577199\n",
      "[287]\ttraining's binary_logloss: 0.577173\n",
      "[288]\ttraining's binary_logloss: 0.577156\n",
      "[289]\ttraining's binary_logloss: 0.577129\n",
      "[290]\ttraining's binary_logloss: 0.57711\n",
      "[291]\ttraining's binary_logloss: 0.577097\n",
      "[292]\ttraining's binary_logloss: 0.577082\n",
      "[293]\ttraining's binary_logloss: 0.577066\n",
      "[294]\ttraining's binary_logloss: 0.577049\n",
      "[295]\ttraining's binary_logloss: 0.577022\n",
      "[296]\ttraining's binary_logloss: 0.576994\n",
      "[297]\ttraining's binary_logloss: 0.576969\n",
      "[298]\ttraining's binary_logloss: 0.576945\n",
      "[299]\ttraining's binary_logloss: 0.576933\n",
      "[300]\ttraining's binary_logloss: 0.576921\n",
      "[301]\ttraining's binary_logloss: 0.576907\n",
      "[302]\ttraining's binary_logloss: 0.576882\n",
      "[303]\ttraining's binary_logloss: 0.576864\n",
      "[304]\ttraining's binary_logloss: 0.576852\n",
      "[305]\ttraining's binary_logloss: 0.57684\n",
      "[306]\ttraining's binary_logloss: 0.576832\n",
      "[307]\ttraining's binary_logloss: 0.576825\n",
      "[308]\ttraining's binary_logloss: 0.576818\n",
      "[309]\ttraining's binary_logloss: 0.576792\n",
      "[310]\ttraining's binary_logloss: 0.576778\n",
      "[311]\ttraining's binary_logloss: 0.576768\n",
      "[312]\ttraining's binary_logloss: 0.576748\n",
      "[313]\ttraining's binary_logloss: 0.576723\n",
      "[314]\ttraining's binary_logloss: 0.576717\n",
      "[315]\ttraining's binary_logloss: 0.576695\n",
      "[316]\ttraining's binary_logloss: 0.576673\n",
      "[317]\ttraining's binary_logloss: 0.576652\n",
      "[318]\ttraining's binary_logloss: 0.576631\n",
      "[319]\ttraining's binary_logloss: 0.576624\n",
      "[320]\ttraining's binary_logloss: 0.576601\n",
      "[321]\ttraining's binary_logloss: 0.576584\n",
      "[322]\ttraining's binary_logloss: 0.576566\n",
      "[323]\ttraining's binary_logloss: 0.576542\n",
      "[324]\ttraining's binary_logloss: 0.576519\n",
      "[325]\ttraining's binary_logloss: 0.576506\n",
      "[326]\ttraining's binary_logloss: 0.576487\n",
      "[327]\ttraining's binary_logloss: 0.576472\n",
      "[328]\ttraining's binary_logloss: 0.576458\n",
      "[329]\ttraining's binary_logloss: 0.57644\n",
      "[330]\ttraining's binary_logloss: 0.576423\n",
      "[331]\ttraining's binary_logloss: 0.576411\n",
      "[332]\ttraining's binary_logloss: 0.5764\n",
      "[333]\ttraining's binary_logloss: 0.576384\n",
      "[334]\ttraining's binary_logloss: 0.576373\n",
      "[335]\ttraining's binary_logloss: 0.576355\n",
      "[336]\ttraining's binary_logloss: 0.576341\n",
      "[337]\ttraining's binary_logloss: 0.576332\n",
      "[338]\ttraining's binary_logloss: 0.576325\n",
      "[339]\ttraining's binary_logloss: 0.576305\n",
      "[340]\ttraining's binary_logloss: 0.576282\n",
      "[341]\ttraining's binary_logloss: 0.57627\n",
      "[342]\ttraining's binary_logloss: 0.576259\n",
      "[343]\ttraining's binary_logloss: 0.576244\n",
      "[344]\ttraining's binary_logloss: 0.576227\n",
      "[345]\ttraining's binary_logloss: 0.576218\n",
      "[346]\ttraining's binary_logloss: 0.576206\n",
      "[347]\ttraining's binary_logloss: 0.576198\n",
      "[348]\ttraining's binary_logloss: 0.576191\n",
      "[349]\ttraining's binary_logloss: 0.576176\n",
      "[350]\ttraining's binary_logloss: 0.576161\n",
      "[351]\ttraining's binary_logloss: 0.576153\n",
      "[352]\ttraining's binary_logloss: 0.576141\n",
      "[353]\ttraining's binary_logloss: 0.576136\n",
      "[354]\ttraining's binary_logloss: 0.576126\n",
      "[355]\ttraining's binary_logloss: 0.576113\n",
      "[356]\ttraining's binary_logloss: 0.576103\n",
      "[357]\ttraining's binary_logloss: 0.576089\n",
      "[358]\ttraining's binary_logloss: 0.576077\n",
      "[359]\ttraining's binary_logloss: 0.576067\n",
      "[360]\ttraining's binary_logloss: 0.57606\n",
      "[361]\ttraining's binary_logloss: 0.576049\n",
      "[362]\ttraining's binary_logloss: 0.576037\n",
      "[363]\ttraining's binary_logloss: 0.57603\n",
      "[364]\ttraining's binary_logloss: 0.576026\n",
      "[365]\ttraining's binary_logloss: 0.576012\n",
      "[366]\ttraining's binary_logloss: 0.576003\n",
      "[367]\ttraining's binary_logloss: 0.57599\n",
      "[368]\ttraining's binary_logloss: 0.575985\n",
      "[369]\ttraining's binary_logloss: 0.575978\n",
      "[370]\ttraining's binary_logloss: 0.575967\n",
      "[371]\ttraining's binary_logloss: 0.575956\n",
      "[372]\ttraining's binary_logloss: 0.575949\n",
      "[373]\ttraining's binary_logloss: 0.575939\n",
      "[374]\ttraining's binary_logloss: 0.575928\n",
      "[375]\ttraining's binary_logloss: 0.575915\n",
      "[376]\ttraining's binary_logloss: 0.57591\n",
      "[377]\ttraining's binary_logloss: 0.575906\n",
      "[378]\ttraining's binary_logloss: 0.575897\n",
      "[379]\ttraining's binary_logloss: 0.575893\n",
      "[380]\ttraining's binary_logloss: 0.575879\n",
      "[381]\ttraining's binary_logloss: 0.575876\n",
      "[382]\ttraining's binary_logloss: 0.575869\n",
      "[383]\ttraining's binary_logloss: 0.575858\n",
      "[384]\ttraining's binary_logloss: 0.575847\n",
      "[385]\ttraining's binary_logloss: 0.575828\n",
      "[386]\ttraining's binary_logloss: 0.575819\n",
      "[387]\ttraining's binary_logloss: 0.575812\n",
      "[388]\ttraining's binary_logloss: 0.575796\n",
      "[389]\ttraining's binary_logloss: 0.575787\n",
      "[390]\ttraining's binary_logloss: 0.575769\n",
      "[391]\ttraining's binary_logloss: 0.575758\n",
      "[392]\ttraining's binary_logloss: 0.575741\n",
      "[393]\ttraining's binary_logloss: 0.575735\n",
      "[394]\ttraining's binary_logloss: 0.57573\n",
      "[395]\ttraining's binary_logloss: 0.575724\n",
      "[396]\ttraining's binary_logloss: 0.575715\n",
      "[397]\ttraining's binary_logloss: 0.575703\n",
      "[398]\ttraining's binary_logloss: 0.575695\n",
      "[399]\ttraining's binary_logloss: 0.575685\n",
      "[400]\ttraining's binary_logloss: 0.575678\n",
      "[401]\ttraining's binary_logloss: 0.575675\n",
      "[402]\ttraining's binary_logloss: 0.575669\n",
      "[403]\ttraining's binary_logloss: 0.575666\n",
      "[404]\ttraining's binary_logloss: 0.575658\n",
      "[405]\ttraining's binary_logloss: 0.575643\n",
      "[406]\ttraining's binary_logloss: 0.575634\n",
      "[407]\ttraining's binary_logloss: 0.575628\n",
      "[408]\ttraining's binary_logloss: 0.575621\n",
      "[409]\ttraining's binary_logloss: 0.575614\n",
      "[410]\ttraining's binary_logloss: 0.575604\n",
      "[411]\ttraining's binary_logloss: 0.575588\n",
      "[412]\ttraining's binary_logloss: 0.575583\n",
      "[413]\ttraining's binary_logloss: 0.575575\n",
      "[414]\ttraining's binary_logloss: 0.575562\n",
      "[415]\ttraining's binary_logloss: 0.575553\n",
      "[416]\ttraining's binary_logloss: 0.575548\n",
      "[417]\ttraining's binary_logloss: 0.575545\n",
      "[418]\ttraining's binary_logloss: 0.575539\n",
      "[419]\ttraining's binary_logloss: 0.575534\n",
      "[420]\ttraining's binary_logloss: 0.575527\n",
      "[421]\ttraining's binary_logloss: 0.57552\n",
      "[422]\ttraining's binary_logloss: 0.575516\n",
      "[423]\ttraining's binary_logloss: 0.575512\n",
      "[424]\ttraining's binary_logloss: 0.575502\n",
      "[425]\ttraining's binary_logloss: 0.575495\n",
      "[426]\ttraining's binary_logloss: 0.575488\n",
      "[427]\ttraining's binary_logloss: 0.575479\n",
      "[428]\ttraining's binary_logloss: 0.575472\n",
      "[429]\ttraining's binary_logloss: 0.575463\n",
      "[430]\ttraining's binary_logloss: 0.57546\n",
      "[431]\ttraining's binary_logloss: 0.575456\n",
      "[432]\ttraining's binary_logloss: 0.575451\n",
      "[433]\ttraining's binary_logloss: 0.575448\n",
      "[434]\ttraining's binary_logloss: 0.575434\n",
      "[435]\ttraining's binary_logloss: 0.575427\n",
      "[436]\ttraining's binary_logloss: 0.575421\n",
      "[437]\ttraining's binary_logloss: 0.575413\n",
      "[438]\ttraining's binary_logloss: 0.575405\n",
      "[439]\ttraining's binary_logloss: 0.575397\n",
      "[440]\ttraining's binary_logloss: 0.575389\n",
      "[441]\ttraining's binary_logloss: 0.575384\n",
      "[442]\ttraining's binary_logloss: 0.575376\n",
      "[443]\ttraining's binary_logloss: 0.57537\n",
      "[444]\ttraining's binary_logloss: 0.575366\n",
      "[445]\ttraining's binary_logloss: 0.575361\n",
      "[446]\ttraining's binary_logloss: 0.575353\n",
      "[447]\ttraining's binary_logloss: 0.575351\n",
      "[448]\ttraining's binary_logloss: 0.575345\n",
      "[449]\ttraining's binary_logloss: 0.57534\n",
      "[450]\ttraining's binary_logloss: 0.575332\n",
      "[451]\ttraining's binary_logloss: 0.57533\n",
      "[452]\ttraining's binary_logloss: 0.575324\n",
      "[453]\ttraining's binary_logloss: 0.57532\n",
      "[454]\ttraining's binary_logloss: 0.575313\n",
      "[455]\ttraining's binary_logloss: 0.575306\n",
      "[456]\ttraining's binary_logloss: 0.5753\n",
      "[457]\ttraining's binary_logloss: 0.575294\n",
      "[458]\ttraining's binary_logloss: 0.575288\n",
      "[459]\ttraining's binary_logloss: 0.575281\n",
      "[460]\ttraining's binary_logloss: 0.575275\n",
      "[461]\ttraining's binary_logloss: 0.575272\n",
      "[462]\ttraining's binary_logloss: 0.57527\n",
      "[463]\ttraining's binary_logloss: 0.575267\n",
      "[464]\ttraining's binary_logloss: 0.575264\n",
      "[465]\ttraining's binary_logloss: 0.575258\n",
      "[466]\ttraining's binary_logloss: 0.575254\n",
      "[467]\ttraining's binary_logloss: 0.575251\n",
      "[468]\ttraining's binary_logloss: 0.575248\n",
      "[469]\ttraining's binary_logloss: 0.57524\n",
      "[470]\ttraining's binary_logloss: 0.575237\n",
      "[471]\ttraining's binary_logloss: 0.575232\n",
      "[472]\ttraining's binary_logloss: 0.575227\n",
      "[473]\ttraining's binary_logloss: 0.575223\n",
      "[474]\ttraining's binary_logloss: 0.575221\n",
      "[475]\ttraining's binary_logloss: 0.575217\n",
      "[476]\ttraining's binary_logloss: 0.575211\n",
      "[477]\ttraining's binary_logloss: 0.575207\n",
      "[478]\ttraining's binary_logloss: 0.575203\n",
      "[479]\ttraining's binary_logloss: 0.575199\n",
      "[480]\ttraining's binary_logloss: 0.575194\n",
      "[481]\ttraining's binary_logloss: 0.57519\n",
      "[482]\ttraining's binary_logloss: 0.575186\n",
      "[483]\ttraining's binary_logloss: 0.57518\n",
      "[484]\ttraining's binary_logloss: 0.575174\n",
      "[485]\ttraining's binary_logloss: 0.57517\n",
      "[486]\ttraining's binary_logloss: 0.575165\n",
      "[487]\ttraining's binary_logloss: 0.575162\n",
      "[488]\ttraining's binary_logloss: 0.575159\n",
      "[489]\ttraining's binary_logloss: 0.575156\n",
      "[490]\ttraining's binary_logloss: 0.575152\n",
      "[491]\ttraining's binary_logloss: 0.575146\n",
      "[492]\ttraining's binary_logloss: 0.575141\n",
      "[493]\ttraining's binary_logloss: 0.575138\n",
      "[494]\ttraining's binary_logloss: 0.575132\n",
      "[495]\ttraining's binary_logloss: 0.575128\n",
      "[496]\ttraining's binary_logloss: 0.575126\n",
      "[497]\ttraining's binary_logloss: 0.575119\n",
      "[498]\ttraining's binary_logloss: 0.575114\n",
      "[499]\ttraining's binary_logloss: 0.575111\n",
      "[500]\ttraining's binary_logloss: 0.575106\n",
      "[501]\ttraining's binary_logloss: 0.5751\n",
      "[502]\ttraining's binary_logloss: 0.575096\n",
      "[503]\ttraining's binary_logloss: 0.575092\n",
      "[504]\ttraining's binary_logloss: 0.575085\n",
      "[505]\ttraining's binary_logloss: 0.575081\n",
      "[506]\ttraining's binary_logloss: 0.575079\n",
      "[507]\ttraining's binary_logloss: 0.575076\n",
      "[508]\ttraining's binary_logloss: 0.575073\n",
      "[509]\ttraining's binary_logloss: 0.575071\n",
      "[510]\ttraining's binary_logloss: 0.575068\n",
      "[511]\ttraining's binary_logloss: 0.575066\n",
      "[512]\ttraining's binary_logloss: 0.575065\n",
      "[513]\ttraining's binary_logloss: 0.575062\n",
      "[514]\ttraining's binary_logloss: 0.575058\n",
      "[515]\ttraining's binary_logloss: 0.575056\n",
      "[516]\ttraining's binary_logloss: 0.575051\n",
      "[517]\ttraining's binary_logloss: 0.575049\n",
      "[518]\ttraining's binary_logloss: 0.575045\n",
      "[519]\ttraining's binary_logloss: 0.575042\n",
      "[520]\ttraining's binary_logloss: 0.575039\n",
      "[521]\ttraining's binary_logloss: 0.575037\n",
      "[522]\ttraining's binary_logloss: 0.575033\n",
      "[523]\ttraining's binary_logloss: 0.575032\n",
      "[524]\ttraining's binary_logloss: 0.575029\n",
      "[525]\ttraining's binary_logloss: 0.575026\n",
      "[526]\ttraining's binary_logloss: 0.575021\n",
      "[527]\ttraining's binary_logloss: 0.575018\n",
      "[528]\ttraining's binary_logloss: 0.575015\n",
      "[529]\ttraining's binary_logloss: 0.575011\n",
      "[530]\ttraining's binary_logloss: 0.575008\n",
      "[531]\ttraining's binary_logloss: 0.575004\n",
      "[532]\ttraining's binary_logloss: 0.575003\n",
      "[533]\ttraining's binary_logloss: 0.574999\n",
      "[534]\ttraining's binary_logloss: 0.574996\n",
      "[535]\ttraining's binary_logloss: 0.574993\n",
      "[536]\ttraining's binary_logloss: 0.574991\n",
      "[537]\ttraining's binary_logloss: 0.574986\n",
      "[538]\ttraining's binary_logloss: 0.574983\n",
      "[539]\ttraining's binary_logloss: 0.574978\n",
      "[540]\ttraining's binary_logloss: 0.574974\n",
      "[541]\ttraining's binary_logloss: 0.57497\n",
      "[542]\ttraining's binary_logloss: 0.574967\n",
      "[543]\ttraining's binary_logloss: 0.574965\n",
      "[544]\ttraining's binary_logloss: 0.574962\n",
      "[545]\ttraining's binary_logloss: 0.574959\n",
      "[546]\ttraining's binary_logloss: 0.574958\n",
      "[547]\ttraining's binary_logloss: 0.574955\n",
      "[548]\ttraining's binary_logloss: 0.574953\n",
      "[549]\ttraining's binary_logloss: 0.574951\n",
      "[550]\ttraining's binary_logloss: 0.574949\n",
      "[551]\ttraining's binary_logloss: 0.574944\n",
      "[552]\ttraining's binary_logloss: 0.57494\n",
      "[553]\ttraining's binary_logloss: 0.574938\n",
      "[554]\ttraining's binary_logloss: 0.574934\n",
      "[555]\ttraining's binary_logloss: 0.574931\n",
      "[556]\ttraining's binary_logloss: 0.574929\n",
      "[557]\ttraining's binary_logloss: 0.574927\n",
      "[558]\ttraining's binary_logloss: 0.574924\n",
      "[559]\ttraining's binary_logloss: 0.574921\n",
      "[560]\ttraining's binary_logloss: 0.574918\n",
      "[561]\ttraining's binary_logloss: 0.574916\n",
      "[562]\ttraining's binary_logloss: 0.574914\n",
      "[563]\ttraining's binary_logloss: 0.574911\n",
      "[564]\ttraining's binary_logloss: 0.574909\n",
      "[565]\ttraining's binary_logloss: 0.574908\n",
      "[566]\ttraining's binary_logloss: 0.574904\n",
      "[567]\ttraining's binary_logloss: 0.5749\n",
      "[568]\ttraining's binary_logloss: 0.574898\n",
      "[569]\ttraining's binary_logloss: 0.574896\n",
      "[570]\ttraining's binary_logloss: 0.574893\n",
      "[571]\ttraining's binary_logloss: 0.574891\n",
      "[572]\ttraining's binary_logloss: 0.574889\n",
      "[573]\ttraining's binary_logloss: 0.574888\n",
      "[574]\ttraining's binary_logloss: 0.574887\n",
      "[575]\ttraining's binary_logloss: 0.574884\n",
      "[576]\ttraining's binary_logloss: 0.574878\n",
      "[577]\ttraining's binary_logloss: 0.574878\n",
      "[578]\ttraining's binary_logloss: 0.574876\n",
      "[579]\ttraining's binary_logloss: 0.574874\n",
      "[580]\ttraining's binary_logloss: 0.574871\n",
      "[581]\ttraining's binary_logloss: 0.57487\n",
      "[582]\ttraining's binary_logloss: 0.574866\n",
      "[583]\ttraining's binary_logloss: 0.574865\n",
      "[584]\ttraining's binary_logloss: 0.574863\n",
      "[585]\ttraining's binary_logloss: 0.574861\n",
      "[586]\ttraining's binary_logloss: 0.574859\n",
      "[587]\ttraining's binary_logloss: 0.574856\n",
      "[588]\ttraining's binary_logloss: 0.574854\n",
      "[589]\ttraining's binary_logloss: 0.574851\n",
      "[590]\ttraining's binary_logloss: 0.574849\n",
      "[591]\ttraining's binary_logloss: 0.574847\n",
      "[592]\ttraining's binary_logloss: 0.574846\n",
      "[593]\ttraining's binary_logloss: 0.574843\n",
      "[594]\ttraining's binary_logloss: 0.574841\n",
      "[595]\ttraining's binary_logloss: 0.57484\n",
      "[596]\ttraining's binary_logloss: 0.574838\n",
      "[597]\ttraining's binary_logloss: 0.574836\n",
      "[598]\ttraining's binary_logloss: 0.574835\n",
      "[599]\ttraining's binary_logloss: 0.574833\n",
      "[600]\ttraining's binary_logloss: 0.574829\n",
      "[1]\ttraining's binary_logloss: 0.63033\n",
      "[2]\ttraining's binary_logloss: 0.625711\n",
      "[3]\ttraining's binary_logloss: 0.621914\n",
      "[4]\ttraining's binary_logloss: 0.61874\n",
      "[5]\ttraining's binary_logloss: 0.616073\n",
      "[6]\ttraining's binary_logloss: 0.613752\n",
      "[7]\ttraining's binary_logloss: 0.611686\n",
      "[8]\ttraining's binary_logloss: 0.609981\n",
      "[9]\ttraining's binary_logloss: 0.608295\n",
      "[10]\ttraining's binary_logloss: 0.606883\n",
      "[11]\ttraining's binary_logloss: 0.605663\n",
      "[12]\ttraining's binary_logloss: 0.604389\n",
      "[13]\ttraining's binary_logloss: 0.603298\n",
      "[14]\ttraining's binary_logloss: 0.60245\n",
      "[15]\ttraining's binary_logloss: 0.601499\n",
      "[16]\ttraining's binary_logloss: 0.600647\n",
      "[17]\ttraining's binary_logloss: 0.599937\n",
      "[18]\ttraining's binary_logloss: 0.599225\n",
      "[19]\ttraining's binary_logloss: 0.59841\n",
      "[20]\ttraining's binary_logloss: 0.597804\n",
      "[21]\ttraining's binary_logloss: 0.597085\n",
      "[22]\ttraining's binary_logloss: 0.59655\n",
      "[23]\ttraining's binary_logloss: 0.596017\n",
      "[24]\ttraining's binary_logloss: 0.595417\n",
      "[25]\ttraining's binary_logloss: 0.594902\n",
      "[26]\ttraining's binary_logloss: 0.594394\n",
      "[27]\ttraining's binary_logloss: 0.593898\n",
      "[28]\ttraining's binary_logloss: 0.593461\n",
      "[29]\ttraining's binary_logloss: 0.592999\n",
      "[30]\ttraining's binary_logloss: 0.592399\n",
      "[31]\ttraining's binary_logloss: 0.592041\n",
      "[32]\ttraining's binary_logloss: 0.591588\n",
      "[33]\ttraining's binary_logloss: 0.591201\n",
      "[34]\ttraining's binary_logloss: 0.590832\n",
      "[35]\ttraining's binary_logloss: 0.590363\n",
      "[36]\ttraining's binary_logloss: 0.589924\n",
      "[37]\ttraining's binary_logloss: 0.589368\n",
      "[38]\ttraining's binary_logloss: 0.589051\n",
      "[39]\ttraining's binary_logloss: 0.588721\n",
      "[40]\ttraining's binary_logloss: 0.588386\n",
      "[41]\ttraining's binary_logloss: 0.588044\n",
      "[42]\ttraining's binary_logloss: 0.587785\n",
      "[43]\ttraining's binary_logloss: 0.587471\n",
      "[44]\ttraining's binary_logloss: 0.587148\n",
      "[45]\ttraining's binary_logloss: 0.586785\n",
      "[46]\ttraining's binary_logloss: 0.58646\n",
      "[47]\ttraining's binary_logloss: 0.586138\n",
      "[48]\ttraining's binary_logloss: 0.585829\n",
      "[49]\ttraining's binary_logloss: 0.585611\n",
      "[50]\ttraining's binary_logloss: 0.585372\n",
      "[51]\ttraining's binary_logloss: 0.584879\n",
      "[52]\ttraining's binary_logloss: 0.584558\n",
      "[53]\ttraining's binary_logloss: 0.584103\n",
      "[54]\ttraining's binary_logloss: 0.583793\n",
      "[55]\ttraining's binary_logloss: 0.58353\n",
      "[56]\ttraining's binary_logloss: 0.583233\n",
      "[57]\ttraining's binary_logloss: 0.58303\n",
      "[58]\ttraining's binary_logloss: 0.582667\n",
      "[59]\ttraining's binary_logloss: 0.582378\n",
      "[60]\ttraining's binary_logloss: 0.581995\n",
      "[61]\ttraining's binary_logloss: 0.581775\n",
      "[62]\ttraining's binary_logloss: 0.581544\n",
      "[63]\ttraining's binary_logloss: 0.581217\n",
      "[64]\ttraining's binary_logloss: 0.581029\n",
      "[65]\ttraining's binary_logloss: 0.58076\n",
      "[66]\ttraining's binary_logloss: 0.580515\n",
      "[67]\ttraining's binary_logloss: 0.580212\n",
      "[68]\ttraining's binary_logloss: 0.579797\n",
      "[69]\ttraining's binary_logloss: 0.57959\n",
      "[70]\ttraining's binary_logloss: 0.57932\n",
      "[71]\ttraining's binary_logloss: 0.579103\n",
      "[72]\ttraining's binary_logloss: 0.578801\n",
      "[73]\ttraining's binary_logloss: 0.57848\n",
      "[74]\ttraining's binary_logloss: 0.57811\n",
      "[75]\ttraining's binary_logloss: 0.577713\n",
      "[76]\ttraining's binary_logloss: 0.577465\n",
      "[77]\ttraining's binary_logloss: 0.577218\n",
      "[78]\ttraining's binary_logloss: 0.5769\n",
      "[79]\ttraining's binary_logloss: 0.576678\n",
      "[80]\ttraining's binary_logloss: 0.57637\n",
      "[81]\ttraining's binary_logloss: 0.576128\n",
      "[82]\ttraining's binary_logloss: 0.57596\n",
      "[83]\ttraining's binary_logloss: 0.575725\n",
      "[84]\ttraining's binary_logloss: 0.575415\n",
      "[85]\ttraining's binary_logloss: 0.575157\n",
      "[86]\ttraining's binary_logloss: 0.574965\n",
      "[87]\ttraining's binary_logloss: 0.57465\n",
      "[88]\ttraining's binary_logloss: 0.574459\n",
      "[89]\ttraining's binary_logloss: 0.57431\n",
      "[90]\ttraining's binary_logloss: 0.573962\n",
      "[91]\ttraining's binary_logloss: 0.573782\n",
      "[92]\ttraining's binary_logloss: 0.57356\n",
      "[93]\ttraining's binary_logloss: 0.573298\n",
      "[94]\ttraining's binary_logloss: 0.573006\n",
      "[95]\ttraining's binary_logloss: 0.572666\n",
      "[96]\ttraining's binary_logloss: 0.57254\n",
      "[97]\ttraining's binary_logloss: 0.572376\n",
      "[98]\ttraining's binary_logloss: 0.572133\n",
      "[99]\ttraining's binary_logloss: 0.571877\n",
      "[100]\ttraining's binary_logloss: 0.571583\n",
      "[101]\ttraining's binary_logloss: 0.571247\n",
      "[102]\ttraining's binary_logloss: 0.571062\n",
      "[103]\ttraining's binary_logloss: 0.570765\n",
      "[104]\ttraining's binary_logloss: 0.570541\n",
      "[105]\ttraining's binary_logloss: 0.570185\n",
      "[106]\ttraining's binary_logloss: 0.569948\n",
      "[107]\ttraining's binary_logloss: 0.569645\n",
      "[108]\ttraining's binary_logloss: 0.569406\n",
      "[109]\ttraining's binary_logloss: 0.569178\n",
      "[110]\ttraining's binary_logloss: 0.568957\n",
      "[111]\ttraining's binary_logloss: 0.568663\n",
      "[112]\ttraining's binary_logloss: 0.568299\n",
      "[113]\ttraining's binary_logloss: 0.568147\n",
      "[114]\ttraining's binary_logloss: 0.567994\n",
      "[115]\ttraining's binary_logloss: 0.567686\n",
      "[116]\ttraining's binary_logloss: 0.567431\n",
      "[117]\ttraining's binary_logloss: 0.567281\n",
      "[118]\ttraining's binary_logloss: 0.567116\n",
      "[119]\ttraining's binary_logloss: 0.566916\n",
      "[120]\ttraining's binary_logloss: 0.566589\n",
      "[121]\ttraining's binary_logloss: 0.56637\n",
      "[122]\ttraining's binary_logloss: 0.566238\n",
      "[123]\ttraining's binary_logloss: 0.56606\n",
      "[124]\ttraining's binary_logloss: 0.565775\n",
      "[125]\ttraining's binary_logloss: 0.565453\n",
      "[126]\ttraining's binary_logloss: 0.565263\n",
      "[127]\ttraining's binary_logloss: 0.56501\n",
      "[128]\ttraining's binary_logloss: 0.564699\n",
      "[129]\ttraining's binary_logloss: 0.564521\n",
      "[130]\ttraining's binary_logloss: 0.564269\n",
      "[131]\ttraining's binary_logloss: 0.564027\n",
      "[132]\ttraining's binary_logloss: 0.563905\n",
      "[133]\ttraining's binary_logloss: 0.563705\n",
      "[134]\ttraining's binary_logloss: 0.563422\n",
      "[135]\ttraining's binary_logloss: 0.563199\n",
      "[136]\ttraining's binary_logloss: 0.563004\n",
      "[137]\ttraining's binary_logloss: 0.562888\n",
      "[138]\ttraining's binary_logloss: 0.562615\n",
      "[139]\ttraining's binary_logloss: 0.562466\n",
      "[140]\ttraining's binary_logloss: 0.562102\n",
      "[141]\ttraining's binary_logloss: 0.561859\n",
      "[142]\ttraining's binary_logloss: 0.561589\n",
      "[143]\ttraining's binary_logloss: 0.561269\n",
      "[144]\ttraining's binary_logloss: 0.561055\n",
      "[145]\ttraining's binary_logloss: 0.560773\n",
      "[146]\ttraining's binary_logloss: 0.560586\n",
      "[147]\ttraining's binary_logloss: 0.560423\n",
      "[148]\ttraining's binary_logloss: 0.560238\n",
      "[149]\ttraining's binary_logloss: 0.560039\n",
      "[150]\ttraining's binary_logloss: 0.559776\n",
      "[151]\ttraining's binary_logloss: 0.559618\n",
      "[152]\ttraining's binary_logloss: 0.559444\n",
      "[153]\ttraining's binary_logloss: 0.559244\n",
      "[154]\ttraining's binary_logloss: 0.558971\n",
      "[155]\ttraining's binary_logloss: 0.558755\n",
      "[156]\ttraining's binary_logloss: 0.558647\n",
      "[157]\ttraining's binary_logloss: 0.558452\n",
      "[158]\ttraining's binary_logloss: 0.558333\n",
      "[159]\ttraining's binary_logloss: 0.558193\n",
      "[160]\ttraining's binary_logloss: 0.558032\n",
      "[161]\ttraining's binary_logloss: 0.55778\n",
      "[162]\ttraining's binary_logloss: 0.557563\n",
      "[163]\ttraining's binary_logloss: 0.557406\n",
      "[164]\ttraining's binary_logloss: 0.557307\n",
      "[165]\ttraining's binary_logloss: 0.557067\n",
      "[166]\ttraining's binary_logloss: 0.556812\n",
      "[167]\ttraining's binary_logloss: 0.55658\n",
      "[168]\ttraining's binary_logloss: 0.556433\n",
      "[169]\ttraining's binary_logloss: 0.556184\n",
      "[170]\ttraining's binary_logloss: 0.555981\n",
      "[171]\ttraining's binary_logloss: 0.555846\n",
      "[172]\ttraining's binary_logloss: 0.555745\n",
      "[173]\ttraining's binary_logloss: 0.555516\n",
      "[174]\ttraining's binary_logloss: 0.555308\n",
      "[175]\ttraining's binary_logloss: 0.555128\n",
      "[176]\ttraining's binary_logloss: 0.554924\n",
      "[177]\ttraining's binary_logloss: 0.554775\n",
      "[178]\ttraining's binary_logloss: 0.554546\n",
      "[179]\ttraining's binary_logloss: 0.554323\n",
      "[180]\ttraining's binary_logloss: 0.554109\n",
      "[181]\ttraining's binary_logloss: 0.553948\n",
      "[182]\ttraining's binary_logloss: 0.553793\n",
      "[183]\ttraining's binary_logloss: 0.553544\n",
      "[184]\ttraining's binary_logloss: 0.553435\n",
      "[185]\ttraining's binary_logloss: 0.553248\n",
      "[186]\ttraining's binary_logloss: 0.553128\n",
      "[187]\ttraining's binary_logloss: 0.552893\n",
      "[188]\ttraining's binary_logloss: 0.55273\n",
      "[189]\ttraining's binary_logloss: 0.55256\n",
      "[190]\ttraining's binary_logloss: 0.55239\n",
      "[191]\ttraining's binary_logloss: 0.552228\n",
      "[192]\ttraining's binary_logloss: 0.552017\n",
      "[193]\ttraining's binary_logloss: 0.551804\n",
      "[194]\ttraining's binary_logloss: 0.551576\n",
      "[195]\ttraining's binary_logloss: 0.55132\n",
      "[196]\ttraining's binary_logloss: 0.551141\n",
      "[197]\ttraining's binary_logloss: 0.551037\n",
      "[198]\ttraining's binary_logloss: 0.550744\n",
      "[199]\ttraining's binary_logloss: 0.550546\n",
      "[200]\ttraining's binary_logloss: 0.550368\n",
      "[201]\ttraining's binary_logloss: 0.550077\n",
      "[202]\ttraining's binary_logloss: 0.549906\n",
      "[203]\ttraining's binary_logloss: 0.549704\n",
      "[204]\ttraining's binary_logloss: 0.549554\n",
      "[205]\ttraining's binary_logloss: 0.549262\n",
      "[206]\ttraining's binary_logloss: 0.549039\n",
      "[207]\ttraining's binary_logloss: 0.548913\n",
      "[208]\ttraining's binary_logloss: 0.548675\n",
      "[209]\ttraining's binary_logloss: 0.548492\n",
      "[210]\ttraining's binary_logloss: 0.548218\n",
      "[211]\ttraining's binary_logloss: 0.54806\n",
      "[212]\ttraining's binary_logloss: 0.547833\n",
      "[213]\ttraining's binary_logloss: 0.547621\n",
      "[214]\ttraining's binary_logloss: 0.547517\n",
      "[215]\ttraining's binary_logloss: 0.5474\n",
      "[216]\ttraining's binary_logloss: 0.547234\n",
      "[217]\ttraining's binary_logloss: 0.547049\n",
      "[218]\ttraining's binary_logloss: 0.54683\n",
      "[219]\ttraining's binary_logloss: 0.546741\n",
      "[220]\ttraining's binary_logloss: 0.546587\n",
      "[221]\ttraining's binary_logloss: 0.546439\n",
      "[222]\ttraining's binary_logloss: 0.546327\n",
      "[223]\ttraining's binary_logloss: 0.546107\n",
      "[224]\ttraining's binary_logloss: 0.545924\n",
      "[225]\ttraining's binary_logloss: 0.545703\n",
      "[226]\ttraining's binary_logloss: 0.545517\n",
      "[227]\ttraining's binary_logloss: 0.545356\n",
      "[228]\ttraining's binary_logloss: 0.545143\n",
      "[229]\ttraining's binary_logloss: 0.544949\n",
      "[230]\ttraining's binary_logloss: 0.544671\n",
      "[231]\ttraining's binary_logloss: 0.544511\n",
      "[232]\ttraining's binary_logloss: 0.544363\n",
      "[233]\ttraining's binary_logloss: 0.544141\n",
      "[234]\ttraining's binary_logloss: 0.543983\n",
      "[235]\ttraining's binary_logloss: 0.543812\n",
      "[236]\ttraining's binary_logloss: 0.543613\n",
      "[237]\ttraining's binary_logloss: 0.543438\n",
      "[238]\ttraining's binary_logloss: 0.543177\n",
      "[239]\ttraining's binary_logloss: 0.542964\n",
      "[240]\ttraining's binary_logloss: 0.542763\n",
      "[241]\ttraining's binary_logloss: 0.542671\n",
      "[242]\ttraining's binary_logloss: 0.542581\n",
      "[243]\ttraining's binary_logloss: 0.542492\n",
      "[244]\ttraining's binary_logloss: 0.54231\n",
      "[245]\ttraining's binary_logloss: 0.542142\n",
      "[246]\ttraining's binary_logloss: 0.541969\n",
      "[247]\ttraining's binary_logloss: 0.541836\n",
      "[248]\ttraining's binary_logloss: 0.541664\n",
      "[249]\ttraining's binary_logloss: 0.541569\n",
      "[250]\ttraining's binary_logloss: 0.541431\n",
      "[251]\ttraining's binary_logloss: 0.541205\n",
      "[252]\ttraining's binary_logloss: 0.541042\n",
      "[253]\ttraining's binary_logloss: 0.540843\n",
      "[254]\ttraining's binary_logloss: 0.540595\n",
      "[255]\ttraining's binary_logloss: 0.540359\n",
      "[256]\ttraining's binary_logloss: 0.540104\n",
      "[257]\ttraining's binary_logloss: 0.539869\n",
      "[258]\ttraining's binary_logloss: 0.539702\n",
      "[259]\ttraining's binary_logloss: 0.539507\n",
      "[260]\ttraining's binary_logloss: 0.539419\n",
      "[261]\ttraining's binary_logloss: 0.539249\n",
      "[262]\ttraining's binary_logloss: 0.53918\n",
      "[263]\ttraining's binary_logloss: 0.538995\n",
      "[264]\ttraining's binary_logloss: 0.538824\n",
      "[265]\ttraining's binary_logloss: 0.538662\n",
      "[266]\ttraining's binary_logloss: 0.538552\n",
      "[267]\ttraining's binary_logloss: 0.538333\n",
      "[268]\ttraining's binary_logloss: 0.538164\n",
      "[269]\ttraining's binary_logloss: 0.538006\n",
      "[270]\ttraining's binary_logloss: 0.537803\n",
      "[271]\ttraining's binary_logloss: 0.537648\n",
      "[272]\ttraining's binary_logloss: 0.537479\n",
      "[273]\ttraining's binary_logloss: 0.537343\n",
      "[274]\ttraining's binary_logloss: 0.5372\n",
      "[275]\ttraining's binary_logloss: 0.536988\n",
      "[276]\ttraining's binary_logloss: 0.536893\n",
      "[277]\ttraining's binary_logloss: 0.536703\n",
      "[278]\ttraining's binary_logloss: 0.536599\n",
      "[279]\ttraining's binary_logloss: 0.536448\n",
      "[280]\ttraining's binary_logloss: 0.536308\n",
      "[281]\ttraining's binary_logloss: 0.536138\n",
      "[282]\ttraining's binary_logloss: 0.536005\n",
      "[283]\ttraining's binary_logloss: 0.535811\n",
      "[284]\ttraining's binary_logloss: 0.53563\n",
      "[285]\ttraining's binary_logloss: 0.535419\n",
      "[286]\ttraining's binary_logloss: 0.535265\n",
      "[287]\ttraining's binary_logloss: 0.535137\n",
      "[288]\ttraining's binary_logloss: 0.534998\n",
      "[289]\ttraining's binary_logloss: 0.53482\n",
      "[290]\ttraining's binary_logloss: 0.534651\n",
      "[291]\ttraining's binary_logloss: 0.534466\n",
      "[292]\ttraining's binary_logloss: 0.534206\n",
      "[293]\ttraining's binary_logloss: 0.534069\n",
      "[294]\ttraining's binary_logloss: 0.533899\n",
      "[295]\ttraining's binary_logloss: 0.533779\n",
      "[296]\ttraining's binary_logloss: 0.533649\n",
      "[297]\ttraining's binary_logloss: 0.53347\n",
      "[298]\ttraining's binary_logloss: 0.53329\n",
      "[299]\ttraining's binary_logloss: 0.53307\n",
      "[300]\ttraining's binary_logloss: 0.532953\n",
      "[301]\ttraining's binary_logloss: 0.532858\n",
      "[302]\ttraining's binary_logloss: 0.532741\n",
      "[303]\ttraining's binary_logloss: 0.532476\n",
      "[304]\ttraining's binary_logloss: 0.532312\n",
      "[305]\ttraining's binary_logloss: 0.532155\n",
      "[306]\ttraining's binary_logloss: 0.531941\n",
      "[307]\ttraining's binary_logloss: 0.531825\n",
      "[308]\ttraining's binary_logloss: 0.531735\n",
      "[309]\ttraining's binary_logloss: 0.53144\n",
      "[310]\ttraining's binary_logloss: 0.531248\n",
      "[311]\ttraining's binary_logloss: 0.531095\n",
      "[312]\ttraining's binary_logloss: 0.530974\n",
      "[313]\ttraining's binary_logloss: 0.530791\n",
      "[314]\ttraining's binary_logloss: 0.530616\n",
      "[315]\ttraining's binary_logloss: 0.530497\n",
      "[316]\ttraining's binary_logloss: 0.530338\n",
      "[317]\ttraining's binary_logloss: 0.530188\n",
      "[318]\ttraining's binary_logloss: 0.530017\n",
      "[319]\ttraining's binary_logloss: 0.529866\n",
      "[320]\ttraining's binary_logloss: 0.52966\n",
      "[321]\ttraining's binary_logloss: 0.529458\n",
      "[322]\ttraining's binary_logloss: 0.529371\n",
      "[323]\ttraining's binary_logloss: 0.529223\n",
      "[324]\ttraining's binary_logloss: 0.52906\n",
      "[325]\ttraining's binary_logloss: 0.528961\n",
      "[326]\ttraining's binary_logloss: 0.528742\n",
      "[327]\ttraining's binary_logloss: 0.52862\n",
      "[328]\ttraining's binary_logloss: 0.528519\n",
      "[329]\ttraining's binary_logloss: 0.528322\n",
      "[330]\ttraining's binary_logloss: 0.528149\n",
      "[331]\ttraining's binary_logloss: 0.528008\n",
      "[332]\ttraining's binary_logloss: 0.527822\n",
      "[333]\ttraining's binary_logloss: 0.527697\n",
      "[334]\ttraining's binary_logloss: 0.527606\n",
      "[335]\ttraining's binary_logloss: 0.527466\n",
      "[336]\ttraining's binary_logloss: 0.527401\n",
      "[337]\ttraining's binary_logloss: 0.527261\n",
      "[338]\ttraining's binary_logloss: 0.527172\n",
      "[339]\ttraining's binary_logloss: 0.527021\n",
      "[340]\ttraining's binary_logloss: 0.5269\n",
      "[341]\ttraining's binary_logloss: 0.526749\n",
      "[342]\ttraining's binary_logloss: 0.526646\n",
      "[343]\ttraining's binary_logloss: 0.526518\n",
      "[344]\ttraining's binary_logloss: 0.526398\n",
      "[345]\ttraining's binary_logloss: 0.526234\n",
      "[346]\ttraining's binary_logloss: 0.526048\n",
      "[347]\ttraining's binary_logloss: 0.525934\n",
      "[348]\ttraining's binary_logloss: 0.525794\n",
      "[349]\ttraining's binary_logloss: 0.525567\n",
      "[350]\ttraining's binary_logloss: 0.525418\n",
      "[351]\ttraining's binary_logloss: 0.525253\n",
      "[352]\ttraining's binary_logloss: 0.525072\n",
      "[353]\ttraining's binary_logloss: 0.524954\n",
      "[354]\ttraining's binary_logloss: 0.524869\n",
      "[355]\ttraining's binary_logloss: 0.524725\n",
      "[356]\ttraining's binary_logloss: 0.524497\n",
      "[357]\ttraining's binary_logloss: 0.524308\n",
      "[358]\ttraining's binary_logloss: 0.524217\n",
      "[359]\ttraining's binary_logloss: 0.52407\n",
      "[360]\ttraining's binary_logloss: 0.523916\n",
      "[361]\ttraining's binary_logloss: 0.523714\n",
      "[362]\ttraining's binary_logloss: 0.5236\n",
      "[363]\ttraining's binary_logloss: 0.52334\n",
      "[364]\ttraining's binary_logloss: 0.523237\n",
      "[365]\ttraining's binary_logloss: 0.523015\n",
      "[366]\ttraining's binary_logloss: 0.52289\n",
      "[367]\ttraining's binary_logloss: 0.52271\n",
      "[368]\ttraining's binary_logloss: 0.522551\n",
      "[369]\ttraining's binary_logloss: 0.522435\n",
      "[370]\ttraining's binary_logloss: 0.522313\n",
      "[371]\ttraining's binary_logloss: 0.522183\n",
      "[372]\ttraining's binary_logloss: 0.522082\n",
      "[373]\ttraining's binary_logloss: 0.521989\n",
      "[374]\ttraining's binary_logloss: 0.52186\n",
      "[375]\ttraining's binary_logloss: 0.521704\n",
      "[376]\ttraining's binary_logloss: 0.521585\n",
      "[377]\ttraining's binary_logloss: 0.521352\n",
      "[378]\ttraining's binary_logloss: 0.521124\n",
      "[379]\ttraining's binary_logloss: 0.521037\n",
      "[380]\ttraining's binary_logloss: 0.520844\n",
      "[381]\ttraining's binary_logloss: 0.520693\n",
      "[382]\ttraining's binary_logloss: 0.520567\n",
      "[383]\ttraining's binary_logloss: 0.520429\n",
      "[384]\ttraining's binary_logloss: 0.520272\n",
      "[385]\ttraining's binary_logloss: 0.520158\n",
      "[386]\ttraining's binary_logloss: 0.51999\n",
      "[387]\ttraining's binary_logloss: 0.519795\n",
      "[388]\ttraining's binary_logloss: 0.519685\n",
      "[389]\ttraining's binary_logloss: 0.519611\n",
      "[390]\ttraining's binary_logloss: 0.519472\n",
      "[391]\ttraining's binary_logloss: 0.519311\n",
      "[392]\ttraining's binary_logloss: 0.519205\n",
      "[393]\ttraining's binary_logloss: 0.519043\n",
      "[394]\ttraining's binary_logloss: 0.518955\n",
      "[395]\ttraining's binary_logloss: 0.51884\n",
      "[396]\ttraining's binary_logloss: 0.518704\n",
      "[397]\ttraining's binary_logloss: 0.518499\n",
      "[398]\ttraining's binary_logloss: 0.518403\n",
      "[399]\ttraining's binary_logloss: 0.518296\n",
      "[400]\ttraining's binary_logloss: 0.518184\n",
      "[401]\ttraining's binary_logloss: 0.518082\n",
      "[402]\ttraining's binary_logloss: 0.517874\n",
      "[403]\ttraining's binary_logloss: 0.517795\n",
      "[404]\ttraining's binary_logloss: 0.517717\n",
      "[405]\ttraining's binary_logloss: 0.517535\n",
      "[406]\ttraining's binary_logloss: 0.517378\n",
      "[407]\ttraining's binary_logloss: 0.517237\n",
      "[408]\ttraining's binary_logloss: 0.517047\n",
      "[409]\ttraining's binary_logloss: 0.516967\n",
      "[410]\ttraining's binary_logloss: 0.516826\n",
      "[411]\ttraining's binary_logloss: 0.516726\n",
      "[412]\ttraining's binary_logloss: 0.516545\n",
      "[413]\ttraining's binary_logloss: 0.516462\n",
      "[414]\ttraining's binary_logloss: 0.516392\n",
      "[415]\ttraining's binary_logloss: 0.516145\n",
      "[416]\ttraining's binary_logloss: 0.515981\n",
      "[417]\ttraining's binary_logloss: 0.515764\n",
      "[418]\ttraining's binary_logloss: 0.515667\n",
      "[419]\ttraining's binary_logloss: 0.515512\n",
      "[420]\ttraining's binary_logloss: 0.515425\n",
      "[421]\ttraining's binary_logloss: 0.515259\n",
      "[422]\ttraining's binary_logloss: 0.51513\n",
      "[423]\ttraining's binary_logloss: 0.514954\n",
      "[424]\ttraining's binary_logloss: 0.514863\n",
      "[425]\ttraining's binary_logloss: 0.514793\n",
      "[426]\ttraining's binary_logloss: 0.514659\n",
      "[427]\ttraining's binary_logloss: 0.514507\n",
      "[428]\ttraining's binary_logloss: 0.514321\n",
      "[429]\ttraining's binary_logloss: 0.514177\n",
      "[430]\ttraining's binary_logloss: 0.514038\n",
      "[431]\ttraining's binary_logloss: 0.513907\n",
      "[432]\ttraining's binary_logloss: 0.51372\n",
      "[433]\ttraining's binary_logloss: 0.513629\n",
      "[434]\ttraining's binary_logloss: 0.513429\n",
      "[435]\ttraining's binary_logloss: 0.513352\n",
      "[436]\ttraining's binary_logloss: 0.513207\n",
      "[437]\ttraining's binary_logloss: 0.513109\n",
      "[438]\ttraining's binary_logloss: 0.512974\n",
      "[439]\ttraining's binary_logloss: 0.512914\n",
      "[440]\ttraining's binary_logloss: 0.512696\n",
      "[441]\ttraining's binary_logloss: 0.512554\n",
      "[442]\ttraining's binary_logloss: 0.512505\n",
      "[443]\ttraining's binary_logloss: 0.512331\n",
      "[444]\ttraining's binary_logloss: 0.51221\n",
      "[445]\ttraining's binary_logloss: 0.512051\n",
      "[446]\ttraining's binary_logloss: 0.511877\n",
      "[447]\ttraining's binary_logloss: 0.511788\n",
      "[448]\ttraining's binary_logloss: 0.51172\n",
      "[449]\ttraining's binary_logloss: 0.511571\n",
      "[450]\ttraining's binary_logloss: 0.511428\n",
      "[451]\ttraining's binary_logloss: 0.511209\n",
      "[452]\ttraining's binary_logloss: 0.511045\n",
      "[453]\ttraining's binary_logloss: 0.510825\n",
      "[454]\ttraining's binary_logloss: 0.510714\n",
      "[455]\ttraining's binary_logloss: 0.510519\n",
      "[456]\ttraining's binary_logloss: 0.510427\n",
      "[457]\ttraining's binary_logloss: 0.510332\n",
      "[458]\ttraining's binary_logloss: 0.510248\n",
      "[459]\ttraining's binary_logloss: 0.510152\n",
      "[460]\ttraining's binary_logloss: 0.510073\n",
      "[461]\ttraining's binary_logloss: 0.50994\n",
      "[462]\ttraining's binary_logloss: 0.50989\n",
      "[463]\ttraining's binary_logloss: 0.509771\n",
      "[464]\ttraining's binary_logloss: 0.509651\n",
      "[465]\ttraining's binary_logloss: 0.509498\n",
      "[466]\ttraining's binary_logloss: 0.509267\n",
      "[467]\ttraining's binary_logloss: 0.509164\n",
      "[468]\ttraining's binary_logloss: 0.509008\n",
      "[469]\ttraining's binary_logloss: 0.508873\n",
      "[470]\ttraining's binary_logloss: 0.508767\n",
      "[471]\ttraining's binary_logloss: 0.508666\n",
      "[472]\ttraining's binary_logloss: 0.508525\n",
      "[473]\ttraining's binary_logloss: 0.508418\n",
      "[474]\ttraining's binary_logloss: 0.508252\n",
      "[475]\ttraining's binary_logloss: 0.508107\n",
      "[476]\ttraining's binary_logloss: 0.507975\n",
      "[477]\ttraining's binary_logloss: 0.507807\n",
      "[478]\ttraining's binary_logloss: 0.507661\n",
      "[479]\ttraining's binary_logloss: 0.507459\n",
      "[480]\ttraining's binary_logloss: 0.507246\n",
      "[481]\ttraining's binary_logloss: 0.507088\n",
      "[482]\ttraining's binary_logloss: 0.506954\n",
      "[483]\ttraining's binary_logloss: 0.506867\n",
      "[484]\ttraining's binary_logloss: 0.506788\n",
      "[485]\ttraining's binary_logloss: 0.506641\n",
      "[486]\ttraining's binary_logloss: 0.50648\n",
      "[487]\ttraining's binary_logloss: 0.506422\n",
      "[488]\ttraining's binary_logloss: 0.506322\n",
      "[489]\ttraining's binary_logloss: 0.506192\n",
      "[490]\ttraining's binary_logloss: 0.506065\n",
      "[491]\ttraining's binary_logloss: 0.505948\n",
      "[492]\ttraining's binary_logloss: 0.505796\n",
      "[493]\ttraining's binary_logloss: 0.505714\n",
      "[494]\ttraining's binary_logloss: 0.505574\n",
      "[495]\ttraining's binary_logloss: 0.505508\n",
      "[496]\ttraining's binary_logloss: 0.505344\n",
      "[497]\ttraining's binary_logloss: 0.505193\n",
      "[498]\ttraining's binary_logloss: 0.505082\n",
      "[499]\ttraining's binary_logloss: 0.504942\n",
      "[500]\ttraining's binary_logloss: 0.504811\n",
      "[501]\ttraining's binary_logloss: 0.504674\n",
      "[502]\ttraining's binary_logloss: 0.504568\n",
      "[503]\ttraining's binary_logloss: 0.504483\n",
      "[504]\ttraining's binary_logloss: 0.50434\n",
      "[505]\ttraining's binary_logloss: 0.504237\n",
      "[506]\ttraining's binary_logloss: 0.504193\n",
      "[507]\ttraining's binary_logloss: 0.503963\n",
      "[508]\ttraining's binary_logloss: 0.503833\n",
      "[509]\ttraining's binary_logloss: 0.503759\n",
      "[510]\ttraining's binary_logloss: 0.503671\n",
      "[511]\ttraining's binary_logloss: 0.503589\n",
      "[512]\ttraining's binary_logloss: 0.503459\n",
      "[513]\ttraining's binary_logloss: 0.503288\n",
      "[514]\ttraining's binary_logloss: 0.503167\n",
      "[515]\ttraining's binary_logloss: 0.503061\n",
      "[516]\ttraining's binary_logloss: 0.5029\n",
      "[517]\ttraining's binary_logloss: 0.502822\n",
      "[518]\ttraining's binary_logloss: 0.502711\n",
      "[519]\ttraining's binary_logloss: 0.502581\n",
      "[520]\ttraining's binary_logloss: 0.502476\n",
      "[521]\ttraining's binary_logloss: 0.502372\n",
      "[522]\ttraining's binary_logloss: 0.502257\n",
      "[523]\ttraining's binary_logloss: 0.50215\n",
      "[524]\ttraining's binary_logloss: 0.502043\n",
      "[525]\ttraining's binary_logloss: 0.501926\n",
      "[526]\ttraining's binary_logloss: 0.501799\n",
      "[527]\ttraining's binary_logloss: 0.501672\n",
      "[528]\ttraining's binary_logloss: 0.501568\n",
      "[529]\ttraining's binary_logloss: 0.501499\n",
      "[530]\ttraining's binary_logloss: 0.501396\n",
      "[531]\ttraining's binary_logloss: 0.501256\n",
      "[532]\ttraining's binary_logloss: 0.50112\n",
      "[533]\ttraining's binary_logloss: 0.50105\n",
      "[534]\ttraining's binary_logloss: 0.500955\n",
      "[535]\ttraining's binary_logloss: 0.500853\n",
      "[536]\ttraining's binary_logloss: 0.50074\n",
      "[537]\ttraining's binary_logloss: 0.500563\n",
      "[538]\ttraining's binary_logloss: 0.500446\n",
      "[539]\ttraining's binary_logloss: 0.500394\n",
      "[540]\ttraining's binary_logloss: 0.500291\n",
      "[541]\ttraining's binary_logloss: 0.500127\n",
      "[542]\ttraining's binary_logloss: 0.500006\n",
      "[543]\ttraining's binary_logloss: 0.499831\n",
      "[544]\ttraining's binary_logloss: 0.49973\n",
      "[545]\ttraining's binary_logloss: 0.499675\n",
      "[546]\ttraining's binary_logloss: 0.499566\n",
      "[547]\ttraining's binary_logloss: 0.499498\n",
      "[548]\ttraining's binary_logloss: 0.499446\n",
      "[549]\ttraining's binary_logloss: 0.499315\n",
      "[550]\ttraining's binary_logloss: 0.499242\n",
      "[551]\ttraining's binary_logloss: 0.499185\n",
      "[552]\ttraining's binary_logloss: 0.49913\n",
      "[553]\ttraining's binary_logloss: 0.498998\n",
      "[554]\ttraining's binary_logloss: 0.498916\n",
      "[555]\ttraining's binary_logloss: 0.498782\n",
      "[556]\ttraining's binary_logloss: 0.498651\n",
      "[557]\ttraining's binary_logloss: 0.498547\n",
      "[558]\ttraining's binary_logloss: 0.498388\n",
      "[559]\ttraining's binary_logloss: 0.498208\n",
      "[560]\ttraining's binary_logloss: 0.498128\n",
      "[561]\ttraining's binary_logloss: 0.498002\n",
      "[562]\ttraining's binary_logloss: 0.497902\n",
      "[563]\ttraining's binary_logloss: 0.497801\n",
      "[564]\ttraining's binary_logloss: 0.497697\n",
      "[565]\ttraining's binary_logloss: 0.497649\n",
      "[566]\ttraining's binary_logloss: 0.497488\n",
      "[567]\ttraining's binary_logloss: 0.497347\n",
      "[568]\ttraining's binary_logloss: 0.49725\n",
      "[569]\ttraining's binary_logloss: 0.497142\n",
      "[570]\ttraining's binary_logloss: 0.497005\n",
      "[571]\ttraining's binary_logloss: 0.496929\n",
      "[572]\ttraining's binary_logloss: 0.496772\n",
      "[573]\ttraining's binary_logloss: 0.496555\n",
      "[574]\ttraining's binary_logloss: 0.496425\n",
      "[575]\ttraining's binary_logloss: 0.496235\n",
      "[576]\ttraining's binary_logloss: 0.496039\n",
      "[577]\ttraining's binary_logloss: 0.495917\n",
      "[578]\ttraining's binary_logloss: 0.49577\n",
      "[579]\ttraining's binary_logloss: 0.495668\n",
      "[580]\ttraining's binary_logloss: 0.49555\n",
      "[581]\ttraining's binary_logloss: 0.495394\n",
      "[582]\ttraining's binary_logloss: 0.495231\n",
      "[583]\ttraining's binary_logloss: 0.495086\n",
      "[584]\ttraining's binary_logloss: 0.494968\n",
      "[585]\ttraining's binary_logloss: 0.494892\n",
      "[586]\ttraining's binary_logloss: 0.494806\n",
      "[587]\ttraining's binary_logloss: 0.49466\n",
      "[588]\ttraining's binary_logloss: 0.494559\n",
      "[589]\ttraining's binary_logloss: 0.494379\n",
      "[590]\ttraining's binary_logloss: 0.494218\n",
      "[591]\ttraining's binary_logloss: 0.49408\n",
      "[592]\ttraining's binary_logloss: 0.493933\n",
      "[593]\ttraining's binary_logloss: 0.493844\n",
      "[594]\ttraining's binary_logloss: 0.493668\n",
      "[595]\ttraining's binary_logloss: 0.49356\n",
      "[596]\ttraining's binary_logloss: 0.493441\n",
      "[597]\ttraining's binary_logloss: 0.493337\n",
      "[598]\ttraining's binary_logloss: 0.493266\n",
      "[599]\ttraining's binary_logloss: 0.493071\n",
      "[600]\ttraining's binary_logloss: 0.492855\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as LightGBM\n",
    "\n",
    "feature_set = []\n",
    "feature_set = make_feature_set(x_train)\n",
    "model = []\n",
    "\n",
    "## train\n",
    "for x in feature_set :\n",
    "    lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=600,\n",
    "                               max_depth = 50,\n",
    "                               min_data_in_leaf = 50,\n",
    "                               class_weight={True: 10, False: 1},\n",
    "                               learning_rate= 0.1\n",
    "                              ) \n",
    "    evals = [(x, y_train_bool)]\n",
    "    lgbm.fit(x, y_train_bool, eval_metric='logloss', eval_set=evals)\n",
    "    model.append(lgbm)\n",
    "\n",
    "## prediction\n",
    "def predict_ensemble_model(x_) :\n",
    "    feature_set = make_feature_set(x_)\n",
    "    y_pred = []\n",
    "    i = 0\n",
    "    for x in feature_set :\n",
    "        pred = model[i].predict(x)\n",
    "        y_pred.append(pred)\n",
    "        i = i+1\n",
    "\n",
    "    y_pred_sum = y_pred[0] &(y_pred[1] | y_pred[2] | y_pred[3] | y_pred[4])\n",
    "    return y_pred_sum\n",
    "#y_pred = np.array([vote(xi) for xi in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.99      0.45      0.62     63391\n",
      "        risk       0.26      0.98      0.41     12724\n",
      "\n",
      "    accuracy                           0.54     76115\n",
      "   macro avg       0.63      0.71      0.52     76115\n",
      "weighted avg       0.87      0.54      0.58     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = predict_ensemble_model(x_train)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_train_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.41      0.56     21052\n",
      "        risk       0.22      0.80      0.34      4344\n",
      "\n",
      "    accuracy                           0.47     25396\n",
      "   macro avg       0.56      0.60      0.45     25396\n",
      "weighted avg       0.79      0.47      0.52     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = predict_ensemble_model(x_valid)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dc4a390a9b2827902a4747c64ce7fc589728c7a1b50ebf341cedb4beb3b25a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
