{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = ['?', '??', 'N/A', 'NA', 'nan', 'NaN', '-nan', '-NaN', 'null', '-']\n",
    "x_train = pd.read_csv('./data/track1/features/x_train_normal.csv', na_values = null_values)\n",
    "x_valid = pd.read_csv('./data/track1/features/x_valid_normal.csv', na_values = null_values)\n",
    "x_test = pd.read_csv('./data/track1/features/x_test_normal.csv', na_values = null_values)\n",
    "y_train = pd.read_csv('./data/track1/features/y_train_normal.csv', na_values = null_values)\n",
    "y_valid = pd.read_csv('./data/track1/features/y_valid_normal.csv', na_values = null_values)\n",
    "y_test = pd.read_csv('./data/track1/features/y_test_normal.csv', na_values = null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = x_train.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_valid_features = x_valid.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_test_features = x_test.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "y_train_bool = y_train['Y'] <-2.0\n",
    "y_valid_bool = y_valid['Y'] <-2.0\n",
    "y_test_bool = y_test['Y'] <-2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tree Base Simple Classifers\n",
    "Tree 기반 분류 모델인 Decision Tree와 Random Forest 를 사용하여 재무데이터로 리스크 주식 분류 모델을 만들기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-1 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test/models/decisionTree.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier(\n",
    "    max_depth=15,\n",
    "    min_samples_split=100,\n",
    "    class_weight={True: 10, False: 1}\n",
    ")\n",
    "decisionTree.fit(x_train_features, y_train_bool)\n",
    "\n",
    "joblib.dump(decisionTree, './test/models/decisionTree.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.98      0.34      0.50     63391\n",
      "        risk       0.22      0.96      0.36     12724\n",
      "\n",
      "    accuracy                           0.44     76115\n",
      "   macro avg       0.60      0.65      0.43     76115\n",
      "weighted avg       0.85      0.44      0.48     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = decisionTree.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_train_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.90      0.31      0.46     21052\n",
      "        risk       0.20      0.83      0.32      4344\n",
      "\n",
      "    accuracy                           0.40     25396\n",
      "   macro avg       0.55      0.57      0.39     25396\n",
      "weighted avg       0.78      0.40      0.44     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = decisionTree.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_valid_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.90      0.31      0.46     21040\n",
      "        risk       0.20      0.83      0.32      4311\n",
      "\n",
      "    accuracy                           0.40     25351\n",
      "   macro avg       0.55      0.57      0.39     25351\n",
      "weighted avg       0.78      0.40      0.43     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = decisionTree.predict(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test/models/randomForest.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    criterion='entropy', \n",
    "    min_samples_split = 100,\n",
    "    bootstrap=True,\n",
    "    max_depth=20,\n",
    "    class_weight={True: 10, False: 1}\n",
    "    )\n",
    "rf.fit(x_train_features, y_train_bool)\n",
    "\n",
    "joblib.dump(rf, './test/models/randomForest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.97      0.40      0.57     63391\n",
      "        risk       0.24      0.95      0.38     12724\n",
      "\n",
      "    accuracy                           0.49     76115\n",
      "   macro avg       0.61      0.67      0.48     76115\n",
      "weighted avg       0.85      0.49      0.54     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = rf.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_train_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.38      0.53     21052\n",
      "        risk       0.22      0.83      0.34      4344\n",
      "\n",
      "    accuracy                           0.45     25396\n",
      "   macro avg       0.56      0.60      0.44     25396\n",
      "weighted avg       0.79      0.45      0.50     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.38      0.54     21040\n",
      "        risk       0.21      0.82      0.34      4311\n",
      "\n",
      "    accuracy                           0.46     25351\n",
      "   macro avg       0.56      0.60      0.44     25351\n",
      "weighted avg       0.79      0.46      0.50     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Selection\n",
    "Forward, Backward 방식으로 Feature Selection을 시도 하였음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 2-1. RFECV\n",
    "Backward 방식으로 simple classifier들에 대해 feature selection 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train = pd.read_csv(\"features2021/x_train_normal.csv\")            # 데이터 불러오기\n",
    "y_train = pd.read_csv(\"features2021/y_train_normal.csv\")\n",
    "\n",
    "x_train.drop(['날짜', 'CODE'], axis=1, inplace=True)\n",
    "y_train['Y'] = y_train['Y'].apply(lambda x:True if x<-2 else False)\n",
    "\n",
    "rf = RandomForestClassifier(            # simple classifier에서 구성한 RandomForestClassifier 재사용\n",
    "    n_estimators=200,\n",
    "    bootstrap=True,\n",
    "    max_depth=10,\n",
    "    class_weight={True: 10, False: 1}\n",
    "    )\n",
    "cv = RepeatedStratifiedKFold()          # validation으로 RepeatedStratifiedKFold활용, n_splits=5(default), n_repeats=10(default)\n",
    "selector = RFECV(rf, cv=cv)             # RFECV 진행\n",
    "selector = selector.fit(x_train, y_train)\n",
    "\n",
    "print(x_train.columns[selector.support_])   # 선택된 feature 리스트 출력\n",
    "plt.figure()                                # feature 갯수와 CV accuracy에 대한 그래프\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BPS', 'PER', 'PBR', 'EPS', 'DIV', 'DPS', '거래량', '시가총액', '금리', '유동자산',\n",
      "       '비유동자산', '자산총계', '유동부채', '비유동부채', '부채총계', '이익잉여금', '자본총계', '매출액',\n",
      "       '영업이익', '법인세차감전 순이익', '당기순이익', '자본금'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_train = pd.read_csv(\"featuress/x_train_normal.csv\")               # 데이터 불러오기\n",
    "y_train = pd.read_csv(\"featuress/y_train_normal.csv\")\n",
    "\n",
    "x_train.drop(['날짜', 'CODE', '종가'], axis=1, inplace=True)\n",
    "y_train['Y'] = y_train['Y'].apply(lambda x:True if x<-2 else False)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=15,               # simple classifier에서 구성한 DecisionTreeClassifier 재사용\n",
    "                            min_samples_split=100,\n",
    "                            class_weight={True:10, False:1})\n",
    "cv = RepeatedStratifiedKFold()                          # 위와 같이 RepeatedStratifiedKFold 선언\n",
    "selector = RFECV(dt, cv=cv)\n",
    "selector = selector.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "print(x_train.columns[selector.support_])               # 선택된 feature 리스트 및 그래프 출력\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 2-2. Sequential Feature Selector\n",
    "forward 방식으로 feature selection 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BPS', 'DIV', '거래량', '금리', '비유동자산', '자산총계', '부채총계', '법인세차감전 순이익',\n",
      "       '당기순이익'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_train = pd.read_csv(\"featuress/x_train_normal.csv\")       # 데이터 로드\n",
    "y_train = pd.read_csv(\"featuress/y_train_normal.csv\")\n",
    "\n",
    "x_train.drop(['날짜', 'CODE', '종가'], axis=1, inplace=True)\n",
    "y_train['Y'] = y_train['Y'].apply(lambda x:True if x<-2 else False)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=15,                   # simple classifier 중 실행시간이 짧은 DecisionTreeClassifier 선택\n",
    "                            min_samples_split=100,\n",
    "                            class_weight={True:10, False:1})\n",
    "\n",
    "cv = RepeatedStratifiedKFold()\n",
    "sfs = SequentialFeatureSelector(dt,                         # SequentialFeatureSelector 선언, 목표 feature 선택수는 9개, forward 방식으로 cv와 함께 진행\n",
    "                                n_features_to_select=9,\n",
    "                                direction='forward',\n",
    "                                cv=cv)\n",
    "sfs.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "print(x_train.columns[sfs.get_support()])                   # 선택된 feature 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LightGBM and Weak Bagging\n",
    "gradient boosting 기반의 tree classifier인 lightGBM 모델을 사용하여 Risk 종목 분류를 하였습니다\n",
    "LightGBM 모델들을 Feature selection 결과를 기반으로 Bagging의 아이디어를 활용하여 앙상블 하는 모델을 만들었습니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-1 Light GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.3-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: wheel in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.3\n"
     ]
    }
   ],
   "source": [
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[1]\ttraining's binary_logloss: 0.628971\n",
      "[2]\ttraining's binary_logloss: 0.623177\n",
      "[3]\ttraining's binary_logloss: 0.61815\n",
      "[4]\ttraining's binary_logloss: 0.613883\n",
      "[5]\ttraining's binary_logloss: 0.610448\n",
      "[6]\ttraining's binary_logloss: 0.607506\n",
      "[7]\ttraining's binary_logloss: 0.604738\n",
      "[8]\ttraining's binary_logloss: 0.602413\n",
      "[9]\ttraining's binary_logloss: 0.600206\n",
      "[10]\ttraining's binary_logloss: 0.598273\n",
      "[11]\ttraining's binary_logloss: 0.596473\n",
      "[12]\ttraining's binary_logloss: 0.594786\n",
      "[13]\ttraining's binary_logloss: 0.593277\n",
      "[14]\ttraining's binary_logloss: 0.591786\n",
      "[15]\ttraining's binary_logloss: 0.590473\n",
      "[16]\ttraining's binary_logloss: 0.589261\n",
      "[17]\ttraining's binary_logloss: 0.588205\n",
      "[18]\ttraining's binary_logloss: 0.587172\n",
      "[19]\ttraining's binary_logloss: 0.586137\n",
      "[20]\ttraining's binary_logloss: 0.585207\n",
      "[21]\ttraining's binary_logloss: 0.584248\n",
      "[22]\ttraining's binary_logloss: 0.583436\n",
      "[23]\ttraining's binary_logloss: 0.582647\n",
      "[24]\ttraining's binary_logloss: 0.581928\n",
      "[25]\ttraining's binary_logloss: 0.581184\n",
      "[26]\ttraining's binary_logloss: 0.580455\n",
      "[27]\ttraining's binary_logloss: 0.579781\n",
      "[28]\ttraining's binary_logloss: 0.579132\n",
      "[29]\ttraining's binary_logloss: 0.578465\n",
      "[30]\ttraining's binary_logloss: 0.577807\n",
      "[31]\ttraining's binary_logloss: 0.577219\n",
      "[32]\ttraining's binary_logloss: 0.576682\n",
      "[33]\ttraining's binary_logloss: 0.576121\n",
      "[34]\ttraining's binary_logloss: 0.575556\n",
      "[35]\ttraining's binary_logloss: 0.575021\n",
      "[36]\ttraining's binary_logloss: 0.574541\n",
      "[37]\ttraining's binary_logloss: 0.574051\n",
      "[38]\ttraining's binary_logloss: 0.573477\n",
      "[39]\ttraining's binary_logloss: 0.573022\n",
      "[40]\ttraining's binary_logloss: 0.572539\n",
      "[41]\ttraining's binary_logloss: 0.572078\n",
      "[42]\ttraining's binary_logloss: 0.571693\n",
      "[43]\ttraining's binary_logloss: 0.571209\n",
      "[44]\ttraining's binary_logloss: 0.570761\n",
      "[45]\ttraining's binary_logloss: 0.570248\n",
      "[46]\ttraining's binary_logloss: 0.569861\n",
      "[47]\ttraining's binary_logloss: 0.569441\n",
      "[48]\ttraining's binary_logloss: 0.569067\n",
      "[49]\ttraining's binary_logloss: 0.56858\n",
      "[50]\ttraining's binary_logloss: 0.568146\n",
      "[51]\ttraining's binary_logloss: 0.567781\n",
      "[52]\ttraining's binary_logloss: 0.567388\n",
      "[53]\ttraining's binary_logloss: 0.566968\n",
      "[54]\ttraining's binary_logloss: 0.56658\n",
      "[55]\ttraining's binary_logloss: 0.566269\n",
      "[56]\ttraining's binary_logloss: 0.565846\n",
      "[57]\ttraining's binary_logloss: 0.565499\n",
      "[58]\ttraining's binary_logloss: 0.565142\n",
      "[59]\ttraining's binary_logloss: 0.564792\n",
      "[60]\ttraining's binary_logloss: 0.564459\n",
      "[61]\ttraining's binary_logloss: 0.564127\n",
      "[62]\ttraining's binary_logloss: 0.563827\n",
      "[63]\ttraining's binary_logloss: 0.563512\n",
      "[64]\ttraining's binary_logloss: 0.563144\n",
      "[65]\ttraining's binary_logloss: 0.562781\n",
      "[66]\ttraining's binary_logloss: 0.562482\n",
      "[67]\ttraining's binary_logloss: 0.562179\n",
      "[68]\ttraining's binary_logloss: 0.561863\n",
      "[69]\ttraining's binary_logloss: 0.561489\n",
      "[70]\ttraining's binary_logloss: 0.561266\n",
      "[71]\ttraining's binary_logloss: 0.560915\n",
      "[72]\ttraining's binary_logloss: 0.560636\n",
      "[73]\ttraining's binary_logloss: 0.560396\n",
      "[74]\ttraining's binary_logloss: 0.560101\n",
      "[75]\ttraining's binary_logloss: 0.559797\n",
      "[76]\ttraining's binary_logloss: 0.559446\n",
      "[77]\ttraining's binary_logloss: 0.559158\n",
      "[78]\ttraining's binary_logloss: 0.558832\n",
      "[79]\ttraining's binary_logloss: 0.558664\n",
      "[80]\ttraining's binary_logloss: 0.558425\n",
      "[81]\ttraining's binary_logloss: 0.558092\n",
      "[82]\ttraining's binary_logloss: 0.557828\n",
      "[83]\ttraining's binary_logloss: 0.557464\n",
      "[84]\ttraining's binary_logloss: 0.557161\n",
      "[85]\ttraining's binary_logloss: 0.556947\n",
      "[86]\ttraining's binary_logloss: 0.556649\n",
      "[87]\ttraining's binary_logloss: 0.556341\n",
      "[88]\ttraining's binary_logloss: 0.556076\n",
      "[89]\ttraining's binary_logloss: 0.555792\n",
      "[90]\ttraining's binary_logloss: 0.555572\n",
      "[91]\ttraining's binary_logloss: 0.555348\n",
      "[92]\ttraining's binary_logloss: 0.555103\n",
      "[93]\ttraining's binary_logloss: 0.55482\n",
      "[94]\ttraining's binary_logloss: 0.554614\n",
      "[95]\ttraining's binary_logloss: 0.554367\n",
      "[96]\ttraining's binary_logloss: 0.554113\n",
      "[97]\ttraining's binary_logloss: 0.553832\n",
      "[98]\ttraining's binary_logloss: 0.553579\n",
      "[99]\ttraining's binary_logloss: 0.553222\n",
      "[100]\ttraining's binary_logloss: 0.553036\n",
      "[101]\ttraining's binary_logloss: 0.55277\n",
      "[102]\ttraining's binary_logloss: 0.5525\n",
      "[103]\ttraining's binary_logloss: 0.552267\n",
      "[104]\ttraining's binary_logloss: 0.552042\n",
      "[105]\ttraining's binary_logloss: 0.551783\n",
      "[106]\ttraining's binary_logloss: 0.551622\n",
      "[107]\ttraining's binary_logloss: 0.55145\n",
      "[108]\ttraining's binary_logloss: 0.551272\n",
      "[109]\ttraining's binary_logloss: 0.551063\n",
      "[110]\ttraining's binary_logloss: 0.550855\n",
      "[111]\ttraining's binary_logloss: 0.550609\n",
      "[112]\ttraining's binary_logloss: 0.5504\n",
      "[113]\ttraining's binary_logloss: 0.550099\n",
      "[114]\ttraining's binary_logloss: 0.549872\n",
      "[115]\ttraining's binary_logloss: 0.549655\n",
      "[116]\ttraining's binary_logloss: 0.549419\n",
      "[117]\ttraining's binary_logloss: 0.549098\n",
      "[118]\ttraining's binary_logloss: 0.548808\n",
      "[119]\ttraining's binary_logloss: 0.548558\n",
      "[120]\ttraining's binary_logloss: 0.548306\n",
      "[121]\ttraining's binary_logloss: 0.548006\n",
      "[122]\ttraining's binary_logloss: 0.547838\n",
      "[123]\ttraining's binary_logloss: 0.547682\n",
      "[124]\ttraining's binary_logloss: 0.547478\n",
      "[125]\ttraining's binary_logloss: 0.54721\n",
      "[126]\ttraining's binary_logloss: 0.546999\n",
      "[127]\ttraining's binary_logloss: 0.546847\n",
      "[128]\ttraining's binary_logloss: 0.546672\n",
      "[129]\ttraining's binary_logloss: 0.546429\n",
      "[130]\ttraining's binary_logloss: 0.546189\n",
      "[131]\ttraining's binary_logloss: 0.546017\n",
      "[132]\ttraining's binary_logloss: 0.545774\n",
      "[133]\ttraining's binary_logloss: 0.545565\n",
      "[134]\ttraining's binary_logloss: 0.545313\n",
      "[135]\ttraining's binary_logloss: 0.545149\n",
      "[136]\ttraining's binary_logloss: 0.544947\n",
      "[137]\ttraining's binary_logloss: 0.544806\n",
      "[138]\ttraining's binary_logloss: 0.54456\n",
      "[139]\ttraining's binary_logloss: 0.54435\n",
      "[140]\ttraining's binary_logloss: 0.544139\n",
      "[141]\ttraining's binary_logloss: 0.543933\n",
      "[142]\ttraining's binary_logloss: 0.543745\n",
      "[143]\ttraining's binary_logloss: 0.543566\n",
      "[144]\ttraining's binary_logloss: 0.543448\n",
      "[145]\ttraining's binary_logloss: 0.543166\n",
      "[146]\ttraining's binary_logloss: 0.543002\n",
      "[147]\ttraining's binary_logloss: 0.54277\n",
      "[148]\ttraining's binary_logloss: 0.5426\n",
      "[149]\ttraining's binary_logloss: 0.542433\n",
      "[150]\ttraining's binary_logloss: 0.542274\n",
      "[151]\ttraining's binary_logloss: 0.542161\n",
      "[152]\ttraining's binary_logloss: 0.541961\n",
      "[153]\ttraining's binary_logloss: 0.541761\n",
      "[154]\ttraining's binary_logloss: 0.541529\n",
      "[155]\ttraining's binary_logloss: 0.541305\n",
      "[156]\ttraining's binary_logloss: 0.541127\n",
      "[157]\ttraining's binary_logloss: 0.540908\n",
      "[158]\ttraining's binary_logloss: 0.540787\n",
      "[159]\ttraining's binary_logloss: 0.540631\n",
      "[160]\ttraining's binary_logloss: 0.540341\n",
      "[161]\ttraining's binary_logloss: 0.540187\n",
      "[162]\ttraining's binary_logloss: 0.539922\n",
      "[163]\ttraining's binary_logloss: 0.539634\n",
      "[164]\ttraining's binary_logloss: 0.53944\n",
      "[165]\ttraining's binary_logloss: 0.539297\n",
      "[166]\ttraining's binary_logloss: 0.539113\n",
      "[167]\ttraining's binary_logloss: 0.538979\n",
      "[168]\ttraining's binary_logloss: 0.538755\n",
      "[169]\ttraining's binary_logloss: 0.538556\n",
      "[170]\ttraining's binary_logloss: 0.538288\n",
      "[171]\ttraining's binary_logloss: 0.538015\n",
      "[172]\ttraining's binary_logloss: 0.537769\n",
      "[173]\ttraining's binary_logloss: 0.53758\n",
      "[174]\ttraining's binary_logloss: 0.537397\n",
      "[175]\ttraining's binary_logloss: 0.537119\n",
      "[176]\ttraining's binary_logloss: 0.536973\n",
      "[177]\ttraining's binary_logloss: 0.536731\n",
      "[178]\ttraining's binary_logloss: 0.536575\n",
      "[179]\ttraining's binary_logloss: 0.536425\n",
      "[180]\ttraining's binary_logloss: 0.536252\n",
      "[181]\ttraining's binary_logloss: 0.536065\n",
      "[182]\ttraining's binary_logloss: 0.5358\n",
      "[183]\ttraining's binary_logloss: 0.535664\n",
      "[184]\ttraining's binary_logloss: 0.535538\n",
      "[185]\ttraining's binary_logloss: 0.53539\n",
      "[186]\ttraining's binary_logloss: 0.535245\n",
      "[187]\ttraining's binary_logloss: 0.535016\n",
      "[188]\ttraining's binary_logloss: 0.53487\n",
      "[189]\ttraining's binary_logloss: 0.534773\n",
      "[190]\ttraining's binary_logloss: 0.534589\n",
      "[191]\ttraining's binary_logloss: 0.534343\n",
      "[192]\ttraining's binary_logloss: 0.534184\n",
      "[193]\ttraining's binary_logloss: 0.534056\n",
      "[194]\ttraining's binary_logloss: 0.533945\n",
      "[195]\ttraining's binary_logloss: 0.533675\n",
      "[196]\ttraining's binary_logloss: 0.533532\n",
      "[197]\ttraining's binary_logloss: 0.533356\n",
      "[198]\ttraining's binary_logloss: 0.533212\n",
      "[199]\ttraining's binary_logloss: 0.532938\n",
      "[200]\ttraining's binary_logloss: 0.532762\n",
      "[201]\ttraining's binary_logloss: 0.53256\n",
      "[202]\ttraining's binary_logloss: 0.532371\n",
      "[203]\ttraining's binary_logloss: 0.532237\n",
      "[204]\ttraining's binary_logloss: 0.532083\n",
      "[205]\ttraining's binary_logloss: 0.531883\n",
      "[206]\ttraining's binary_logloss: 0.531719\n",
      "[207]\ttraining's binary_logloss: 0.531553\n",
      "[208]\ttraining's binary_logloss: 0.531403\n",
      "[209]\ttraining's binary_logloss: 0.531265\n",
      "[210]\ttraining's binary_logloss: 0.531079\n",
      "[211]\ttraining's binary_logloss: 0.530885\n",
      "[212]\ttraining's binary_logloss: 0.530725\n",
      "[213]\ttraining's binary_logloss: 0.530561\n",
      "[214]\ttraining's binary_logloss: 0.53038\n",
      "[215]\ttraining's binary_logloss: 0.530178\n",
      "[216]\ttraining's binary_logloss: 0.529881\n",
      "[217]\ttraining's binary_logloss: 0.529737\n",
      "[218]\ttraining's binary_logloss: 0.529533\n",
      "[219]\ttraining's binary_logloss: 0.529345\n",
      "[220]\ttraining's binary_logloss: 0.52919\n",
      "[221]\ttraining's binary_logloss: 0.529063\n",
      "[222]\ttraining's binary_logloss: 0.528803\n",
      "[223]\ttraining's binary_logloss: 0.528692\n",
      "[224]\ttraining's binary_logloss: 0.528556\n",
      "[225]\ttraining's binary_logloss: 0.528407\n",
      "[226]\ttraining's binary_logloss: 0.528204\n",
      "[227]\ttraining's binary_logloss: 0.527983\n",
      "[228]\ttraining's binary_logloss: 0.527743\n",
      "[229]\ttraining's binary_logloss: 0.527593\n",
      "[230]\ttraining's binary_logloss: 0.527364\n",
      "[231]\ttraining's binary_logloss: 0.527263\n",
      "[232]\ttraining's binary_logloss: 0.527094\n",
      "[233]\ttraining's binary_logloss: 0.526921\n",
      "[234]\ttraining's binary_logloss: 0.526812\n",
      "[235]\ttraining's binary_logloss: 0.526596\n",
      "[236]\ttraining's binary_logloss: 0.526408\n",
      "[237]\ttraining's binary_logloss: 0.526237\n",
      "[238]\ttraining's binary_logloss: 0.526103\n",
      "[239]\ttraining's binary_logloss: 0.525907\n",
      "[240]\ttraining's binary_logloss: 0.525683\n",
      "[241]\ttraining's binary_logloss: 0.525522\n",
      "[242]\ttraining's binary_logloss: 0.525413\n",
      "[243]\ttraining's binary_logloss: 0.52527\n",
      "[244]\ttraining's binary_logloss: 0.525135\n",
      "[245]\ttraining's binary_logloss: 0.525012\n",
      "[246]\ttraining's binary_logloss: 0.524884\n",
      "[247]\ttraining's binary_logloss: 0.524712\n",
      "[248]\ttraining's binary_logloss: 0.524575\n",
      "[249]\ttraining's binary_logloss: 0.524462\n",
      "[250]\ttraining's binary_logloss: 0.524286\n",
      "[251]\ttraining's binary_logloss: 0.524091\n",
      "[252]\ttraining's binary_logloss: 0.523931\n",
      "[253]\ttraining's binary_logloss: 0.523707\n",
      "[254]\ttraining's binary_logloss: 0.523554\n",
      "[255]\ttraining's binary_logloss: 0.523376\n",
      "[256]\ttraining's binary_logloss: 0.523222\n",
      "[257]\ttraining's binary_logloss: 0.522943\n",
      "[258]\ttraining's binary_logloss: 0.522782\n",
      "[259]\ttraining's binary_logloss: 0.522654\n",
      "[260]\ttraining's binary_logloss: 0.522419\n",
      "[261]\ttraining's binary_logloss: 0.522187\n",
      "[262]\ttraining's binary_logloss: 0.52206\n",
      "[263]\ttraining's binary_logloss: 0.521855\n",
      "[264]\ttraining's binary_logloss: 0.521743\n",
      "[265]\ttraining's binary_logloss: 0.521589\n",
      "[266]\ttraining's binary_logloss: 0.521382\n",
      "[267]\ttraining's binary_logloss: 0.521246\n",
      "[268]\ttraining's binary_logloss: 0.521118\n",
      "[269]\ttraining's binary_logloss: 0.520944\n",
      "[270]\ttraining's binary_logloss: 0.520757\n",
      "[271]\ttraining's binary_logloss: 0.520477\n",
      "[272]\ttraining's binary_logloss: 0.520281\n",
      "[273]\ttraining's binary_logloss: 0.520166\n",
      "[274]\ttraining's binary_logloss: 0.520044\n",
      "[275]\ttraining's binary_logloss: 0.519926\n",
      "[276]\ttraining's binary_logloss: 0.519761\n",
      "[277]\ttraining's binary_logloss: 0.519647\n",
      "[278]\ttraining's binary_logloss: 0.519507\n",
      "[279]\ttraining's binary_logloss: 0.51933\n",
      "[280]\ttraining's binary_logloss: 0.51917\n",
      "[281]\ttraining's binary_logloss: 0.519002\n",
      "[282]\ttraining's binary_logloss: 0.518849\n",
      "[283]\ttraining's binary_logloss: 0.518707\n",
      "[284]\ttraining's binary_logloss: 0.518594\n",
      "[285]\ttraining's binary_logloss: 0.518391\n",
      "[286]\ttraining's binary_logloss: 0.518171\n",
      "[287]\ttraining's binary_logloss: 0.517989\n",
      "[288]\ttraining's binary_logloss: 0.517821\n",
      "[289]\ttraining's binary_logloss: 0.517631\n",
      "[290]\ttraining's binary_logloss: 0.517503\n",
      "[291]\ttraining's binary_logloss: 0.517322\n",
      "[292]\ttraining's binary_logloss: 0.517057\n",
      "[293]\ttraining's binary_logloss: 0.516941\n",
      "[294]\ttraining's binary_logloss: 0.516762\n",
      "[295]\ttraining's binary_logloss: 0.51663\n",
      "[296]\ttraining's binary_logloss: 0.516463\n",
      "[297]\ttraining's binary_logloss: 0.516239\n",
      "[298]\ttraining's binary_logloss: 0.516098\n",
      "[299]\ttraining's binary_logloss: 0.515931\n",
      "[300]\ttraining's binary_logloss: 0.515751\n",
      "[301]\ttraining's binary_logloss: 0.515641\n",
      "[302]\ttraining's binary_logloss: 0.515497\n",
      "[303]\ttraining's binary_logloss: 0.515327\n",
      "[304]\ttraining's binary_logloss: 0.515137\n",
      "[305]\ttraining's binary_logloss: 0.514942\n",
      "[306]\ttraining's binary_logloss: 0.514814\n",
      "[307]\ttraining's binary_logloss: 0.514748\n",
      "[308]\ttraining's binary_logloss: 0.514628\n",
      "[309]\ttraining's binary_logloss: 0.51453\n",
      "[310]\ttraining's binary_logloss: 0.514298\n",
      "[311]\ttraining's binary_logloss: 0.514122\n",
      "[312]\ttraining's binary_logloss: 0.513989\n",
      "[313]\ttraining's binary_logloss: 0.513805\n",
      "[314]\ttraining's binary_logloss: 0.5136\n",
      "[315]\ttraining's binary_logloss: 0.513473\n",
      "[316]\ttraining's binary_logloss: 0.513289\n",
      "[317]\ttraining's binary_logloss: 0.513176\n",
      "[318]\ttraining's binary_logloss: 0.512968\n",
      "[319]\ttraining's binary_logloss: 0.512765\n",
      "[320]\ttraining's binary_logloss: 0.512645\n",
      "[321]\ttraining's binary_logloss: 0.512507\n",
      "[322]\ttraining's binary_logloss: 0.512344\n",
      "[323]\ttraining's binary_logloss: 0.512241\n",
      "[324]\ttraining's binary_logloss: 0.512098\n",
      "[325]\ttraining's binary_logloss: 0.511865\n",
      "[326]\ttraining's binary_logloss: 0.511747\n",
      "[327]\ttraining's binary_logloss: 0.511577\n",
      "[328]\ttraining's binary_logloss: 0.51134\n",
      "[329]\ttraining's binary_logloss: 0.511269\n",
      "[330]\ttraining's binary_logloss: 0.511141\n",
      "[331]\ttraining's binary_logloss: 0.511015\n",
      "[332]\ttraining's binary_logloss: 0.510886\n",
      "[333]\ttraining's binary_logloss: 0.510774\n",
      "[334]\ttraining's binary_logloss: 0.510666\n",
      "[335]\ttraining's binary_logloss: 0.510526\n",
      "[336]\ttraining's binary_logloss: 0.510405\n",
      "[337]\ttraining's binary_logloss: 0.510267\n",
      "[338]\ttraining's binary_logloss: 0.510033\n",
      "[339]\ttraining's binary_logloss: 0.509849\n",
      "[340]\ttraining's binary_logloss: 0.509668\n",
      "[341]\ttraining's binary_logloss: 0.509578\n",
      "[342]\ttraining's binary_logloss: 0.509415\n",
      "[343]\ttraining's binary_logloss: 0.509311\n",
      "[344]\ttraining's binary_logloss: 0.509124\n",
      "[345]\ttraining's binary_logloss: 0.508849\n",
      "[346]\ttraining's binary_logloss: 0.508621\n",
      "[347]\ttraining's binary_logloss: 0.508533\n",
      "[348]\ttraining's binary_logloss: 0.508397\n",
      "[349]\ttraining's binary_logloss: 0.508264\n",
      "[350]\ttraining's binary_logloss: 0.508175\n",
      "[351]\ttraining's binary_logloss: 0.508007\n",
      "[352]\ttraining's binary_logloss: 0.507769\n",
      "[353]\ttraining's binary_logloss: 0.507599\n",
      "[354]\ttraining's binary_logloss: 0.507496\n",
      "[355]\ttraining's binary_logloss: 0.507394\n",
      "[356]\ttraining's binary_logloss: 0.507264\n",
      "[357]\ttraining's binary_logloss: 0.507079\n",
      "[358]\ttraining's binary_logloss: 0.506945\n",
      "[359]\ttraining's binary_logloss: 0.506792\n",
      "[360]\ttraining's binary_logloss: 0.506632\n",
      "[361]\ttraining's binary_logloss: 0.506514\n",
      "[362]\ttraining's binary_logloss: 0.506342\n",
      "[363]\ttraining's binary_logloss: 0.506214\n",
      "[364]\ttraining's binary_logloss: 0.506069\n",
      "[365]\ttraining's binary_logloss: 0.505962\n",
      "[366]\ttraining's binary_logloss: 0.505864\n",
      "[367]\ttraining's binary_logloss: 0.505727\n",
      "[368]\ttraining's binary_logloss: 0.505573\n",
      "[369]\ttraining's binary_logloss: 0.505421\n",
      "[370]\ttraining's binary_logloss: 0.505272\n",
      "[371]\ttraining's binary_logloss: 0.505135\n",
      "[372]\ttraining's binary_logloss: 0.504997\n",
      "[373]\ttraining's binary_logloss: 0.5049\n",
      "[374]\ttraining's binary_logloss: 0.504782\n",
      "[375]\ttraining's binary_logloss: 0.504676\n",
      "[376]\ttraining's binary_logloss: 0.504552\n",
      "[377]\ttraining's binary_logloss: 0.504443\n",
      "[378]\ttraining's binary_logloss: 0.504248\n",
      "[379]\ttraining's binary_logloss: 0.504139\n",
      "[380]\ttraining's binary_logloss: 0.503987\n",
      "[381]\ttraining's binary_logloss: 0.50379\n",
      "[382]\ttraining's binary_logloss: 0.503672\n",
      "[383]\ttraining's binary_logloss: 0.503581\n",
      "[384]\ttraining's binary_logloss: 0.503386\n",
      "[385]\ttraining's binary_logloss: 0.503283\n",
      "[386]\ttraining's binary_logloss: 0.503147\n",
      "[387]\ttraining's binary_logloss: 0.502971\n",
      "[388]\ttraining's binary_logloss: 0.502869\n",
      "[389]\ttraining's binary_logloss: 0.502728\n",
      "[390]\ttraining's binary_logloss: 0.502608\n",
      "[391]\ttraining's binary_logloss: 0.502477\n",
      "[392]\ttraining's binary_logloss: 0.502333\n",
      "[393]\ttraining's binary_logloss: 0.502191\n",
      "[394]\ttraining's binary_logloss: 0.502079\n",
      "[395]\ttraining's binary_logloss: 0.501955\n",
      "[396]\ttraining's binary_logloss: 0.501834\n",
      "[397]\ttraining's binary_logloss: 0.501708\n",
      "[398]\ttraining's binary_logloss: 0.501516\n",
      "[399]\ttraining's binary_logloss: 0.501393\n",
      "[400]\ttraining's binary_logloss: 0.501286\n",
      "[401]\ttraining's binary_logloss: 0.50119\n",
      "[402]\ttraining's binary_logloss: 0.501071\n",
      "[403]\ttraining's binary_logloss: 0.50092\n",
      "[404]\ttraining's binary_logloss: 0.500791\n",
      "[405]\ttraining's binary_logloss: 0.500582\n",
      "[406]\ttraining's binary_logloss: 0.50044\n",
      "[407]\ttraining's binary_logloss: 0.50032\n",
      "[408]\ttraining's binary_logloss: 0.500189\n",
      "[409]\ttraining's binary_logloss: 0.500057\n",
      "[410]\ttraining's binary_logloss: 0.499956\n",
      "[411]\ttraining's binary_logloss: 0.499745\n",
      "[412]\ttraining's binary_logloss: 0.499593\n",
      "[413]\ttraining's binary_logloss: 0.49939\n",
      "[414]\ttraining's binary_logloss: 0.499312\n",
      "[415]\ttraining's binary_logloss: 0.499144\n",
      "[416]\ttraining's binary_logloss: 0.498997\n",
      "[417]\ttraining's binary_logloss: 0.498826\n",
      "[418]\ttraining's binary_logloss: 0.498715\n",
      "[419]\ttraining's binary_logloss: 0.498578\n",
      "[420]\ttraining's binary_logloss: 0.498482\n",
      "[421]\ttraining's binary_logloss: 0.498328\n",
      "[422]\ttraining's binary_logloss: 0.498185\n",
      "[423]\ttraining's binary_logloss: 0.498004\n",
      "[424]\ttraining's binary_logloss: 0.497866\n",
      "[425]\ttraining's binary_logloss: 0.497688\n",
      "[426]\ttraining's binary_logloss: 0.497563\n",
      "[427]\ttraining's binary_logloss: 0.497426\n",
      "[428]\ttraining's binary_logloss: 0.497283\n",
      "[429]\ttraining's binary_logloss: 0.497165\n",
      "[430]\ttraining's binary_logloss: 0.497068\n",
      "[431]\ttraining's binary_logloss: 0.496937\n",
      "[432]\ttraining's binary_logloss: 0.496761\n",
      "[433]\ttraining's binary_logloss: 0.496599\n",
      "[434]\ttraining's binary_logloss: 0.496466\n",
      "[435]\ttraining's binary_logloss: 0.496384\n",
      "[436]\ttraining's binary_logloss: 0.496282\n",
      "[437]\ttraining's binary_logloss: 0.496127\n",
      "[438]\ttraining's binary_logloss: 0.495988\n",
      "[439]\ttraining's binary_logloss: 0.495896\n",
      "[440]\ttraining's binary_logloss: 0.495806\n",
      "[441]\ttraining's binary_logloss: 0.495629\n",
      "[442]\ttraining's binary_logloss: 0.495454\n",
      "[443]\ttraining's binary_logloss: 0.495322\n",
      "[444]\ttraining's binary_logloss: 0.4952\n",
      "[445]\ttraining's binary_logloss: 0.495089\n",
      "[446]\ttraining's binary_logloss: 0.494898\n",
      "[447]\ttraining's binary_logloss: 0.494749\n",
      "[448]\ttraining's binary_logloss: 0.494653\n",
      "[449]\ttraining's binary_logloss: 0.494479\n",
      "[450]\ttraining's binary_logloss: 0.494401\n",
      "[451]\ttraining's binary_logloss: 0.494292\n",
      "[452]\ttraining's binary_logloss: 0.494103\n",
      "[453]\ttraining's binary_logloss: 0.493954\n",
      "[454]\ttraining's binary_logloss: 0.493765\n",
      "[455]\ttraining's binary_logloss: 0.493587\n",
      "[456]\ttraining's binary_logloss: 0.493456\n",
      "[457]\ttraining's binary_logloss: 0.493327\n",
      "[458]\ttraining's binary_logloss: 0.49322\n",
      "[459]\ttraining's binary_logloss: 0.49312\n",
      "[460]\ttraining's binary_logloss: 0.493041\n",
      "[461]\ttraining's binary_logloss: 0.492863\n",
      "[462]\ttraining's binary_logloss: 0.492769\n",
      "[463]\ttraining's binary_logloss: 0.492603\n",
      "[464]\ttraining's binary_logloss: 0.492403\n",
      "[465]\ttraining's binary_logloss: 0.492278\n",
      "[466]\ttraining's binary_logloss: 0.492176\n",
      "[467]\ttraining's binary_logloss: 0.492013\n",
      "[468]\ttraining's binary_logloss: 0.491904\n",
      "[469]\ttraining's binary_logloss: 0.491797\n",
      "[470]\ttraining's binary_logloss: 0.491647\n",
      "[471]\ttraining's binary_logloss: 0.491462\n",
      "[472]\ttraining's binary_logloss: 0.491306\n",
      "[473]\ttraining's binary_logloss: 0.491138\n",
      "[474]\ttraining's binary_logloss: 0.491027\n",
      "[475]\ttraining's binary_logloss: 0.490901\n",
      "[476]\ttraining's binary_logloss: 0.490764\n",
      "[477]\ttraining's binary_logloss: 0.490659\n",
      "[478]\ttraining's binary_logloss: 0.490549\n",
      "[479]\ttraining's binary_logloss: 0.490443\n",
      "[480]\ttraining's binary_logloss: 0.49033\n",
      "[481]\ttraining's binary_logloss: 0.490113\n",
      "[482]\ttraining's binary_logloss: 0.49\n",
      "[483]\ttraining's binary_logloss: 0.489874\n",
      "[484]\ttraining's binary_logloss: 0.489766\n",
      "[485]\ttraining's binary_logloss: 0.489634\n",
      "[486]\ttraining's binary_logloss: 0.489552\n",
      "[487]\ttraining's binary_logloss: 0.48934\n",
      "[488]\ttraining's binary_logloss: 0.489189\n",
      "[489]\ttraining's binary_logloss: 0.489108\n",
      "[490]\ttraining's binary_logloss: 0.489013\n",
      "[491]\ttraining's binary_logloss: 0.488904\n",
      "[492]\ttraining's binary_logloss: 0.488776\n",
      "[493]\ttraining's binary_logloss: 0.488675\n",
      "[494]\ttraining's binary_logloss: 0.488533\n",
      "[495]\ttraining's binary_logloss: 0.488384\n",
      "[496]\ttraining's binary_logloss: 0.488255\n",
      "[497]\ttraining's binary_logloss: 0.488076\n",
      "[498]\ttraining's binary_logloss: 0.487978\n",
      "[499]\ttraining's binary_logloss: 0.487855\n",
      "[500]\ttraining's binary_logloss: 0.48774\n",
      "[501]\ttraining's binary_logloss: 0.48762\n",
      "[502]\ttraining's binary_logloss: 0.487468\n",
      "[503]\ttraining's binary_logloss: 0.487371\n",
      "[504]\ttraining's binary_logloss: 0.487168\n",
      "[505]\ttraining's binary_logloss: 0.487031\n",
      "[506]\ttraining's binary_logloss: 0.486924\n",
      "[507]\ttraining's binary_logloss: 0.486822\n",
      "[508]\ttraining's binary_logloss: 0.486658\n",
      "[509]\ttraining's binary_logloss: 0.48657\n",
      "[510]\ttraining's binary_logloss: 0.486446\n",
      "[511]\ttraining's binary_logloss: 0.486338\n",
      "[512]\ttraining's binary_logloss: 0.48623\n",
      "[513]\ttraining's binary_logloss: 0.486135\n",
      "[514]\ttraining's binary_logloss: 0.486075\n",
      "[515]\ttraining's binary_logloss: 0.485986\n",
      "[516]\ttraining's binary_logloss: 0.48588\n",
      "[517]\ttraining's binary_logloss: 0.485753\n",
      "[518]\ttraining's binary_logloss: 0.485588\n",
      "[519]\ttraining's binary_logloss: 0.485482\n",
      "[520]\ttraining's binary_logloss: 0.485384\n",
      "[521]\ttraining's binary_logloss: 0.485276\n",
      "[522]\ttraining's binary_logloss: 0.485138\n",
      "[523]\ttraining's binary_logloss: 0.48505\n",
      "[524]\ttraining's binary_logloss: 0.484904\n",
      "[525]\ttraining's binary_logloss: 0.48474\n",
      "[526]\ttraining's binary_logloss: 0.484549\n",
      "[527]\ttraining's binary_logloss: 0.484366\n",
      "[528]\ttraining's binary_logloss: 0.484283\n",
      "[529]\ttraining's binary_logloss: 0.484131\n",
      "[530]\ttraining's binary_logloss: 0.484036\n",
      "[531]\ttraining's binary_logloss: 0.483889\n",
      "[532]\ttraining's binary_logloss: 0.483763\n",
      "[533]\ttraining's binary_logloss: 0.48363\n",
      "[534]\ttraining's binary_logloss: 0.483535\n",
      "[535]\ttraining's binary_logloss: 0.483222\n",
      "[536]\ttraining's binary_logloss: 0.483143\n",
      "[537]\ttraining's binary_logloss: 0.482973\n",
      "[538]\ttraining's binary_logloss: 0.482823\n",
      "[539]\ttraining's binary_logloss: 0.482664\n",
      "[540]\ttraining's binary_logloss: 0.482546\n",
      "[541]\ttraining's binary_logloss: 0.482406\n",
      "[542]\ttraining's binary_logloss: 0.48226\n",
      "[543]\ttraining's binary_logloss: 0.482122\n",
      "[544]\ttraining's binary_logloss: 0.48195\n",
      "[545]\ttraining's binary_logloss: 0.481823\n",
      "[546]\ttraining's binary_logloss: 0.481645\n",
      "[547]\ttraining's binary_logloss: 0.481525\n",
      "[548]\ttraining's binary_logloss: 0.481355\n",
      "[549]\ttraining's binary_logloss: 0.481221\n",
      "[550]\ttraining's binary_logloss: 0.481045\n",
      "[551]\ttraining's binary_logloss: 0.480954\n",
      "[552]\ttraining's binary_logloss: 0.480671\n",
      "[553]\ttraining's binary_logloss: 0.480601\n",
      "[554]\ttraining's binary_logloss: 0.480445\n",
      "[555]\ttraining's binary_logloss: 0.480367\n",
      "[556]\ttraining's binary_logloss: 0.480243\n",
      "[557]\ttraining's binary_logloss: 0.480155\n",
      "[558]\ttraining's binary_logloss: 0.48\n",
      "[559]\ttraining's binary_logloss: 0.479842\n",
      "[560]\ttraining's binary_logloss: 0.479766\n",
      "[561]\ttraining's binary_logloss: 0.479672\n",
      "[562]\ttraining's binary_logloss: 0.479611\n",
      "[563]\ttraining's binary_logloss: 0.479525\n",
      "[564]\ttraining's binary_logloss: 0.479379\n",
      "[565]\ttraining's binary_logloss: 0.479263\n",
      "[566]\ttraining's binary_logloss: 0.479179\n",
      "[567]\ttraining's binary_logloss: 0.47906\n",
      "[568]\ttraining's binary_logloss: 0.478926\n",
      "[569]\ttraining's binary_logloss: 0.478835\n",
      "[570]\ttraining's binary_logloss: 0.478571\n",
      "[571]\ttraining's binary_logloss: 0.478478\n",
      "[572]\ttraining's binary_logloss: 0.478366\n",
      "[573]\ttraining's binary_logloss: 0.478246\n",
      "[574]\ttraining's binary_logloss: 0.478161\n",
      "[575]\ttraining's binary_logloss: 0.478075\n",
      "[576]\ttraining's binary_logloss: 0.47792\n",
      "[577]\ttraining's binary_logloss: 0.477746\n",
      "[578]\ttraining's binary_logloss: 0.477629\n",
      "[579]\ttraining's binary_logloss: 0.477431\n",
      "[580]\ttraining's binary_logloss: 0.477291\n",
      "[581]\ttraining's binary_logloss: 0.477146\n",
      "[582]\ttraining's binary_logloss: 0.47704\n",
      "[583]\ttraining's binary_logloss: 0.476972\n",
      "[584]\ttraining's binary_logloss: 0.476863\n",
      "[585]\ttraining's binary_logloss: 0.476764\n",
      "[586]\ttraining's binary_logloss: 0.476642\n",
      "[587]\ttraining's binary_logloss: 0.476498\n",
      "[588]\ttraining's binary_logloss: 0.476348\n",
      "[589]\ttraining's binary_logloss: 0.476252\n",
      "[590]\ttraining's binary_logloss: 0.476164\n",
      "[591]\ttraining's binary_logloss: 0.476013\n",
      "[592]\ttraining's binary_logloss: 0.475934\n",
      "[593]\ttraining's binary_logloss: 0.475866\n",
      "[594]\ttraining's binary_logloss: 0.475662\n",
      "[595]\ttraining's binary_logloss: 0.475553\n",
      "[596]\ttraining's binary_logloss: 0.475446\n",
      "[597]\ttraining's binary_logloss: 0.475295\n",
      "[598]\ttraining's binary_logloss: 0.475082\n",
      "[599]\ttraining's binary_logloss: 0.474982\n",
      "[600]\ttraining's binary_logloss: 0.474886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./test/models/LightGBM.pkl']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as LightGBM\n",
    "\n",
    "lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=600,\n",
    "                               max_depth = 20,\n",
    "                               min_data_in_leaf = 100,\n",
    "                               class_weight={True: 10, False: 1},\n",
    "                               learning_rate= 0.1,\n",
    "                               objective='binary'\n",
    "                              ) \n",
    "\n",
    "evals = [(x_train_features, y_train_bool)]\n",
    "lgbm.fit(x_train_features, y_train_bool, eval_metric='logloss', eval_set=evals)\n",
    "y_pred = lgbm.predict(x_train_features)\n",
    "\n",
    "joblib.dump(lgbm, './test/models/LightGBM.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.99      0.43      0.60     63391\n",
      "        risk       0.26      0.97      0.40     12724\n",
      "\n",
      "    accuracy                           0.52     76115\n",
      "   macro avg       0.62      0.70      0.50     76115\n",
      "weighted avg       0.86      0.52      0.57     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_train_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.40      0.55     21052\n",
      "        risk       0.22      0.81      0.34      4344\n",
      "\n",
      "    accuracy                           0.47     25396\n",
      "   macro avg       0.56      0.61      0.45     25396\n",
      "weighted avg       0.79      0.47      0.52     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2 Light GBM and Ensembles\n",
    "Light GBM 모델을 feature selection feature set 기반으로 앙상블하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light gbm 앙상블을 위해 feature set을 생성하는 코드\n",
    "rfecv_feature_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "sfs_feature_list = ['BPS', 'DIV', '거래량', '금리', '비유동자산', '자산총계', '부채총계', '법인세차감전 순이익', '당기순이익']\n",
    "\n",
    "\n",
    "def make_feature_set(x) :\n",
    "    x_whole = x\n",
    "    x_rfecv = x[rfecv_feature_list]\n",
    "    x_sfs = x[sfs_feature_list]\n",
    "\n",
    "    return x_whole, x_rfecv, x_sfs\n",
    "\n",
    "\n",
    "x_whole, x_rfecv, x_sfs= make_feature_set(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_logloss: 0.628963\n",
      "[2]\ttraining's binary_logloss: 0.623169\n",
      "[3]\ttraining's binary_logloss: 0.618132\n",
      "[4]\ttraining's binary_logloss: 0.613868\n",
      "[5]\ttraining's binary_logloss: 0.610407\n",
      "[6]\ttraining's binary_logloss: 0.607459\n",
      "[7]\ttraining's binary_logloss: 0.604669\n",
      "[8]\ttraining's binary_logloss: 0.602451\n",
      "[9]\ttraining's binary_logloss: 0.600126\n",
      "[10]\ttraining's binary_logloss: 0.598127\n",
      "[11]\ttraining's binary_logloss: 0.596193\n",
      "[12]\ttraining's binary_logloss: 0.594546\n",
      "[13]\ttraining's binary_logloss: 0.593102\n",
      "[14]\ttraining's binary_logloss: 0.591732\n",
      "[15]\ttraining's binary_logloss: 0.590413\n",
      "[16]\ttraining's binary_logloss: 0.589211\n",
      "[17]\ttraining's binary_logloss: 0.588091\n",
      "[18]\ttraining's binary_logloss: 0.587018\n",
      "[19]\ttraining's binary_logloss: 0.585988\n",
      "[20]\ttraining's binary_logloss: 0.585031\n",
      "[21]\ttraining's binary_logloss: 0.584051\n",
      "[22]\ttraining's binary_logloss: 0.58326\n",
      "[23]\ttraining's binary_logloss: 0.582493\n",
      "[24]\ttraining's binary_logloss: 0.581635\n",
      "[25]\ttraining's binary_logloss: 0.580872\n",
      "[26]\ttraining's binary_logloss: 0.580154\n",
      "[27]\ttraining's binary_logloss: 0.579441\n",
      "[28]\ttraining's binary_logloss: 0.578824\n",
      "[29]\ttraining's binary_logloss: 0.57811\n",
      "[30]\ttraining's binary_logloss: 0.577437\n",
      "[31]\ttraining's binary_logloss: 0.576835\n",
      "[32]\ttraining's binary_logloss: 0.576283\n",
      "[33]\ttraining's binary_logloss: 0.575666\n",
      "[34]\ttraining's binary_logloss: 0.575064\n",
      "[35]\ttraining's binary_logloss: 0.57452\n",
      "[36]\ttraining's binary_logloss: 0.574011\n",
      "[37]\ttraining's binary_logloss: 0.573532\n",
      "[38]\ttraining's binary_logloss: 0.572951\n",
      "[39]\ttraining's binary_logloss: 0.572392\n",
      "[40]\ttraining's binary_logloss: 0.571964\n",
      "[41]\ttraining's binary_logloss: 0.571437\n",
      "[42]\ttraining's binary_logloss: 0.570946\n",
      "[43]\ttraining's binary_logloss: 0.570487\n",
      "[44]\ttraining's binary_logloss: 0.570051\n",
      "[45]\ttraining's binary_logloss: 0.569539\n",
      "[46]\ttraining's binary_logloss: 0.569159\n",
      "[47]\ttraining's binary_logloss: 0.568708\n",
      "[48]\ttraining's binary_logloss: 0.568197\n",
      "[49]\ttraining's binary_logloss: 0.567752\n",
      "[50]\ttraining's binary_logloss: 0.56734\n",
      "[51]\ttraining's binary_logloss: 0.566889\n",
      "[52]\ttraining's binary_logloss: 0.56649\n",
      "[53]\ttraining's binary_logloss: 0.56608\n",
      "[54]\ttraining's binary_logloss: 0.565764\n",
      "[55]\ttraining's binary_logloss: 0.565339\n",
      "[56]\ttraining's binary_logloss: 0.564958\n",
      "[57]\ttraining's binary_logloss: 0.564519\n",
      "[58]\ttraining's binary_logloss: 0.564238\n",
      "[59]\ttraining's binary_logloss: 0.563877\n",
      "[60]\ttraining's binary_logloss: 0.563488\n",
      "[61]\ttraining's binary_logloss: 0.563102\n",
      "[62]\ttraining's binary_logloss: 0.562772\n",
      "[63]\ttraining's binary_logloss: 0.562396\n",
      "[64]\ttraining's binary_logloss: 0.562085\n",
      "[65]\ttraining's binary_logloss: 0.561705\n",
      "[66]\ttraining's binary_logloss: 0.561431\n",
      "[67]\ttraining's binary_logloss: 0.561093\n",
      "[68]\ttraining's binary_logloss: 0.560786\n",
      "[69]\ttraining's binary_logloss: 0.560433\n",
      "[70]\ttraining's binary_logloss: 0.560096\n",
      "[71]\ttraining's binary_logloss: 0.559874\n",
      "[72]\ttraining's binary_logloss: 0.559599\n",
      "[73]\ttraining's binary_logloss: 0.559252\n",
      "[74]\ttraining's binary_logloss: 0.558956\n",
      "[75]\ttraining's binary_logloss: 0.558668\n",
      "[76]\ttraining's binary_logloss: 0.558314\n",
      "[77]\ttraining's binary_logloss: 0.558076\n",
      "[78]\ttraining's binary_logloss: 0.557822\n",
      "[79]\ttraining's binary_logloss: 0.557471\n",
      "[80]\ttraining's binary_logloss: 0.557253\n",
      "[81]\ttraining's binary_logloss: 0.557024\n",
      "[82]\ttraining's binary_logloss: 0.556733\n",
      "[83]\ttraining's binary_logloss: 0.556431\n",
      "[84]\ttraining's binary_logloss: 0.556088\n",
      "[85]\ttraining's binary_logloss: 0.555829\n",
      "[86]\ttraining's binary_logloss: 0.555629\n",
      "[87]\ttraining's binary_logloss: 0.555208\n",
      "[88]\ttraining's binary_logloss: 0.554933\n",
      "[89]\ttraining's binary_logloss: 0.554658\n",
      "[90]\ttraining's binary_logloss: 0.554377\n",
      "[91]\ttraining's binary_logloss: 0.554078\n",
      "[92]\ttraining's binary_logloss: 0.553841\n",
      "[93]\ttraining's binary_logloss: 0.553642\n",
      "[94]\ttraining's binary_logloss: 0.553311\n",
      "[95]\ttraining's binary_logloss: 0.553111\n",
      "[96]\ttraining's binary_logloss: 0.552854\n",
      "[97]\ttraining's binary_logloss: 0.552546\n",
      "[98]\ttraining's binary_logloss: 0.552333\n",
      "[99]\ttraining's binary_logloss: 0.552051\n",
      "[100]\ttraining's binary_logloss: 0.551763\n",
      "[101]\ttraining's binary_logloss: 0.551425\n",
      "[102]\ttraining's binary_logloss: 0.551169\n",
      "[103]\ttraining's binary_logloss: 0.550913\n",
      "[104]\ttraining's binary_logloss: 0.55065\n",
      "[105]\ttraining's binary_logloss: 0.55043\n",
      "[106]\ttraining's binary_logloss: 0.550183\n",
      "[107]\ttraining's binary_logloss: 0.549872\n",
      "[108]\ttraining's binary_logloss: 0.549713\n",
      "[109]\ttraining's binary_logloss: 0.549458\n",
      "[110]\ttraining's binary_logloss: 0.549188\n",
      "[111]\ttraining's binary_logloss: 0.54897\n",
      "[112]\ttraining's binary_logloss: 0.548748\n",
      "[113]\ttraining's binary_logloss: 0.548499\n",
      "[114]\ttraining's binary_logloss: 0.548164\n",
      "[115]\ttraining's binary_logloss: 0.547928\n",
      "[116]\ttraining's binary_logloss: 0.54767\n",
      "[117]\ttraining's binary_logloss: 0.547412\n",
      "[118]\ttraining's binary_logloss: 0.547176\n",
      "[119]\ttraining's binary_logloss: 0.546962\n",
      "[120]\ttraining's binary_logloss: 0.546642\n",
      "[121]\ttraining's binary_logloss: 0.546368\n",
      "[122]\ttraining's binary_logloss: 0.546221\n",
      "[123]\ttraining's binary_logloss: 0.545972\n",
      "[124]\ttraining's binary_logloss: 0.545736\n",
      "[125]\ttraining's binary_logloss: 0.545469\n",
      "[126]\ttraining's binary_logloss: 0.54529\n",
      "[127]\ttraining's binary_logloss: 0.545093\n",
      "[128]\ttraining's binary_logloss: 0.544786\n",
      "[129]\ttraining's binary_logloss: 0.54451\n",
      "[130]\ttraining's binary_logloss: 0.544338\n",
      "[131]\ttraining's binary_logloss: 0.544032\n",
      "[132]\ttraining's binary_logloss: 0.543758\n",
      "[133]\ttraining's binary_logloss: 0.543488\n",
      "[134]\ttraining's binary_logloss: 0.543303\n",
      "[135]\ttraining's binary_logloss: 0.542975\n",
      "[136]\ttraining's binary_logloss: 0.542708\n",
      "[137]\ttraining's binary_logloss: 0.542454\n",
      "[138]\ttraining's binary_logloss: 0.542089\n",
      "[139]\ttraining's binary_logloss: 0.541921\n",
      "[140]\ttraining's binary_logloss: 0.541694\n",
      "[141]\ttraining's binary_logloss: 0.541396\n",
      "[142]\ttraining's binary_logloss: 0.541202\n",
      "[143]\ttraining's binary_logloss: 0.541023\n",
      "[144]\ttraining's binary_logloss: 0.540866\n",
      "[145]\ttraining's binary_logloss: 0.540646\n",
      "[146]\ttraining's binary_logloss: 0.540402\n",
      "[147]\ttraining's binary_logloss: 0.540107\n",
      "[148]\ttraining's binary_logloss: 0.539891\n",
      "[149]\ttraining's binary_logloss: 0.53965\n",
      "[150]\ttraining's binary_logloss: 0.539406\n",
      "[151]\ttraining's binary_logloss: 0.53918\n",
      "[152]\ttraining's binary_logloss: 0.538933\n",
      "[153]\ttraining's binary_logloss: 0.538629\n",
      "[154]\ttraining's binary_logloss: 0.53841\n",
      "[155]\ttraining's binary_logloss: 0.53821\n",
      "[156]\ttraining's binary_logloss: 0.537919\n",
      "[157]\ttraining's binary_logloss: 0.537743\n",
      "[158]\ttraining's binary_logloss: 0.537606\n",
      "[159]\ttraining's binary_logloss: 0.537413\n",
      "[160]\ttraining's binary_logloss: 0.537169\n",
      "[161]\ttraining's binary_logloss: 0.536936\n",
      "[162]\ttraining's binary_logloss: 0.536653\n",
      "[163]\ttraining's binary_logloss: 0.536487\n",
      "[164]\ttraining's binary_logloss: 0.536316\n",
      "[165]\ttraining's binary_logloss: 0.536026\n",
      "[166]\ttraining's binary_logloss: 0.535833\n",
      "[167]\ttraining's binary_logloss: 0.535549\n",
      "[168]\ttraining's binary_logloss: 0.53532\n",
      "[169]\ttraining's binary_logloss: 0.535192\n",
      "[170]\ttraining's binary_logloss: 0.534994\n",
      "[171]\ttraining's binary_logloss: 0.534719\n",
      "[172]\ttraining's binary_logloss: 0.534529\n",
      "[173]\ttraining's binary_logloss: 0.534327\n",
      "[174]\ttraining's binary_logloss: 0.534118\n",
      "[175]\ttraining's binary_logloss: 0.533936\n",
      "[176]\ttraining's binary_logloss: 0.533752\n",
      "[177]\ttraining's binary_logloss: 0.533587\n",
      "[178]\ttraining's binary_logloss: 0.533286\n",
      "[179]\ttraining's binary_logloss: 0.533048\n",
      "[180]\ttraining's binary_logloss: 0.532822\n",
      "[181]\ttraining's binary_logloss: 0.532626\n",
      "[182]\ttraining's binary_logloss: 0.532497\n",
      "[183]\ttraining's binary_logloss: 0.532311\n",
      "[184]\ttraining's binary_logloss: 0.532081\n",
      "[185]\ttraining's binary_logloss: 0.531808\n",
      "[186]\ttraining's binary_logloss: 0.531524\n",
      "[187]\ttraining's binary_logloss: 0.531321\n",
      "[188]\ttraining's binary_logloss: 0.531119\n",
      "[189]\ttraining's binary_logloss: 0.530978\n",
      "[190]\ttraining's binary_logloss: 0.530783\n",
      "[191]\ttraining's binary_logloss: 0.530557\n",
      "[192]\ttraining's binary_logloss: 0.530346\n",
      "[193]\ttraining's binary_logloss: 0.5302\n",
      "[194]\ttraining's binary_logloss: 0.529961\n",
      "[195]\ttraining's binary_logloss: 0.529692\n",
      "[196]\ttraining's binary_logloss: 0.529504\n",
      "[197]\ttraining's binary_logloss: 0.529278\n",
      "[198]\ttraining's binary_logloss: 0.529107\n",
      "[199]\ttraining's binary_logloss: 0.528869\n",
      "[200]\ttraining's binary_logloss: 0.528637\n",
      "[201]\ttraining's binary_logloss: 0.52835\n",
      "[202]\ttraining's binary_logloss: 0.528146\n",
      "[203]\ttraining's binary_logloss: 0.527961\n",
      "[204]\ttraining's binary_logloss: 0.527748\n",
      "[205]\ttraining's binary_logloss: 0.527561\n",
      "[206]\ttraining's binary_logloss: 0.527285\n",
      "[207]\ttraining's binary_logloss: 0.527118\n",
      "[208]\ttraining's binary_logloss: 0.526915\n",
      "[209]\ttraining's binary_logloss: 0.526719\n",
      "[210]\ttraining's binary_logloss: 0.526545\n",
      "[211]\ttraining's binary_logloss: 0.526362\n",
      "[212]\ttraining's binary_logloss: 0.526227\n",
      "[213]\ttraining's binary_logloss: 0.526104\n",
      "[214]\ttraining's binary_logloss: 0.525888\n",
      "[215]\ttraining's binary_logloss: 0.525649\n",
      "[216]\ttraining's binary_logloss: 0.525397\n",
      "[217]\ttraining's binary_logloss: 0.525249\n",
      "[218]\ttraining's binary_logloss: 0.525113\n",
      "[219]\ttraining's binary_logloss: 0.524808\n",
      "[220]\ttraining's binary_logloss: 0.524572\n",
      "[221]\ttraining's binary_logloss: 0.524386\n",
      "[222]\ttraining's binary_logloss: 0.524177\n",
      "[223]\ttraining's binary_logloss: 0.524064\n",
      "[224]\ttraining's binary_logloss: 0.523976\n",
      "[225]\ttraining's binary_logloss: 0.523782\n",
      "[226]\ttraining's binary_logloss: 0.523664\n",
      "[227]\ttraining's binary_logloss: 0.523469\n",
      "[228]\ttraining's binary_logloss: 0.523261\n",
      "[229]\ttraining's binary_logloss: 0.523089\n",
      "[230]\ttraining's binary_logloss: 0.522814\n",
      "[231]\ttraining's binary_logloss: 0.522642\n",
      "[232]\ttraining's binary_logloss: 0.522444\n",
      "[233]\ttraining's binary_logloss: 0.522336\n",
      "[234]\ttraining's binary_logloss: 0.522145\n",
      "[235]\ttraining's binary_logloss: 0.521952\n",
      "[236]\ttraining's binary_logloss: 0.521791\n",
      "[237]\ttraining's binary_logloss: 0.521519\n",
      "[238]\ttraining's binary_logloss: 0.521284\n",
      "[239]\ttraining's binary_logloss: 0.521173\n",
      "[240]\ttraining's binary_logloss: 0.521047\n",
      "[241]\ttraining's binary_logloss: 0.520912\n",
      "[242]\ttraining's binary_logloss: 0.520719\n",
      "[243]\ttraining's binary_logloss: 0.520576\n",
      "[244]\ttraining's binary_logloss: 0.520355\n",
      "[245]\ttraining's binary_logloss: 0.520102\n",
      "[246]\ttraining's binary_logloss: 0.519953\n",
      "[247]\ttraining's binary_logloss: 0.519811\n",
      "[248]\ttraining's binary_logloss: 0.519668\n",
      "[249]\ttraining's binary_logloss: 0.519487\n",
      "[250]\ttraining's binary_logloss: 0.519278\n",
      "[251]\ttraining's binary_logloss: 0.519018\n",
      "[252]\ttraining's binary_logloss: 0.518886\n",
      "[253]\ttraining's binary_logloss: 0.518701\n",
      "[254]\ttraining's binary_logloss: 0.518482\n",
      "[255]\ttraining's binary_logloss: 0.518367\n",
      "[256]\ttraining's binary_logloss: 0.518133\n",
      "[257]\ttraining's binary_logloss: 0.51803\n",
      "[258]\ttraining's binary_logloss: 0.517828\n",
      "[259]\ttraining's binary_logloss: 0.517612\n",
      "[260]\ttraining's binary_logloss: 0.517405\n",
      "[261]\ttraining's binary_logloss: 0.517199\n",
      "[262]\ttraining's binary_logloss: 0.517035\n",
      "[263]\ttraining's binary_logloss: 0.516878\n",
      "[264]\ttraining's binary_logloss: 0.516717\n",
      "[265]\ttraining's binary_logloss: 0.516588\n",
      "[266]\ttraining's binary_logloss: 0.516337\n",
      "[267]\ttraining's binary_logloss: 0.516185\n",
      "[268]\ttraining's binary_logloss: 0.515968\n",
      "[269]\ttraining's binary_logloss: 0.515692\n",
      "[270]\ttraining's binary_logloss: 0.515519\n",
      "[271]\ttraining's binary_logloss: 0.515204\n",
      "[272]\ttraining's binary_logloss: 0.515073\n",
      "[273]\ttraining's binary_logloss: 0.514892\n",
      "[274]\ttraining's binary_logloss: 0.514688\n",
      "[275]\ttraining's binary_logloss: 0.514448\n",
      "[276]\ttraining's binary_logloss: 0.514294\n",
      "[277]\ttraining's binary_logloss: 0.514108\n",
      "[278]\ttraining's binary_logloss: 0.513991\n",
      "[279]\ttraining's binary_logloss: 0.513746\n",
      "[280]\ttraining's binary_logloss: 0.513496\n",
      "[281]\ttraining's binary_logloss: 0.513302\n",
      "[282]\ttraining's binary_logloss: 0.513158\n",
      "[283]\ttraining's binary_logloss: 0.512831\n",
      "[284]\ttraining's binary_logloss: 0.512672\n",
      "[285]\ttraining's binary_logloss: 0.512494\n",
      "[286]\ttraining's binary_logloss: 0.512292\n",
      "[287]\ttraining's binary_logloss: 0.512111\n",
      "[288]\ttraining's binary_logloss: 0.511889\n",
      "[289]\ttraining's binary_logloss: 0.511692\n",
      "[290]\ttraining's binary_logloss: 0.511464\n",
      "[291]\ttraining's binary_logloss: 0.511172\n",
      "[292]\ttraining's binary_logloss: 0.51093\n",
      "[293]\ttraining's binary_logloss: 0.510798\n",
      "[294]\ttraining's binary_logloss: 0.510695\n",
      "[295]\ttraining's binary_logloss: 0.510527\n",
      "[296]\ttraining's binary_logloss: 0.51041\n",
      "[297]\ttraining's binary_logloss: 0.510314\n",
      "[298]\ttraining's binary_logloss: 0.510186\n",
      "[299]\ttraining's binary_logloss: 0.509989\n",
      "[300]\ttraining's binary_logloss: 0.509745\n",
      "[301]\ttraining's binary_logloss: 0.509635\n",
      "[302]\ttraining's binary_logloss: 0.509511\n",
      "[303]\ttraining's binary_logloss: 0.509292\n",
      "[304]\ttraining's binary_logloss: 0.509114\n",
      "[305]\ttraining's binary_logloss: 0.508991\n",
      "[306]\ttraining's binary_logloss: 0.508901\n",
      "[307]\ttraining's binary_logloss: 0.508667\n",
      "[308]\ttraining's binary_logloss: 0.508367\n",
      "[309]\ttraining's binary_logloss: 0.508164\n",
      "[310]\ttraining's binary_logloss: 0.508044\n",
      "[311]\ttraining's binary_logloss: 0.507893\n",
      "[312]\ttraining's binary_logloss: 0.507704\n",
      "[313]\ttraining's binary_logloss: 0.507546\n",
      "[314]\ttraining's binary_logloss: 0.507384\n",
      "[315]\ttraining's binary_logloss: 0.507197\n",
      "[316]\ttraining's binary_logloss: 0.507062\n",
      "[317]\ttraining's binary_logloss: 0.506856\n",
      "[318]\ttraining's binary_logloss: 0.506741\n",
      "[319]\ttraining's binary_logloss: 0.50665\n",
      "[320]\ttraining's binary_logloss: 0.506501\n",
      "[321]\ttraining's binary_logloss: 0.506332\n",
      "[322]\ttraining's binary_logloss: 0.506158\n",
      "[323]\ttraining's binary_logloss: 0.505978\n",
      "[324]\ttraining's binary_logloss: 0.505731\n",
      "[325]\ttraining's binary_logloss: 0.50557\n",
      "[326]\ttraining's binary_logloss: 0.505449\n",
      "[327]\ttraining's binary_logloss: 0.505225\n",
      "[328]\ttraining's binary_logloss: 0.505094\n",
      "[329]\ttraining's binary_logloss: 0.504949\n",
      "[330]\ttraining's binary_logloss: 0.504764\n",
      "[331]\ttraining's binary_logloss: 0.504538\n",
      "[332]\ttraining's binary_logloss: 0.504404\n",
      "[333]\ttraining's binary_logloss: 0.504289\n",
      "[334]\ttraining's binary_logloss: 0.504131\n",
      "[335]\ttraining's binary_logloss: 0.504032\n",
      "[336]\ttraining's binary_logloss: 0.503835\n",
      "[337]\ttraining's binary_logloss: 0.503614\n",
      "[338]\ttraining's binary_logloss: 0.503388\n",
      "[339]\ttraining's binary_logloss: 0.503211\n",
      "[340]\ttraining's binary_logloss: 0.503037\n",
      "[341]\ttraining's binary_logloss: 0.50295\n",
      "[342]\ttraining's binary_logloss: 0.502754\n",
      "[343]\ttraining's binary_logloss: 0.502552\n",
      "[344]\ttraining's binary_logloss: 0.50243\n",
      "[345]\ttraining's binary_logloss: 0.502327\n",
      "[346]\ttraining's binary_logloss: 0.502166\n",
      "[347]\ttraining's binary_logloss: 0.501959\n",
      "[348]\ttraining's binary_logloss: 0.50174\n",
      "[349]\ttraining's binary_logloss: 0.501642\n",
      "[350]\ttraining's binary_logloss: 0.501499\n",
      "[351]\ttraining's binary_logloss: 0.501405\n",
      "[352]\ttraining's binary_logloss: 0.501314\n",
      "[353]\ttraining's binary_logloss: 0.501153\n",
      "[354]\ttraining's binary_logloss: 0.50102\n",
      "[355]\ttraining's binary_logloss: 0.50088\n",
      "[356]\ttraining's binary_logloss: 0.50074\n",
      "[357]\ttraining's binary_logloss: 0.50062\n",
      "[358]\ttraining's binary_logloss: 0.500518\n",
      "[359]\ttraining's binary_logloss: 0.500343\n",
      "[360]\ttraining's binary_logloss: 0.500258\n",
      "[361]\ttraining's binary_logloss: 0.500166\n",
      "[362]\ttraining's binary_logloss: 0.499967\n",
      "[363]\ttraining's binary_logloss: 0.499782\n",
      "[364]\ttraining's binary_logloss: 0.499592\n",
      "[365]\ttraining's binary_logloss: 0.499376\n",
      "[366]\ttraining's binary_logloss: 0.49921\n",
      "[367]\ttraining's binary_logloss: 0.499135\n",
      "[368]\ttraining's binary_logloss: 0.498984\n",
      "[369]\ttraining's binary_logloss: 0.498841\n",
      "[370]\ttraining's binary_logloss: 0.498646\n",
      "[371]\ttraining's binary_logloss: 0.498474\n",
      "[372]\ttraining's binary_logloss: 0.498284\n",
      "[373]\ttraining's binary_logloss: 0.498098\n",
      "[374]\ttraining's binary_logloss: 0.497951\n",
      "[375]\ttraining's binary_logloss: 0.49783\n",
      "[376]\ttraining's binary_logloss: 0.497534\n",
      "[377]\ttraining's binary_logloss: 0.497322\n",
      "[378]\ttraining's binary_logloss: 0.497167\n",
      "[379]\ttraining's binary_logloss: 0.497058\n",
      "[380]\ttraining's binary_logloss: 0.496861\n",
      "[381]\ttraining's binary_logloss: 0.496741\n",
      "[382]\ttraining's binary_logloss: 0.496597\n",
      "[383]\ttraining's binary_logloss: 0.496449\n",
      "[384]\ttraining's binary_logloss: 0.496341\n",
      "[385]\ttraining's binary_logloss: 0.496181\n",
      "[386]\ttraining's binary_logloss: 0.496054\n",
      "[387]\ttraining's binary_logloss: 0.495881\n",
      "[388]\ttraining's binary_logloss: 0.495717\n",
      "[389]\ttraining's binary_logloss: 0.495503\n",
      "[390]\ttraining's binary_logloss: 0.495371\n",
      "[391]\ttraining's binary_logloss: 0.495277\n",
      "[392]\ttraining's binary_logloss: 0.495139\n",
      "[393]\ttraining's binary_logloss: 0.49506\n",
      "[394]\ttraining's binary_logloss: 0.494994\n",
      "[395]\ttraining's binary_logloss: 0.494764\n",
      "[396]\ttraining's binary_logloss: 0.494592\n",
      "[397]\ttraining's binary_logloss: 0.494502\n",
      "[398]\ttraining's binary_logloss: 0.494305\n",
      "[399]\ttraining's binary_logloss: 0.494221\n",
      "[400]\ttraining's binary_logloss: 0.494103\n",
      "[401]\ttraining's binary_logloss: 0.493948\n",
      "[402]\ttraining's binary_logloss: 0.493731\n",
      "[403]\ttraining's binary_logloss: 0.493633\n",
      "[404]\ttraining's binary_logloss: 0.493503\n",
      "[405]\ttraining's binary_logloss: 0.493387\n",
      "[406]\ttraining's binary_logloss: 0.493278\n",
      "[407]\ttraining's binary_logloss: 0.49309\n",
      "[408]\ttraining's binary_logloss: 0.4929\n",
      "[409]\ttraining's binary_logloss: 0.492772\n",
      "[410]\ttraining's binary_logloss: 0.492607\n",
      "[411]\ttraining's binary_logloss: 0.492454\n",
      "[412]\ttraining's binary_logloss: 0.492254\n",
      "[413]\ttraining's binary_logloss: 0.492026\n",
      "[414]\ttraining's binary_logloss: 0.491872\n",
      "[415]\ttraining's binary_logloss: 0.491686\n",
      "[416]\ttraining's binary_logloss: 0.491526\n",
      "[417]\ttraining's binary_logloss: 0.491369\n",
      "[418]\ttraining's binary_logloss: 0.491156\n",
      "[419]\ttraining's binary_logloss: 0.49102\n",
      "[420]\ttraining's binary_logloss: 0.490799\n",
      "[421]\ttraining's binary_logloss: 0.490684\n",
      "[422]\ttraining's binary_logloss: 0.490483\n",
      "[423]\ttraining's binary_logloss: 0.490284\n",
      "[424]\ttraining's binary_logloss: 0.490122\n",
      "[425]\ttraining's binary_logloss: 0.489985\n",
      "[426]\ttraining's binary_logloss: 0.489819\n",
      "[427]\ttraining's binary_logloss: 0.48961\n",
      "[428]\ttraining's binary_logloss: 0.489448\n",
      "[429]\ttraining's binary_logloss: 0.489314\n",
      "[430]\ttraining's binary_logloss: 0.489149\n",
      "[431]\ttraining's binary_logloss: 0.489017\n",
      "[432]\ttraining's binary_logloss: 0.488903\n",
      "[433]\ttraining's binary_logloss: 0.488755\n",
      "[434]\ttraining's binary_logloss: 0.488679\n",
      "[435]\ttraining's binary_logloss: 0.488583\n",
      "[436]\ttraining's binary_logloss: 0.488471\n",
      "[437]\ttraining's binary_logloss: 0.488335\n",
      "[438]\ttraining's binary_logloss: 0.488173\n",
      "[439]\ttraining's binary_logloss: 0.488063\n",
      "[440]\ttraining's binary_logloss: 0.487934\n",
      "[441]\ttraining's binary_logloss: 0.48779\n",
      "[442]\ttraining's binary_logloss: 0.487718\n",
      "[443]\ttraining's binary_logloss: 0.48765\n",
      "[444]\ttraining's binary_logloss: 0.487507\n",
      "[445]\ttraining's binary_logloss: 0.48738\n",
      "[446]\ttraining's binary_logloss: 0.487264\n",
      "[447]\ttraining's binary_logloss: 0.487094\n",
      "[448]\ttraining's binary_logloss: 0.486952\n",
      "[449]\ttraining's binary_logloss: 0.486816\n",
      "[450]\ttraining's binary_logloss: 0.486654\n",
      "[451]\ttraining's binary_logloss: 0.486544\n",
      "[452]\ttraining's binary_logloss: 0.486435\n",
      "[453]\ttraining's binary_logloss: 0.486215\n",
      "[454]\ttraining's binary_logloss: 0.486037\n",
      "[455]\ttraining's binary_logloss: 0.48585\n",
      "[456]\ttraining's binary_logloss: 0.48572\n",
      "[457]\ttraining's binary_logloss: 0.485635\n",
      "[458]\ttraining's binary_logloss: 0.485496\n",
      "[459]\ttraining's binary_logloss: 0.485301\n",
      "[460]\ttraining's binary_logloss: 0.485147\n",
      "[461]\ttraining's binary_logloss: 0.484997\n",
      "[462]\ttraining's binary_logloss: 0.484863\n",
      "[463]\ttraining's binary_logloss: 0.484703\n",
      "[464]\ttraining's binary_logloss: 0.484537\n",
      "[465]\ttraining's binary_logloss: 0.484333\n",
      "[466]\ttraining's binary_logloss: 0.484162\n",
      "[467]\ttraining's binary_logloss: 0.484026\n",
      "[468]\ttraining's binary_logloss: 0.483888\n",
      "[469]\ttraining's binary_logloss: 0.483826\n",
      "[470]\ttraining's binary_logloss: 0.483737\n",
      "[471]\ttraining's binary_logloss: 0.483616\n",
      "[472]\ttraining's binary_logloss: 0.483554\n",
      "[473]\ttraining's binary_logloss: 0.48344\n",
      "[474]\ttraining's binary_logloss: 0.483367\n",
      "[475]\ttraining's binary_logloss: 0.483258\n",
      "[476]\ttraining's binary_logloss: 0.483075\n",
      "[477]\ttraining's binary_logloss: 0.48292\n",
      "[478]\ttraining's binary_logloss: 0.482771\n",
      "[479]\ttraining's binary_logloss: 0.48262\n",
      "[480]\ttraining's binary_logloss: 0.482448\n",
      "[481]\ttraining's binary_logloss: 0.482272\n",
      "[482]\ttraining's binary_logloss: 0.482111\n",
      "[483]\ttraining's binary_logloss: 0.481985\n",
      "[484]\ttraining's binary_logloss: 0.481815\n",
      "[485]\ttraining's binary_logloss: 0.481725\n",
      "[486]\ttraining's binary_logloss: 0.48162\n",
      "[487]\ttraining's binary_logloss: 0.481492\n",
      "[488]\ttraining's binary_logloss: 0.481331\n",
      "[489]\ttraining's binary_logloss: 0.481201\n",
      "[490]\ttraining's binary_logloss: 0.481136\n",
      "[491]\ttraining's binary_logloss: 0.481032\n",
      "[492]\ttraining's binary_logloss: 0.480885\n",
      "[493]\ttraining's binary_logloss: 0.480762\n",
      "[494]\ttraining's binary_logloss: 0.480692\n",
      "[495]\ttraining's binary_logloss: 0.480563\n",
      "[496]\ttraining's binary_logloss: 0.480399\n",
      "[497]\ttraining's binary_logloss: 0.480186\n",
      "[498]\ttraining's binary_logloss: 0.480066\n",
      "[499]\ttraining's binary_logloss: 0.479982\n",
      "[500]\ttraining's binary_logloss: 0.479761\n",
      "[501]\ttraining's binary_logloss: 0.479581\n",
      "[502]\ttraining's binary_logloss: 0.479387\n",
      "[503]\ttraining's binary_logloss: 0.479279\n",
      "[504]\ttraining's binary_logloss: 0.479193\n",
      "[505]\ttraining's binary_logloss: 0.478995\n",
      "[506]\ttraining's binary_logloss: 0.478767\n",
      "[507]\ttraining's binary_logloss: 0.478631\n",
      "[508]\ttraining's binary_logloss: 0.478505\n",
      "[509]\ttraining's binary_logloss: 0.47838\n",
      "[510]\ttraining's binary_logloss: 0.478168\n",
      "[511]\ttraining's binary_logloss: 0.478009\n",
      "[512]\ttraining's binary_logloss: 0.47791\n",
      "[513]\ttraining's binary_logloss: 0.477835\n",
      "[514]\ttraining's binary_logloss: 0.477716\n",
      "[515]\ttraining's binary_logloss: 0.47762\n",
      "[516]\ttraining's binary_logloss: 0.477475\n",
      "[517]\ttraining's binary_logloss: 0.477348\n",
      "[518]\ttraining's binary_logloss: 0.477177\n",
      "[519]\ttraining's binary_logloss: 0.477073\n",
      "[520]\ttraining's binary_logloss: 0.476862\n",
      "[521]\ttraining's binary_logloss: 0.476692\n",
      "[522]\ttraining's binary_logloss: 0.476576\n",
      "[523]\ttraining's binary_logloss: 0.476419\n",
      "[524]\ttraining's binary_logloss: 0.476318\n",
      "[525]\ttraining's binary_logloss: 0.476224\n",
      "[526]\ttraining's binary_logloss: 0.476126\n",
      "[527]\ttraining's binary_logloss: 0.475943\n",
      "[528]\ttraining's binary_logloss: 0.47589\n",
      "[529]\ttraining's binary_logloss: 0.475805\n",
      "[530]\ttraining's binary_logloss: 0.475638\n",
      "[531]\ttraining's binary_logloss: 0.475502\n",
      "[532]\ttraining's binary_logloss: 0.47541\n",
      "[533]\ttraining's binary_logloss: 0.475277\n",
      "[534]\ttraining's binary_logloss: 0.475232\n",
      "[535]\ttraining's binary_logloss: 0.475085\n",
      "[536]\ttraining's binary_logloss: 0.474991\n",
      "[537]\ttraining's binary_logloss: 0.474771\n",
      "[538]\ttraining's binary_logloss: 0.474699\n",
      "[539]\ttraining's binary_logloss: 0.474572\n",
      "[540]\ttraining's binary_logloss: 0.474442\n",
      "[541]\ttraining's binary_logloss: 0.474373\n",
      "[542]\ttraining's binary_logloss: 0.474241\n",
      "[543]\ttraining's binary_logloss: 0.474164\n",
      "[544]\ttraining's binary_logloss: 0.47398\n",
      "[545]\ttraining's binary_logloss: 0.473862\n",
      "[546]\ttraining's binary_logloss: 0.473504\n",
      "[547]\ttraining's binary_logloss: 0.47334\n",
      "[548]\ttraining's binary_logloss: 0.473258\n",
      "[549]\ttraining's binary_logloss: 0.47316\n",
      "[550]\ttraining's binary_logloss: 0.473052\n",
      "[551]\ttraining's binary_logloss: 0.472875\n",
      "[552]\ttraining's binary_logloss: 0.472728\n",
      "[553]\ttraining's binary_logloss: 0.472631\n",
      "[554]\ttraining's binary_logloss: 0.472494\n",
      "[555]\ttraining's binary_logloss: 0.472395\n",
      "[556]\ttraining's binary_logloss: 0.472122\n",
      "[557]\ttraining's binary_logloss: 0.472003\n",
      "[558]\ttraining's binary_logloss: 0.471867\n",
      "[559]\ttraining's binary_logloss: 0.471736\n",
      "[560]\ttraining's binary_logloss: 0.471607\n",
      "[561]\ttraining's binary_logloss: 0.471419\n",
      "[562]\ttraining's binary_logloss: 0.471248\n",
      "[563]\ttraining's binary_logloss: 0.471153\n",
      "[564]\ttraining's binary_logloss: 0.471032\n",
      "[565]\ttraining's binary_logloss: 0.470869\n",
      "[566]\ttraining's binary_logloss: 0.470742\n",
      "[567]\ttraining's binary_logloss: 0.47054\n",
      "[568]\ttraining's binary_logloss: 0.47038\n",
      "[569]\ttraining's binary_logloss: 0.470244\n",
      "[570]\ttraining's binary_logloss: 0.470083\n",
      "[571]\ttraining's binary_logloss: 0.469976\n",
      "[572]\ttraining's binary_logloss: 0.469779\n",
      "[573]\ttraining's binary_logloss: 0.469666\n",
      "[574]\ttraining's binary_logloss: 0.469504\n",
      "[575]\ttraining's binary_logloss: 0.469384\n",
      "[576]\ttraining's binary_logloss: 0.469214\n",
      "[577]\ttraining's binary_logloss: 0.468921\n",
      "[578]\ttraining's binary_logloss: 0.46878\n",
      "[579]\ttraining's binary_logloss: 0.468612\n",
      "[580]\ttraining's binary_logloss: 0.46852\n",
      "[581]\ttraining's binary_logloss: 0.468378\n",
      "[582]\ttraining's binary_logloss: 0.468214\n",
      "[583]\ttraining's binary_logloss: 0.468023\n",
      "[584]\ttraining's binary_logloss: 0.467852\n",
      "[585]\ttraining's binary_logloss: 0.467737\n",
      "[586]\ttraining's binary_logloss: 0.467614\n",
      "[587]\ttraining's binary_logloss: 0.467507\n",
      "[588]\ttraining's binary_logloss: 0.467342\n",
      "[589]\ttraining's binary_logloss: 0.467104\n",
      "[590]\ttraining's binary_logloss: 0.46702\n",
      "[591]\ttraining's binary_logloss: 0.466902\n",
      "[592]\ttraining's binary_logloss: 0.466784\n",
      "[593]\ttraining's binary_logloss: 0.466684\n",
      "[594]\ttraining's binary_logloss: 0.466494\n",
      "[595]\ttraining's binary_logloss: 0.466348\n",
      "[596]\ttraining's binary_logloss: 0.466235\n",
      "[597]\ttraining's binary_logloss: 0.466055\n",
      "[598]\ttraining's binary_logloss: 0.465919\n",
      "[599]\ttraining's binary_logloss: 0.465723\n",
      "[600]\ttraining's binary_logloss: 0.465571\n",
      "[1]\ttraining's binary_logloss: 0.628988\n",
      "[2]\ttraining's binary_logloss: 0.623421\n",
      "[3]\ttraining's binary_logloss: 0.618595\n",
      "[4]\ttraining's binary_logloss: 0.614736\n",
      "[5]\ttraining's binary_logloss: 0.611145\n",
      "[6]\ttraining's binary_logloss: 0.608083\n",
      "[7]\ttraining's binary_logloss: 0.605599\n",
      "[8]\ttraining's binary_logloss: 0.60318\n",
      "[9]\ttraining's binary_logloss: 0.601156\n",
      "[10]\ttraining's binary_logloss: 0.599245\n",
      "[11]\ttraining's binary_logloss: 0.597674\n",
      "[12]\ttraining's binary_logloss: 0.596315\n",
      "[13]\ttraining's binary_logloss: 0.594907\n",
      "[14]\ttraining's binary_logloss: 0.593697\n",
      "[15]\ttraining's binary_logloss: 0.592459\n",
      "[16]\ttraining's binary_logloss: 0.591341\n",
      "[17]\ttraining's binary_logloss: 0.590368\n",
      "[18]\ttraining's binary_logloss: 0.589415\n",
      "[19]\ttraining's binary_logloss: 0.588542\n",
      "[20]\ttraining's binary_logloss: 0.587626\n",
      "[21]\ttraining's binary_logloss: 0.586785\n",
      "[22]\ttraining's binary_logloss: 0.585927\n",
      "[23]\ttraining's binary_logloss: 0.585217\n",
      "[24]\ttraining's binary_logloss: 0.584518\n",
      "[25]\ttraining's binary_logloss: 0.583815\n",
      "[26]\ttraining's binary_logloss: 0.583215\n",
      "[27]\ttraining's binary_logloss: 0.582612\n",
      "[28]\ttraining's binary_logloss: 0.582014\n",
      "[29]\ttraining's binary_logloss: 0.581421\n",
      "[30]\ttraining's binary_logloss: 0.580827\n",
      "[31]\ttraining's binary_logloss: 0.580219\n",
      "[32]\ttraining's binary_logloss: 0.579627\n",
      "[33]\ttraining's binary_logloss: 0.57907\n",
      "[34]\ttraining's binary_logloss: 0.578461\n",
      "[35]\ttraining's binary_logloss: 0.57794\n",
      "[36]\ttraining's binary_logloss: 0.577413\n",
      "[37]\ttraining's binary_logloss: 0.576984\n",
      "[38]\ttraining's binary_logloss: 0.576446\n",
      "[39]\ttraining's binary_logloss: 0.576013\n",
      "[40]\ttraining's binary_logloss: 0.575554\n",
      "[41]\ttraining's binary_logloss: 0.575091\n",
      "[42]\ttraining's binary_logloss: 0.574702\n",
      "[43]\ttraining's binary_logloss: 0.574224\n",
      "[44]\ttraining's binary_logloss: 0.573776\n",
      "[45]\ttraining's binary_logloss: 0.573357\n",
      "[46]\ttraining's binary_logloss: 0.572885\n",
      "[47]\ttraining's binary_logloss: 0.572478\n",
      "[48]\ttraining's binary_logloss: 0.572078\n",
      "[49]\ttraining's binary_logloss: 0.571684\n",
      "[50]\ttraining's binary_logloss: 0.571342\n",
      "[51]\ttraining's binary_logloss: 0.570987\n",
      "[52]\ttraining's binary_logloss: 0.570604\n",
      "[53]\ttraining's binary_logloss: 0.570223\n",
      "[54]\ttraining's binary_logloss: 0.569866\n",
      "[55]\ttraining's binary_logloss: 0.569547\n",
      "[56]\ttraining's binary_logloss: 0.569147\n",
      "[57]\ttraining's binary_logloss: 0.568802\n",
      "[58]\ttraining's binary_logloss: 0.568329\n",
      "[59]\ttraining's binary_logloss: 0.567903\n",
      "[60]\ttraining's binary_logloss: 0.567521\n",
      "[61]\ttraining's binary_logloss: 0.567223\n",
      "[62]\ttraining's binary_logloss: 0.566953\n",
      "[63]\ttraining's binary_logloss: 0.566651\n",
      "[64]\ttraining's binary_logloss: 0.566302\n",
      "[65]\ttraining's binary_logloss: 0.565949\n",
      "[66]\ttraining's binary_logloss: 0.565623\n",
      "[67]\ttraining's binary_logloss: 0.565279\n",
      "[68]\ttraining's binary_logloss: 0.564954\n",
      "[69]\ttraining's binary_logloss: 0.564539\n",
      "[70]\ttraining's binary_logloss: 0.564261\n",
      "[71]\ttraining's binary_logloss: 0.563952\n",
      "[72]\ttraining's binary_logloss: 0.563678\n",
      "[73]\ttraining's binary_logloss: 0.563482\n",
      "[74]\ttraining's binary_logloss: 0.563098\n",
      "[75]\ttraining's binary_logloss: 0.562868\n",
      "[76]\ttraining's binary_logloss: 0.562527\n",
      "[77]\ttraining's binary_logloss: 0.562208\n",
      "[78]\ttraining's binary_logloss: 0.561945\n",
      "[79]\ttraining's binary_logloss: 0.561602\n",
      "[80]\ttraining's binary_logloss: 0.561302\n",
      "[81]\ttraining's binary_logloss: 0.561\n",
      "[82]\ttraining's binary_logloss: 0.560753\n",
      "[83]\ttraining's binary_logloss: 0.560448\n",
      "[84]\ttraining's binary_logloss: 0.560219\n",
      "[85]\ttraining's binary_logloss: 0.559976\n",
      "[86]\ttraining's binary_logloss: 0.559661\n",
      "[87]\ttraining's binary_logloss: 0.559427\n",
      "[88]\ttraining's binary_logloss: 0.559171\n",
      "[89]\ttraining's binary_logloss: 0.558935\n",
      "[90]\ttraining's binary_logloss: 0.558638\n",
      "[91]\ttraining's binary_logloss: 0.558326\n",
      "[92]\ttraining's binary_logloss: 0.557987\n",
      "[93]\ttraining's binary_logloss: 0.557749\n",
      "[94]\ttraining's binary_logloss: 0.557557\n",
      "[95]\ttraining's binary_logloss: 0.557268\n",
      "[96]\ttraining's binary_logloss: 0.556977\n",
      "[97]\ttraining's binary_logloss: 0.556674\n",
      "[98]\ttraining's binary_logloss: 0.556364\n",
      "[99]\ttraining's binary_logloss: 0.556105\n",
      "[100]\ttraining's binary_logloss: 0.555852\n",
      "[101]\ttraining's binary_logloss: 0.555561\n",
      "[102]\ttraining's binary_logloss: 0.555314\n",
      "[103]\ttraining's binary_logloss: 0.554996\n",
      "[104]\ttraining's binary_logloss: 0.554814\n",
      "[105]\ttraining's binary_logloss: 0.554589\n",
      "[106]\ttraining's binary_logloss: 0.554296\n",
      "[107]\ttraining's binary_logloss: 0.554059\n",
      "[108]\ttraining's binary_logloss: 0.553875\n",
      "[109]\ttraining's binary_logloss: 0.553637\n",
      "[110]\ttraining's binary_logloss: 0.553318\n",
      "[111]\ttraining's binary_logloss: 0.553091\n",
      "[112]\ttraining's binary_logloss: 0.552917\n",
      "[113]\ttraining's binary_logloss: 0.552749\n",
      "[114]\ttraining's binary_logloss: 0.552485\n",
      "[115]\ttraining's binary_logloss: 0.552213\n",
      "[116]\ttraining's binary_logloss: 0.55188\n",
      "[117]\ttraining's binary_logloss: 0.551536\n",
      "[118]\ttraining's binary_logloss: 0.551318\n",
      "[119]\ttraining's binary_logloss: 0.551114\n",
      "[120]\ttraining's binary_logloss: 0.550837\n",
      "[121]\ttraining's binary_logloss: 0.550632\n",
      "[122]\ttraining's binary_logloss: 0.550362\n",
      "[123]\ttraining's binary_logloss: 0.550071\n",
      "[124]\ttraining's binary_logloss: 0.549858\n",
      "[125]\ttraining's binary_logloss: 0.549697\n",
      "[126]\ttraining's binary_logloss: 0.54944\n",
      "[127]\ttraining's binary_logloss: 0.549271\n",
      "[128]\ttraining's binary_logloss: 0.548994\n",
      "[129]\ttraining's binary_logloss: 0.548805\n",
      "[130]\ttraining's binary_logloss: 0.548558\n",
      "[131]\ttraining's binary_logloss: 0.548351\n",
      "[132]\ttraining's binary_logloss: 0.548144\n",
      "[133]\ttraining's binary_logloss: 0.547895\n",
      "[134]\ttraining's binary_logloss: 0.547648\n",
      "[135]\ttraining's binary_logloss: 0.547381\n",
      "[136]\ttraining's binary_logloss: 0.547155\n",
      "[137]\ttraining's binary_logloss: 0.546884\n",
      "[138]\ttraining's binary_logloss: 0.546658\n",
      "[139]\ttraining's binary_logloss: 0.546403\n",
      "[140]\ttraining's binary_logloss: 0.546274\n",
      "[141]\ttraining's binary_logloss: 0.546016\n",
      "[142]\ttraining's binary_logloss: 0.545819\n",
      "[143]\ttraining's binary_logloss: 0.545659\n",
      "[144]\ttraining's binary_logloss: 0.545511\n",
      "[145]\ttraining's binary_logloss: 0.545351\n",
      "[146]\ttraining's binary_logloss: 0.545155\n",
      "[147]\ttraining's binary_logloss: 0.544932\n",
      "[148]\ttraining's binary_logloss: 0.544764\n",
      "[149]\ttraining's binary_logloss: 0.544435\n",
      "[150]\ttraining's binary_logloss: 0.544229\n",
      "[151]\ttraining's binary_logloss: 0.543983\n",
      "[152]\ttraining's binary_logloss: 0.543728\n",
      "[153]\ttraining's binary_logloss: 0.543585\n",
      "[154]\ttraining's binary_logloss: 0.543335\n",
      "[155]\ttraining's binary_logloss: 0.543105\n",
      "[156]\ttraining's binary_logloss: 0.542911\n",
      "[157]\ttraining's binary_logloss: 0.542675\n",
      "[158]\ttraining's binary_logloss: 0.542439\n",
      "[159]\ttraining's binary_logloss: 0.5423\n",
      "[160]\ttraining's binary_logloss: 0.542053\n",
      "[161]\ttraining's binary_logloss: 0.541883\n",
      "[162]\ttraining's binary_logloss: 0.541685\n",
      "[163]\ttraining's binary_logloss: 0.541495\n",
      "[164]\ttraining's binary_logloss: 0.541345\n",
      "[165]\ttraining's binary_logloss: 0.541225\n",
      "[166]\ttraining's binary_logloss: 0.541012\n",
      "[167]\ttraining's binary_logloss: 0.540823\n",
      "[168]\ttraining's binary_logloss: 0.540631\n",
      "[169]\ttraining's binary_logloss: 0.540449\n",
      "[170]\ttraining's binary_logloss: 0.540231\n",
      "[171]\ttraining's binary_logloss: 0.539952\n",
      "[172]\ttraining's binary_logloss: 0.539802\n",
      "[173]\ttraining's binary_logloss: 0.539524\n",
      "[174]\ttraining's binary_logloss: 0.53938\n",
      "[175]\ttraining's binary_logloss: 0.53917\n",
      "[176]\ttraining's binary_logloss: 0.538953\n",
      "[177]\ttraining's binary_logloss: 0.538717\n",
      "[178]\ttraining's binary_logloss: 0.538486\n",
      "[179]\ttraining's binary_logloss: 0.538317\n",
      "[180]\ttraining's binary_logloss: 0.538166\n",
      "[181]\ttraining's binary_logloss: 0.537896\n",
      "[182]\ttraining's binary_logloss: 0.537605\n",
      "[183]\ttraining's binary_logloss: 0.537406\n",
      "[184]\ttraining's binary_logloss: 0.537169\n",
      "[185]\ttraining's binary_logloss: 0.536928\n",
      "[186]\ttraining's binary_logloss: 0.536683\n",
      "[187]\ttraining's binary_logloss: 0.536511\n",
      "[188]\ttraining's binary_logloss: 0.536364\n",
      "[189]\ttraining's binary_logloss: 0.536229\n",
      "[190]\ttraining's binary_logloss: 0.535961\n",
      "[191]\ttraining's binary_logloss: 0.535815\n",
      "[192]\ttraining's binary_logloss: 0.535647\n",
      "[193]\ttraining's binary_logloss: 0.535393\n",
      "[194]\ttraining's binary_logloss: 0.535139\n",
      "[195]\ttraining's binary_logloss: 0.534887\n",
      "[196]\ttraining's binary_logloss: 0.534687\n",
      "[197]\ttraining's binary_logloss: 0.534507\n",
      "[198]\ttraining's binary_logloss: 0.534299\n",
      "[199]\ttraining's binary_logloss: 0.534101\n",
      "[200]\ttraining's binary_logloss: 0.533881\n",
      "[201]\ttraining's binary_logloss: 0.533707\n",
      "[202]\ttraining's binary_logloss: 0.533481\n",
      "[203]\ttraining's binary_logloss: 0.533228\n",
      "[204]\ttraining's binary_logloss: 0.533083\n",
      "[205]\ttraining's binary_logloss: 0.532924\n",
      "[206]\ttraining's binary_logloss: 0.532758\n",
      "[207]\ttraining's binary_logloss: 0.532574\n",
      "[208]\ttraining's binary_logloss: 0.532385\n",
      "[209]\ttraining's binary_logloss: 0.532069\n",
      "[210]\ttraining's binary_logloss: 0.53196\n",
      "[211]\ttraining's binary_logloss: 0.531817\n",
      "[212]\ttraining's binary_logloss: 0.531615\n",
      "[213]\ttraining's binary_logloss: 0.531348\n",
      "[214]\ttraining's binary_logloss: 0.531182\n",
      "[215]\ttraining's binary_logloss: 0.531006\n",
      "[216]\ttraining's binary_logloss: 0.53084\n",
      "[217]\ttraining's binary_logloss: 0.530633\n",
      "[218]\ttraining's binary_logloss: 0.530433\n",
      "[219]\ttraining's binary_logloss: 0.530198\n",
      "[220]\ttraining's binary_logloss: 0.53008\n",
      "[221]\ttraining's binary_logloss: 0.529862\n",
      "[222]\ttraining's binary_logloss: 0.529695\n",
      "[223]\ttraining's binary_logloss: 0.529583\n",
      "[224]\ttraining's binary_logloss: 0.529468\n",
      "[225]\ttraining's binary_logloss: 0.529249\n",
      "[226]\ttraining's binary_logloss: 0.529043\n",
      "[227]\ttraining's binary_logloss: 0.528809\n",
      "[228]\ttraining's binary_logloss: 0.528607\n",
      "[229]\ttraining's binary_logloss: 0.528413\n",
      "[230]\ttraining's binary_logloss: 0.528164\n",
      "[231]\ttraining's binary_logloss: 0.527949\n",
      "[232]\ttraining's binary_logloss: 0.527773\n",
      "[233]\ttraining's binary_logloss: 0.52744\n",
      "[234]\ttraining's binary_logloss: 0.527209\n",
      "[235]\ttraining's binary_logloss: 0.526984\n",
      "[236]\ttraining's binary_logloss: 0.526783\n",
      "[237]\ttraining's binary_logloss: 0.526549\n",
      "[238]\ttraining's binary_logloss: 0.526364\n",
      "[239]\ttraining's binary_logloss: 0.526233\n",
      "[240]\ttraining's binary_logloss: 0.526064\n",
      "[241]\ttraining's binary_logloss: 0.525812\n",
      "[242]\ttraining's binary_logloss: 0.525578\n",
      "[243]\ttraining's binary_logloss: 0.525466\n",
      "[244]\ttraining's binary_logloss: 0.525309\n",
      "[245]\ttraining's binary_logloss: 0.525075\n",
      "[246]\ttraining's binary_logloss: 0.524932\n",
      "[247]\ttraining's binary_logloss: 0.524748\n",
      "[248]\ttraining's binary_logloss: 0.524624\n",
      "[249]\ttraining's binary_logloss: 0.524385\n",
      "[250]\ttraining's binary_logloss: 0.524211\n",
      "[251]\ttraining's binary_logloss: 0.523941\n",
      "[252]\ttraining's binary_logloss: 0.523787\n",
      "[253]\ttraining's binary_logloss: 0.523612\n",
      "[254]\ttraining's binary_logloss: 0.523421\n",
      "[255]\ttraining's binary_logloss: 0.523211\n",
      "[256]\ttraining's binary_logloss: 0.523096\n",
      "[257]\ttraining's binary_logloss: 0.522911\n",
      "[258]\ttraining's binary_logloss: 0.522737\n",
      "[259]\ttraining's binary_logloss: 0.522592\n",
      "[260]\ttraining's binary_logloss: 0.522367\n",
      "[261]\ttraining's binary_logloss: 0.522237\n",
      "[262]\ttraining's binary_logloss: 0.522139\n",
      "[263]\ttraining's binary_logloss: 0.521994\n",
      "[264]\ttraining's binary_logloss: 0.521802\n",
      "[265]\ttraining's binary_logloss: 0.521594\n",
      "[266]\ttraining's binary_logloss: 0.521445\n",
      "[267]\ttraining's binary_logloss: 0.521302\n",
      "[268]\ttraining's binary_logloss: 0.521156\n",
      "[269]\ttraining's binary_logloss: 0.520924\n",
      "[270]\ttraining's binary_logloss: 0.520802\n",
      "[271]\ttraining's binary_logloss: 0.520695\n",
      "[272]\ttraining's binary_logloss: 0.52045\n",
      "[273]\ttraining's binary_logloss: 0.520258\n",
      "[274]\ttraining's binary_logloss: 0.520119\n",
      "[275]\ttraining's binary_logloss: 0.519904\n",
      "[276]\ttraining's binary_logloss: 0.519718\n",
      "[277]\ttraining's binary_logloss: 0.519517\n",
      "[278]\ttraining's binary_logloss: 0.519358\n",
      "[279]\ttraining's binary_logloss: 0.519137\n",
      "[280]\ttraining's binary_logloss: 0.518928\n",
      "[281]\ttraining's binary_logloss: 0.518758\n",
      "[282]\ttraining's binary_logloss: 0.518617\n",
      "[283]\ttraining's binary_logloss: 0.518432\n",
      "[284]\ttraining's binary_logloss: 0.518235\n",
      "[285]\ttraining's binary_logloss: 0.518076\n",
      "[286]\ttraining's binary_logloss: 0.517902\n",
      "[287]\ttraining's binary_logloss: 0.517765\n",
      "[288]\ttraining's binary_logloss: 0.517566\n",
      "[289]\ttraining's binary_logloss: 0.517363\n",
      "[290]\ttraining's binary_logloss: 0.517185\n",
      "[291]\ttraining's binary_logloss: 0.517063\n",
      "[292]\ttraining's binary_logloss: 0.516949\n",
      "[293]\ttraining's binary_logloss: 0.516724\n",
      "[294]\ttraining's binary_logloss: 0.516606\n",
      "[295]\ttraining's binary_logloss: 0.516478\n",
      "[296]\ttraining's binary_logloss: 0.516358\n",
      "[297]\ttraining's binary_logloss: 0.516134\n",
      "[298]\ttraining's binary_logloss: 0.515972\n",
      "[299]\ttraining's binary_logloss: 0.515855\n",
      "[300]\ttraining's binary_logloss: 0.515758\n",
      "[301]\ttraining's binary_logloss: 0.515549\n",
      "[302]\ttraining's binary_logloss: 0.515324\n",
      "[303]\ttraining's binary_logloss: 0.515135\n",
      "[304]\ttraining's binary_logloss: 0.515002\n",
      "[305]\ttraining's binary_logloss: 0.514869\n",
      "[306]\ttraining's binary_logloss: 0.514722\n",
      "[307]\ttraining's binary_logloss: 0.514604\n",
      "[308]\ttraining's binary_logloss: 0.514416\n",
      "[309]\ttraining's binary_logloss: 0.514274\n",
      "[310]\ttraining's binary_logloss: 0.514173\n",
      "[311]\ttraining's binary_logloss: 0.514027\n",
      "[312]\ttraining's binary_logloss: 0.513902\n",
      "[313]\ttraining's binary_logloss: 0.513697\n",
      "[314]\ttraining's binary_logloss: 0.513577\n",
      "[315]\ttraining's binary_logloss: 0.513477\n",
      "[316]\ttraining's binary_logloss: 0.513385\n",
      "[317]\ttraining's binary_logloss: 0.513213\n",
      "[318]\ttraining's binary_logloss: 0.513084\n",
      "[319]\ttraining's binary_logloss: 0.512925\n",
      "[320]\ttraining's binary_logloss: 0.512728\n",
      "[321]\ttraining's binary_logloss: 0.512534\n",
      "[322]\ttraining's binary_logloss: 0.512377\n",
      "[323]\ttraining's binary_logloss: 0.512206\n",
      "[324]\ttraining's binary_logloss: 0.511964\n",
      "[325]\ttraining's binary_logloss: 0.511767\n",
      "[326]\ttraining's binary_logloss: 0.511654\n",
      "[327]\ttraining's binary_logloss: 0.511566\n",
      "[328]\ttraining's binary_logloss: 0.511438\n",
      "[329]\ttraining's binary_logloss: 0.511314\n",
      "[330]\ttraining's binary_logloss: 0.511167\n",
      "[331]\ttraining's binary_logloss: 0.511005\n",
      "[332]\ttraining's binary_logloss: 0.510866\n",
      "[333]\ttraining's binary_logloss: 0.510709\n",
      "[334]\ttraining's binary_logloss: 0.510605\n",
      "[335]\ttraining's binary_logloss: 0.510477\n",
      "[336]\ttraining's binary_logloss: 0.510289\n",
      "[337]\ttraining's binary_logloss: 0.510155\n",
      "[338]\ttraining's binary_logloss: 0.509992\n",
      "[339]\ttraining's binary_logloss: 0.509879\n",
      "[340]\ttraining's binary_logloss: 0.509773\n",
      "[341]\ttraining's binary_logloss: 0.509652\n",
      "[342]\ttraining's binary_logloss: 0.509516\n",
      "[343]\ttraining's binary_logloss: 0.509373\n",
      "[344]\ttraining's binary_logloss: 0.509185\n",
      "[345]\ttraining's binary_logloss: 0.509024\n",
      "[346]\ttraining's binary_logloss: 0.508872\n",
      "[347]\ttraining's binary_logloss: 0.508713\n",
      "[348]\ttraining's binary_logloss: 0.508581\n",
      "[349]\ttraining's binary_logloss: 0.508506\n",
      "[350]\ttraining's binary_logloss: 0.508319\n",
      "[351]\ttraining's binary_logloss: 0.508188\n",
      "[352]\ttraining's binary_logloss: 0.507979\n",
      "[353]\ttraining's binary_logloss: 0.507885\n",
      "[354]\ttraining's binary_logloss: 0.507774\n",
      "[355]\ttraining's binary_logloss: 0.507569\n",
      "[356]\ttraining's binary_logloss: 0.507382\n",
      "[357]\ttraining's binary_logloss: 0.507216\n",
      "[358]\ttraining's binary_logloss: 0.507041\n",
      "[359]\ttraining's binary_logloss: 0.506883\n",
      "[360]\ttraining's binary_logloss: 0.506773\n",
      "[361]\ttraining's binary_logloss: 0.506625\n",
      "[362]\ttraining's binary_logloss: 0.506412\n",
      "[363]\ttraining's binary_logloss: 0.506283\n",
      "[364]\ttraining's binary_logloss: 0.506137\n",
      "[365]\ttraining's binary_logloss: 0.506006\n",
      "[366]\ttraining's binary_logloss: 0.505889\n",
      "[367]\ttraining's binary_logloss: 0.505767\n",
      "[368]\ttraining's binary_logloss: 0.505686\n",
      "[369]\ttraining's binary_logloss: 0.505584\n",
      "[370]\ttraining's binary_logloss: 0.505375\n",
      "[371]\ttraining's binary_logloss: 0.505292\n",
      "[372]\ttraining's binary_logloss: 0.505186\n",
      "[373]\ttraining's binary_logloss: 0.504896\n",
      "[374]\ttraining's binary_logloss: 0.504785\n",
      "[375]\ttraining's binary_logloss: 0.504651\n",
      "[376]\ttraining's binary_logloss: 0.504527\n",
      "[377]\ttraining's binary_logloss: 0.504437\n",
      "[378]\ttraining's binary_logloss: 0.504246\n",
      "[379]\ttraining's binary_logloss: 0.504133\n",
      "[380]\ttraining's binary_logloss: 0.503959\n",
      "[381]\ttraining's binary_logloss: 0.503819\n",
      "[382]\ttraining's binary_logloss: 0.503661\n",
      "[383]\ttraining's binary_logloss: 0.503573\n",
      "[384]\ttraining's binary_logloss: 0.503427\n",
      "[385]\ttraining's binary_logloss: 0.503211\n",
      "[386]\ttraining's binary_logloss: 0.503052\n",
      "[387]\ttraining's binary_logloss: 0.502924\n",
      "[388]\ttraining's binary_logloss: 0.502776\n",
      "[389]\ttraining's binary_logloss: 0.502645\n",
      "[390]\ttraining's binary_logloss: 0.502505\n",
      "[391]\ttraining's binary_logloss: 0.502316\n",
      "[392]\ttraining's binary_logloss: 0.502187\n",
      "[393]\ttraining's binary_logloss: 0.502058\n",
      "[394]\ttraining's binary_logloss: 0.501894\n",
      "[395]\ttraining's binary_logloss: 0.501759\n",
      "[396]\ttraining's binary_logloss: 0.501558\n",
      "[397]\ttraining's binary_logloss: 0.501413\n",
      "[398]\ttraining's binary_logloss: 0.501316\n",
      "[399]\ttraining's binary_logloss: 0.501176\n",
      "[400]\ttraining's binary_logloss: 0.501055\n",
      "[401]\ttraining's binary_logloss: 0.500928\n",
      "[402]\ttraining's binary_logloss: 0.500761\n",
      "[403]\ttraining's binary_logloss: 0.500668\n",
      "[404]\ttraining's binary_logloss: 0.500543\n",
      "[405]\ttraining's binary_logloss: 0.500402\n",
      "[406]\ttraining's binary_logloss: 0.500267\n",
      "[407]\ttraining's binary_logloss: 0.500161\n",
      "[408]\ttraining's binary_logloss: 0.500062\n",
      "[409]\ttraining's binary_logloss: 0.499921\n",
      "[410]\ttraining's binary_logloss: 0.499807\n",
      "[411]\ttraining's binary_logloss: 0.499607\n",
      "[412]\ttraining's binary_logloss: 0.499425\n",
      "[413]\ttraining's binary_logloss: 0.49924\n",
      "[414]\ttraining's binary_logloss: 0.499064\n",
      "[415]\ttraining's binary_logloss: 0.498891\n",
      "[416]\ttraining's binary_logloss: 0.498716\n",
      "[417]\ttraining's binary_logloss: 0.498522\n",
      "[418]\ttraining's binary_logloss: 0.498374\n",
      "[419]\ttraining's binary_logloss: 0.498228\n",
      "[420]\ttraining's binary_logloss: 0.498068\n",
      "[421]\ttraining's binary_logloss: 0.497929\n",
      "[422]\ttraining's binary_logloss: 0.497798\n",
      "[423]\ttraining's binary_logloss: 0.497711\n",
      "[424]\ttraining's binary_logloss: 0.49759\n",
      "[425]\ttraining's binary_logloss: 0.497517\n",
      "[426]\ttraining's binary_logloss: 0.497441\n",
      "[427]\ttraining's binary_logloss: 0.497311\n",
      "[428]\ttraining's binary_logloss: 0.497187\n",
      "[429]\ttraining's binary_logloss: 0.497078\n",
      "[430]\ttraining's binary_logloss: 0.496973\n",
      "[431]\ttraining's binary_logloss: 0.496839\n",
      "[432]\ttraining's binary_logloss: 0.496677\n",
      "[433]\ttraining's binary_logloss: 0.496414\n",
      "[434]\ttraining's binary_logloss: 0.496302\n",
      "[435]\ttraining's binary_logloss: 0.4961\n",
      "[436]\ttraining's binary_logloss: 0.495954\n",
      "[437]\ttraining's binary_logloss: 0.495792\n",
      "[438]\ttraining's binary_logloss: 0.495639\n",
      "[439]\ttraining's binary_logloss: 0.495489\n",
      "[440]\ttraining's binary_logloss: 0.495268\n",
      "[441]\ttraining's binary_logloss: 0.495081\n",
      "[442]\ttraining's binary_logloss: 0.494945\n",
      "[443]\ttraining's binary_logloss: 0.494788\n",
      "[444]\ttraining's binary_logloss: 0.494641\n",
      "[445]\ttraining's binary_logloss: 0.494469\n",
      "[446]\ttraining's binary_logloss: 0.494293\n",
      "[447]\ttraining's binary_logloss: 0.494149\n",
      "[448]\ttraining's binary_logloss: 0.49404\n",
      "[449]\ttraining's binary_logloss: 0.493929\n",
      "[450]\ttraining's binary_logloss: 0.493802\n",
      "[451]\ttraining's binary_logloss: 0.493594\n",
      "[452]\ttraining's binary_logloss: 0.493461\n",
      "[453]\ttraining's binary_logloss: 0.493257\n",
      "[454]\ttraining's binary_logloss: 0.493138\n",
      "[455]\ttraining's binary_logloss: 0.4929\n",
      "[456]\ttraining's binary_logloss: 0.492708\n",
      "[457]\ttraining's binary_logloss: 0.492523\n",
      "[458]\ttraining's binary_logloss: 0.492346\n",
      "[459]\ttraining's binary_logloss: 0.492225\n",
      "[460]\ttraining's binary_logloss: 0.492074\n",
      "[461]\ttraining's binary_logloss: 0.491995\n",
      "[462]\ttraining's binary_logloss: 0.491841\n",
      "[463]\ttraining's binary_logloss: 0.491669\n",
      "[464]\ttraining's binary_logloss: 0.491585\n",
      "[465]\ttraining's binary_logloss: 0.49137\n",
      "[466]\ttraining's binary_logloss: 0.491247\n",
      "[467]\ttraining's binary_logloss: 0.491023\n",
      "[468]\ttraining's binary_logloss: 0.490787\n",
      "[469]\ttraining's binary_logloss: 0.490642\n",
      "[470]\ttraining's binary_logloss: 0.49047\n",
      "[471]\ttraining's binary_logloss: 0.49032\n",
      "[472]\ttraining's binary_logloss: 0.49025\n",
      "[473]\ttraining's binary_logloss: 0.490111\n",
      "[474]\ttraining's binary_logloss: 0.490028\n",
      "[475]\ttraining's binary_logloss: 0.489879\n",
      "[476]\ttraining's binary_logloss: 0.489743\n",
      "[477]\ttraining's binary_logloss: 0.489652\n",
      "[478]\ttraining's binary_logloss: 0.489449\n",
      "[479]\ttraining's binary_logloss: 0.489248\n",
      "[480]\ttraining's binary_logloss: 0.489098\n",
      "[481]\ttraining's binary_logloss: 0.488954\n",
      "[482]\ttraining's binary_logloss: 0.48875\n",
      "[483]\ttraining's binary_logloss: 0.488541\n",
      "[484]\ttraining's binary_logloss: 0.488423\n",
      "[485]\ttraining's binary_logloss: 0.488296\n",
      "[486]\ttraining's binary_logloss: 0.488137\n",
      "[487]\ttraining's binary_logloss: 0.487985\n",
      "[488]\ttraining's binary_logloss: 0.487906\n",
      "[489]\ttraining's binary_logloss: 0.487767\n",
      "[490]\ttraining's binary_logloss: 0.487648\n",
      "[491]\ttraining's binary_logloss: 0.487574\n",
      "[492]\ttraining's binary_logloss: 0.487436\n",
      "[493]\ttraining's binary_logloss: 0.487338\n",
      "[494]\ttraining's binary_logloss: 0.48721\n",
      "[495]\ttraining's binary_logloss: 0.487064\n",
      "[496]\ttraining's binary_logloss: 0.486942\n",
      "[497]\ttraining's binary_logloss: 0.486792\n",
      "[498]\ttraining's binary_logloss: 0.486698\n",
      "[499]\ttraining's binary_logloss: 0.48659\n",
      "[500]\ttraining's binary_logloss: 0.486445\n",
      "[501]\ttraining's binary_logloss: 0.486385\n",
      "[502]\ttraining's binary_logloss: 0.486237\n",
      "[503]\ttraining's binary_logloss: 0.486115\n",
      "[504]\ttraining's binary_logloss: 0.485927\n",
      "[505]\ttraining's binary_logloss: 0.485731\n",
      "[506]\ttraining's binary_logloss: 0.485578\n",
      "[507]\ttraining's binary_logloss: 0.485432\n",
      "[508]\ttraining's binary_logloss: 0.485369\n",
      "[509]\ttraining's binary_logloss: 0.485264\n",
      "[510]\ttraining's binary_logloss: 0.485204\n",
      "[511]\ttraining's binary_logloss: 0.485053\n",
      "[512]\ttraining's binary_logloss: 0.484837\n",
      "[513]\ttraining's binary_logloss: 0.484697\n",
      "[514]\ttraining's binary_logloss: 0.484614\n",
      "[515]\ttraining's binary_logloss: 0.484471\n",
      "[516]\ttraining's binary_logloss: 0.484355\n",
      "[517]\ttraining's binary_logloss: 0.484278\n",
      "[518]\ttraining's binary_logloss: 0.484161\n",
      "[519]\ttraining's binary_logloss: 0.484017\n",
      "[520]\ttraining's binary_logloss: 0.483871\n",
      "[521]\ttraining's binary_logloss: 0.483718\n",
      "[522]\ttraining's binary_logloss: 0.483561\n",
      "[523]\ttraining's binary_logloss: 0.483437\n",
      "[524]\ttraining's binary_logloss: 0.48327\n",
      "[525]\ttraining's binary_logloss: 0.483139\n",
      "[526]\ttraining's binary_logloss: 0.483041\n",
      "[527]\ttraining's binary_logloss: 0.482976\n",
      "[528]\ttraining's binary_logloss: 0.482896\n",
      "[529]\ttraining's binary_logloss: 0.482774\n",
      "[530]\ttraining's binary_logloss: 0.482655\n",
      "[531]\ttraining's binary_logloss: 0.482452\n",
      "[532]\ttraining's binary_logloss: 0.482208\n",
      "[533]\ttraining's binary_logloss: 0.482015\n",
      "[534]\ttraining's binary_logloss: 0.481875\n",
      "[535]\ttraining's binary_logloss: 0.48182\n",
      "[536]\ttraining's binary_logloss: 0.481705\n",
      "[537]\ttraining's binary_logloss: 0.481619\n",
      "[538]\ttraining's binary_logloss: 0.481566\n",
      "[539]\ttraining's binary_logloss: 0.481444\n",
      "[540]\ttraining's binary_logloss: 0.481317\n",
      "[541]\ttraining's binary_logloss: 0.481087\n",
      "[542]\ttraining's binary_logloss: 0.4809\n",
      "[543]\ttraining's binary_logloss: 0.480727\n",
      "[544]\ttraining's binary_logloss: 0.48066\n",
      "[545]\ttraining's binary_logloss: 0.480512\n",
      "[546]\ttraining's binary_logloss: 0.480356\n",
      "[547]\ttraining's binary_logloss: 0.480271\n",
      "[548]\ttraining's binary_logloss: 0.480147\n",
      "[549]\ttraining's binary_logloss: 0.480047\n",
      "[550]\ttraining's binary_logloss: 0.479899\n",
      "[551]\ttraining's binary_logloss: 0.47976\n",
      "[552]\ttraining's binary_logloss: 0.479687\n",
      "[553]\ttraining's binary_logloss: 0.479503\n",
      "[554]\ttraining's binary_logloss: 0.479271\n",
      "[555]\ttraining's binary_logloss: 0.47911\n",
      "[556]\ttraining's binary_logloss: 0.478939\n",
      "[557]\ttraining's binary_logloss: 0.478815\n",
      "[558]\ttraining's binary_logloss: 0.478681\n",
      "[559]\ttraining's binary_logloss: 0.478574\n",
      "[560]\ttraining's binary_logloss: 0.478449\n",
      "[561]\ttraining's binary_logloss: 0.478283\n",
      "[562]\ttraining's binary_logloss: 0.478128\n",
      "[563]\ttraining's binary_logloss: 0.477999\n",
      "[564]\ttraining's binary_logloss: 0.477863\n",
      "[565]\ttraining's binary_logloss: 0.477737\n",
      "[566]\ttraining's binary_logloss: 0.477555\n",
      "[567]\ttraining's binary_logloss: 0.477435\n",
      "[568]\ttraining's binary_logloss: 0.477335\n",
      "[569]\ttraining's binary_logloss: 0.477182\n",
      "[570]\ttraining's binary_logloss: 0.476987\n",
      "[571]\ttraining's binary_logloss: 0.476769\n",
      "[572]\ttraining's binary_logloss: 0.47669\n",
      "[573]\ttraining's binary_logloss: 0.476541\n",
      "[574]\ttraining's binary_logloss: 0.476399\n",
      "[575]\ttraining's binary_logloss: 0.476295\n",
      "[576]\ttraining's binary_logloss: 0.476219\n",
      "[577]\ttraining's binary_logloss: 0.476067\n",
      "[578]\ttraining's binary_logloss: 0.475861\n",
      "[579]\ttraining's binary_logloss: 0.475688\n",
      "[580]\ttraining's binary_logloss: 0.475621\n",
      "[581]\ttraining's binary_logloss: 0.47549\n",
      "[582]\ttraining's binary_logloss: 0.475425\n",
      "[583]\ttraining's binary_logloss: 0.475299\n",
      "[584]\ttraining's binary_logloss: 0.475218\n",
      "[585]\ttraining's binary_logloss: 0.475103\n",
      "[586]\ttraining's binary_logloss: 0.475023\n",
      "[587]\ttraining's binary_logloss: 0.474877\n",
      "[588]\ttraining's binary_logloss: 0.474806\n",
      "[589]\ttraining's binary_logloss: 0.474642\n",
      "[590]\ttraining's binary_logloss: 0.47448\n",
      "[591]\ttraining's binary_logloss: 0.474361\n",
      "[592]\ttraining's binary_logloss: 0.4742\n",
      "[593]\ttraining's binary_logloss: 0.474037\n",
      "[594]\ttraining's binary_logloss: 0.473923\n",
      "[595]\ttraining's binary_logloss: 0.47379\n",
      "[596]\ttraining's binary_logloss: 0.473665\n",
      "[597]\ttraining's binary_logloss: 0.473546\n",
      "[598]\ttraining's binary_logloss: 0.473405\n",
      "[599]\ttraining's binary_logloss: 0.473277\n",
      "[600]\ttraining's binary_logloss: 0.473123\n",
      "[1]\ttraining's binary_logloss: 0.62902\n",
      "[2]\ttraining's binary_logloss: 0.6234\n",
      "[3]\ttraining's binary_logloss: 0.618577\n",
      "[4]\ttraining's binary_logloss: 0.614496\n",
      "[5]\ttraining's binary_logloss: 0.611207\n",
      "[6]\ttraining's binary_logloss: 0.60842\n",
      "[7]\ttraining's binary_logloss: 0.605822\n",
      "[8]\ttraining's binary_logloss: 0.60356\n",
      "[9]\ttraining's binary_logloss: 0.601606\n",
      "[10]\ttraining's binary_logloss: 0.599688\n",
      "[11]\ttraining's binary_logloss: 0.597967\n",
      "[12]\ttraining's binary_logloss: 0.596567\n",
      "[13]\ttraining's binary_logloss: 0.595091\n",
      "[14]\ttraining's binary_logloss: 0.593747\n",
      "[15]\ttraining's binary_logloss: 0.592669\n",
      "[16]\ttraining's binary_logloss: 0.591589\n",
      "[17]\ttraining's binary_logloss: 0.590556\n",
      "[18]\ttraining's binary_logloss: 0.589746\n",
      "[19]\ttraining's binary_logloss: 0.588872\n",
      "[20]\ttraining's binary_logloss: 0.588017\n",
      "[21]\ttraining's binary_logloss: 0.587175\n",
      "[22]\ttraining's binary_logloss: 0.586436\n",
      "[23]\ttraining's binary_logloss: 0.585739\n",
      "[24]\ttraining's binary_logloss: 0.584979\n",
      "[25]\ttraining's binary_logloss: 0.584267\n",
      "[26]\ttraining's binary_logloss: 0.583645\n",
      "[27]\ttraining's binary_logloss: 0.583049\n",
      "[28]\ttraining's binary_logloss: 0.582465\n",
      "[29]\ttraining's binary_logloss: 0.581815\n",
      "[30]\ttraining's binary_logloss: 0.581266\n",
      "[31]\ttraining's binary_logloss: 0.580749\n",
      "[32]\ttraining's binary_logloss: 0.580222\n",
      "[33]\ttraining's binary_logloss: 0.579616\n",
      "[34]\ttraining's binary_logloss: 0.579029\n",
      "[35]\ttraining's binary_logloss: 0.578498\n",
      "[36]\ttraining's binary_logloss: 0.578025\n",
      "[37]\ttraining's binary_logloss: 0.57757\n",
      "[38]\ttraining's binary_logloss: 0.577018\n",
      "[39]\ttraining's binary_logloss: 0.57663\n",
      "[40]\ttraining's binary_logloss: 0.576198\n",
      "[41]\ttraining's binary_logloss: 0.575726\n",
      "[42]\ttraining's binary_logloss: 0.575231\n",
      "[43]\ttraining's binary_logloss: 0.574813\n",
      "[44]\ttraining's binary_logloss: 0.574326\n",
      "[45]\ttraining's binary_logloss: 0.573923\n",
      "[46]\ttraining's binary_logloss: 0.573571\n",
      "[47]\ttraining's binary_logloss: 0.573124\n",
      "[48]\ttraining's binary_logloss: 0.572715\n",
      "[49]\ttraining's binary_logloss: 0.572362\n",
      "[50]\ttraining's binary_logloss: 0.571973\n",
      "[51]\ttraining's binary_logloss: 0.571657\n",
      "[52]\ttraining's binary_logloss: 0.571248\n",
      "[53]\ttraining's binary_logloss: 0.570899\n",
      "[54]\ttraining's binary_logloss: 0.570596\n",
      "[55]\ttraining's binary_logloss: 0.570286\n",
      "[56]\ttraining's binary_logloss: 0.569993\n",
      "[57]\ttraining's binary_logloss: 0.569618\n",
      "[58]\ttraining's binary_logloss: 0.569217\n",
      "[59]\ttraining's binary_logloss: 0.568946\n",
      "[60]\ttraining's binary_logloss: 0.568611\n",
      "[61]\ttraining's binary_logloss: 0.568327\n",
      "[62]\ttraining's binary_logloss: 0.567982\n",
      "[63]\ttraining's binary_logloss: 0.567609\n",
      "[64]\ttraining's binary_logloss: 0.567316\n",
      "[65]\ttraining's binary_logloss: 0.567069\n",
      "[66]\ttraining's binary_logloss: 0.566753\n",
      "[67]\ttraining's binary_logloss: 0.566477\n",
      "[68]\ttraining's binary_logloss: 0.566274\n",
      "[69]\ttraining's binary_logloss: 0.56599\n",
      "[70]\ttraining's binary_logloss: 0.565699\n",
      "[71]\ttraining's binary_logloss: 0.565355\n",
      "[72]\ttraining's binary_logloss: 0.565077\n",
      "[73]\ttraining's binary_logloss: 0.564866\n",
      "[74]\ttraining's binary_logloss: 0.564603\n",
      "[75]\ttraining's binary_logloss: 0.564309\n",
      "[76]\ttraining's binary_logloss: 0.564114\n",
      "[77]\ttraining's binary_logloss: 0.56374\n",
      "[78]\ttraining's binary_logloss: 0.563463\n",
      "[79]\ttraining's binary_logloss: 0.563196\n",
      "[80]\ttraining's binary_logloss: 0.562946\n",
      "[81]\ttraining's binary_logloss: 0.562724\n",
      "[82]\ttraining's binary_logloss: 0.562472\n",
      "[83]\ttraining's binary_logloss: 0.562188\n",
      "[84]\ttraining's binary_logloss: 0.562035\n",
      "[85]\ttraining's binary_logloss: 0.561802\n",
      "[86]\ttraining's binary_logloss: 0.561515\n",
      "[87]\ttraining's binary_logloss: 0.561322\n",
      "[88]\ttraining's binary_logloss: 0.561039\n",
      "[89]\ttraining's binary_logloss: 0.560828\n",
      "[90]\ttraining's binary_logloss: 0.560637\n",
      "[91]\ttraining's binary_logloss: 0.560489\n",
      "[92]\ttraining's binary_logloss: 0.560295\n",
      "[93]\ttraining's binary_logloss: 0.560094\n",
      "[94]\ttraining's binary_logloss: 0.559781\n",
      "[95]\ttraining's binary_logloss: 0.559491\n",
      "[96]\ttraining's binary_logloss: 0.559243\n",
      "[97]\ttraining's binary_logloss: 0.559034\n",
      "[98]\ttraining's binary_logloss: 0.558807\n",
      "[99]\ttraining's binary_logloss: 0.558456\n",
      "[100]\ttraining's binary_logloss: 0.558107\n",
      "[101]\ttraining's binary_logloss: 0.55792\n",
      "[102]\ttraining's binary_logloss: 0.557615\n",
      "[103]\ttraining's binary_logloss: 0.557366\n",
      "[104]\ttraining's binary_logloss: 0.55706\n",
      "[105]\ttraining's binary_logloss: 0.556762\n",
      "[106]\ttraining's binary_logloss: 0.556548\n",
      "[107]\ttraining's binary_logloss: 0.55636\n",
      "[108]\ttraining's binary_logloss: 0.556117\n",
      "[109]\ttraining's binary_logloss: 0.555889\n",
      "[110]\ttraining's binary_logloss: 0.555599\n",
      "[111]\ttraining's binary_logloss: 0.555453\n",
      "[112]\ttraining's binary_logloss: 0.555264\n",
      "[113]\ttraining's binary_logloss: 0.555055\n",
      "[114]\ttraining's binary_logloss: 0.554874\n",
      "[115]\ttraining's binary_logloss: 0.554548\n",
      "[116]\ttraining's binary_logloss: 0.554312\n",
      "[117]\ttraining's binary_logloss: 0.554038\n",
      "[118]\ttraining's binary_logloss: 0.553781\n",
      "[119]\ttraining's binary_logloss: 0.553576\n",
      "[120]\ttraining's binary_logloss: 0.553368\n",
      "[121]\ttraining's binary_logloss: 0.553219\n",
      "[122]\ttraining's binary_logloss: 0.553059\n",
      "[123]\ttraining's binary_logloss: 0.552736\n",
      "[124]\ttraining's binary_logloss: 0.552407\n",
      "[125]\ttraining's binary_logloss: 0.552057\n",
      "[126]\ttraining's binary_logloss: 0.551885\n",
      "[127]\ttraining's binary_logloss: 0.551618\n",
      "[128]\ttraining's binary_logloss: 0.551394\n",
      "[129]\ttraining's binary_logloss: 0.551128\n",
      "[130]\ttraining's binary_logloss: 0.550991\n",
      "[131]\ttraining's binary_logloss: 0.550749\n",
      "[132]\ttraining's binary_logloss: 0.550588\n",
      "[133]\ttraining's binary_logloss: 0.550433\n",
      "[134]\ttraining's binary_logloss: 0.55016\n",
      "[135]\ttraining's binary_logloss: 0.549909\n",
      "[136]\ttraining's binary_logloss: 0.549702\n",
      "[137]\ttraining's binary_logloss: 0.549511\n",
      "[138]\ttraining's binary_logloss: 0.549267\n",
      "[139]\ttraining's binary_logloss: 0.54899\n",
      "[140]\ttraining's binary_logloss: 0.54884\n",
      "[141]\ttraining's binary_logloss: 0.548576\n",
      "[142]\ttraining's binary_logloss: 0.548332\n",
      "[143]\ttraining's binary_logloss: 0.548132\n",
      "[144]\ttraining's binary_logloss: 0.547832\n",
      "[145]\ttraining's binary_logloss: 0.547618\n",
      "[146]\ttraining's binary_logloss: 0.547414\n",
      "[147]\ttraining's binary_logloss: 0.547258\n",
      "[148]\ttraining's binary_logloss: 0.547043\n",
      "[149]\ttraining's binary_logloss: 0.546848\n",
      "[150]\ttraining's binary_logloss: 0.546669\n",
      "[151]\ttraining's binary_logloss: 0.546543\n",
      "[152]\ttraining's binary_logloss: 0.546398\n",
      "[153]\ttraining's binary_logloss: 0.546194\n",
      "[154]\ttraining's binary_logloss: 0.546021\n",
      "[155]\ttraining's binary_logloss: 0.545793\n",
      "[156]\ttraining's binary_logloss: 0.54565\n",
      "[157]\ttraining's binary_logloss: 0.545465\n",
      "[158]\ttraining's binary_logloss: 0.545259\n",
      "[159]\ttraining's binary_logloss: 0.54505\n",
      "[160]\ttraining's binary_logloss: 0.544863\n",
      "[161]\ttraining's binary_logloss: 0.544582\n",
      "[162]\ttraining's binary_logloss: 0.544371\n",
      "[163]\ttraining's binary_logloss: 0.544225\n",
      "[164]\ttraining's binary_logloss: 0.544002\n",
      "[165]\ttraining's binary_logloss: 0.543868\n",
      "[166]\ttraining's binary_logloss: 0.543608\n",
      "[167]\ttraining's binary_logloss: 0.543415\n",
      "[168]\ttraining's binary_logloss: 0.543287\n",
      "[169]\ttraining's binary_logloss: 0.543092\n",
      "[170]\ttraining's binary_logloss: 0.542949\n",
      "[171]\ttraining's binary_logloss: 0.542765\n",
      "[172]\ttraining's binary_logloss: 0.54256\n",
      "[173]\ttraining's binary_logloss: 0.5423\n",
      "[174]\ttraining's binary_logloss: 0.542097\n",
      "[175]\ttraining's binary_logloss: 0.541871\n",
      "[176]\ttraining's binary_logloss: 0.541625\n",
      "[177]\ttraining's binary_logloss: 0.541456\n",
      "[178]\ttraining's binary_logloss: 0.541275\n",
      "[179]\ttraining's binary_logloss: 0.541133\n",
      "[180]\ttraining's binary_logloss: 0.540857\n",
      "[181]\ttraining's binary_logloss: 0.540649\n",
      "[182]\ttraining's binary_logloss: 0.540477\n",
      "[183]\ttraining's binary_logloss: 0.540284\n",
      "[184]\ttraining's binary_logloss: 0.540031\n",
      "[185]\ttraining's binary_logloss: 0.53987\n",
      "[186]\ttraining's binary_logloss: 0.539659\n",
      "[187]\ttraining's binary_logloss: 0.539432\n",
      "[188]\ttraining's binary_logloss: 0.539272\n",
      "[189]\ttraining's binary_logloss: 0.539083\n",
      "[190]\ttraining's binary_logloss: 0.538897\n",
      "[191]\ttraining's binary_logloss: 0.538785\n",
      "[192]\ttraining's binary_logloss: 0.538549\n",
      "[193]\ttraining's binary_logloss: 0.538331\n",
      "[194]\ttraining's binary_logloss: 0.538231\n",
      "[195]\ttraining's binary_logloss: 0.537988\n",
      "[196]\ttraining's binary_logloss: 0.537876\n",
      "[197]\ttraining's binary_logloss: 0.537685\n",
      "[198]\ttraining's binary_logloss: 0.537563\n",
      "[199]\ttraining's binary_logloss: 0.537409\n",
      "[200]\ttraining's binary_logloss: 0.53724\n",
      "[201]\ttraining's binary_logloss: 0.537066\n",
      "[202]\ttraining's binary_logloss: 0.536855\n",
      "[203]\ttraining's binary_logloss: 0.536688\n",
      "[204]\ttraining's binary_logloss: 0.536488\n",
      "[205]\ttraining's binary_logloss: 0.536288\n",
      "[206]\ttraining's binary_logloss: 0.536194\n",
      "[207]\ttraining's binary_logloss: 0.535991\n",
      "[208]\ttraining's binary_logloss: 0.535832\n",
      "[209]\ttraining's binary_logloss: 0.535643\n",
      "[210]\ttraining's binary_logloss: 0.535389\n",
      "[211]\ttraining's binary_logloss: 0.535207\n",
      "[212]\ttraining's binary_logloss: 0.534977\n",
      "[213]\ttraining's binary_logloss: 0.534787\n",
      "[214]\ttraining's binary_logloss: 0.534595\n",
      "[215]\ttraining's binary_logloss: 0.534499\n",
      "[216]\ttraining's binary_logloss: 0.534405\n",
      "[217]\ttraining's binary_logloss: 0.534291\n",
      "[218]\ttraining's binary_logloss: 0.534097\n",
      "[219]\ttraining's binary_logloss: 0.534004\n",
      "[220]\ttraining's binary_logloss: 0.533878\n",
      "[221]\ttraining's binary_logloss: 0.533719\n",
      "[222]\ttraining's binary_logloss: 0.5336\n",
      "[223]\ttraining's binary_logloss: 0.53347\n",
      "[224]\ttraining's binary_logloss: 0.533379\n",
      "[225]\ttraining's binary_logloss: 0.533244\n",
      "[226]\ttraining's binary_logloss: 0.533117\n",
      "[227]\ttraining's binary_logloss: 0.532926\n",
      "[228]\ttraining's binary_logloss: 0.532736\n",
      "[229]\ttraining's binary_logloss: 0.532503\n",
      "[230]\ttraining's binary_logloss: 0.532413\n",
      "[231]\ttraining's binary_logloss: 0.532278\n",
      "[232]\ttraining's binary_logloss: 0.532116\n",
      "[233]\ttraining's binary_logloss: 0.531987\n",
      "[234]\ttraining's binary_logloss: 0.531779\n",
      "[235]\ttraining's binary_logloss: 0.531552\n",
      "[236]\ttraining's binary_logloss: 0.531346\n",
      "[237]\ttraining's binary_logloss: 0.531243\n",
      "[238]\ttraining's binary_logloss: 0.531032\n",
      "[239]\ttraining's binary_logloss: 0.53088\n",
      "[240]\ttraining's binary_logloss: 0.530762\n",
      "[241]\ttraining's binary_logloss: 0.530514\n",
      "[242]\ttraining's binary_logloss: 0.530389\n",
      "[243]\ttraining's binary_logloss: 0.530193\n",
      "[244]\ttraining's binary_logloss: 0.529989\n",
      "[245]\ttraining's binary_logloss: 0.529795\n",
      "[246]\ttraining's binary_logloss: 0.52961\n",
      "[247]\ttraining's binary_logloss: 0.5294\n",
      "[248]\ttraining's binary_logloss: 0.52912\n",
      "[249]\ttraining's binary_logloss: 0.529011\n",
      "[250]\ttraining's binary_logloss: 0.528856\n",
      "[251]\ttraining's binary_logloss: 0.528667\n",
      "[252]\ttraining's binary_logloss: 0.528479\n",
      "[253]\ttraining's binary_logloss: 0.528303\n",
      "[254]\ttraining's binary_logloss: 0.528221\n",
      "[255]\ttraining's binary_logloss: 0.528077\n",
      "[256]\ttraining's binary_logloss: 0.527853\n",
      "[257]\ttraining's binary_logloss: 0.527786\n",
      "[258]\ttraining's binary_logloss: 0.527586\n",
      "[259]\ttraining's binary_logloss: 0.527443\n",
      "[260]\ttraining's binary_logloss: 0.527319\n",
      "[261]\ttraining's binary_logloss: 0.527069\n",
      "[262]\ttraining's binary_logloss: 0.526921\n",
      "[263]\ttraining's binary_logloss: 0.526808\n",
      "[264]\ttraining's binary_logloss: 0.526669\n",
      "[265]\ttraining's binary_logloss: 0.526466\n",
      "[266]\ttraining's binary_logloss: 0.526302\n",
      "[267]\ttraining's binary_logloss: 0.526144\n",
      "[268]\ttraining's binary_logloss: 0.526034\n",
      "[269]\ttraining's binary_logloss: 0.525855\n",
      "[270]\ttraining's binary_logloss: 0.525671\n",
      "[271]\ttraining's binary_logloss: 0.525482\n",
      "[272]\ttraining's binary_logloss: 0.525298\n",
      "[273]\ttraining's binary_logloss: 0.525117\n",
      "[274]\ttraining's binary_logloss: 0.524943\n",
      "[275]\ttraining's binary_logloss: 0.524761\n",
      "[276]\ttraining's binary_logloss: 0.524673\n",
      "[277]\ttraining's binary_logloss: 0.524503\n",
      "[278]\ttraining's binary_logloss: 0.524265\n",
      "[279]\ttraining's binary_logloss: 0.524057\n",
      "[280]\ttraining's binary_logloss: 0.523914\n",
      "[281]\ttraining's binary_logloss: 0.523791\n",
      "[282]\ttraining's binary_logloss: 0.523644\n",
      "[283]\ttraining's binary_logloss: 0.523525\n",
      "[284]\ttraining's binary_logloss: 0.523371\n",
      "[285]\ttraining's binary_logloss: 0.523202\n",
      "[286]\ttraining's binary_logloss: 0.523083\n",
      "[287]\ttraining's binary_logloss: 0.522977\n",
      "[288]\ttraining's binary_logloss: 0.522871\n",
      "[289]\ttraining's binary_logloss: 0.522763\n",
      "[290]\ttraining's binary_logloss: 0.522564\n",
      "[291]\ttraining's binary_logloss: 0.522355\n",
      "[292]\ttraining's binary_logloss: 0.52221\n",
      "[293]\ttraining's binary_logloss: 0.522072\n",
      "[294]\ttraining's binary_logloss: 0.521901\n",
      "[295]\ttraining's binary_logloss: 0.521751\n",
      "[296]\ttraining's binary_logloss: 0.521576\n",
      "[297]\ttraining's binary_logloss: 0.521401\n",
      "[298]\ttraining's binary_logloss: 0.521221\n",
      "[299]\ttraining's binary_logloss: 0.521095\n",
      "[300]\ttraining's binary_logloss: 0.520937\n",
      "[301]\ttraining's binary_logloss: 0.520784\n",
      "[302]\ttraining's binary_logloss: 0.520655\n",
      "[303]\ttraining's binary_logloss: 0.520557\n",
      "[304]\ttraining's binary_logloss: 0.520481\n",
      "[305]\ttraining's binary_logloss: 0.520412\n",
      "[306]\ttraining's binary_logloss: 0.520227\n",
      "[307]\ttraining's binary_logloss: 0.520058\n",
      "[308]\ttraining's binary_logloss: 0.519934\n",
      "[309]\ttraining's binary_logloss: 0.519835\n",
      "[310]\ttraining's binary_logloss: 0.519718\n",
      "[311]\ttraining's binary_logloss: 0.519525\n",
      "[312]\ttraining's binary_logloss: 0.519229\n",
      "[313]\ttraining's binary_logloss: 0.519013\n",
      "[314]\ttraining's binary_logloss: 0.5189\n",
      "[315]\ttraining's binary_logloss: 0.518793\n",
      "[316]\ttraining's binary_logloss: 0.518626\n",
      "[317]\ttraining's binary_logloss: 0.518445\n",
      "[318]\ttraining's binary_logloss: 0.518348\n",
      "[319]\ttraining's binary_logloss: 0.518174\n",
      "[320]\ttraining's binary_logloss: 0.517933\n",
      "[321]\ttraining's binary_logloss: 0.517862\n",
      "[322]\ttraining's binary_logloss: 0.51773\n",
      "[323]\ttraining's binary_logloss: 0.517529\n",
      "[324]\ttraining's binary_logloss: 0.517377\n",
      "[325]\ttraining's binary_logloss: 0.51728\n",
      "[326]\ttraining's binary_logloss: 0.51709\n",
      "[327]\ttraining's binary_logloss: 0.516934\n",
      "[328]\ttraining's binary_logloss: 0.516854\n",
      "[329]\ttraining's binary_logloss: 0.51672\n",
      "[330]\ttraining's binary_logloss: 0.51658\n",
      "[331]\ttraining's binary_logloss: 0.516441\n",
      "[332]\ttraining's binary_logloss: 0.516308\n",
      "[333]\ttraining's binary_logloss: 0.516223\n",
      "[334]\ttraining's binary_logloss: 0.516134\n",
      "[335]\ttraining's binary_logloss: 0.515982\n",
      "[336]\ttraining's binary_logloss: 0.515836\n",
      "[337]\ttraining's binary_logloss: 0.515669\n",
      "[338]\ttraining's binary_logloss: 0.51553\n",
      "[339]\ttraining's binary_logloss: 0.515406\n",
      "[340]\ttraining's binary_logloss: 0.515258\n",
      "[341]\ttraining's binary_logloss: 0.51515\n",
      "[342]\ttraining's binary_logloss: 0.514993\n",
      "[343]\ttraining's binary_logloss: 0.514847\n",
      "[344]\ttraining's binary_logloss: 0.514781\n",
      "[345]\ttraining's binary_logloss: 0.514686\n",
      "[346]\ttraining's binary_logloss: 0.514527\n",
      "[347]\ttraining's binary_logloss: 0.514412\n",
      "[348]\ttraining's binary_logloss: 0.514179\n",
      "[349]\ttraining's binary_logloss: 0.514028\n",
      "[350]\ttraining's binary_logloss: 0.513931\n",
      "[351]\ttraining's binary_logloss: 0.513786\n",
      "[352]\ttraining's binary_logloss: 0.513621\n",
      "[353]\ttraining's binary_logloss: 0.513488\n",
      "[354]\ttraining's binary_logloss: 0.513327\n",
      "[355]\ttraining's binary_logloss: 0.513201\n",
      "[356]\ttraining's binary_logloss: 0.513108\n",
      "[357]\ttraining's binary_logloss: 0.512865\n",
      "[358]\ttraining's binary_logloss: 0.512746\n",
      "[359]\ttraining's binary_logloss: 0.512634\n",
      "[360]\ttraining's binary_logloss: 0.512495\n",
      "[361]\ttraining's binary_logloss: 0.512328\n",
      "[362]\ttraining's binary_logloss: 0.512158\n",
      "[363]\ttraining's binary_logloss: 0.51206\n",
      "[364]\ttraining's binary_logloss: 0.511903\n",
      "[365]\ttraining's binary_logloss: 0.51179\n",
      "[366]\ttraining's binary_logloss: 0.511678\n",
      "[367]\ttraining's binary_logloss: 0.51156\n",
      "[368]\ttraining's binary_logloss: 0.511395\n",
      "[369]\ttraining's binary_logloss: 0.511238\n",
      "[370]\ttraining's binary_logloss: 0.511114\n",
      "[371]\ttraining's binary_logloss: 0.510987\n",
      "[372]\ttraining's binary_logloss: 0.510908\n",
      "[373]\ttraining's binary_logloss: 0.510835\n",
      "[374]\ttraining's binary_logloss: 0.510694\n",
      "[375]\ttraining's binary_logloss: 0.510543\n",
      "[376]\ttraining's binary_logloss: 0.510425\n",
      "[377]\ttraining's binary_logloss: 0.510277\n",
      "[378]\ttraining's binary_logloss: 0.510152\n",
      "[379]\ttraining's binary_logloss: 0.510013\n",
      "[380]\ttraining's binary_logloss: 0.509897\n",
      "[381]\ttraining's binary_logloss: 0.509793\n",
      "[382]\ttraining's binary_logloss: 0.509642\n",
      "[383]\ttraining's binary_logloss: 0.509524\n",
      "[384]\ttraining's binary_logloss: 0.509392\n",
      "[385]\ttraining's binary_logloss: 0.509279\n",
      "[386]\ttraining's binary_logloss: 0.509159\n",
      "[387]\ttraining's binary_logloss: 0.508986\n",
      "[388]\ttraining's binary_logloss: 0.508871\n",
      "[389]\ttraining's binary_logloss: 0.508686\n",
      "[390]\ttraining's binary_logloss: 0.508488\n",
      "[391]\ttraining's binary_logloss: 0.508367\n",
      "[392]\ttraining's binary_logloss: 0.508187\n",
      "[393]\ttraining's binary_logloss: 0.50805\n",
      "[394]\ttraining's binary_logloss: 0.507983\n",
      "[395]\ttraining's binary_logloss: 0.507867\n",
      "[396]\ttraining's binary_logloss: 0.507762\n",
      "[397]\ttraining's binary_logloss: 0.50756\n",
      "[398]\ttraining's binary_logloss: 0.50745\n",
      "[399]\ttraining's binary_logloss: 0.5073\n",
      "[400]\ttraining's binary_logloss: 0.507134\n",
      "[401]\ttraining's binary_logloss: 0.507007\n",
      "[402]\ttraining's binary_logloss: 0.506868\n",
      "[403]\ttraining's binary_logloss: 0.506757\n",
      "[404]\ttraining's binary_logloss: 0.506563\n",
      "[405]\ttraining's binary_logloss: 0.506447\n",
      "[406]\ttraining's binary_logloss: 0.506314\n",
      "[407]\ttraining's binary_logloss: 0.506249\n",
      "[408]\ttraining's binary_logloss: 0.506142\n",
      "[409]\ttraining's binary_logloss: 0.505933\n",
      "[410]\ttraining's binary_logloss: 0.505864\n",
      "[411]\ttraining's binary_logloss: 0.505722\n",
      "[412]\ttraining's binary_logloss: 0.50553\n",
      "[413]\ttraining's binary_logloss: 0.505417\n",
      "[414]\ttraining's binary_logloss: 0.505301\n",
      "[415]\ttraining's binary_logloss: 0.505142\n",
      "[416]\ttraining's binary_logloss: 0.504981\n",
      "[417]\ttraining's binary_logloss: 0.504905\n",
      "[418]\ttraining's binary_logloss: 0.504745\n",
      "[419]\ttraining's binary_logloss: 0.504637\n",
      "[420]\ttraining's binary_logloss: 0.504562\n",
      "[421]\ttraining's binary_logloss: 0.504445\n",
      "[422]\ttraining's binary_logloss: 0.504372\n",
      "[423]\ttraining's binary_logloss: 0.504293\n",
      "[424]\ttraining's binary_logloss: 0.504158\n",
      "[425]\ttraining's binary_logloss: 0.504059\n",
      "[426]\ttraining's binary_logloss: 0.503909\n",
      "[427]\ttraining's binary_logloss: 0.503748\n",
      "[428]\ttraining's binary_logloss: 0.503543\n",
      "[429]\ttraining's binary_logloss: 0.503356\n",
      "[430]\ttraining's binary_logloss: 0.503216\n",
      "[431]\ttraining's binary_logloss: 0.503093\n",
      "[432]\ttraining's binary_logloss: 0.502927\n",
      "[433]\ttraining's binary_logloss: 0.502802\n",
      "[434]\ttraining's binary_logloss: 0.502673\n",
      "[435]\ttraining's binary_logloss: 0.502565\n",
      "[436]\ttraining's binary_logloss: 0.502373\n",
      "[437]\ttraining's binary_logloss: 0.502306\n",
      "[438]\ttraining's binary_logloss: 0.502191\n",
      "[439]\ttraining's binary_logloss: 0.502061\n",
      "[440]\ttraining's binary_logloss: 0.501928\n",
      "[441]\ttraining's binary_logloss: 0.501795\n",
      "[442]\ttraining's binary_logloss: 0.501647\n",
      "[443]\ttraining's binary_logloss: 0.501575\n",
      "[444]\ttraining's binary_logloss: 0.501402\n",
      "[445]\ttraining's binary_logloss: 0.501311\n",
      "[446]\ttraining's binary_logloss: 0.501197\n",
      "[447]\ttraining's binary_logloss: 0.501066\n",
      "[448]\ttraining's binary_logloss: 0.500913\n",
      "[449]\ttraining's binary_logloss: 0.500782\n",
      "[450]\ttraining's binary_logloss: 0.500614\n",
      "[451]\ttraining's binary_logloss: 0.500482\n",
      "[452]\ttraining's binary_logloss: 0.500353\n",
      "[453]\ttraining's binary_logloss: 0.500188\n",
      "[454]\ttraining's binary_logloss: 0.500018\n",
      "[455]\ttraining's binary_logloss: 0.499897\n",
      "[456]\ttraining's binary_logloss: 0.499728\n",
      "[457]\ttraining's binary_logloss: 0.49961\n",
      "[458]\ttraining's binary_logloss: 0.499476\n",
      "[459]\ttraining's binary_logloss: 0.499295\n",
      "[460]\ttraining's binary_logloss: 0.499146\n",
      "[461]\ttraining's binary_logloss: 0.499032\n",
      "[462]\ttraining's binary_logloss: 0.498827\n",
      "[463]\ttraining's binary_logloss: 0.498666\n",
      "[464]\ttraining's binary_logloss: 0.49858\n",
      "[465]\ttraining's binary_logloss: 0.498474\n",
      "[466]\ttraining's binary_logloss: 0.498352\n",
      "[467]\ttraining's binary_logloss: 0.498237\n",
      "[468]\ttraining's binary_logloss: 0.498078\n",
      "[469]\ttraining's binary_logloss: 0.498003\n",
      "[470]\ttraining's binary_logloss: 0.497932\n",
      "[471]\ttraining's binary_logloss: 0.497833\n",
      "[472]\ttraining's binary_logloss: 0.497678\n",
      "[473]\ttraining's binary_logloss: 0.497566\n",
      "[474]\ttraining's binary_logloss: 0.497468\n",
      "[475]\ttraining's binary_logloss: 0.497346\n",
      "[476]\ttraining's binary_logloss: 0.497238\n",
      "[477]\ttraining's binary_logloss: 0.497077\n",
      "[478]\ttraining's binary_logloss: 0.496935\n",
      "[479]\ttraining's binary_logloss: 0.496836\n",
      "[480]\ttraining's binary_logloss: 0.49673\n",
      "[481]\ttraining's binary_logloss: 0.496599\n",
      "[482]\ttraining's binary_logloss: 0.496546\n",
      "[483]\ttraining's binary_logloss: 0.496401\n",
      "[484]\ttraining's binary_logloss: 0.496267\n",
      "[485]\ttraining's binary_logloss: 0.496097\n",
      "[486]\ttraining's binary_logloss: 0.496039\n",
      "[487]\ttraining's binary_logloss: 0.495903\n",
      "[488]\ttraining's binary_logloss: 0.495799\n",
      "[489]\ttraining's binary_logloss: 0.495656\n",
      "[490]\ttraining's binary_logloss: 0.495517\n",
      "[491]\ttraining's binary_logloss: 0.495438\n",
      "[492]\ttraining's binary_logloss: 0.495292\n",
      "[493]\ttraining's binary_logloss: 0.495179\n",
      "[494]\ttraining's binary_logloss: 0.49507\n",
      "[495]\ttraining's binary_logloss: 0.494979\n",
      "[496]\ttraining's binary_logloss: 0.494858\n",
      "[497]\ttraining's binary_logloss: 0.49478\n",
      "[498]\ttraining's binary_logloss: 0.494704\n",
      "[499]\ttraining's binary_logloss: 0.494584\n",
      "[500]\ttraining's binary_logloss: 0.494476\n",
      "[501]\ttraining's binary_logloss: 0.494306\n",
      "[502]\ttraining's binary_logloss: 0.494177\n",
      "[503]\ttraining's binary_logloss: 0.494072\n",
      "[504]\ttraining's binary_logloss: 0.493959\n",
      "[505]\ttraining's binary_logloss: 0.493801\n",
      "[506]\ttraining's binary_logloss: 0.49371\n",
      "[507]\ttraining's binary_logloss: 0.493573\n",
      "[508]\ttraining's binary_logloss: 0.493465\n",
      "[509]\ttraining's binary_logloss: 0.493341\n",
      "[510]\ttraining's binary_logloss: 0.493241\n",
      "[511]\ttraining's binary_logloss: 0.493192\n",
      "[512]\ttraining's binary_logloss: 0.493097\n",
      "[513]\ttraining's binary_logloss: 0.493043\n",
      "[514]\ttraining's binary_logloss: 0.492924\n",
      "[515]\ttraining's binary_logloss: 0.492717\n",
      "[516]\ttraining's binary_logloss: 0.492651\n",
      "[517]\ttraining's binary_logloss: 0.492605\n",
      "[518]\ttraining's binary_logloss: 0.492507\n",
      "[519]\ttraining's binary_logloss: 0.492377\n",
      "[520]\ttraining's binary_logloss: 0.492269\n",
      "[521]\ttraining's binary_logloss: 0.492169\n",
      "[522]\ttraining's binary_logloss: 0.492001\n",
      "[523]\ttraining's binary_logloss: 0.491866\n",
      "[524]\ttraining's binary_logloss: 0.491747\n",
      "[525]\ttraining's binary_logloss: 0.491603\n",
      "[526]\ttraining's binary_logloss: 0.491433\n",
      "[527]\ttraining's binary_logloss: 0.491316\n",
      "[528]\ttraining's binary_logloss: 0.49118\n",
      "[529]\ttraining's binary_logloss: 0.491105\n",
      "[530]\ttraining's binary_logloss: 0.490932\n",
      "[531]\ttraining's binary_logloss: 0.490838\n",
      "[532]\ttraining's binary_logloss: 0.490646\n",
      "[533]\ttraining's binary_logloss: 0.490458\n",
      "[534]\ttraining's binary_logloss: 0.490349\n",
      "[535]\ttraining's binary_logloss: 0.490248\n",
      "[536]\ttraining's binary_logloss: 0.490073\n",
      "[537]\ttraining's binary_logloss: 0.489914\n",
      "[538]\ttraining's binary_logloss: 0.489823\n",
      "[539]\ttraining's binary_logloss: 0.489745\n",
      "[540]\ttraining's binary_logloss: 0.489606\n",
      "[541]\ttraining's binary_logloss: 0.489513\n",
      "[542]\ttraining's binary_logloss: 0.489403\n",
      "[543]\ttraining's binary_logloss: 0.489303\n",
      "[544]\ttraining's binary_logloss: 0.489153\n",
      "[545]\ttraining's binary_logloss: 0.489077\n",
      "[546]\ttraining's binary_logloss: 0.488923\n",
      "[547]\ttraining's binary_logloss: 0.488741\n",
      "[548]\ttraining's binary_logloss: 0.488655\n",
      "[549]\ttraining's binary_logloss: 0.488573\n",
      "[550]\ttraining's binary_logloss: 0.488407\n",
      "[551]\ttraining's binary_logloss: 0.488261\n",
      "[552]\ttraining's binary_logloss: 0.488129\n",
      "[553]\ttraining's binary_logloss: 0.487942\n",
      "[554]\ttraining's binary_logloss: 0.48786\n",
      "[555]\ttraining's binary_logloss: 0.487747\n",
      "[556]\ttraining's binary_logloss: 0.487647\n",
      "[557]\ttraining's binary_logloss: 0.487498\n",
      "[558]\ttraining's binary_logloss: 0.487383\n",
      "[559]\ttraining's binary_logloss: 0.487254\n",
      "[560]\ttraining's binary_logloss: 0.486981\n",
      "[561]\ttraining's binary_logloss: 0.48685\n",
      "[562]\ttraining's binary_logloss: 0.486624\n",
      "[563]\ttraining's binary_logloss: 0.486508\n",
      "[564]\ttraining's binary_logloss: 0.486365\n",
      "[565]\ttraining's binary_logloss: 0.486237\n",
      "[566]\ttraining's binary_logloss: 0.486136\n",
      "[567]\ttraining's binary_logloss: 0.485947\n",
      "[568]\ttraining's binary_logloss: 0.485824\n",
      "[569]\ttraining's binary_logloss: 0.485753\n",
      "[570]\ttraining's binary_logloss: 0.48564\n",
      "[571]\ttraining's binary_logloss: 0.485565\n",
      "[572]\ttraining's binary_logloss: 0.485465\n",
      "[573]\ttraining's binary_logloss: 0.48532\n",
      "[574]\ttraining's binary_logloss: 0.485174\n",
      "[575]\ttraining's binary_logloss: 0.485032\n",
      "[576]\ttraining's binary_logloss: 0.484962\n",
      "[577]\ttraining's binary_logloss: 0.484863\n",
      "[578]\ttraining's binary_logloss: 0.484665\n",
      "[579]\ttraining's binary_logloss: 0.4845\n",
      "[580]\ttraining's binary_logloss: 0.484392\n",
      "[581]\ttraining's binary_logloss: 0.484326\n",
      "[582]\ttraining's binary_logloss: 0.484243\n",
      "[583]\ttraining's binary_logloss: 0.484137\n",
      "[584]\ttraining's binary_logloss: 0.483989\n",
      "[585]\ttraining's binary_logloss: 0.483835\n",
      "[586]\ttraining's binary_logloss: 0.483721\n",
      "[587]\ttraining's binary_logloss: 0.483571\n",
      "[588]\ttraining's binary_logloss: 0.483473\n",
      "[589]\ttraining's binary_logloss: 0.483333\n",
      "[590]\ttraining's binary_logloss: 0.483222\n",
      "[591]\ttraining's binary_logloss: 0.483136\n",
      "[592]\ttraining's binary_logloss: 0.483011\n",
      "[593]\ttraining's binary_logloss: 0.482885\n",
      "[594]\ttraining's binary_logloss: 0.482754\n",
      "[595]\ttraining's binary_logloss: 0.482637\n",
      "[596]\ttraining's binary_logloss: 0.482559\n",
      "[597]\ttraining's binary_logloss: 0.482444\n",
      "[598]\ttraining's binary_logloss: 0.482242\n",
      "[599]\ttraining's binary_logloss: 0.482134\n",
      "[600]\ttraining's binary_logloss: 0.482029\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as LightGBM\n",
    "\n",
    "feature_set = []\n",
    "feature_set = make_feature_set(x_train_features)\n",
    "model = []\n",
    "\n",
    "## train\n",
    "i=0\n",
    "for x in feature_set :\n",
    "    lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=600,\n",
    "                               max_depth = 50,\n",
    "                               min_data_in_leaf = 50,\n",
    "                               class_weight={True: 10, False: 1},\n",
    "                               learning_rate= 0.1\n",
    "                              ) \n",
    "    evals = [(x, y_train_bool)]\n",
    "    lgbm.fit(x, y_train_bool, eval_metric='logloss', eval_set=evals)\n",
    "    joblib.dump(lgbm, './test/models/lgbm_ensembles' + str(i) + '.pkl')\n",
    "    i = i+1\n",
    "\n",
    "## prediction\n",
    "def predict_ensemble_model(x_) :\n",
    "    feature_set = make_feature_set(x_)\n",
    "    y_pred = []\n",
    "    i = 0\n",
    "    for x in feature_set :\n",
    "        pred = model[i].predict(x)\n",
    "        y_pred.append(pred)\n",
    "        i = i+1\n",
    "\n",
    "    y_pred_sum = y_pred[0] | (y_pred[1] & y_pred[2])# & y_pred[3] & y_pred[4])\n",
    "    return y_pred_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.99      0.40      0.57     63391\n",
      "        risk       0.25      0.99      0.40     12724\n",
      "\n",
      "    accuracy                           0.50     76115\n",
      "   macro avg       0.62      0.69      0.49     76115\n",
      "weighted avg       0.87      0.50      0.55     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = predict_ensemble_model(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_train_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.92      0.37      0.53     21052\n",
      "        risk       0.21      0.84      0.34      4344\n",
      "\n",
      "    accuracy                           0.45     25396\n",
      "   macro avg       0.57      0.60      0.43     25396\n",
      "weighted avg       0.80      0.45      0.50     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = predict_ensemble_model(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.37      0.52     21040\n",
      "        risk       0.21      0.82      0.34      4311\n",
      "\n",
      "    accuracy                           0.45     25351\n",
      "   macro avg       0.56      0.60      0.43     25351\n",
      "weighted avg       0.79      0.45      0.49     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = predict_ensemble_model(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_test_bool, y, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MLP Base Models\n",
    "multi-layer perceptron 기반의 classifier를 만들었고\n",
    "encoder-decoder로 데이터를 임베딩하여 거래량(시계열) 데이터 예측에 사용 할 수 있도록 하였습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-0 Data loader \n",
    "pytorch로 mlp classifier를 구현하였는데 이 때 필요한 데이터 로더를 구현하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "executionInfo": {
     "elapsed": 4280,
     "status": "ok",
     "timestamp": 1670484610585,
     "user": {
      "displayName": "전병국",
      "userId": "04716627198164050618"
     },
     "user_tz": -540
    },
    "id": "w4zZXzY6Okgy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x.iloc[idx])\n",
    "        y = torch.FloatTensor(self.y.iloc[idx])\n",
    "        return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1 Multi-Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "Ncua0eWeOaCR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class Simple_MLP_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_MLP_Net, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(22, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(32, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def embedding_output(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dRF1r8XOc-R",
    "outputId": "77b1f226-9adf-4364-c77c-e7c8ce5c32fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :   10, Cost : 0.429\n",
      "Epoch :   20, Cost : 0.425\n",
      "Epoch :   30, Cost : 0.421\n",
      "Epoch :   40, Cost : 0.417\n",
      "Epoch :   50, Cost : 0.414\n",
      "Epoch :   60, Cost : 0.410\n"
     ]
    }
   ],
   "source": [
    "y_train_int = pd.DataFrame()\n",
    "y_train_int['y'] = y_train_bool.astype(int)\n",
    "train_dataset = StockDataset(x_train_features, y_train_int)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Simple_MLP_Net().to(device)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    cost = 0.0\n",
    "    model.train()\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(train_dataloader)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
    "        torch.save(model.state_dict(), './history/mlp_net_checkpoint' + str(epoch) +  '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :  tensor(0.8290) loss :  0.43288516200552085\n"
     ]
    }
   ],
   "source": [
    "#from torcheval.metrics import BinaryAccuracy\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "\n",
    "y_train_int = pd.DataFrame()\n",
    "y_train_int['y'] = y_train_bool.astype(int)\n",
    "train_dataset = StockDataset(x_train_features, y_train_int)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "y_valid_int = pd.DataFrame()\n",
    "y_valid_int['y'] = y_valid_bool.astype(int)\n",
    "valid_dataset = StockDataset(x_valid_features, y_valid_int)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Simple_MLP_Net().to(device)\n",
    "\n",
    "PATH = './history/mlp_net_checkpoint19.pth'\n",
    "checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "\n",
    "model.eval()\n",
    "total_acc = 0\n",
    "total_loss = 0\n",
    "num_batch = 0\n",
    "for x, y in valid_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(x)\n",
    "        #pred = (outputs > 0.5).float\n",
    "        loss = criterion(outputs, y)\n",
    "        metric = BinaryAccuracy().to(device)\n",
    "        metric(outputs, y)\n",
    "        acc = metric.compute()\n",
    "        total_acc += acc\n",
    "        total_loss += loss.cpu().item()\n",
    "        num_batch = num_batch + 1\n",
    "        \n",
    "total_acc = total_acc/(num_batch) \n",
    "total_loss = total_loss/(num_batch)\n",
    "\n",
    "print(\"acc : \", total_acc, \"loss : \" , total_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-2 Encoder Decoder\n",
    "Multi-Layer Perceptron 네트워크로 재무데이터를 22 차원에서 16차원으로 줄이는 encoder와 다시 embedding을 22차원으로 늘리는 decoder를 학습켰습니다. 임베딩은 이후 시계열 데이터 예측 모델 (LSTM)에서 feature로 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1670484617333,
     "user": {
      "displayName": "전병국",
      "userId": "04716627198164050618"
     },
     "user_tz": -540
    },
    "id": "vNinSxd-U9Up"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class Encoder_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(22, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 22, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def calcEncoding(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22426,
     "status": "ok",
     "timestamp": 1670484917158,
     "user": {
      "displayName": "전병국",
      "userId": "04716627198164050618"
     },
     "user_tz": -540
    },
    "id": "PKw_dDVkYn0Z",
    "outputId": "e458b3dc-d1f9-41eb-de02-50dd6bff08ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :    1, Cost : 0.003\n"
     ]
    }
   ],
   "source": [
    "### 아래 결과는 150 epoch 후 한번 돌린 후의 결과입니다\n",
    "train_dataset = StockDataset(x_train_features, x_train_features)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Encoder_Decoder().to(device)\n",
    "\n",
    "PATH = './history/embedding_net5_150_checkpoint.pth'\n",
    "checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "\n",
    "for epoch in range(1):\n",
    "    cost = 0.0\n",
    "\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(train_dataloader)\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
    "        #torch.save(model.state_dict(), '/content/drive/MyDrive/data/embedding_net5_' + str(epoch) + '_checkpoint.pth')\n",
    "\n",
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/data/embedding_net5_100_checkpoint.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5514,
     "status": "ok",
     "timestamp": 1670485292125,
     "user": {
      "displayName": "전병국",
      "userId": "04716627198164050618"
     },
     "user_tz": -540
    },
    "id": "zWFzLGjdiG9T",
    "outputId": "ba04d695-9b53-46c9-e152-dbce6312709f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 0.0022947683464735746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_dataset = StockDataset(x_valid_features, x_valid_features)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "y_train_int = pd.DataFrame()\n",
    "y_train_int['y'] = y_train_bool.astype(int)\n",
    "train_dataset = StockDataset(x_train_features, y_train_int)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Encoder_Decoder().to(device)\n",
    "\n",
    "PATH = './history/embedding_net5_150_checkpoint.pth'\n",
    "checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "total_loss = 0\n",
    "num_batch = 0\n",
    "for x, y in valid_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        total_loss += loss\n",
    "        \n",
    "total_loss = total_loss / len(valid_dataloader)\n",
    "print(\"validation loss : \" + str(float(total_loss)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Timeseries Analysis With LSTM\n",
    "거래량 데이터를 LSTM 모델을 이용하여 시계열 분석을 하였습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 5-1. Convert data to time series and save data\n",
    "기존 데이터를 시계열 데이터로 변환 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"modified_stock_trade_volume.csv\")\n",
    "column = ['기관합계', '기타법인', '개인', '외국인합계', '전체']          # 거래량 정보에 담긴 feature들\n",
    "\n",
    "data['code'] = data['code'].apply(lambda x : \"0\" *(6-len(str(x))) + str(x))\n",
    "data.set_index('날짜', inplace=True)\n",
    "\n",
    "codes = set(data['code'])\n",
    "\n",
    "for code in codes:\n",
    "  for col in column:\n",
    "    a = pd.DataFrame()\n",
    "    for i in range(10):                                             # 데이터프레임을 shift하여 10일간의 데이터를 하나로 묶음\n",
    "      a['shift'+str(i)] = data[data['code']==code][col].shift(i)    # 각 feature마다 10일씩 shift하여 1~10일, 2~11일, ...의 데이터가 하나의 데이터프레임을 이루게 함\n",
    "    a['code'] = data[data['code']==code]['code']\n",
    "    a['y'] = data[data['code']==code]['Y']\n",
    "    a = a.dropna()                                                  # 결측치 제거\n",
    "    a.to_csv(\"lstm/\" + str(code) + str(col) + '_info.csv')          # 기업별, feature별로 10일단위 시계열 데이터에 대한 데이터프레임 저장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 5-2. Load data, convert into 3D data, and fit model\n",
    "저장된 각각의 데이터를 불러와 3차원 데이터 형성 후 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 1.7510 - acc: 0.8099 - val_loss: 1.4329 - val_acc: 0.8625\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.8411 - acc: 0.8192 - val_loss: 1.5754 - val_acc: 0.8550\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 1.4103 - acc: 0.8444 - val_loss: 1.0718 - val_acc: 0.8959\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.4459 - acc: 0.8593 - val_loss: 1.0117 - val_acc: 0.8959\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.8902 - acc: 0.8984 - val_loss: 0.4636 - val_acc: 0.9554\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8976 - acc: 0.9161 - val_loss: 0.4135 - val_acc: 0.9628\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 1.9023 - acc: 0.8518 - val_loss: 2.8752 - val_acc: 0.7732\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.8781 - acc: 0.8546 - val_loss: 2.8842 - val_acc: 0.7732\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.8876 - acc: 0.8555 - val_loss: 2.8846 - val_acc: 0.7732\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 2.2410 - acc: 0.8300 - val_loss: 3.6198 - val_acc: 0.7538\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 2.2775 - acc: 0.8300 - val_loss: 3.6232 - val_acc: 0.7538\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 2.6430 - acc: 0.8109 - val_loss: 4.2558 - val_acc: 0.7143\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 2.6372 - acc: 0.8081 - val_loss: 4.2546 - val_acc: 0.7143\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 2.6342 - acc: 0.8071 - val_loss: 4.2251 - val_acc: 0.7143\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 2.6327 - acc: 0.8090 - val_loss: 4.2139 - val_acc: 0.7143\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 2.6315 - acc: 0.8100 - val_loss: 4.2106 - val_acc: 0.7143\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 2.6315 - acc: 0.8100 - val_loss: 4.2079 - val_acc: 0.7143\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 1.6227 - acc: 0.8683 - val_loss: 3.0243 - val_acc: 0.7632\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.6526 - acc: 0.8692 - val_loss: 3.0458 - val_acc: 0.7632\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 13ms/step - loss: 3.8319 - acc: 0.7451 - val_loss: 2.8906 - val_acc: 0.8128\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.8035 - acc: 0.7451 - val_loss: 2.8928 - val_acc: 0.8082\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.6556 - acc: 0.7451 - val_loss: 2.8944 - val_acc: 0.8082\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.6063 - acc: 0.7440 - val_loss: 2.8948 - val_acc: 0.8082\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.5775 - acc: 0.7417 - val_loss: 2.7370 - val_acc: 0.8082\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 2.6632 - acc: 0.7440 - val_loss: 1.9149 - val_acc: 0.8037\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 3.0395 - acc: 0.7371 - val_loss: 1.7121 - val_acc: 0.7900\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 1.3820 - acc: 0.7083 - val_loss: 1.6241 - val_acc: 0.7063\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.3443 - acc: 0.7605 - val_loss: 1.6103 - val_acc: 0.7993\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.3345 - acc: 0.7959 - val_loss: 1.5201 - val_acc: 0.7993\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.3504 - acc: 0.8052 - val_loss: 1.5009 - val_acc: 0.8141\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 1.4094 - acc: 0.7046 - val_loss: 1.6756 - val_acc: 0.7546\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.3791 - acc: 0.7297 - val_loss: 1.4810 - val_acc: 0.7546\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.2815 - acc: 0.7213 - val_loss: 1.3299 - val_acc: 0.7435\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.0355 - acc: 0.6962 - val_loss: 0.6468 - val_acc: 0.6952\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8287 - acc: 0.6682 - val_loss: 0.6350 - val_acc: 0.7212\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7986 - acc: 0.7092 - val_loss: 0.6693 - val_acc: 0.7212\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8293 - acc: 0.7139 - val_loss: 0.6695 - val_acc: 0.7472\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.7550 - acc: 0.7556 - val_loss: 0.9201 - val_acc: 0.7873\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7451 - acc: 0.8032 - val_loss: 0.8041 - val_acc: 0.7910\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8280 - acc: 0.8349 - val_loss: 0.9600 - val_acc: 0.8209\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 1.0324 - acc: 0.8500 - val_loss: 1.0095 - val_acc: 0.8327\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.0179 - acc: 0.8444 - val_loss: 1.0059 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.0384 - acc: 0.8490 - val_loss: 1.0020 - val_acc: 0.8401\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 1.1262 - acc: 0.8136 - val_loss: 1.0243 - val_acc: 0.7993\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 1.0026 - acc: 0.7978 - val_loss: 1.0180 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.9807 - acc: 0.8062 - val_loss: 0.9700 - val_acc: 0.8067\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.9354 - acc: 0.8127 - val_loss: 0.8759 - val_acc: 0.8030\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8757 - acc: 0.8062 - val_loss: 0.8690 - val_acc: 0.8141\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8760 - acc: 0.8183 - val_loss: 0.8647 - val_acc: 0.8141\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.8166 - acc: 0.8826 - val_loss: 1.1379 - val_acc: 0.8513\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7924 - acc: 0.8956 - val_loss: 1.1628 - val_acc: 0.8513\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.8257 - acc: 0.8975 - val_loss: 1.2202 - val_acc: 0.8513\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 10ms/step - loss: 0.8858 - acc: 0.8611 - val_loss: 1.1108 - val_acc: 0.7472\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7862 - acc: 0.8500 - val_loss: 0.9413 - val_acc: 0.7509\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7430 - acc: 0.8537 - val_loss: 1.1946 - val_acc: 0.7472\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7807 - acc: 0.8602 - val_loss: 1.3828 - val_acc: 0.7509\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.6126 - acc: 0.7884 - val_loss: 0.7003 - val_acc: 0.7286\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5994 - acc: 0.7884 - val_loss: 0.6957 - val_acc: 0.7286\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5969 - acc: 0.7884 - val_loss: 0.7431 - val_acc: 0.7286\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6195 - acc: 0.7884 - val_loss: 0.7965 - val_acc: 0.7286\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5363 - acc: 0.9105 - val_loss: 0.5191 - val_acc: 0.8885\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4781 - acc: 0.9031 - val_loss: 0.5010 - val_acc: 0.8885\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4874 - acc: 0.9068 - val_loss: 0.5324 - val_acc: 0.8996\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "27/27 [==============================] - 2s 15ms/step - loss: 0.8321 - acc: 0.8084 - val_loss: 0.9285 - val_acc: 0.7857\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.7445 - acc: 0.8084 - val_loss: 0.7368 - val_acc: 0.7806\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6561 - acc: 0.8008 - val_loss: 0.7425 - val_acc: 0.7806\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6479 - acc: 0.8020 - val_loss: 0.7433 - val_acc: 0.7806\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6299 - acc: 0.8020 - val_loss: 0.7920 - val_acc: 0.7806\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6132 - acc: 0.7982 - val_loss: 0.7160 - val_acc: 0.7806\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6096 - acc: 0.8033 - val_loss: 0.7200 - val_acc: 0.7806\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6051 - acc: 0.8020 - val_loss: 0.7136 - val_acc: 0.7806\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6044 - acc: 0.8072 - val_loss: 0.7330 - val_acc: 0.7806\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.6059 - acc: 0.8084 - val_loss: 0.7158 - val_acc: 0.7806\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5940 - acc: 0.7717 - val_loss: 0.6114 - val_acc: 0.7138\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5664 - acc: 0.7679 - val_loss: 0.6597 - val_acc: 0.7138\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5501 - acc: 0.7717 - val_loss: 0.6179 - val_acc: 0.7100\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5461 - acc: 0.7735 - val_loss: 0.6171 - val_acc: 0.7100\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5441 - acc: 0.7735 - val_loss: 0.6205 - val_acc: 0.7138\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5419 - acc: 0.7735 - val_loss: 0.6193 - val_acc: 0.7138\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5405 - acc: 0.7726 - val_loss: 0.6220 - val_acc: 0.7138\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5389 - acc: 0.7726 - val_loss: 0.6196 - val_acc: 0.7138\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5391 - acc: 0.7717 - val_loss: 0.6235 - val_acc: 0.7138\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4694 - acc: 0.8463 - val_loss: 0.4224 - val_acc: 0.8523\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4550 - acc: 0.8491 - val_loss: 0.4149 - val_acc: 0.8598\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4342 - acc: 0.8539 - val_loss: 0.4242 - val_acc: 0.8561\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4633 - acc: 0.8463 - val_loss: 0.4627 - val_acc: 0.8598\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 30ms/step - loss: 0.5367 - acc: 0.7778 - val_loss: 0.5033 - val_acc: 0.8242\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5254 - acc: 0.7778 - val_loss: 0.5047 - val_acc: 0.8242\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5214 - acc: 0.7806 - val_loss: 0.5134 - val_acc: 0.8242\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5132 - acc: 0.7806 - val_loss: 0.6523 - val_acc: 0.8242\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5095 - acc: 0.7806 - val_loss: 0.6534 - val_acc: 0.8242\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5155 - acc: 0.7833 - val_loss: 0.6533 - val_acc: 0.8242\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5184 - acc: 0.8675 - val_loss: 0.5771 - val_acc: 0.7669\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4798 - acc: 0.8675 - val_loss: 0.5797 - val_acc: 0.7669\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4580 - acc: 0.8665 - val_loss: 0.5671 - val_acc: 0.7707\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4476 - acc: 0.8665 - val_loss: 0.5658 - val_acc: 0.7707\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4434 - acc: 0.8675 - val_loss: 0.5725 - val_acc: 0.7707\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4393 - acc: 0.8665 - val_loss: 0.5734 - val_acc: 0.7707\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4385 - acc: 0.8675 - val_loss: 0.5832 - val_acc: 0.7707\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4378 - acc: 0.8675 - val_loss: 0.5960 - val_acc: 0.7744\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4363 - acc: 0.8656 - val_loss: 0.5833 - val_acc: 0.7744\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4352 - acc: 0.8656 - val_loss: 0.5921 - val_acc: 0.7744\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4177 - acc: 0.8675 - val_loss: 0.6251 - val_acc: 0.7707\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3995 - acc: 0.8665 - val_loss: 0.6286 - val_acc: 0.7707\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3990 - acc: 0.8684 - val_loss: 0.6222 - val_acc: 0.7707\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3976 - acc: 0.8675 - val_loss: 0.5845 - val_acc: 0.7707\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3977 - acc: 0.8675 - val_loss: 0.6261 - val_acc: 0.7707\n",
      "Epoch 15: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3083 - acc: 0.9664 - val_loss: 0.1106 - val_acc: 0.9926\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2715 - acc: 0.9581 - val_loss: 0.1315 - val_acc: 0.9851\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2746 - acc: 0.9581 - val_loss: 0.1102 - val_acc: 0.9926\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.8145 - acc: 0.8276 - val_loss: 0.5740 - val_acc: 0.7249\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6484 - acc: 0.6570 - val_loss: 0.5174 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5461 - acc: 0.7502 - val_loss: 0.4198 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4760 - acc: 0.7987 - val_loss: 0.3761 - val_acc: 0.8662\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4561 - acc: 0.8313 - val_loss: 0.3602 - val_acc: 0.8848\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4530 - acc: 0.8406 - val_loss: 0.3694 - val_acc: 0.8996\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4418 - acc: 0.8369 - val_loss: 0.3566 - val_acc: 0.8959\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4324 - acc: 0.8453 - val_loss: 0.4056 - val_acc: 0.9033\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4177 - acc: 0.8565 - val_loss: 0.3886 - val_acc: 0.9108\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4215 - acc: 0.8621 - val_loss: 0.3731 - val_acc: 0.9182\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4808 - acc: 0.8856 - val_loss: 0.6291 - val_acc: 0.8340\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4789 - acc: 0.8970 - val_loss: 0.6096 - val_acc: 0.8453\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4995 - acc: 0.8998 - val_loss: 0.7191 - val_acc: 0.8415\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5662 - acc: 0.8145 - val_loss: 0.5696 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5217 - acc: 0.8136 - val_loss: 0.4668 - val_acc: 0.8550\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4980 - acc: 0.8108 - val_loss: 0.4573 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4930 - acc: 0.8108 - val_loss: 0.4513 - val_acc: 0.8550\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4926 - acc: 0.8099 - val_loss: 0.4598 - val_acc: 0.8513\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4909 - acc: 0.8062 - val_loss: 0.4508 - val_acc: 0.8513\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4857 - acc: 0.8080 - val_loss: 0.4465 - val_acc: 0.8513\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4839 - acc: 0.8099 - val_loss: 0.4430 - val_acc: 0.8476\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4809 - acc: 0.8099 - val_loss: 0.4435 - val_acc: 0.8476\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4796 - acc: 0.8099 - val_loss: 0.4430 - val_acc: 0.8476\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4897 - acc: 0.8108 - val_loss: 0.4437 - val_acc: 0.8439\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4895 - acc: 0.8416 - val_loss: 0.6424 - val_acc: 0.7286\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4819 - acc: 0.8416 - val_loss: 0.7009 - val_acc: 0.7286\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4687 - acc: 0.8462 - val_loss: 0.6460 - val_acc: 0.7286\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4582 - acc: 0.8462 - val_loss: 0.6233 - val_acc: 0.7286\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4665 - acc: 0.8472 - val_loss: 0.6290 - val_acc: 0.7286\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5306 - acc: 0.8145 - val_loss: 0.4050 - val_acc: 0.8773\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5026 - acc: 0.8155 - val_loss: 0.4134 - val_acc: 0.8810\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4963 - acc: 0.8164 - val_loss: 0.4000 - val_acc: 0.8848\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4958 - acc: 0.8164 - val_loss: 0.4417 - val_acc: 0.8885\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4835 - acc: 0.8173 - val_loss: 0.4361 - val_acc: 0.8736\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4900 - acc: 0.8136 - val_loss: 0.4384 - val_acc: 0.8848\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4631 - acc: 0.8611 - val_loss: 0.4650 - val_acc: 0.8253\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4591 - acc: 0.8639 - val_loss: 0.4656 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4652 - acc: 0.8639 - val_loss: 0.5062 - val_acc: 0.8327\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5101 - acc: 0.8630 - val_loss: 0.3724 - val_acc: 0.8922\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4457 - acc: 0.8677 - val_loss: 0.3735 - val_acc: 0.8959\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4288 - acc: 0.8667 - val_loss: 0.3674 - val_acc: 0.8996\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4367 - acc: 0.8686 - val_loss: 0.3719 - val_acc: 0.8996\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4618 - acc: 0.8541 - val_loss: 0.4736 - val_acc: 0.8321\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4500 - acc: 0.8541 - val_loss: 0.4781 - val_acc: 0.8321\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4323 - acc: 0.8550 - val_loss: 0.4674 - val_acc: 0.8358\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4297 - acc: 0.8550 - val_loss: 0.4691 - val_acc: 0.8358\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4260 - acc: 0.8550 - val_loss: 0.4694 - val_acc: 0.8358\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4251 - acc: 0.8550 - val_loss: 0.4707 - val_acc: 0.8358\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4237 - acc: 0.8559 - val_loss: 0.4722 - val_acc: 0.8358\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4222 - acc: 0.8559 - val_loss: 0.4725 - val_acc: 0.8358\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4213 - acc: 0.8569 - val_loss: 0.4698 - val_acc: 0.8358\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4200 - acc: 0.8569 - val_loss: 0.4719 - val_acc: 0.8358\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4194 - acc: 0.8569 - val_loss: 0.5120 - val_acc: 0.8358\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4182 - acc: 0.8559 - val_loss: 0.5124 - val_acc: 0.8358\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4174 - acc: 0.8559 - val_loss: 0.5112 - val_acc: 0.8358\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4171 - acc: 0.8559 - val_loss: 0.5086 - val_acc: 0.8358\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4161 - acc: 0.8559 - val_loss: 0.5126 - val_acc: 0.8358\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4152 - acc: 0.8559 - val_loss: 0.5096 - val_acc: 0.8358\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4139 - acc: 0.8559 - val_loss: 0.5107 - val_acc: 0.8358\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4137 - acc: 0.8559 - val_loss: 0.5096 - val_acc: 0.8358\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4130 - acc: 0.8559 - val_loss: 0.5076 - val_acc: 0.8358\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4131 - acc: 0.8559 - val_loss: 0.5089 - val_acc: 0.8358\n",
      "Epoch 20: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.6628 - acc: 0.7204 - val_loss: 0.5437 - val_acc: 0.7621\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6017 - acc: 0.7204 - val_loss: 0.5422 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5987 - acc: 0.7204 - val_loss: 0.5440 - val_acc: 0.7621\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5987 - acc: 0.7204 - val_loss: 0.5411 - val_acc: 0.7621\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5990 - acc: 0.7204 - val_loss: 0.5420 - val_acc: 0.7621\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4116 - acc: 0.8841 - val_loss: 0.3348 - val_acc: 0.9394\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3703 - acc: 0.8841 - val_loss: 0.2988 - val_acc: 0.9394\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3652 - acc: 0.8841 - val_loss: 0.2940 - val_acc: 0.9432\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3619 - acc: 0.8841 - val_loss: 0.2828 - val_acc: 0.9432\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3555 - acc: 0.8841 - val_loss: 0.2792 - val_acc: 0.9432\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3536 - acc: 0.8841 - val_loss: 0.2782 - val_acc: 0.9432\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3496 - acc: 0.8841 - val_loss: 0.2754 - val_acc: 0.9432\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3445 - acc: 0.8841 - val_loss: 0.3140 - val_acc: 0.9432\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3452 - acc: 0.8841 - val_loss: 0.2851 - val_acc: 0.9432\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4507 - acc: 0.8425 - val_loss: 0.5269 - val_acc: 0.7774\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4335 - acc: 0.8425 - val_loss: 0.5358 - val_acc: 0.7774\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4277 - acc: 0.8425 - val_loss: 0.5350 - val_acc: 0.7774\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4258 - acc: 0.8425 - val_loss: 0.5346 - val_acc: 0.7774\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4180 - acc: 0.8425 - val_loss: 0.5319 - val_acc: 0.7774\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4151 - acc: 0.8425 - val_loss: 0.5339 - val_acc: 0.7774\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4127 - acc: 0.8425 - val_loss: 0.5342 - val_acc: 0.7774\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4116 - acc: 0.8425 - val_loss: 0.5327 - val_acc: 0.7774\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4104 - acc: 0.8425 - val_loss: 0.5390 - val_acc: 0.7774\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4097 - acc: 0.8425 - val_loss: 0.5335 - val_acc: 0.7774\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4064 - acc: 0.8425 - val_loss: 0.5340 - val_acc: 0.7774\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4057 - acc: 0.8425 - val_loss: 0.5357 - val_acc: 0.7774\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4030 - acc: 0.8425 - val_loss: 0.5357 - val_acc: 0.7774\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4019 - acc: 0.8425 - val_loss: 0.5336 - val_acc: 0.7774\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4018 - acc: 0.8425 - val_loss: 0.5336 - val_acc: 0.7774\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4016 - acc: 0.8425 - val_loss: 0.5403 - val_acc: 0.7774\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3991 - acc: 0.8425 - val_loss: 0.5348 - val_acc: 0.7774\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3970 - acc: 0.8425 - val_loss: 0.5306 - val_acc: 0.7774\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3974 - acc: 0.8425 - val_loss: 0.5336 - val_acc: 0.7774\n",
      "Epoch 19: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4845 - acc: 0.8639 - val_loss: 0.4932 - val_acc: 0.8550\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4327 - acc: 0.8639 - val_loss: 0.4347 - val_acc: 0.8550\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4122 - acc: 0.8639 - val_loss: 0.4402 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4195 - acc: 0.8639 - val_loss: 0.4647 - val_acc: 0.8550\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 2s 11ms/step - loss: 0.3735 - acc: 0.9083 - val_loss: 0.4436 - val_acc: 0.8405\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.3331 - acc: 0.9083 - val_loss: 0.4893 - val_acc: 0.8405\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.3313 - acc: 0.9083 - val_loss: 0.4346 - val_acc: 0.8405\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.3219 - acc: 0.9083 - val_loss: 0.4481 - val_acc: 0.8405\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.3218 - acc: 0.9083 - val_loss: 0.4591 - val_acc: 0.8405\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.3208 - acc: 0.9083 - val_loss: 0.4395 - val_acc: 0.8405\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.3170 - acc: 0.9083 - val_loss: 0.4494 - val_acc: 0.8405\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.3200 - acc: 0.9083 - val_loss: 0.4546 - val_acc: 0.8405\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 20ms/step - loss: 0.3105 - acc: 0.9320 - val_loss: 0.4372 - val_acc: 0.8662\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2706 - acc: 0.9320 - val_loss: 0.4085 - val_acc: 0.8662\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2733 - acc: 0.9320 - val_loss: 0.4000 - val_acc: 0.8662\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5716 - acc: 0.7586 - val_loss: 0.6639 - val_acc: 0.6208\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6034 - acc: 0.6925 - val_loss: 0.6162 - val_acc: 0.6952\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4419 - acc: 0.8350 - val_loss: 0.7471 - val_acc: 0.6766\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3633 - acc: 0.8788 - val_loss: 0.8853 - val_acc: 0.7175\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3678 - acc: 0.9012 - val_loss: 1.0574 - val_acc: 0.7398\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 0.5902 - acc: 0.8195 - val_loss: 0.4759 - val_acc: 0.8197\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4917 - acc: 0.8195 - val_loss: 0.4681 - val_acc: 0.8279\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.4968 - acc: 0.8256 - val_loss: 0.4699 - val_acc: 0.8156\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 14ms/step - loss: 0.5591 - acc: 0.7897 - val_loss: 0.5643 - val_acc: 0.7837\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5390 - acc: 0.7969 - val_loss: 0.5646 - val_acc: 0.7837\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5360 - acc: 0.7969 - val_loss: 0.5674 - val_acc: 0.7788\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5338 - acc: 0.8005 - val_loss: 0.5676 - val_acc: 0.7788\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5325 - acc: 0.8029 - val_loss: 0.5682 - val_acc: 0.7788\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5308 - acc: 0.8077 - val_loss: 0.5692 - val_acc: 0.7788\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5274 - acc: 0.8089 - val_loss: 0.5674 - val_acc: 0.7837\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5272 - acc: 0.8113 - val_loss: 0.5682 - val_acc: 0.7837\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5270 - acc: 0.8113 - val_loss: 0.5712 - val_acc: 0.7837\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5253 - acc: 0.8113 - val_loss: 0.5676 - val_acc: 0.7837\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5244 - acc: 0.8113 - val_loss: 0.5674 - val_acc: 0.7837\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5237 - acc: 0.8113 - val_loss: 0.5685 - val_acc: 0.7837\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5226 - acc: 0.8113 - val_loss: 0.5688 - val_acc: 0.7837\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5227 - acc: 0.8113 - val_loss: 0.5727 - val_acc: 0.7837\n",
      "Epoch 14: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.5219 - acc: 0.7978 - val_loss: 0.5590 - val_acc: 0.7695\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5191 - acc: 0.7959 - val_loss: 0.5981 - val_acc: 0.7695\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5163 - acc: 0.7968 - val_loss: 0.5981 - val_acc: 0.7695\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5134 - acc: 0.7978 - val_loss: 0.5974 - val_acc: 0.7695\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5102 - acc: 0.7978 - val_loss: 0.5937 - val_acc: 0.7732\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5086 - acc: 0.7996 - val_loss: 0.5941 - val_acc: 0.7732\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5106 - acc: 0.7996 - val_loss: 0.5944 - val_acc: 0.7732\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4360 - acc: 0.9003 - val_loss: 0.4197 - val_acc: 0.8662\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4268 - acc: 0.9003 - val_loss: 0.4186 - val_acc: 0.8699\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3853 - acc: 0.8984 - val_loss: 0.4235 - val_acc: 0.8662\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4044 - acc: 0.8984 - val_loss: 0.4613 - val_acc: 0.8699\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 13ms/step - loss: 0.5871 - acc: 0.7978 - val_loss: 0.5338 - val_acc: 0.7993\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5273 - acc: 0.7978 - val_loss: 0.5242 - val_acc: 0.7993\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5227 - acc: 0.7996 - val_loss: 0.5296 - val_acc: 0.7955\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5200 - acc: 0.8006 - val_loss: 0.5282 - val_acc: 0.7955\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5179 - acc: 0.8006 - val_loss: 0.5223 - val_acc: 0.7955\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5151 - acc: 0.8006 - val_loss: 0.5217 - val_acc: 0.7955\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5139 - acc: 0.8015 - val_loss: 0.5204 - val_acc: 0.7955\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5134 - acc: 0.8015 - val_loss: 0.5195 - val_acc: 0.7955\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5122 - acc: 0.8015 - val_loss: 0.5181 - val_acc: 0.7955\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5114 - acc: 0.8015 - val_loss: 0.5157 - val_acc: 0.7955\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5102 - acc: 0.8024 - val_loss: 0.5159 - val_acc: 0.7955\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5102 - acc: 0.8015 - val_loss: 0.5156 - val_acc: 0.7955\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 17ms/step - loss: 0.5574 - acc: 0.7779 - val_loss: 0.5785 - val_acc: 0.7585\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5508 - acc: 0.7779 - val_loss: 0.5716 - val_acc: 0.7585\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5454 - acc: 0.7779 - val_loss: 0.5712 - val_acc: 0.7585\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5461 - acc: 0.7779 - val_loss: 0.5691 - val_acc: 0.7585\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 13ms/step - loss: 0.4113 - acc: 0.8767 - val_loss: 0.3543 - val_acc: 0.8950\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3844 - acc: 0.8779 - val_loss: 0.3222 - val_acc: 0.8950\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3751 - acc: 0.8801 - val_loss: 0.3301 - val_acc: 0.8950\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3695 - acc: 0.8801 - val_loss: 0.3201 - val_acc: 0.8950\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3677 - acc: 0.8801 - val_loss: 0.3245 - val_acc: 0.8950\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3666 - acc: 0.8801 - val_loss: 0.3196 - val_acc: 0.8950\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.3653 - acc: 0.8790 - val_loss: 0.3210 - val_acc: 0.8950\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.3649 - acc: 0.8790 - val_loss: 0.3264 - val_acc: 0.8950\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.3640 - acc: 0.8790 - val_loss: 0.3291 - val_acc: 0.8950\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.3608 - acc: 0.8790 - val_loss: 0.3287 - val_acc: 0.8950\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.3591 - acc: 0.8790 - val_loss: 0.3266 - val_acc: 0.8950\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3563 - acc: 0.8790 - val_loss: 0.3279 - val_acc: 0.8950\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3552 - acc: 0.8790 - val_loss: 0.3278 - val_acc: 0.8950\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3514 - acc: 0.8790 - val_loss: 0.3357 - val_acc: 0.8950\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3510 - acc: 0.8790 - val_loss: 0.3430 - val_acc: 0.8950\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.3530 - acc: 0.8790 - val_loss: 0.3575 - val_acc: 0.8950\n",
      "Epoch 16: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3326 - acc: 0.9096 - val_loss: 0.3002 - val_acc: 0.9182\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3235 - acc: 0.9096 - val_loss: 0.2987 - val_acc: 0.9182\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3140 - acc: 0.9096 - val_loss: 0.2934 - val_acc: 0.9182\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3055 - acc: 0.9096 - val_loss: 0.3003 - val_acc: 0.9182\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3153 - acc: 0.9096 - val_loss: 0.3063 - val_acc: 0.9182\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4848 - acc: 0.8239 - val_loss: 0.4141 - val_acc: 0.8736\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4742 - acc: 0.8229 - val_loss: 0.4179 - val_acc: 0.8736\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4741 - acc: 0.8229 - val_loss: 0.4174 - val_acc: 0.8736\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4718 - acc: 0.8229 - val_loss: 0.4184 - val_acc: 0.8736\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4698 - acc: 0.8229 - val_loss: 0.4161 - val_acc: 0.8736\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4691 - acc: 0.8229 - val_loss: 0.4152 - val_acc: 0.8736\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4685 - acc: 0.8239 - val_loss: 0.4178 - val_acc: 0.8736\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4684 - acc: 0.8239 - val_loss: 0.4202 - val_acc: 0.8736\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4686 - acc: 0.8239 - val_loss: 0.4168 - val_acc: 0.8736\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.5632 - acc: 0.7526 - val_loss: 0.6335 - val_acc: 0.6828\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5566 - acc: 0.7526 - val_loss: 0.6315 - val_acc: 0.6828\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5554 - acc: 0.7526 - val_loss: 0.6311 - val_acc: 0.6866\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5539 - acc: 0.7526 - val_loss: 0.6286 - val_acc: 0.6866\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5531 - acc: 0.7535 - val_loss: 0.6254 - val_acc: 0.6866\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5526 - acc: 0.7535 - val_loss: 0.6278 - val_acc: 0.6866\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5519 - acc: 0.7535 - val_loss: 0.6276 - val_acc: 0.6866\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5510 - acc: 0.7535 - val_loss: 0.6263 - val_acc: 0.6866\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5516 - acc: 0.7554 - val_loss: 0.6267 - val_acc: 0.6903\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5058 - acc: 0.8006 - val_loss: 0.4709 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5034 - acc: 0.8006 - val_loss: 0.4676 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5012 - acc: 0.8006 - val_loss: 0.4720 - val_acc: 0.8290\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4995 - acc: 0.8015 - val_loss: 0.4723 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4984 - acc: 0.8015 - val_loss: 0.4721 - val_acc: 0.8253\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4972 - acc: 0.8015 - val_loss: 0.4728 - val_acc: 0.8253\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4982 - acc: 0.8015 - val_loss: 0.4720 - val_acc: 0.8253\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3580 - acc: 0.9077 - val_loss: 0.2046 - val_acc: 0.9665\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3391 - acc: 0.9087 - val_loss: 0.2349 - val_acc: 0.9665\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3322 - acc: 0.9087 - val_loss: 0.2295 - val_acc: 0.9665\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3292 - acc: 0.9087 - val_loss: 0.1889 - val_acc: 0.9665\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3138 - acc: 0.9087 - val_loss: 0.1968 - val_acc: 0.9665\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3136 - acc: 0.9087 - val_loss: 0.2043 - val_acc: 0.9665\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3128 - acc: 0.9087 - val_loss: 0.1990 - val_acc: 0.9665\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3096 - acc: 0.9087 - val_loss: 0.1878 - val_acc: 0.9665\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3106 - acc: 0.9087 - val_loss: 0.1889 - val_acc: 0.9665\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4897 - acc: 0.8341 - val_loss: 0.4720 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4562 - acc: 0.8341 - val_loss: 0.4670 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4534 - acc: 0.8341 - val_loss: 0.4642 - val_acc: 0.8290\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4513 - acc: 0.8341 - val_loss: 0.4639 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4502 - acc: 0.8341 - val_loss: 0.4633 - val_acc: 0.8290\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4491 - acc: 0.8341 - val_loss: 0.4631 - val_acc: 0.8290\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4484 - acc: 0.8341 - val_loss: 0.4630 - val_acc: 0.8290\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4493 - acc: 0.8341 - val_loss: 0.4627 - val_acc: 0.8290\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3842 - acc: 0.8788 - val_loss: 0.3880 - val_acc: 0.8959\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4101 - acc: 0.8788 - val_loss: 0.3857 - val_acc: 0.8959\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4932 - acc: 0.8071 - val_loss: 0.5069 - val_acc: 0.7955\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4884 - acc: 0.8071 - val_loss: 0.5040 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4875 - acc: 0.8071 - val_loss: 0.5035 - val_acc: 0.7955\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4879 - acc: 0.8071 - val_loss: 0.5027 - val_acc: 0.7955\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5513 - acc: 0.7661 - val_loss: 0.5583 - val_acc: 0.7584\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5462 - acc: 0.7661 - val_loss: 0.5573 - val_acc: 0.7584\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5460 - acc: 0.7661 - val_loss: 0.5544 - val_acc: 0.7584\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5446 - acc: 0.7661 - val_loss: 0.5570 - val_acc: 0.7584\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5444 - acc: 0.7661 - val_loss: 0.5551 - val_acc: 0.7584\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5441 - acc: 0.7661 - val_loss: 0.5554 - val_acc: 0.7584\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5434 - acc: 0.7661 - val_loss: 0.5558 - val_acc: 0.7584\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5434 - acc: 0.7661 - val_loss: 0.5556 - val_acc: 0.7584\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4471 - acc: 0.8453 - val_loss: 0.5338 - val_acc: 0.7881\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4405 - acc: 0.8453 - val_loss: 0.5392 - val_acc: 0.7881\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4389 - acc: 0.8453 - val_loss: 0.5346 - val_acc: 0.7881\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4374 - acc: 0.8453 - val_loss: 0.5337 - val_acc: 0.7881\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4362 - acc: 0.8453 - val_loss: 0.5395 - val_acc: 0.7881\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4370 - acc: 0.8453 - val_loss: 0.5394 - val_acc: 0.7881\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 2s 11ms/step - loss: 0.5221 - acc: 0.7908 - val_loss: 0.5206 - val_acc: 0.7901\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5131 - acc: 0.7908 - val_loss: 0.5188 - val_acc: 0.7901\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5102 - acc: 0.7908 - val_loss: 0.5262 - val_acc: 0.7901\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5116 - acc: 0.7908 - val_loss: 0.5220 - val_acc: 0.7901\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 13ms/step - loss: 0.4838 - acc: 0.8200 - val_loss: 0.5170 - val_acc: 0.7854\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4750 - acc: 0.8200 - val_loss: 0.5205 - val_acc: 0.7854\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 0.4714 - acc: 0.8200 - val_loss: 0.5192 - val_acc: 0.7854\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4694 - acc: 0.8200 - val_loss: 0.5163 - val_acc: 0.7854\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4675 - acc: 0.8200 - val_loss: 0.5147 - val_acc: 0.7854\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4666 - acc: 0.8200 - val_loss: 0.5187 - val_acc: 0.7854\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4663 - acc: 0.8200 - val_loss: 0.5185 - val_acc: 0.7854\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4685 - acc: 0.8200 - val_loss: 0.5332 - val_acc: 0.7854\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4491 - acc: 0.8378 - val_loss: 0.5236 - val_acc: 0.7955\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4424 - acc: 0.8378 - val_loss: 0.5214 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4431 - acc: 0.8378 - val_loss: 0.5249 - val_acc: 0.7955\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 13ms/step - loss: 0.3434 - acc: 0.8974 - val_loss: 0.6430 - val_acc: 0.7377\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.3341 - acc: 0.8974 - val_loss: 0.6118 - val_acc: 0.7377\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.3344 - acc: 0.8974 - val_loss: 0.6702 - val_acc: 0.7377\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.3802 - acc: 0.8779 - val_loss: 0.4663 - val_acc: 0.8364\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3727 - acc: 0.8779 - val_loss: 0.4823 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.3695 - acc: 0.8779 - val_loss: 0.4800 - val_acc: 0.8364\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3675 - acc: 0.8779 - val_loss: 0.4890 - val_acc: 0.8364\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3648 - acc: 0.8779 - val_loss: 0.4813 - val_acc: 0.8364\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3656 - acc: 0.8779 - val_loss: 0.4893 - val_acc: 0.8364\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2273 - acc: 0.9581 - val_loss: 0.1338 - val_acc: 0.9926\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2050 - acc: 0.9581 - val_loss: 0.1094 - val_acc: 0.9926\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2733 - acc: 0.9497 - val_loss: 0.3271 - val_acc: 0.9554\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4479 - acc: 0.8854 - val_loss: 0.5419 - val_acc: 0.7770\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3638 - acc: 0.8938 - val_loss: 0.5748 - val_acc: 0.7770\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3450 - acc: 0.8938 - val_loss: 0.5937 - val_acc: 0.7770\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3435 - acc: 0.8938 - val_loss: 0.6137 - val_acc: 0.7770\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3415 - acc: 0.8938 - val_loss: 0.5839 - val_acc: 0.7770\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3416 - acc: 0.8938 - val_loss: 0.5927 - val_acc: 0.7770\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 13ms/step - loss: 0.3604 - acc: 0.8835 - val_loss: 0.3577 - val_acc: 0.8848\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3588 - acc: 0.8835 - val_loss: 0.3651 - val_acc: 0.8848\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3571 - acc: 0.8835 - val_loss: 0.3651 - val_acc: 0.8848\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3570 - acc: 0.8835 - val_loss: 0.3659 - val_acc: 0.8848\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3568 - acc: 0.8835 - val_loss: 0.3679 - val_acc: 0.8848\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3591 - acc: 0.8835 - val_loss: 0.3681 - val_acc: 0.8848\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 2s 11ms/step - loss: 0.5810 - acc: 0.7522 - val_loss: 0.5217 - val_acc: 0.7821\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5625 - acc: 0.7522 - val_loss: 0.5241 - val_acc: 0.7821\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5615 - acc: 0.7522 - val_loss: 0.5255 - val_acc: 0.7821\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5602 - acc: 0.7522 - val_loss: 0.5275 - val_acc: 0.7821\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5590 - acc: 0.7522 - val_loss: 0.5296 - val_acc: 0.7821\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5585 - acc: 0.7522 - val_loss: 0.5305 - val_acc: 0.7821\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5582 - acc: 0.7522 - val_loss: 0.5325 - val_acc: 0.7821\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5586 - acc: 0.7522 - val_loss: 0.5312 - val_acc: 0.7821\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4486 - acc: 0.8444 - val_loss: 0.4322 - val_acc: 0.8476\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4378 - acc: 0.8444 - val_loss: 0.4310 - val_acc: 0.8476\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4359 - acc: 0.8444 - val_loss: 0.4315 - val_acc: 0.8476\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4324 - acc: 0.8444 - val_loss: 0.4323 - val_acc: 0.8476\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4315 - acc: 0.8444 - val_loss: 0.4307 - val_acc: 0.8476\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4319 - acc: 0.8444 - val_loss: 0.4303 - val_acc: 0.8476\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4764 - acc: 0.8201 - val_loss: 0.4115 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4744 - acc: 0.8201 - val_loss: 0.4094 - val_acc: 0.8587\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4727 - acc: 0.8201 - val_loss: 0.4086 - val_acc: 0.8587\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4722 - acc: 0.8201 - val_loss: 0.4079 - val_acc: 0.8587\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4711 - acc: 0.8201 - val_loss: 0.4134 - val_acc: 0.8587\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4707 - acc: 0.8201 - val_loss: 0.4065 - val_acc: 0.8587\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4718 - acc: 0.8201 - val_loss: 0.4034 - val_acc: 0.8587\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3954 - acc: 0.8675 - val_loss: 0.2690 - val_acc: 0.9368\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3964 - acc: 0.8675 - val_loss: 0.2979 - val_acc: 0.9368\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5051 - acc: 0.8006 - val_loss: 0.5671 - val_acc: 0.7509\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5002 - acc: 0.8006 - val_loss: 0.5699 - val_acc: 0.7509\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4989 - acc: 0.8006 - val_loss: 0.5722 - val_acc: 0.7509\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4987 - acc: 0.8006 - val_loss: 0.5706 - val_acc: 0.7509\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4976 - acc: 0.8006 - val_loss: 0.5714 - val_acc: 0.7509\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.4990 - acc: 0.8006 - val_loss: 0.5713 - val_acc: 0.7509\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.4029 - acc: 0.8658 - val_loss: 0.3775 - val_acc: 0.8773\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3953 - acc: 0.8658 - val_loss: 0.3794 - val_acc: 0.8773\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3945 - acc: 0.8658 - val_loss: 0.3753 - val_acc: 0.8773\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3937 - acc: 0.8658 - val_loss: 0.3749 - val_acc: 0.8773\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3917 - acc: 0.8658 - val_loss: 0.3857 - val_acc: 0.8773\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3949 - acc: 0.8658 - val_loss: 0.3872 - val_acc: 0.8773\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 2s 20ms/step - loss: 0.3705 - acc: 0.8832 - val_loss: 0.7561 - val_acc: 0.6715\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3636 - acc: 0.8832 - val_loss: 0.7674 - val_acc: 0.6715\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3631 - acc: 0.8832 - val_loss: 0.7498 - val_acc: 0.6715\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3592 - acc: 0.8832 - val_loss: 0.7492 - val_acc: 0.6715\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3593 - acc: 0.8832 - val_loss: 0.7563 - val_acc: 0.6715\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4044 - acc: 0.8630 - val_loss: 0.6005 - val_acc: 0.7621\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4027 - acc: 0.8630 - val_loss: 0.5813 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3993 - acc: 0.8630 - val_loss: 0.5862 - val_acc: 0.7621\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3995 - acc: 0.8630 - val_loss: 0.5915 - val_acc: 0.7621\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5470 - acc: 0.7745 - val_loss: 0.4989 - val_acc: 0.8030\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5323 - acc: 0.7745 - val_loss: 0.4967 - val_acc: 0.8030\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5339 - acc: 0.7745 - val_loss: 0.5007 - val_acc: 0.8030\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 18ms/step - loss: 0.4758 - acc: 0.8211 - val_loss: 0.5844 - val_acc: 0.7472\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4673 - acc: 0.8211 - val_loss: 0.5750 - val_acc: 0.7472\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4644 - acc: 0.8211 - val_loss: 0.5770 - val_acc: 0.7472\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4656 - acc: 0.8211 - val_loss: 0.5717 - val_acc: 0.7472\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4787 - acc: 0.8183 - val_loss: 0.4444 - val_acc: 0.8401\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4723 - acc: 0.8183 - val_loss: 0.4453 - val_acc: 0.8401\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4692 - acc: 0.8183 - val_loss: 0.4395 - val_acc: 0.8401\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4691 - acc: 0.8183 - val_loss: 0.4411 - val_acc: 0.8401\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4679 - acc: 0.8183 - val_loss: 0.4360 - val_acc: 0.8401\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4689 - acc: 0.8183 - val_loss: 0.4361 - val_acc: 0.8401\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3320 - acc: 0.9105 - val_loss: 0.4195 - val_acc: 0.8513\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3066 - acc: 0.9105 - val_loss: 0.4343 - val_acc: 0.8513\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3039 - acc: 0.9105 - val_loss: 0.4276 - val_acc: 0.8513\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3099 - acc: 0.9105 - val_loss: 0.4225 - val_acc: 0.8513\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 10ms/step - loss: 0.4231 - acc: 0.8565 - val_loss: 0.3212 - val_acc: 0.9145\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4151 - acc: 0.8565 - val_loss: 0.3177 - val_acc: 0.9145\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4143 - acc: 0.8565 - val_loss: 0.3268 - val_acc: 0.9145\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4126 - acc: 0.8565 - val_loss: 0.3113 - val_acc: 0.9145\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4119 - acc: 0.8565 - val_loss: 0.3158 - val_acc: 0.9145\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4099 - acc: 0.8565 - val_loss: 0.3158 - val_acc: 0.9145\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4085 - acc: 0.8565 - val_loss: 0.3068 - val_acc: 0.9145\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4102 - acc: 0.8565 - val_loss: 0.3220 - val_acc: 0.9145\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3228 - acc: 0.9515 - val_loss: 0.2508 - val_acc: 0.9554\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3065 - acc: 0.9515 - val_loss: 0.3041 - val_acc: 0.9554\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2663 - acc: 0.9515 - val_loss: 0.3701 - val_acc: 0.9554\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2577 - acc: 0.9515 - val_loss: 0.4035 - val_acc: 0.9554\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3089 - acc: 0.9515 - val_loss: 0.3608 - val_acc: 0.9554\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.6045 - acc: 0.7782 - val_loss: 0.3633 - val_acc: 0.9219\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5344 - acc: 0.7782 - val_loss: 0.3474 - val_acc: 0.9219\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5294 - acc: 0.7782 - val_loss: 0.3531 - val_acc: 0.9219\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5286 - acc: 0.7782 - val_loss: 0.3552 - val_acc: 0.9219\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5264 - acc: 0.7782 - val_loss: 0.3524 - val_acc: 0.9219\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5257 - acc: 0.7782 - val_loss: 0.3548 - val_acc: 0.9219\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5255 - acc: 0.7782 - val_loss: 0.3508 - val_acc: 0.9219\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5240 - acc: 0.7782 - val_loss: 0.3504 - val_acc: 0.9219\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5249 - acc: 0.7782 - val_loss: 0.3475 - val_acc: 0.9219\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 2s 11ms/step - loss: 0.4796 - acc: 0.8166 - val_loss: 0.4229 - val_acc: 0.8521\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.4773 - acc: 0.8166 - val_loss: 0.4229 - val_acc: 0.8521\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.4794 - acc: 0.8166 - val_loss: 0.4238 - val_acc: 0.8521\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4224 - acc: 0.8565 - val_loss: 0.4367 - val_acc: 0.8476\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4150 - acc: 0.8565 - val_loss: 0.4389 - val_acc: 0.8476\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4090 - acc: 0.8565 - val_loss: 0.4422 - val_acc: 0.8476\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4080 - acc: 0.8565 - val_loss: 0.4352 - val_acc: 0.8476\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4086 - acc: 0.8565 - val_loss: 0.4281 - val_acc: 0.8476\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 2s 78ms/step - loss: 0.7473 - acc: 0.6571 - val_loss: 0.7696 - val_acc: 0.6286\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7112 - acc: 0.6571 - val_loss: 0.7299 - val_acc: 0.6286\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6778 - acc: 0.6571 - val_loss: 0.7035 - val_acc: 0.6286\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6507 - acc: 0.6571 - val_loss: 0.6894 - val_acc: 0.6286\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6434 - acc: 0.6571 - val_loss: 0.6799 - val_acc: 0.6286\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6380 - acc: 0.6571 - val_loss: 0.6744 - val_acc: 0.6286\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6335 - acc: 0.6571 - val_loss: 0.6725 - val_acc: 0.6286\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6325 - acc: 0.6571 - val_loss: 0.6710 - val_acc: 0.6286\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6306 - acc: 0.6571 - val_loss: 0.6700 - val_acc: 0.6286\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6287 - acc: 0.6571 - val_loss: 0.6695 - val_acc: 0.6286\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6274 - acc: 0.6571 - val_loss: 0.6690 - val_acc: 0.6286\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6257 - acc: 0.6571 - val_loss: 0.6682 - val_acc: 0.6286\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6240 - acc: 0.6571 - val_loss: 0.6684 - val_acc: 0.6286\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6225 - acc: 0.6571 - val_loss: 0.6682 - val_acc: 0.6286\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6212 - acc: 0.6571 - val_loss: 0.6683 - val_acc: 0.5714\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6200 - acc: 0.6571 - val_loss: 0.6680 - val_acc: 0.5714\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6189 - acc: 0.6571 - val_loss: 0.6687 - val_acc: 0.5714\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6178 - acc: 0.6571 - val_loss: 0.6694 - val_acc: 0.5714\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6163 - acc: 0.6643 - val_loss: 0.6698 - val_acc: 0.5429\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6157 - acc: 0.6714 - val_loss: 0.6697 - val_acc: 0.5429\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6147 - acc: 0.6714 - val_loss: 0.6696 - val_acc: 0.5429\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6135 - acc: 0.6714 - val_loss: 0.6707 - val_acc: 0.5429\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6126 - acc: 0.6714 - val_loss: 0.6711 - val_acc: 0.5429\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6118 - acc: 0.6714 - val_loss: 0.6716 - val_acc: 0.5429\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6107 - acc: 0.6714 - val_loss: 0.6723 - val_acc: 0.5429\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6098 - acc: 0.6714 - val_loss: 0.6731 - val_acc: 0.5429\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6090 - acc: 0.6714 - val_loss: 0.6734 - val_acc: 0.5429\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6083 - acc: 0.6714 - val_loss: 0.6742 - val_acc: 0.5429\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6073 - acc: 0.6786 - val_loss: 0.6754 - val_acc: 0.5429\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6068 - acc: 0.6857 - val_loss: 0.6756 - val_acc: 0.5429\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6062 - acc: 0.6857 - val_loss: 0.6767 - val_acc: 0.5429\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6048 - acc: 0.6857 - val_loss: 0.6765 - val_acc: 0.5429\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6043 - acc: 0.6929 - val_loss: 0.6770 - val_acc: 0.5714\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6041 - acc: 0.7000 - val_loss: 0.6777 - val_acc: 0.5429\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6030 - acc: 0.7000 - val_loss: 0.6774 - val_acc: 0.5429\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6022 - acc: 0.7000 - val_loss: 0.6766 - val_acc: 0.5429\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6017 - acc: 0.7000 - val_loss: 0.6772 - val_acc: 0.5429\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6009 - acc: 0.6929 - val_loss: 0.6771 - val_acc: 0.5429\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6002 - acc: 0.6929 - val_loss: 0.6784 - val_acc: 0.5429\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6000 - acc: 0.6857 - val_loss: 0.6796 - val_acc: 0.5429\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5988 - acc: 0.6786 - val_loss: 0.6804 - val_acc: 0.5429\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5983 - acc: 0.6786 - val_loss: 0.6807 - val_acc: 0.5429\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5976 - acc: 0.6786 - val_loss: 0.6814 - val_acc: 0.5429\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5972 - acc: 0.6786 - val_loss: 0.6818 - val_acc: 0.5429\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5967 - acc: 0.6857 - val_loss: 0.6821 - val_acc: 0.5143\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5963 - acc: 0.6786 - val_loss: 0.6826 - val_acc: 0.5143\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5957 - acc: 0.6786 - val_loss: 0.6830 - val_acc: 0.5143\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5951 - acc: 0.6857 - val_loss: 0.6832 - val_acc: 0.5429\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5948 - acc: 0.6857 - val_loss: 0.6833 - val_acc: 0.5429\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5939 - acc: 0.6857 - val_loss: 0.6841 - val_acc: 0.5429\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 18ms/step - loss: 0.4413 - acc: 0.8509 - val_loss: 0.5209 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4422 - acc: 0.8695 - val_loss: 0.6008 - val_acc: 0.8587\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5773 - acc: 0.7847 - val_loss: 0.5193 - val_acc: 0.8401\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5444 - acc: 0.7838 - val_loss: 0.4562 - val_acc: 0.8439\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5338 - acc: 0.7847 - val_loss: 0.4548 - val_acc: 0.8439\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5291 - acc: 0.7847 - val_loss: 0.4513 - val_acc: 0.8439\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5269 - acc: 0.7856 - val_loss: 0.4504 - val_acc: 0.8439\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5247 - acc: 0.7866 - val_loss: 0.4492 - val_acc: 0.8439\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5224 - acc: 0.7866 - val_loss: 0.4437 - val_acc: 0.8439\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5222 - acc: 0.7866 - val_loss: 0.4545 - val_acc: 0.8439\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5199 - acc: 0.7866 - val_loss: 0.4445 - val_acc: 0.8439\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5181 - acc: 0.7875 - val_loss: 0.4439 - val_acc: 0.8439\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5173 - acc: 0.7866 - val_loss: 0.4439 - val_acc: 0.8439\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5159 - acc: 0.7856 - val_loss: 0.4462 - val_acc: 0.8439\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5157 - acc: 0.7856 - val_loss: 0.4445 - val_acc: 0.8439\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5145 - acc: 0.7856 - val_loss: 0.4455 - val_acc: 0.8439\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5147 - acc: 0.7856 - val_loss: 0.4498 - val_acc: 0.8439\n",
      "Epoch 15: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5962 - acc: 0.7232 - val_loss: 0.6239 - val_acc: 0.6989\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5912 - acc: 0.7241 - val_loss: 0.6194 - val_acc: 0.6989\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5883 - acc: 0.7241 - val_loss: 0.6202 - val_acc: 0.6989\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5873 - acc: 0.7241 - val_loss: 0.6190 - val_acc: 0.6989\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5860 - acc: 0.7241 - val_loss: 0.6175 - val_acc: 0.6989\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5843 - acc: 0.7241 - val_loss: 0.6175 - val_acc: 0.6989\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5840 - acc: 0.7241 - val_loss: 0.6179 - val_acc: 0.6989\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5826 - acc: 0.7241 - val_loss: 0.6185 - val_acc: 0.6989\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5816 - acc: 0.7251 - val_loss: 0.6169 - val_acc: 0.6989\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5800 - acc: 0.7251 - val_loss: 0.6166 - val_acc: 0.6989\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5795 - acc: 0.7260 - val_loss: 0.6180 - val_acc: 0.6989\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5799 - acc: 0.7260 - val_loss: 0.6185 - val_acc: 0.6989\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 12ms/step - loss: 0.5015 - acc: 0.8228 - val_loss: 0.4413 - val_acc: 0.8442\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4744 - acc: 0.8228 - val_loss: 0.4336 - val_acc: 0.8442\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4678 - acc: 0.8228 - val_loss: 0.4339 - val_acc: 0.8442\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4650 - acc: 0.8228 - val_loss: 0.4331 - val_acc: 0.8442\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4639 - acc: 0.8228 - val_loss: 0.4349 - val_acc: 0.8442\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4622 - acc: 0.8228 - val_loss: 0.4355 - val_acc: 0.8442\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4620 - acc: 0.8228 - val_loss: 0.4350 - val_acc: 0.8442\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4623 - acc: 0.8228 - val_loss: 0.4387 - val_acc: 0.8442\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "24/24 [==============================] - 2s 15ms/step - loss: 0.5785 - acc: 0.7486 - val_loss: 0.5003 - val_acc: 0.8305\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.5637 - acc: 0.7486 - val_loss: 0.4859 - val_acc: 0.8305\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.5628 - acc: 0.7486 - val_loss: 0.4792 - val_acc: 0.8305\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.5624 - acc: 0.7486 - val_loss: 0.4824 - val_acc: 0.8305\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 3ms/step - loss: 0.5636 - acc: 0.7486 - val_loss: 0.4758 - val_acc: 0.8305\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4699 - acc: 0.8257 - val_loss: 0.4740 - val_acc: 0.8104\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4587 - acc: 0.8257 - val_loss: 0.4720 - val_acc: 0.8104\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4551 - acc: 0.8257 - val_loss: 0.4707 - val_acc: 0.8104\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4560 - acc: 0.8257 - val_loss: 0.4700 - val_acc: 0.8104\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4607 - acc: 0.8267 - val_loss: 0.6123 - val_acc: 0.7323\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4540 - acc: 0.8267 - val_loss: 0.6183 - val_acc: 0.7323\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4520 - acc: 0.8267 - val_loss: 0.6158 - val_acc: 0.7323\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4535 - acc: 0.8267 - val_loss: 0.5951 - val_acc: 0.7323\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5053 - acc: 0.8052 - val_loss: 0.3985 - val_acc: 0.8736\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4947 - acc: 0.8052 - val_loss: 0.3885 - val_acc: 0.8736\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4883 - acc: 0.8052 - val_loss: 0.3891 - val_acc: 0.8736\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4865 - acc: 0.8052 - val_loss: 0.3940 - val_acc: 0.8736\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4860 - acc: 0.8052 - val_loss: 0.3961 - val_acc: 0.8736\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4867 - acc: 0.8052 - val_loss: 0.3972 - val_acc: 0.8736\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3884 - acc: 0.8938 - val_loss: 0.5609 - val_acc: 0.7807\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3491 - acc: 0.8938 - val_loss: 0.5440 - val_acc: 0.7807\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3407 - acc: 0.8938 - val_loss: 0.5416 - val_acc: 0.7807\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3368 - acc: 0.8938 - val_loss: 0.5466 - val_acc: 0.7807\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3331 - acc: 0.8938 - val_loss: 0.5405 - val_acc: 0.7807\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3301 - acc: 0.8938 - val_loss: 0.5658 - val_acc: 0.7807\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3566 - acc: 0.8938 - val_loss: 0.5585 - val_acc: 0.7807\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5042 - acc: 0.8444 - val_loss: 0.4822 - val_acc: 0.8178\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4314 - acc: 0.8444 - val_loss: 0.4837 - val_acc: 0.8178\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4283 - acc: 0.8444 - val_loss: 0.4826 - val_acc: 0.8178\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4245 - acc: 0.8444 - val_loss: 0.4821 - val_acc: 0.8178\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4271 - acc: 0.8444 - val_loss: 0.4832 - val_acc: 0.8178\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3887 - acc: 0.8723 - val_loss: 0.4647 - val_acc: 0.8253\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3804 - acc: 0.8723 - val_loss: 0.4687 - val_acc: 0.8253\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3791 - acc: 0.8723 - val_loss: 0.4646 - val_acc: 0.8253\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3777 - acc: 0.8723 - val_loss: 0.4715 - val_acc: 0.8253\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3775 - acc: 0.8723 - val_loss: 0.4738 - val_acc: 0.8253\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3753 - acc: 0.8723 - val_loss: 0.4639 - val_acc: 0.8253\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3832 - acc: 0.8723 - val_loss: 0.4712 - val_acc: 0.8253\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4089 - acc: 0.8685 - val_loss: 0.3661 - val_acc: 0.8959\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3926 - acc: 0.8685 - val_loss: 0.3503 - val_acc: 0.8959\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3879 - acc: 0.8685 - val_loss: 0.3414 - val_acc: 0.8959\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3849 - acc: 0.8685 - val_loss: 0.3421 - val_acc: 0.8959\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3848 - acc: 0.8685 - val_loss: 0.3415 - val_acc: 0.8959\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3857 - acc: 0.8685 - val_loss: 0.3382 - val_acc: 0.8959\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5120 - acc: 0.7950 - val_loss: 0.5737 - val_acc: 0.7435\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5048 - acc: 0.7950 - val_loss: 0.5765 - val_acc: 0.7435\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5037 - acc: 0.7950 - val_loss: 0.5707 - val_acc: 0.7435\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5037 - acc: 0.7950 - val_loss: 0.5704 - val_acc: 0.7435\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4000 - acc: 0.8667 - val_loss: 0.5234 - val_acc: 0.8067\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3962 - acc: 0.8667 - val_loss: 0.5233 - val_acc: 0.8067\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3945 - acc: 0.8667 - val_loss: 0.4996 - val_acc: 0.8067\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4028 - acc: 0.8667 - val_loss: 0.5062 - val_acc: 0.8067\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5062 - acc: 0.7996 - val_loss: 0.4309 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5047 - acc: 0.7996 - val_loss: 0.4268 - val_acc: 0.8587\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5028 - acc: 0.7996 - val_loss: 0.4258 - val_acc: 0.8587\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5033 - acc: 0.7996 - val_loss: 0.4177 - val_acc: 0.8587\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4716 - acc: 0.8192 - val_loss: 0.6251 - val_acc: 0.7286\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4668 - acc: 0.8192 - val_loss: 0.6259 - val_acc: 0.7286\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4654 - acc: 0.8192 - val_loss: 0.6336 - val_acc: 0.7286\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4647 - acc: 0.8192 - val_loss: 0.6372 - val_acc: 0.7286\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4642 - acc: 0.8192 - val_loss: 0.6438 - val_acc: 0.7286\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4644 - acc: 0.8192 - val_loss: 0.6503 - val_acc: 0.7286\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 18ms/step - loss: 0.3744 - acc: 0.8863 - val_loss: 0.4524 - val_acc: 0.8327\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3564 - acc: 0.8863 - val_loss: 0.4693 - val_acc: 0.8327\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3508 - acc: 0.8863 - val_loss: 0.4636 - val_acc: 0.8327\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3522 - acc: 0.8863 - val_loss: 0.4765 - val_acc: 0.8327\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3532 - acc: 0.8910 - val_loss: 0.3518 - val_acc: 0.8959\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3550 - acc: 0.8910 - val_loss: 0.3416 - val_acc: 0.8959\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 0.6090 - acc: 0.7632 - val_loss: 0.5482 - val_acc: 0.7708\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5517 - acc: 0.7632 - val_loss: 0.5502 - val_acc: 0.7708\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5393 - acc: 0.7632 - val_loss: 0.5651 - val_acc: 0.7708\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5358 - acc: 0.7632 - val_loss: 0.5684 - val_acc: 0.7708\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5338 - acc: 0.7632 - val_loss: 0.5668 - val_acc: 0.7708\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5308 - acc: 0.7632 - val_loss: 0.5680 - val_acc: 0.7708\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5281 - acc: 0.7632 - val_loss: 0.5692 - val_acc: 0.7708\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5268 - acc: 0.7632 - val_loss: 0.5711 - val_acc: 0.7708\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5244 - acc: 0.7632 - val_loss: 0.5750 - val_acc: 0.7708\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5234 - acc: 0.7632 - val_loss: 0.5776 - val_acc: 0.7708\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5212 - acc: 0.7632 - val_loss: 0.5824 - val_acc: 0.7708\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5201 - acc: 0.7632 - val_loss: 0.5891 - val_acc: 0.7708\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5182 - acc: 0.7632 - val_loss: 0.5903 - val_acc: 0.7708\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5162 - acc: 0.7632 - val_loss: 0.5903 - val_acc: 0.7708\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5154 - acc: 0.7632 - val_loss: 0.5922 - val_acc: 0.7708\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5141 - acc: 0.7632 - val_loss: 0.5964 - val_acc: 0.7708\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5134 - acc: 0.7632 - val_loss: 0.5991 - val_acc: 0.7708\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5127 - acc: 0.7632 - val_loss: 0.5981 - val_acc: 0.7708\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5113 - acc: 0.7632 - val_loss: 0.6029 - val_acc: 0.7708\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5106 - acc: 0.7632 - val_loss: 0.6057 - val_acc: 0.7708\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5091 - acc: 0.7632 - val_loss: 0.6085 - val_acc: 0.7708\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5080 - acc: 0.7632 - val_loss: 0.6132 - val_acc: 0.7708\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5078 - acc: 0.7632 - val_loss: 0.6137 - val_acc: 0.7708\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5067 - acc: 0.7632 - val_loss: 0.6166 - val_acc: 0.7708\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5058 - acc: 0.7632 - val_loss: 0.6175 - val_acc: 0.7708\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5049 - acc: 0.7632 - val_loss: 0.6221 - val_acc: 0.7500\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5046 - acc: 0.7632 - val_loss: 0.6256 - val_acc: 0.7500\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5046 - acc: 0.7632 - val_loss: 0.6299 - val_acc: 0.7500\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5036 - acc: 0.7632 - val_loss: 0.6323 - val_acc: 0.7500\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5032 - acc: 0.7632 - val_loss: 0.6332 - val_acc: 0.7500\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5031 - acc: 0.7632 - val_loss: 0.6374 - val_acc: 0.7500\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5018 - acc: 0.7632 - val_loss: 0.6393 - val_acc: 0.7500\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5024 - acc: 0.7632 - val_loss: 0.6467 - val_acc: 0.7500\n",
      "Epoch 33: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4624 - acc: 0.8741 - val_loss: 0.4785 - val_acc: 0.8476\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3974 - acc: 0.8759 - val_loss: 0.4218 - val_acc: 0.8476\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3891 - acc: 0.8759 - val_loss: 0.4157 - val_acc: 0.8476\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3780 - acc: 0.8759 - val_loss: 0.4134 - val_acc: 0.8476\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3722 - acc: 0.8759 - val_loss: 0.4136 - val_acc: 0.8476\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3750 - acc: 0.8759 - val_loss: 0.4121 - val_acc: 0.8476\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3320 - acc: 0.9044 - val_loss: 0.5852 - val_acc: 0.7652\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3159 - acc: 0.9044 - val_loss: 0.6244 - val_acc: 0.7652\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3120 - acc: 0.9044 - val_loss: 0.6390 - val_acc: 0.7652\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3182 - acc: 0.9044 - val_loss: 0.6543 - val_acc: 0.7652\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4826 - acc: 0.8192 - val_loss: 0.4406 - val_acc: 0.8513\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4728 - acc: 0.8192 - val_loss: 0.4305 - val_acc: 0.8513\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4705 - acc: 0.8192 - val_loss: 0.4325 - val_acc: 0.8513\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4669 - acc: 0.8192 - val_loss: 0.4524 - val_acc: 0.8513\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4664 - acc: 0.8192 - val_loss: 0.4478 - val_acc: 0.8513\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4653 - acc: 0.8192 - val_loss: 0.4877 - val_acc: 0.8513\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4657 - acc: 0.8192 - val_loss: 0.4504 - val_acc: 0.8513\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4881 - acc: 0.8164 - val_loss: 0.5048 - val_acc: 0.8067\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4767 - acc: 0.8164 - val_loss: 0.5002 - val_acc: 0.8067\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4725 - acc: 0.8164 - val_loss: 0.5025 - val_acc: 0.8067\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4714 - acc: 0.8164 - val_loss: 0.4976 - val_acc: 0.8067\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4685 - acc: 0.8164 - val_loss: 0.5110 - val_acc: 0.8067\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4681 - acc: 0.8164 - val_loss: 0.4982 - val_acc: 0.8067\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4657 - acc: 0.8164 - val_loss: 0.4988 - val_acc: 0.8067\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4641 - acc: 0.8164 - val_loss: 0.4980 - val_acc: 0.8067\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4613 - acc: 0.8164 - val_loss: 0.4952 - val_acc: 0.8067\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4570 - acc: 0.8164 - val_loss: 0.4983 - val_acc: 0.8067\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4580 - acc: 0.8164 - val_loss: 0.4935 - val_acc: 0.8067\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.3930 - acc: 0.8788 - val_loss: 0.5240 - val_acc: 0.7844\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3793 - acc: 0.8788 - val_loss: 0.5361 - val_acc: 0.7844\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3734 - acc: 0.8788 - val_loss: 0.5366 - val_acc: 0.7844\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3712 - acc: 0.8788 - val_loss: 0.5386 - val_acc: 0.7844\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3673 - acc: 0.8788 - val_loss: 0.5388 - val_acc: 0.7844\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3689 - acc: 0.8788 - val_loss: 0.5323 - val_acc: 0.7844\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 20ms/step - loss: 0.5723 - acc: 0.7575 - val_loss: 0.6242 - val_acc: 0.6889\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5598 - acc: 0.7575 - val_loss: 0.6312 - val_acc: 0.6889\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5571 - acc: 0.7575 - val_loss: 0.6277 - val_acc: 0.6889\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5512 - acc: 0.7575 - val_loss: 0.6171 - val_acc: 0.6889\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5476 - acc: 0.7575 - val_loss: 0.6237 - val_acc: 0.6889\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5442 - acc: 0.7575 - val_loss: 0.6209 - val_acc: 0.6889\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5431 - acc: 0.7575 - val_loss: 0.6177 - val_acc: 0.6889\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5410 - acc: 0.7575 - val_loss: 0.6214 - val_acc: 0.6889\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5404 - acc: 0.7575 - val_loss: 0.6222 - val_acc: 0.6889\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5403 - acc: 0.7575 - val_loss: 0.6224 - val_acc: 0.6889\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5389 - acc: 0.7575 - val_loss: 0.6169 - val_acc: 0.6889\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5399 - acc: 0.7575 - val_loss: 0.6191 - val_acc: 0.6889\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 2s 26ms/step - loss: 0.6097 - acc: 0.7115 - val_loss: 0.7933 - val_acc: 0.5534\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6007 - acc: 0.7115 - val_loss: 0.7952 - val_acc: 0.5534\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5999 - acc: 0.7115 - val_loss: 0.7771 - val_acc: 0.5534\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5977 - acc: 0.7115 - val_loss: 0.7805 - val_acc: 0.5534\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5988 - acc: 0.7115 - val_loss: 0.7824 - val_acc: 0.5534\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 14ms/step - loss: 0.5192 - acc: 0.8070 - val_loss: 0.5168 - val_acc: 0.7981\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4961 - acc: 0.8070 - val_loss: 0.5154 - val_acc: 0.7981\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4887 - acc: 0.8070 - val_loss: 0.5184 - val_acc: 0.7981\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4846 - acc: 0.8070 - val_loss: 0.5209 - val_acc: 0.7981\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4831 - acc: 0.8070 - val_loss: 0.5189 - val_acc: 0.7981\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4828 - acc: 0.8070 - val_loss: 0.5218 - val_acc: 0.7981\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4815 - acc: 0.8070 - val_loss: 0.5216 - val_acc: 0.7981\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4804 - acc: 0.8070 - val_loss: 0.5256 - val_acc: 0.7981\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4806 - acc: 0.8070 - val_loss: 0.5236 - val_acc: 0.7981\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4800 - acc: 0.8164 - val_loss: 0.3754 - val_acc: 0.8810\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4729 - acc: 0.8164 - val_loss: 0.3853 - val_acc: 0.8810\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4698 - acc: 0.8164 - val_loss: 0.3776 - val_acc: 0.8810\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4654 - acc: 0.8164 - val_loss: 0.3718 - val_acc: 0.8810\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4670 - acc: 0.8164 - val_loss: 0.3767 - val_acc: 0.8810\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4498 - acc: 0.8472 - val_loss: 0.3200 - val_acc: 0.9219\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4340 - acc: 0.8472 - val_loss: 0.3085 - val_acc: 0.9219\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4305 - acc: 0.8472 - val_loss: 0.3161 - val_acc: 0.9219\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4289 - acc: 0.8472 - val_loss: 0.3046 - val_acc: 0.9219\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4291 - acc: 0.8472 - val_loss: 0.3096 - val_acc: 0.9219\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2714 - acc: 0.9394 - val_loss: 0.4024 - val_acc: 0.8810\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2963 - acc: 0.9273 - val_loss: 0.4786 - val_acc: 0.8141\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.5814 - acc: 0.7316 - val_loss: 0.4829 - val_acc: 0.8216\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5453 - acc: 0.7698 - val_loss: 0.4774 - val_acc: 0.8216\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5425 - acc: 0.7698 - val_loss: 0.4796 - val_acc: 0.8216\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5417 - acc: 0.7698 - val_loss: 0.4782 - val_acc: 0.8216\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5409 - acc: 0.7698 - val_loss: 0.4778 - val_acc: 0.8216\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5404 - acc: 0.7698 - val_loss: 0.4794 - val_acc: 0.8216\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5400 - acc: 0.7698 - val_loss: 0.4780 - val_acc: 0.8216\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5403 - acc: 0.7698 - val_loss: 0.4835 - val_acc: 0.8216\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4232 - acc: 0.8649 - val_loss: 0.6599 - val_acc: 0.7138\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4132 - acc: 0.8649 - val_loss: 0.6707 - val_acc: 0.7138\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4081 - acc: 0.8649 - val_loss: 0.6610 - val_acc: 0.7138\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4062 - acc: 0.8649 - val_loss: 0.6480 - val_acc: 0.7138\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4038 - acc: 0.8649 - val_loss: 0.6621 - val_acc: 0.7138\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4230 - acc: 0.8639 - val_loss: 0.6229 - val_acc: 0.7100\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4731 - acc: 0.8319 - val_loss: 0.5158 - val_acc: 0.8134\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4601 - acc: 0.8319 - val_loss: 0.5072 - val_acc: 0.8134\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4561 - acc: 0.8319 - val_loss: 0.5049 - val_acc: 0.8134\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4552 - acc: 0.8319 - val_loss: 0.5052 - val_acc: 0.8134\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4550 - acc: 0.8319 - val_loss: 0.5022 - val_acc: 0.8134\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4551 - acc: 0.8319 - val_loss: 0.5019 - val_acc: 0.8134\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3080 - acc: 0.9189 - val_loss: 0.2777 - val_acc: 0.9517\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2832 - acc: 0.9189 - val_loss: 0.2716 - val_acc: 0.9517\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2877 - acc: 0.9189 - val_loss: 0.2325 - val_acc: 0.9517\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.6172 - acc: 0.7605 - val_loss: 0.5102 - val_acc: 0.7955\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5496 - acc: 0.7605 - val_loss: 0.5095 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5468 - acc: 0.7605 - val_loss: 0.5105 - val_acc: 0.7955\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5446 - acc: 0.7605 - val_loss: 0.5103 - val_acc: 0.7955\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5428 - acc: 0.7605 - val_loss: 0.5071 - val_acc: 0.7955\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5407 - acc: 0.7605 - val_loss: 0.5095 - val_acc: 0.7955\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5381 - acc: 0.7605 - val_loss: 0.5074 - val_acc: 0.7955\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5350 - acc: 0.7605 - val_loss: 0.5121 - val_acc: 0.7955\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5341 - acc: 0.7605 - val_loss: 0.5189 - val_acc: 0.7955\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5377 - acc: 0.7605 - val_loss: 0.5285 - val_acc: 0.7955\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3219 - acc: 0.9208 - val_loss: 0.3070 - val_acc: 0.9219\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2704 - acc: 0.9199 - val_loss: 0.3062 - val_acc: 0.9219\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2555 - acc: 0.9199 - val_loss: 0.3027 - val_acc: 0.9219\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2718 - acc: 0.9199 - val_loss: 0.2978 - val_acc: 0.9219\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5154 - acc: 0.8099 - val_loss: 0.6040 - val_acc: 0.7175\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4896 - acc: 0.8099 - val_loss: 0.6177 - val_acc: 0.7175\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4793 - acc: 0.8099 - val_loss: 0.6216 - val_acc: 0.7175\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4782 - acc: 0.8099 - val_loss: 0.6337 - val_acc: 0.7175\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4776 - acc: 0.8099 - val_loss: 0.6242 - val_acc: 0.7175\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4751 - acc: 0.8099 - val_loss: 0.6193 - val_acc: 0.7175\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4710 - acc: 0.8099 - val_loss: 0.6364 - val_acc: 0.7175\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4736 - acc: 0.8099 - val_loss: 0.6265 - val_acc: 0.7175\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.3244 - acc: 0.9161 - val_loss: 0.4038 - val_acc: 0.8736\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2931 - acc: 0.9161 - val_loss: 0.4005 - val_acc: 0.8736\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2849 - acc: 0.9161 - val_loss: 0.4591 - val_acc: 0.8736\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2833 - acc: 0.9161 - val_loss: 0.4555 - val_acc: 0.8736\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2831 - acc: 0.9161 - val_loss: 0.3930 - val_acc: 0.8736\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2883 - acc: 0.9161 - val_loss: 0.4097 - val_acc: 0.8736\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2878 - acc: 0.9217 - val_loss: 0.2956 - val_acc: 0.9182\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2799 - acc: 0.9217 - val_loss: 0.2981 - val_acc: 0.9182\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2742 - acc: 0.9217 - val_loss: 0.3355 - val_acc: 0.9182\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2741 - acc: 0.9217 - val_loss: 0.3331 - val_acc: 0.9182\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2691 - acc: 0.9217 - val_loss: 0.2912 - val_acc: 0.9182\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2686 - acc: 0.9217 - val_loss: 0.3270 - val_acc: 0.9182\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2769 - acc: 0.9217 - val_loss: 0.3011 - val_acc: 0.9182\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3078 - acc: 0.9180 - val_loss: 0.2571 - val_acc: 0.9517\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3650 - acc: 0.9180 - val_loss: 0.3247 - val_acc: 0.9517\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4462 - acc: 0.8453 - val_loss: 0.4344 - val_acc: 0.8550\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4334 - acc: 0.8453 - val_loss: 0.4667 - val_acc: 0.8550\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4300 - acc: 0.8453 - val_loss: 0.4243 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4281 - acc: 0.8453 - val_loss: 0.4197 - val_acc: 0.8550\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4266 - acc: 0.8453 - val_loss: 0.4177 - val_acc: 0.8550\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4263 - acc: 0.8453 - val_loss: 0.4156 - val_acc: 0.8550\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4255 - acc: 0.8453 - val_loss: 0.4150 - val_acc: 0.8550\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4251 - acc: 0.8453 - val_loss: 0.4145 - val_acc: 0.8550\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4242 - acc: 0.8453 - val_loss: 0.4145 - val_acc: 0.8550\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4238 - acc: 0.8453 - val_loss: 0.4123 - val_acc: 0.8550\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4219 - acc: 0.8453 - val_loss: 0.4143 - val_acc: 0.8550\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4219 - acc: 0.8453 - val_loss: 0.4152 - val_acc: 0.8550\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4095 - acc: 0.8639 - val_loss: 0.5616 - val_acc: 0.7770\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4036 - acc: 0.8639 - val_loss: 0.5632 - val_acc: 0.7770\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3998 - acc: 0.8639 - val_loss: 0.5704 - val_acc: 0.7770\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3982 - acc: 0.8639 - val_loss: 0.5650 - val_acc: 0.7770\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3973 - acc: 0.8639 - val_loss: 0.5649 - val_acc: 0.7770\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3966 - acc: 0.8639 - val_loss: 0.5729 - val_acc: 0.7770\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3960 - acc: 0.8639 - val_loss: 0.5626 - val_acc: 0.7770\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3950 - acc: 0.8639 - val_loss: 0.5685 - val_acc: 0.7770\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3939 - acc: 0.8639 - val_loss: 0.5718 - val_acc: 0.7770\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3940 - acc: 0.8639 - val_loss: 0.5742 - val_acc: 0.7770\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4112 - acc: 0.8658 - val_loss: 0.4030 - val_acc: 0.8699\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3990 - acc: 0.8658 - val_loss: 0.3877 - val_acc: 0.8699\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3891 - acc: 0.8658 - val_loss: 0.3885 - val_acc: 0.8699\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3855 - acc: 0.8658 - val_loss: 0.3920 - val_acc: 0.8699\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3844 - acc: 0.8658 - val_loss: 0.3930 - val_acc: 0.8699\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3830 - acc: 0.8658 - val_loss: 0.3965 - val_acc: 0.8699\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3832 - acc: 0.8658 - val_loss: 0.3963 - val_acc: 0.8699\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 2s 14ms/step - loss: 0.6011 - acc: 0.7571 - val_loss: 0.6893 - val_acc: 0.6495\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.5583 - acc: 0.7571 - val_loss: 0.6856 - val_acc: 0.6495\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5556 - acc: 0.7571 - val_loss: 0.6872 - val_acc: 0.6495\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5513 - acc: 0.7571 - val_loss: 0.6878 - val_acc: 0.6495\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5505 - acc: 0.7571 - val_loss: 0.6819 - val_acc: 0.6495\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5491 - acc: 0.7571 - val_loss: 0.6804 - val_acc: 0.6495\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5489 - acc: 0.7571 - val_loss: 0.6863 - val_acc: 0.6495\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5473 - acc: 0.7571 - val_loss: 0.6822 - val_acc: 0.6495\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5454 - acc: 0.7571 - val_loss: 0.6867 - val_acc: 0.6495\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5440 - acc: 0.7571 - val_loss: 0.6959 - val_acc: 0.6495\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5449 - acc: 0.7571 - val_loss: 0.6989 - val_acc: 0.6495\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4803 - acc: 0.8201 - val_loss: 0.5872 - val_acc: 0.7361\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4756 - acc: 0.8201 - val_loss: 0.5920 - val_acc: 0.7361\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4726 - acc: 0.8201 - val_loss: 0.5926 - val_acc: 0.7361\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4712 - acc: 0.8201 - val_loss: 0.5984 - val_acc: 0.7361\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4703 - acc: 0.8201 - val_loss: 0.5990 - val_acc: 0.7361\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4687 - acc: 0.8201 - val_loss: 0.5986 - val_acc: 0.7361\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4684 - acc: 0.8201 - val_loss: 0.6016 - val_acc: 0.7361\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4675 - acc: 0.8201 - val_loss: 0.5957 - val_acc: 0.7361\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4676 - acc: 0.8201 - val_loss: 0.5926 - val_acc: 0.7361\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4096 - acc: 0.8602 - val_loss: 0.5231 - val_acc: 0.7918\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4038 - acc: 0.8602 - val_loss: 0.5289 - val_acc: 0.7918\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4036 - acc: 0.8602 - val_loss: 0.5196 - val_acc: 0.7918\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4031 - acc: 0.8602 - val_loss: 0.5316 - val_acc: 0.7918\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4038 - acc: 0.8602 - val_loss: 0.5292 - val_acc: 0.7918\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3360 - acc: 0.9171 - val_loss: 0.2229 - val_acc: 0.9442\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3361 - acc: 0.9171 - val_loss: 0.2348 - val_acc: 0.9442\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3462 - acc: 0.8956 - val_loss: 0.4652 - val_acc: 0.8364\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3371 - acc: 0.8956 - val_loss: 0.4650 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3310 - acc: 0.8956 - val_loss: 0.5166 - val_acc: 0.8364\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3302 - acc: 0.8956 - val_loss: 0.5164 - val_acc: 0.8364\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3307 - acc: 0.8956 - val_loss: 0.5311 - val_acc: 0.8364\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2675 - acc: 0.9348 - val_loss: 0.2617 - val_acc: 0.9331\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2483 - acc: 0.9348 - val_loss: 0.2545 - val_acc: 0.9331\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2419 - acc: 0.9348 - val_loss: 0.2465 - val_acc: 0.9331\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2425 - acc: 0.9348 - val_loss: 0.2439 - val_acc: 0.9331\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2583 - acc: 0.9273 - val_loss: 0.3984 - val_acc: 0.8662\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2588 - acc: 0.9273 - val_loss: 0.4345 - val_acc: 0.8662\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5326 - acc: 0.8108 - val_loss: 0.4569 - val_acc: 0.8401\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4841 - acc: 0.8108 - val_loss: 0.4492 - val_acc: 0.8401\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4788 - acc: 0.8108 - val_loss: 0.4455 - val_acc: 0.8401\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4820 - acc: 0.8108 - val_loss: 0.4497 - val_acc: 0.8401\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5094 - acc: 0.8006 - val_loss: 0.5363 - val_acc: 0.7770\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5017 - acc: 0.8006 - val_loss: 0.5364 - val_acc: 0.7770\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4983 - acc: 0.8006 - val_loss: 0.5366 - val_acc: 0.7770\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4960 - acc: 0.8006 - val_loss: 0.5356 - val_acc: 0.7770\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4954 - acc: 0.8006 - val_loss: 0.5367 - val_acc: 0.7770\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4940 - acc: 0.8006 - val_loss: 0.5365 - val_acc: 0.7770\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4917 - acc: 0.8006 - val_loss: 0.5363 - val_acc: 0.7770\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4917 - acc: 0.8006 - val_loss: 0.5375 - val_acc: 0.7770\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4912 - acc: 0.8006 - val_loss: 0.5382 - val_acc: 0.7770\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4906 - acc: 0.8006 - val_loss: 0.5380 - val_acc: 0.7770\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4906 - acc: 0.8006 - val_loss: 0.5377 - val_acc: 0.7770\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3366 - acc: 0.9599 - val_loss: 0.7900 - val_acc: 0.8885\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4419 - acc: 0.9599 - val_loss: 0.4974 - val_acc: 0.8885\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 2s 11ms/step - loss: 0.4335 - acc: 0.8856 - val_loss: 0.3131 - val_acc: 0.9125\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.4127 - acc: 0.8856 - val_loss: 0.3802 - val_acc: 0.9125\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.4405 - acc: 0.8808 - val_loss: 0.3558 - val_acc: 0.9125\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 2s 14ms/step - loss: 0.5592 - acc: 0.7604 - val_loss: 0.6119 - val_acc: 0.7202\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5487 - acc: 0.7604 - val_loss: 0.6057 - val_acc: 0.7202\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5469 - acc: 0.7604 - val_loss: 0.6025 - val_acc: 0.7202\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5449 - acc: 0.7604 - val_loss: 0.6038 - val_acc: 0.7202\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5437 - acc: 0.7604 - val_loss: 0.6072 - val_acc: 0.7202\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5440 - acc: 0.7604 - val_loss: 0.6071 - val_acc: 0.7202\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5398 - acc: 0.7776 - val_loss: 0.6565 - val_acc: 0.6716\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5333 - acc: 0.7776 - val_loss: 0.6596 - val_acc: 0.6716\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5306 - acc: 0.7776 - val_loss: 0.6626 - val_acc: 0.6716\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5297 - acc: 0.7776 - val_loss: 0.6627 - val_acc: 0.6716\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5287 - acc: 0.7776 - val_loss: 0.6635 - val_acc: 0.6716\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5277 - acc: 0.7776 - val_loss: 0.6682 - val_acc: 0.6716\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5267 - acc: 0.7776 - val_loss: 0.6647 - val_acc: 0.6716\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5257 - acc: 0.7776 - val_loss: 0.6661 - val_acc: 0.6716\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5258 - acc: 0.7776 - val_loss: 0.6693 - val_acc: 0.6716\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3755 - acc: 0.8942 - val_loss: 0.5119 - val_acc: 0.8134\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3462 - acc: 0.8942 - val_loss: 0.5247 - val_acc: 0.8134\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3398 - acc: 0.8942 - val_loss: 0.5050 - val_acc: 0.8134\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3341 - acc: 0.8942 - val_loss: 0.5182 - val_acc: 0.8134\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3332 - acc: 0.8942 - val_loss: 0.5190 - val_acc: 0.8134\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3330 - acc: 0.8942 - val_loss: 0.5263 - val_acc: 0.8134\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3291 - acc: 0.8942 - val_loss: 0.5202 - val_acc: 0.8134\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3285 - acc: 0.8942 - val_loss: 0.5445 - val_acc: 0.8134\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3392 - acc: 0.8942 - val_loss: 0.5494 - val_acc: 0.8134\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3372 - acc: 0.9236 - val_loss: 0.2314 - val_acc: 0.9517\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2843 - acc: 0.9236 - val_loss: 0.2209 - val_acc: 0.9517\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2689 - acc: 0.9236 - val_loss: 0.2015 - val_acc: 0.9517\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2620 - acc: 0.9236 - val_loss: 0.2015 - val_acc: 0.9517\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2580 - acc: 0.9236 - val_loss: 0.1991 - val_acc: 0.9517\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2514 - acc: 0.9236 - val_loss: 0.2048 - val_acc: 0.9517\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2504 - acc: 0.9236 - val_loss: 0.2382 - val_acc: 0.9517\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2460 - acc: 0.9236 - val_loss: 0.2289 - val_acc: 0.9517\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2432 - acc: 0.9236 - val_loss: 0.2304 - val_acc: 0.9517\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2454 - acc: 0.9236 - val_loss: 0.2277 - val_acc: 0.9517\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 13ms/step - loss: 0.4686 - acc: 0.9170 - val_loss: 0.3303 - val_acc: 0.9313\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3237 - acc: 0.9170 - val_loss: 0.3038 - val_acc: 0.9313\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.2964 - acc: 0.9170 - val_loss: 0.3351 - val_acc: 0.9313\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.2889 - acc: 0.9170 - val_loss: 0.2757 - val_acc: 0.9313\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.2825 - acc: 0.9170 - val_loss: 0.2746 - val_acc: 0.9313\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.2781 - acc: 0.9170 - val_loss: 0.3754 - val_acc: 0.9313\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.2767 - acc: 0.9170 - val_loss: 0.2807 - val_acc: 0.9313\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.2721 - acc: 0.9170 - val_loss: 0.2819 - val_acc: 0.9313\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.2731 - acc: 0.9170 - val_loss: 0.2652 - val_acc: 0.9313\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4226 - acc: 0.8638 - val_loss: 0.3499 - val_acc: 0.8996\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3973 - acc: 0.8638 - val_loss: 0.3438 - val_acc: 0.8996\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3968 - acc: 0.8638 - val_loss: 0.3428 - val_acc: 0.8996\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3949 - acc: 0.8638 - val_loss: 0.3421 - val_acc: 0.8996\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3941 - acc: 0.8638 - val_loss: 0.3440 - val_acc: 0.8996\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3941 - acc: 0.8638 - val_loss: 0.3400 - val_acc: 0.8996\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 0.6726 - acc: 0.6815 - val_loss: 0.7012 - val_acc: 0.5870\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6236 - acc: 0.6815 - val_loss: 0.6928 - val_acc: 0.5870\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6208 - acc: 0.6815 - val_loss: 0.6937 - val_acc: 0.5870\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6202 - acc: 0.6815 - val_loss: 0.6950 - val_acc: 0.5870\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6189 - acc: 0.6815 - val_loss: 0.6954 - val_acc: 0.5870\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6179 - acc: 0.6815 - val_loss: 0.6956 - val_acc: 0.5870\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.6184 - acc: 0.6815 - val_loss: 0.6960 - val_acc: 0.5870\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4182 - acc: 0.8872 - val_loss: 0.3569 - val_acc: 0.8885\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3643 - acc: 0.8882 - val_loss: 0.3494 - val_acc: 0.8885\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3541 - acc: 0.8882 - val_loss: 0.3491 - val_acc: 0.8885\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3518 - acc: 0.8882 - val_loss: 0.3503 - val_acc: 0.8885\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3495 - acc: 0.8882 - val_loss: 0.3506 - val_acc: 0.8885\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3473 - acc: 0.8882 - val_loss: 0.3506 - val_acc: 0.8885\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3457 - acc: 0.8882 - val_loss: 0.3495 - val_acc: 0.8885\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3446 - acc: 0.8882 - val_loss: 0.3505 - val_acc: 0.8885\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3454 - acc: 0.8882 - val_loss: 0.3514 - val_acc: 0.8885\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5687 - acc: 0.7661 - val_loss: 0.6230 - val_acc: 0.6877\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5475 - acc: 0.7661 - val_loss: 0.6308 - val_acc: 0.6877\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5447 - acc: 0.7661 - val_loss: 0.6337 - val_acc: 0.6877\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5433 - acc: 0.7661 - val_loss: 0.6302 - val_acc: 0.6877\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5419 - acc: 0.7661 - val_loss: 0.6365 - val_acc: 0.6877\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5408 - acc: 0.7661 - val_loss: 0.6358 - val_acc: 0.6877\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5408 - acc: 0.7661 - val_loss: 0.6387 - val_acc: 0.6877\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3848 - acc: 0.8861 - val_loss: 0.3840 - val_acc: 0.8769\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3567 - acc: 0.8861 - val_loss: 0.3752 - val_acc: 0.8769\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3560 - acc: 0.8861 - val_loss: 0.3751 - val_acc: 0.8769\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3551 - acc: 0.8861 - val_loss: 0.3793 - val_acc: 0.8769\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3541 - acc: 0.8861 - val_loss: 0.3757 - val_acc: 0.8769\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3524 - acc: 0.8861 - val_loss: 0.3752 - val_acc: 0.8769\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3522 - acc: 0.8861 - val_loss: 0.3737 - val_acc: 0.8769\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3503 - acc: 0.8861 - val_loss: 0.3753 - val_acc: 0.8769\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3508 - acc: 0.8861 - val_loss: 0.3799 - val_acc: 0.8769\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2083 - acc: 0.9655 - val_loss: 0.3579 - val_acc: 0.9554\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2192 - acc: 0.9655 - val_loss: 0.3657 - val_acc: 0.9554\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5458 - acc: 0.8333 - val_loss: 0.5365 - val_acc: 0.7790\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4526 - acc: 0.8333 - val_loss: 0.5619 - val_acc: 0.7790\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4500 - acc: 0.8333 - val_loss: 0.5649 - val_acc: 0.7790\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4477 - acc: 0.8333 - val_loss: 0.5577 - val_acc: 0.7790\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4453 - acc: 0.8333 - val_loss: 0.5607 - val_acc: 0.7790\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4452 - acc: 0.8333 - val_loss: 0.5605 - val_acc: 0.7790\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4442 - acc: 0.8333 - val_loss: 0.5588 - val_acc: 0.7790\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4441 - acc: 0.8333 - val_loss: 0.5603 - val_acc: 0.7790\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4426 - acc: 0.8333 - val_loss: 0.5676 - val_acc: 0.7790\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4428 - acc: 0.8333 - val_loss: 0.5665 - val_acc: 0.7790\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4717 - acc: 0.8274 - val_loss: 0.4337 - val_acc: 0.8545\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4614 - acc: 0.8274 - val_loss: 0.4257 - val_acc: 0.8545\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4597 - acc: 0.8274 - val_loss: 0.4268 - val_acc: 0.8545\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4570 - acc: 0.8274 - val_loss: 0.4284 - val_acc: 0.8545\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4544 - acc: 0.8274 - val_loss: 0.4263 - val_acc: 0.8545\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4528 - acc: 0.8274 - val_loss: 0.4313 - val_acc: 0.8545\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4507 - acc: 0.8274 - val_loss: 0.4294 - val_acc: 0.8545\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4503 - acc: 0.8274 - val_loss: 0.4316 - val_acc: 0.8545\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4507 - acc: 0.8274 - val_loss: 0.4318 - val_acc: 0.8545\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4517 - acc: 0.8388 - val_loss: 0.4708 - val_acc: 0.8216\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4459 - acc: 0.8388 - val_loss: 0.4731 - val_acc: 0.8216\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4432 - acc: 0.8388 - val_loss: 0.4739 - val_acc: 0.8216\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4407 - acc: 0.8388 - val_loss: 0.4762 - val_acc: 0.8216\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4390 - acc: 0.8388 - val_loss: 0.4786 - val_acc: 0.8216\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4376 - acc: 0.8388 - val_loss: 0.4797 - val_acc: 0.8216\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4379 - acc: 0.8388 - val_loss: 0.4833 - val_acc: 0.8216\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4091 - acc: 0.8593 - val_loss: 0.4422 - val_acc: 0.8401\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4081 - acc: 0.8593 - val_loss: 0.4417 - val_acc: 0.8401\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4054 - acc: 0.8593 - val_loss: 0.4430 - val_acc: 0.8401\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4039 - acc: 0.8593 - val_loss: 0.4469 - val_acc: 0.8401\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4041 - acc: 0.8593 - val_loss: 0.4488 - val_acc: 0.8401\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 14ms/step - loss: 0.5341 - acc: 0.7816 - val_loss: 0.4749 - val_acc: 0.8261\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5233 - acc: 0.7816 - val_loss: 0.4796 - val_acc: 0.8261\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5228 - acc: 0.7816 - val_loss: 0.4780 - val_acc: 0.8261\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5223 - acc: 0.7816 - val_loss: 0.4776 - val_acc: 0.8261\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5219 - acc: 0.7816 - val_loss: 0.4764 - val_acc: 0.8261\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5215 - acc: 0.7816 - val_loss: 0.4782 - val_acc: 0.8261\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5209 - acc: 0.7816 - val_loss: 0.4811 - val_acc: 0.8261\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5209 - acc: 0.7816 - val_loss: 0.4779 - val_acc: 0.8261\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5203 - acc: 0.7816 - val_loss: 0.4779 - val_acc: 0.8261\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5218 - acc: 0.7816 - val_loss: 0.4801 - val_acc: 0.8261\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4439 - acc: 0.8397 - val_loss: 0.4764 - val_acc: 0.8216\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4360 - acc: 0.8397 - val_loss: 0.4750 - val_acc: 0.8216\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4327 - acc: 0.8397 - val_loss: 0.4762 - val_acc: 0.8216\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4311 - acc: 0.8397 - val_loss: 0.4748 - val_acc: 0.8216\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4294 - acc: 0.8397 - val_loss: 0.4796 - val_acc: 0.8216\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4286 - acc: 0.8397 - val_loss: 0.4730 - val_acc: 0.8216\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4291 - acc: 0.8397 - val_loss: 0.4753 - val_acc: 0.8216\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4982 - acc: 0.8090 - val_loss: 0.5376 - val_acc: 0.7782\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4891 - acc: 0.8090 - val_loss: 0.5319 - val_acc: 0.7782\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4875 - acc: 0.8090 - val_loss: 0.5382 - val_acc: 0.7782\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4822 - acc: 0.8090 - val_loss: 0.5400 - val_acc: 0.7782\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4815 - acc: 0.8090 - val_loss: 0.5420 - val_acc: 0.7782\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4812 - acc: 0.8090 - val_loss: 0.5410 - val_acc: 0.7782\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4816 - acc: 0.8090 - val_loss: 0.5380 - val_acc: 0.7782\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3594 - acc: 0.8872 - val_loss: 0.3210 - val_acc: 0.9108\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3484 - acc: 0.8872 - val_loss: 0.3160 - val_acc: 0.9108\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3443 - acc: 0.8872 - val_loss: 0.3714 - val_acc: 0.9108\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3498 - acc: 0.8872 - val_loss: 0.3235 - val_acc: 0.9108\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4600 - acc: 0.8313 - val_loss: 0.5242 - val_acc: 0.7955\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4512 - acc: 0.8313 - val_loss: 0.5267 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4513 - acc: 0.8313 - val_loss: 0.5304 - val_acc: 0.7955\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3125 - acc: 0.9199 - val_loss: 0.3734 - val_acc: 0.9219\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3637 - acc: 0.9087 - val_loss: 0.3908 - val_acc: 0.9257\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4894 - acc: 0.8406 - val_loss: 0.4294 - val_acc: 0.8550\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4527 - acc: 0.8406 - val_loss: 0.4258 - val_acc: 0.8550\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4445 - acc: 0.8406 - val_loss: 0.4221 - val_acc: 0.8550\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4399 - acc: 0.8406 - val_loss: 0.4233 - val_acc: 0.8550\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4379 - acc: 0.8406 - val_loss: 0.4234 - val_acc: 0.8550\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4358 - acc: 0.8406 - val_loss: 0.4252 - val_acc: 0.8550\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4347 - acc: 0.8406 - val_loss: 0.4240 - val_acc: 0.8550\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4348 - acc: 0.8406 - val_loss: 0.4256 - val_acc: 0.8550\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4096 - acc: 0.8760 - val_loss: 0.4503 - val_acc: 0.8364\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3817 - acc: 0.8760 - val_loss: 0.4538 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3759 - acc: 0.8760 - val_loss: 0.4543 - val_acc: 0.8364\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3730 - acc: 0.8760 - val_loss: 0.4608 - val_acc: 0.8364\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3711 - acc: 0.8760 - val_loss: 0.4603 - val_acc: 0.8364\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3700 - acc: 0.8760 - val_loss: 0.4615 - val_acc: 0.8364\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3683 - acc: 0.8760 - val_loss: 0.4622 - val_acc: 0.8364\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3674 - acc: 0.8760 - val_loss: 0.4600 - val_acc: 0.8364\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3668 - acc: 0.8760 - val_loss: 0.4610 - val_acc: 0.8364\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3668 - acc: 0.8760 - val_loss: 0.4608 - val_acc: 0.8364\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3666 - acc: 0.8760 - val_loss: 0.4615 - val_acc: 0.8364\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3654 - acc: 0.8760 - val_loss: 0.4605 - val_acc: 0.8364\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3636 - acc: 0.8760 - val_loss: 0.4653 - val_acc: 0.8364\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3635 - acc: 0.8760 - val_loss: 0.4712 - val_acc: 0.8364\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3613 - acc: 0.8760 - val_loss: 0.4743 - val_acc: 0.8364\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3609 - acc: 0.8760 - val_loss: 0.4651 - val_acc: 0.8364\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3612 - acc: 0.8760 - val_loss: 0.4683 - val_acc: 0.8364\n",
      "Epoch 17: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4650 - acc: 0.8341 - val_loss: 0.5108 - val_acc: 0.7993\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4530 - acc: 0.8341 - val_loss: 0.5107 - val_acc: 0.7993\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4515 - acc: 0.8341 - val_loss: 0.5123 - val_acc: 0.7993\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4473 - acc: 0.8341 - val_loss: 0.5132 - val_acc: 0.7993\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4469 - acc: 0.8341 - val_loss: 0.5102 - val_acc: 0.7993\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4458 - acc: 0.8341 - val_loss: 0.5088 - val_acc: 0.7993\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4461 - acc: 0.8341 - val_loss: 0.5085 - val_acc: 0.7993\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4481 - acc: 0.8369 - val_loss: 0.5559 - val_acc: 0.7658\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4459 - acc: 0.8369 - val_loss: 0.5568 - val_acc: 0.7658\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4450 - acc: 0.8369 - val_loss: 0.5656 - val_acc: 0.7658\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4436 - acc: 0.8369 - val_loss: 0.5597 - val_acc: 0.7658\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4428 - acc: 0.8369 - val_loss: 0.5583 - val_acc: 0.7658\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4419 - acc: 0.8369 - val_loss: 0.5606 - val_acc: 0.7658\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4409 - acc: 0.8369 - val_loss: 0.5616 - val_acc: 0.7658\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4407 - acc: 0.8369 - val_loss: 0.5644 - val_acc: 0.7658\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4401 - acc: 0.8369 - val_loss: 0.5619 - val_acc: 0.7658\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4391 - acc: 0.8369 - val_loss: 0.5600 - val_acc: 0.7658\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4394 - acc: 0.8369 - val_loss: 0.5645 - val_acc: 0.7658\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4694 - acc: 0.8229 - val_loss: 0.5758 - val_acc: 0.7509\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4688 - acc: 0.8229 - val_loss: 0.5846 - val_acc: 0.7509\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4686 - acc: 0.8229 - val_loss: 0.5776 - val_acc: 0.7509\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4679 - acc: 0.8229 - val_loss: 0.5811 - val_acc: 0.7509\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4671 - acc: 0.8229 - val_loss: 0.5787 - val_acc: 0.7509\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4665 - acc: 0.8229 - val_loss: 0.5868 - val_acc: 0.7509\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4675 - acc: 0.8229 - val_loss: 0.5811 - val_acc: 0.7509\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4738 - acc: 0.8183 - val_loss: 0.4738 - val_acc: 0.8216\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4721 - acc: 0.8183 - val_loss: 0.4742 - val_acc: 0.8216\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4735 - acc: 0.8183 - val_loss: 0.4723 - val_acc: 0.8216\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5273 - acc: 0.7857 - val_loss: 0.5872 - val_acc: 0.7303\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5222 - acc: 0.7857 - val_loss: 0.5902 - val_acc: 0.7303\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5200 - acc: 0.7857 - val_loss: 0.5907 - val_acc: 0.7303\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5194 - acc: 0.7857 - val_loss: 0.5908 - val_acc: 0.7303\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5199 - acc: 0.7857 - val_loss: 0.5912 - val_acc: 0.7303\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4757 - acc: 0.8183 - val_loss: 0.4677 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4724 - acc: 0.8183 - val_loss: 0.4679 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4707 - acc: 0.8183 - val_loss: 0.4683 - val_acc: 0.8290\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4696 - acc: 0.8183 - val_loss: 0.4653 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4708 - acc: 0.8183 - val_loss: 0.4648 - val_acc: 0.8290\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4645 - acc: 0.8276 - val_loss: 0.6377 - val_acc: 0.7175\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4598 - acc: 0.8276 - val_loss: 0.6314 - val_acc: 0.7175\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4574 - acc: 0.8276 - val_loss: 0.6349 - val_acc: 0.7175\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4556 - acc: 0.8276 - val_loss: 0.6413 - val_acc: 0.7175\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4549 - acc: 0.8276 - val_loss: 0.6337 - val_acc: 0.7175\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4532 - acc: 0.8276 - val_loss: 0.6328 - val_acc: 0.7175\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4529 - acc: 0.8276 - val_loss: 0.6315 - val_acc: 0.7175\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4518 - acc: 0.8276 - val_loss: 0.6316 - val_acc: 0.7175\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4514 - acc: 0.8276 - val_loss: 0.6340 - val_acc: 0.7175\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4513 - acc: 0.8276 - val_loss: 0.6392 - val_acc: 0.7175\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4506 - acc: 0.8276 - val_loss: 0.6353 - val_acc: 0.7175\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4500 - acc: 0.8276 - val_loss: 0.6341 - val_acc: 0.7175\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4506 - acc: 0.8276 - val_loss: 0.6390 - val_acc: 0.7175\n",
      "Epoch 13: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3582 - acc: 0.8938 - val_loss: 0.3492 - val_acc: 0.8848\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3439 - acc: 0.8938 - val_loss: 0.3456 - val_acc: 0.8848\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3416 - acc: 0.8938 - val_loss: 0.3492 - val_acc: 0.8848\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3350 - acc: 0.8938 - val_loss: 0.3469 - val_acc: 0.8848\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3329 - acc: 0.8938 - val_loss: 0.3443 - val_acc: 0.8848\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3356 - acc: 0.8938 - val_loss: 0.3502 - val_acc: 0.8848\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5218 - acc: 0.8000 - val_loss: 0.5196 - val_acc: 0.7940\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5022 - acc: 0.8000 - val_loss: 0.5181 - val_acc: 0.7940\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5006 - acc: 0.8000 - val_loss: 0.5163 - val_acc: 0.7940\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4995 - acc: 0.8000 - val_loss: 0.5166 - val_acc: 0.7940\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4985 - acc: 0.8000 - val_loss: 0.5151 - val_acc: 0.7940\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4986 - acc: 0.8000 - val_loss: 0.5147 - val_acc: 0.7940\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3804 - acc: 0.8774 - val_loss: 0.2966 - val_acc: 0.9240\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3706 - acc: 0.8774 - val_loss: 0.2922 - val_acc: 0.9240\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3686 - acc: 0.8774 - val_loss: 0.2931 - val_acc: 0.9240\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3672 - acc: 0.8774 - val_loss: 0.2945 - val_acc: 0.9240\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3671 - acc: 0.8774 - val_loss: 0.2958 - val_acc: 0.9240\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3666 - acc: 0.8774 - val_loss: 0.3007 - val_acc: 0.9240\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3660 - acc: 0.8774 - val_loss: 0.2983 - val_acc: 0.9240\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3644 - acc: 0.8774 - val_loss: 0.3011 - val_acc: 0.9240\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3660 - acc: 0.8774 - val_loss: 0.3002 - val_acc: 0.9240\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.5307 - acc: 0.7829 - val_loss: 0.5913 - val_acc: 0.7323\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5253 - acc: 0.7829 - val_loss: 0.6023 - val_acc: 0.7323\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5247 - acc: 0.7829 - val_loss: 0.5927 - val_acc: 0.7323\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5234 - acc: 0.7829 - val_loss: 0.5987 - val_acc: 0.7323\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.5238 - acc: 0.7829 - val_loss: 0.5948 - val_acc: 0.7323\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4370 - acc: 0.8385 - val_loss: 0.5676 - val_acc: 0.7612\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4244 - acc: 0.8385 - val_loss: 0.5610 - val_acc: 0.7612\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4190 - acc: 0.8385 - val_loss: 0.5600 - val_acc: 0.7612\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4142 - acc: 0.8385 - val_loss: 0.5637 - val_acc: 0.7612\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4330 - acc: 0.8385 - val_loss: 0.5620 - val_acc: 0.7612\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4414 - acc: 0.8481 - val_loss: 0.5448 - val_acc: 0.7844\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4319 - acc: 0.8481 - val_loss: 0.5487 - val_acc: 0.7844\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4247 - acc: 0.8481 - val_loss: 0.5443 - val_acc: 0.7844\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4238 - acc: 0.8481 - val_loss: 0.5329 - val_acc: 0.7844\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4187 - acc: 0.8481 - val_loss: 0.5652 - val_acc: 0.7844\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4225 - acc: 0.8481 - val_loss: 0.5366 - val_acc: 0.7844\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4518 - acc: 0.8397 - val_loss: 0.4386 - val_acc: 0.8439\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4445 - acc: 0.8397 - val_loss: 0.4380 - val_acc: 0.8439\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4374 - acc: 0.8397 - val_loss: 0.4404 - val_acc: 0.8439\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4363 - acc: 0.8397 - val_loss: 0.4411 - val_acc: 0.8439\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4338 - acc: 0.8397 - val_loss: 0.4415 - val_acc: 0.8439\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4319 - acc: 0.8397 - val_loss: 0.4351 - val_acc: 0.8439\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4333 - acc: 0.8397 - val_loss: 0.4426 - val_acc: 0.8439\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.6513 - acc: 0.7158 - val_loss: 0.5897 - val_acc: 0.7292\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6236 - acc: 0.7158 - val_loss: 0.5820 - val_acc: 0.7292\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6047 - acc: 0.7158 - val_loss: 0.5790 - val_acc: 0.7292\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5942 - acc: 0.7158 - val_loss: 0.5787 - val_acc: 0.7292\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5898 - acc: 0.7158 - val_loss: 0.5801 - val_acc: 0.7292\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5853 - acc: 0.7158 - val_loss: 0.5799 - val_acc: 0.7292\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5825 - acc: 0.7158 - val_loss: 0.5778 - val_acc: 0.7292\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5795 - acc: 0.7158 - val_loss: 0.5749 - val_acc: 0.7292\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5769 - acc: 0.7158 - val_loss: 0.5734 - val_acc: 0.7292\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5742 - acc: 0.7158 - val_loss: 0.5736 - val_acc: 0.7292\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5719 - acc: 0.7158 - val_loss: 0.5735 - val_acc: 0.7292\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5700 - acc: 0.7158 - val_loss: 0.5737 - val_acc: 0.7292\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5679 - acc: 0.7158 - val_loss: 0.5738 - val_acc: 0.7292\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5666 - acc: 0.7158 - val_loss: 0.5744 - val_acc: 0.7292\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5653 - acc: 0.7158 - val_loss: 0.5754 - val_acc: 0.7292\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5631 - acc: 0.7158 - val_loss: 0.5762 - val_acc: 0.7292\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5619 - acc: 0.7105 - val_loss: 0.5774 - val_acc: 0.7292\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5610 - acc: 0.7105 - val_loss: 0.5782 - val_acc: 0.7292\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5598 - acc: 0.7105 - val_loss: 0.5794 - val_acc: 0.7292\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5581 - acc: 0.7105 - val_loss: 0.5798 - val_acc: 0.7292\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5576 - acc: 0.7105 - val_loss: 0.5808 - val_acc: 0.7292\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5559 - acc: 0.7105 - val_loss: 0.5798 - val_acc: 0.7292\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5550 - acc: 0.7105 - val_loss: 0.5801 - val_acc: 0.7292\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5539 - acc: 0.7105 - val_loss: 0.5805 - val_acc: 0.7292\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5540 - acc: 0.7105 - val_loss: 0.5788 - val_acc: 0.7292\n",
      "Epoch 25: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4011 - acc: 0.8854 - val_loss: 0.3615 - val_acc: 0.9182\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3693 - acc: 0.8872 - val_loss: 0.2984 - val_acc: 0.9182\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3603 - acc: 0.8872 - val_loss: 0.3043 - val_acc: 0.9182\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3541 - acc: 0.8872 - val_loss: 0.2889 - val_acc: 0.9182\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3550 - acc: 0.8872 - val_loss: 0.3050 - val_acc: 0.9182\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3848 - acc: 0.8733 - val_loss: 0.2794 - val_acc: 0.9283\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3794 - acc: 0.8733 - val_loss: 0.2794 - val_acc: 0.9283\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3769 - acc: 0.8733 - val_loss: 0.2765 - val_acc: 0.9283\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3766 - acc: 0.8733 - val_loss: 0.2821 - val_acc: 0.9283\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3749 - acc: 0.8733 - val_loss: 0.2739 - val_acc: 0.9283\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3740 - acc: 0.8733 - val_loss: 0.2767 - val_acc: 0.9283\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3717 - acc: 0.8733 - val_loss: 0.2714 - val_acc: 0.9283\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3717 - acc: 0.8733 - val_loss: 0.2748 - val_acc: 0.9283\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3713 - acc: 0.8733 - val_loss: 0.2735 - val_acc: 0.9283\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3728 - acc: 0.8733 - val_loss: 0.2750 - val_acc: 0.9283\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4404 - acc: 0.8461 - val_loss: 0.4085 - val_acc: 0.8642\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4252 - acc: 0.8461 - val_loss: 0.4056 - val_acc: 0.8642\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4198 - acc: 0.8461 - val_loss: 0.4059 - val_acc: 0.8642\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4174 - acc: 0.8461 - val_loss: 0.4098 - val_acc: 0.8642\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4172 - acc: 0.8461 - val_loss: 0.4086 - val_acc: 0.8642\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4160 - acc: 0.8461 - val_loss: 0.4096 - val_acc: 0.8642\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4152 - acc: 0.8461 - val_loss: 0.4248 - val_acc: 0.8642\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4167 - acc: 0.8461 - val_loss: 0.4127 - val_acc: 0.8642\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4028 - acc: 0.8872 - val_loss: 0.4553 - val_acc: 0.8439\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3832 - acc: 0.8872 - val_loss: 0.4625 - val_acc: 0.8439\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3754 - acc: 0.8872 - val_loss: 0.4520 - val_acc: 0.8439\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3593 - acc: 0.8872 - val_loss: 0.4592 - val_acc: 0.8439\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3586 - acc: 0.8872 - val_loss: 0.4534 - val_acc: 0.8439\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3576 - acc: 0.8872 - val_loss: 0.4512 - val_acc: 0.8439\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3563 - acc: 0.8872 - val_loss: 0.4554 - val_acc: 0.8439\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3555 - acc: 0.8872 - val_loss: 0.4528 - val_acc: 0.8439\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3546 - acc: 0.8872 - val_loss: 0.4462 - val_acc: 0.8439\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3526 - acc: 0.8872 - val_loss: 0.4554 - val_acc: 0.8439\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3508 - acc: 0.8872 - val_loss: 0.4473 - val_acc: 0.8439\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3494 - acc: 0.8872 - val_loss: 0.4476 - val_acc: 0.8439\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3501 - acc: 0.8872 - val_loss: 0.4496 - val_acc: 0.8439\n",
      "Epoch 13: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5013 - acc: 0.8071 - val_loss: 0.5789 - val_acc: 0.7472\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4894 - acc: 0.8071 - val_loss: 0.5791 - val_acc: 0.7472\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4869 - acc: 0.8071 - val_loss: 0.5795 - val_acc: 0.7472\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4872 - acc: 0.8071 - val_loss: 0.5813 - val_acc: 0.7472\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4189 - acc: 0.8602 - val_loss: 0.4477 - val_acc: 0.8364\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4076 - acc: 0.8602 - val_loss: 0.4480 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4025 - acc: 0.8602 - val_loss: 0.4459 - val_acc: 0.8364\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4010 - acc: 0.8602 - val_loss: 0.4455 - val_acc: 0.8364\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4002 - acc: 0.8602 - val_loss: 0.4470 - val_acc: 0.8364\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4005 - acc: 0.8602 - val_loss: 0.4499 - val_acc: 0.8364\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 44ms/step - loss: 0.2893 - acc: 0.9292 - val_loss: 0.1942 - val_acc: 0.9667\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2779 - acc: 0.9292 - val_loss: 0.1774 - val_acc: 0.9667\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2674 - acc: 0.9292 - val_loss: 0.1674 - val_acc: 0.9667\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2579 - acc: 0.9292 - val_loss: 0.1690 - val_acc: 0.9667\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2509 - acc: 0.9292 - val_loss: 0.1754 - val_acc: 0.9667\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2478 - acc: 0.9292 - val_loss: 0.1800 - val_acc: 0.9667\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2450 - acc: 0.9292 - val_loss: 0.1882 - val_acc: 0.9667\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2433 - acc: 0.9292 - val_loss: 0.1919 - val_acc: 0.9667\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2417 - acc: 0.9292 - val_loss: 0.2073 - val_acc: 0.9667\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2402 - acc: 0.9292 - val_loss: 0.2099 - val_acc: 0.9667\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2381 - acc: 0.9292 - val_loss: 0.3760 - val_acc: 0.9667\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2368 - acc: 0.9292 - val_loss: 0.2729 - val_acc: 0.9667\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2366 - acc: 0.9292 - val_loss: 0.3785 - val_acc: 0.9667\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2346 - acc: 0.9292 - val_loss: 0.3793 - val_acc: 0.9667\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2353 - acc: 0.9292 - val_loss: 0.3852 - val_acc: 0.9667\n",
      "Epoch 15: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4591 - acc: 0.8397 - val_loss: 0.4828 - val_acc: 0.8178\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4347 - acc: 0.8397 - val_loss: 0.4681 - val_acc: 0.8178\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4291 - acc: 0.8397 - val_loss: 0.4675 - val_acc: 0.8178\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4262 - acc: 0.8397 - val_loss: 0.4672 - val_acc: 0.8178\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4215 - acc: 0.8397 - val_loss: 0.4721 - val_acc: 0.8178\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4185 - acc: 0.8397 - val_loss: 0.4705 - val_acc: 0.8178\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4151 - acc: 0.8397 - val_loss: 0.4694 - val_acc: 0.8178\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4100 - acc: 0.8397 - val_loss: 0.4667 - val_acc: 0.8178\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4062 - acc: 0.8397 - val_loss: 0.4715 - val_acc: 0.8178\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4024 - acc: 0.8397 - val_loss: 0.4675 - val_acc: 0.8178\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3987 - acc: 0.8397 - val_loss: 0.4740 - val_acc: 0.8178\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3968 - acc: 0.8397 - val_loss: 0.4833 - val_acc: 0.8178\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3944 - acc: 0.8397 - val_loss: 0.4761 - val_acc: 0.8178\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3894 - acc: 0.8397 - val_loss: 0.4750 - val_acc: 0.8178\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3869 - acc: 0.8397 - val_loss: 0.4758 - val_acc: 0.8178\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3826 - acc: 0.8397 - val_loss: 0.4768 - val_acc: 0.8178\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3784 - acc: 0.8397 - val_loss: 0.4806 - val_acc: 0.8178\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3753 - acc: 0.8397 - val_loss: 0.4826 - val_acc: 0.8178\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3716 - acc: 0.8397 - val_loss: 0.4838 - val_acc: 0.8178\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3699 - acc: 0.8397 - val_loss: 0.4850 - val_acc: 0.8178\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3654 - acc: 0.8397 - val_loss: 0.4821 - val_acc: 0.8178\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3619 - acc: 0.8406 - val_loss: 0.4814 - val_acc: 0.8178\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3705 - acc: 0.8416 - val_loss: 0.4855 - val_acc: 0.8178\n",
      "Epoch 23: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.6968 - acc: 0.8322 - val_loss: 0.7111 - val_acc: 0.7881\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5408 - acc: 0.8360 - val_loss: 0.7480 - val_acc: 0.7881\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4667 - acc: 0.8453 - val_loss: 0.5506 - val_acc: 0.7881\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4451 - acc: 0.8453 - val_loss: 0.5342 - val_acc: 0.7881\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4404 - acc: 0.8453 - val_loss: 0.5289 - val_acc: 0.7881\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4332 - acc: 0.8453 - val_loss: 0.5281 - val_acc: 0.7881\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4323 - acc: 0.8453 - val_loss: 0.5285 - val_acc: 0.7881\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4302 - acc: 0.8453 - val_loss: 0.5234 - val_acc: 0.7881\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4309 - acc: 0.8453 - val_loss: 0.5222 - val_acc: 0.7881\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.2666 - acc: 0.9348 - val_loss: 0.2146 - val_acc: 0.9703\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2490 - acc: 0.9348 - val_loss: 0.2127 - val_acc: 0.9703\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2562 - acc: 0.9348 - val_loss: 0.2449 - val_acc: 0.9703\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4866 - acc: 0.8127 - val_loss: 0.5750 - val_acc: 0.7546\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4836 - acc: 0.8127 - val_loss: 0.5687 - val_acc: 0.7546\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4827 - acc: 0.8127 - val_loss: 0.5674 - val_acc: 0.7546\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4803 - acc: 0.8127 - val_loss: 0.5690 - val_acc: 0.7546\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4787 - acc: 0.8127 - val_loss: 0.5670 - val_acc: 0.7546\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4785 - acc: 0.8127 - val_loss: 0.5667 - val_acc: 0.7546\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4771 - acc: 0.8127 - val_loss: 0.5668 - val_acc: 0.7546\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4764 - acc: 0.8127 - val_loss: 0.5685 - val_acc: 0.7546\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4760 - acc: 0.8127 - val_loss: 0.5672 - val_acc: 0.7546\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4755 - acc: 0.8127 - val_loss: 0.5673 - val_acc: 0.7546\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4751 - acc: 0.8127 - val_loss: 0.5683 - val_acc: 0.7546\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4745 - acc: 0.8127 - val_loss: 0.5693 - val_acc: 0.7546\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4745 - acc: 0.8127 - val_loss: 0.5704 - val_acc: 0.7546\n",
      "Epoch 13: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2219 - acc: 0.9618 - val_loss: 0.3876 - val_acc: 0.8885\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2007 - acc: 0.9618 - val_loss: 0.7795 - val_acc: 0.8885\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1962 - acc: 0.9618 - val_loss: 0.3734 - val_acc: 0.8885\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1916 - acc: 0.9618 - val_loss: 0.6502 - val_acc: 0.8885\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.1914 - acc: 0.9618 - val_loss: 0.3544 - val_acc: 0.8885\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2810 - acc: 0.9618 - val_loss: 0.3969 - val_acc: 0.8885\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4723 - acc: 0.8798 - val_loss: 0.5800 - val_acc: 0.7658\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4162 - acc: 0.8798 - val_loss: 0.5722 - val_acc: 0.7658\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4010 - acc: 0.8798 - val_loss: 0.5736 - val_acc: 0.7658\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3891 - acc: 0.8798 - val_loss: 0.6155 - val_acc: 0.7658\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3752 - acc: 0.8798 - val_loss: 0.6223 - val_acc: 0.7658\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3708 - acc: 0.8798 - val_loss: 0.6211 - val_acc: 0.7658\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3697 - acc: 0.8798 - val_loss: 0.6305 - val_acc: 0.7658\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3721 - acc: 0.8798 - val_loss: 0.6292 - val_acc: 0.7658\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5490 - acc: 0.7829 - val_loss: 0.5755 - val_acc: 0.7584\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5341 - acc: 0.7829 - val_loss: 0.5619 - val_acc: 0.7584\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5280 - acc: 0.7829 - val_loss: 0.5565 - val_acc: 0.7584\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5251 - acc: 0.7829 - val_loss: 0.5542 - val_acc: 0.7584\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5243 - acc: 0.7829 - val_loss: 0.5567 - val_acc: 0.7584\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5235 - acc: 0.7829 - val_loss: 0.5559 - val_acc: 0.7584\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5227 - acc: 0.7829 - val_loss: 0.5519 - val_acc: 0.7584\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5216 - acc: 0.7829 - val_loss: 0.5537 - val_acc: 0.7584\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5208 - acc: 0.7829 - val_loss: 0.5528 - val_acc: 0.7584\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5201 - acc: 0.7829 - val_loss: 0.5522 - val_acc: 0.7584\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5194 - acc: 0.7829 - val_loss: 0.5518 - val_acc: 0.7584\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5188 - acc: 0.7829 - val_loss: 0.5528 - val_acc: 0.7584\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5187 - acc: 0.7829 - val_loss: 0.5531 - val_acc: 0.7584\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5180 - acc: 0.7829 - val_loss: 0.5527 - val_acc: 0.7584\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5186 - acc: 0.7829 - val_loss: 0.5512 - val_acc: 0.7584\n",
      "Epoch 15: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5329 - acc: 0.7763 - val_loss: 0.5545 - val_acc: 0.7621\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5302 - acc: 0.7763 - val_loss: 0.5547 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5300 - acc: 0.7763 - val_loss: 0.5560 - val_acc: 0.7621\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5291 - acc: 0.7763 - val_loss: 0.5562 - val_acc: 0.7621\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5288 - acc: 0.7763 - val_loss: 0.5561 - val_acc: 0.7621\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5287 - acc: 0.7763 - val_loss: 0.5559 - val_acc: 0.7621\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5289 - acc: 0.7763 - val_loss: 0.5557 - val_acc: 0.7621\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 0.5488 - acc: 0.7634 - val_loss: 0.6668 - val_acc: 0.6708\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5489 - acc: 0.7634 - val_loss: 0.6625 - val_acc: 0.6708\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5185 - acc: 0.7895 - val_loss: 0.5216 - val_acc: 0.7828\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5169 - acc: 0.7895 - val_loss: 0.5248 - val_acc: 0.7828\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5147 - acc: 0.7895 - val_loss: 0.5248 - val_acc: 0.7828\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5139 - acc: 0.7895 - val_loss: 0.5274 - val_acc: 0.7828\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5137 - acc: 0.7895 - val_loss: 0.5269 - val_acc: 0.7828\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5125 - acc: 0.7895 - val_loss: 0.5267 - val_acc: 0.7828\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5123 - acc: 0.7895 - val_loss: 0.5284 - val_acc: 0.7828\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5124 - acc: 0.7895 - val_loss: 0.5289 - val_acc: 0.7828\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4590 - acc: 0.8346 - val_loss: 0.4667 - val_acc: 0.8233\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4542 - acc: 0.8346 - val_loss: 0.4664 - val_acc: 0.8233\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4515 - acc: 0.8346 - val_loss: 0.4681 - val_acc: 0.8233\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4511 - acc: 0.8346 - val_loss: 0.4671 - val_acc: 0.8233\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4519 - acc: 0.8346 - val_loss: 0.4677 - val_acc: 0.8233\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5256 - acc: 0.7829 - val_loss: 0.4817 - val_acc: 0.8178\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5227 - acc: 0.7829 - val_loss: 0.4820 - val_acc: 0.8178\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5219 - acc: 0.7829 - val_loss: 0.4829 - val_acc: 0.8178\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5213 - acc: 0.7829 - val_loss: 0.4834 - val_acc: 0.8178\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5206 - acc: 0.7829 - val_loss: 0.4819 - val_acc: 0.8178\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5191 - acc: 0.7829 - val_loss: 0.4830 - val_acc: 0.8178\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5186 - acc: 0.7829 - val_loss: 0.4851 - val_acc: 0.8178\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5177 - acc: 0.7829 - val_loss: 0.4821 - val_acc: 0.8178\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5177 - acc: 0.7829 - val_loss: 0.4838 - val_acc: 0.8178\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5173 - acc: 0.7829 - val_loss: 0.4851 - val_acc: 0.8178\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5198 - acc: 0.7829 - val_loss: 0.4865 - val_acc: 0.8178\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3409 - acc: 0.9143 - val_loss: 0.5065 - val_acc: 0.8178\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3101 - acc: 0.9143 - val_loss: 0.4981 - val_acc: 0.8178\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3031 - acc: 0.9143 - val_loss: 0.5199 - val_acc: 0.8178\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3008 - acc: 0.9143 - val_loss: 0.4919 - val_acc: 0.8178\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3009 - acc: 0.9143 - val_loss: 0.4980 - val_acc: 0.8178\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4827 - acc: 0.8248 - val_loss: 0.5477 - val_acc: 0.7621\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4699 - acc: 0.8248 - val_loss: 0.5583 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4629 - acc: 0.8248 - val_loss: 0.5749 - val_acc: 0.7621\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4620 - acc: 0.8248 - val_loss: 0.5720 - val_acc: 0.7621\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4605 - acc: 0.8248 - val_loss: 0.5794 - val_acc: 0.7621\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4600 - acc: 0.8248 - val_loss: 0.5748 - val_acc: 0.7621\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4592 - acc: 0.8248 - val_loss: 0.5742 - val_acc: 0.7621\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4571 - acc: 0.8248 - val_loss: 0.5831 - val_acc: 0.7621\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4566 - acc: 0.8248 - val_loss: 0.5797 - val_acc: 0.7621\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4565 - acc: 0.8248 - val_loss: 0.5823 - val_acc: 0.7621\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4566 - acc: 0.8248 - val_loss: 0.5827 - val_acc: 0.7621\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5096 - acc: 0.7959 - val_loss: 0.4936 - val_acc: 0.8067\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5074 - acc: 0.7959 - val_loss: 0.4943 - val_acc: 0.8067\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5060 - acc: 0.7959 - val_loss: 0.4934 - val_acc: 0.8067\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5065 - acc: 0.7959 - val_loss: 0.4932 - val_acc: 0.8067\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3769 - acc: 0.8844 - val_loss: 0.6124 - val_acc: 0.7472\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3655 - acc: 0.8844 - val_loss: 0.6274 - val_acc: 0.7472\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3629 - acc: 0.8844 - val_loss: 0.6311 - val_acc: 0.7472\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3614 - acc: 0.8844 - val_loss: 0.6460 - val_acc: 0.7472\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3611 - acc: 0.8844 - val_loss: 0.6277 - val_acc: 0.7472\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3608 - acc: 0.8844 - val_loss: 0.6241 - val_acc: 0.7472\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3606 - acc: 0.8844 - val_loss: 0.6297 - val_acc: 0.7472\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3585 - acc: 0.8844 - val_loss: 0.6378 - val_acc: 0.7472\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3612 - acc: 0.8844 - val_loss: 0.6399 - val_acc: 0.7472\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.6305 - acc: 0.7343 - val_loss: 0.5972 - val_acc: 0.7303\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5917 - acc: 0.7343 - val_loss: 0.5871 - val_acc: 0.7303\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5862 - acc: 0.7343 - val_loss: 0.5840 - val_acc: 0.7303\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5833 - acc: 0.7343 - val_loss: 0.5824 - val_acc: 0.7303\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5822 - acc: 0.7343 - val_loss: 0.5817 - val_acc: 0.7303\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5819 - acc: 0.7343 - val_loss: 0.5812 - val_acc: 0.7303\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5808 - acc: 0.7343 - val_loss: 0.5809 - val_acc: 0.7303\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5803 - acc: 0.7343 - val_loss: 0.5807 - val_acc: 0.7303\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5797 - acc: 0.7343 - val_loss: 0.5812 - val_acc: 0.7303\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5802 - acc: 0.7343 - val_loss: 0.5809 - val_acc: 0.7303\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4642 - acc: 0.8378 - val_loss: 0.4623 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4453 - acc: 0.8378 - val_loss: 0.4624 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4420 - acc: 0.8378 - val_loss: 0.4676 - val_acc: 0.8290\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4396 - acc: 0.8378 - val_loss: 0.4701 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4376 - acc: 0.8378 - val_loss: 0.4737 - val_acc: 0.8290\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4366 - acc: 0.8378 - val_loss: 0.4759 - val_acc: 0.8290\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4364 - acc: 0.8378 - val_loss: 0.4752 - val_acc: 0.8290\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4376 - acc: 0.8378 - val_loss: 0.4748 - val_acc: 0.8290\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 19ms/step - loss: 0.5670 - acc: 0.7668 - val_loss: 0.5446 - val_acc: 0.7704\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5427 - acc: 0.7668 - val_loss: 0.5424 - val_acc: 0.7704\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5451 - acc: 0.7668 - val_loss: 0.5421 - val_acc: 0.7704\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5023 - acc: 0.8006 - val_loss: 0.5504 - val_acc: 0.7658\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5021 - acc: 0.8006 - val_loss: 0.5490 - val_acc: 0.7658\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5016 - acc: 0.8006 - val_loss: 0.5517 - val_acc: 0.7658\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5012 - acc: 0.8006 - val_loss: 0.5501 - val_acc: 0.7658\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4996 - acc: 0.8006 - val_loss: 0.5503 - val_acc: 0.7658\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4987 - acc: 0.8006 - val_loss: 0.5498 - val_acc: 0.7658\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4986 - acc: 0.8006 - val_loss: 0.5485 - val_acc: 0.7658\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4988 - acc: 0.8006 - val_loss: 0.5493 - val_acc: 0.7658\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 2s 23ms/step - loss: 0.4904 - acc: 0.8124 - val_loss: 0.4923 - val_acc: 0.8091\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4870 - acc: 0.8124 - val_loss: 0.4916 - val_acc: 0.8091\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4852 - acc: 0.8124 - val_loss: 0.4925 - val_acc: 0.8091\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4822 - acc: 0.8124 - val_loss: 0.4898 - val_acc: 0.8091\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4796 - acc: 0.8124 - val_loss: 0.4902 - val_acc: 0.8091\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4782 - acc: 0.8124 - val_loss: 0.4903 - val_acc: 0.8091\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4773 - acc: 0.8124 - val_loss: 0.4900 - val_acc: 0.8091\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4755 - acc: 0.8124 - val_loss: 0.4890 - val_acc: 0.8091\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4753 - acc: 0.8124 - val_loss: 0.4906 - val_acc: 0.8091\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4738 - acc: 0.8124 - val_loss: 0.4900 - val_acc: 0.8091\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4738 - acc: 0.8124 - val_loss: 0.4905 - val_acc: 0.8091\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5422 - acc: 0.7754 - val_loss: 0.5783 - val_acc: 0.7621\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5367 - acc: 0.7754 - val_loss: 0.5711 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5340 - acc: 0.7754 - val_loss: 0.5688 - val_acc: 0.7621\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5313 - acc: 0.7754 - val_loss: 0.5695 - val_acc: 0.7621\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5305 - acc: 0.7754 - val_loss: 0.5694 - val_acc: 0.7621\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5292 - acc: 0.7754 - val_loss: 0.5692 - val_acc: 0.7621\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5288 - acc: 0.7754 - val_loss: 0.5675 - val_acc: 0.7621\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5270 - acc: 0.7754 - val_loss: 0.5722 - val_acc: 0.7621\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5296 - acc: 0.7754 - val_loss: 0.5682 - val_acc: 0.7621\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 2s 30ms/step - loss: 0.4283 - acc: 0.8613 - val_loss: 0.4507 - val_acc: 0.8391\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4157 - acc: 0.8613 - val_loss: 0.4412 - val_acc: 0.8391\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4031 - acc: 0.8613 - val_loss: 0.4359 - val_acc: 0.8391\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3937 - acc: 0.8613 - val_loss: 0.4348 - val_acc: 0.8391\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3868 - acc: 0.8613 - val_loss: 0.4365 - val_acc: 0.8391\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3813 - acc: 0.8613 - val_loss: 0.4388 - val_acc: 0.8391\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3774 - acc: 0.8613 - val_loss: 0.4432 - val_acc: 0.8391\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3754 - acc: 0.8613 - val_loss: 0.4491 - val_acc: 0.8391\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3713 - acc: 0.8613 - val_loss: 0.4506 - val_acc: 0.8391\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3684 - acc: 0.8613 - val_loss: 0.4554 - val_acc: 0.8391\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3654 - acc: 0.8613 - val_loss: 0.4580 - val_acc: 0.8391\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3628 - acc: 0.8613 - val_loss: 0.4579 - val_acc: 0.8391\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3614 - acc: 0.8613 - val_loss: 0.4652 - val_acc: 0.8391\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3599 - acc: 0.8613 - val_loss: 0.4779 - val_acc: 0.8391\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3588 - acc: 0.8613 - val_loss: 0.4948 - val_acc: 0.8391\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3583 - acc: 0.8613 - val_loss: 0.4942 - val_acc: 0.8391\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3578 - acc: 0.8613 - val_loss: 0.5027 - val_acc: 0.8391\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3542 - acc: 0.8613 - val_loss: 0.5198 - val_acc: 0.8391\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3529 - acc: 0.8613 - val_loss: 0.7751 - val_acc: 0.8391\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.3530 - acc: 0.8613 - val_loss: 0.7775 - val_acc: 0.8391\n",
      "Epoch 20: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 2s 11ms/step - loss: 0.6844 - acc: 0.7463 - val_loss: 0.5628 - val_acc: 0.7704\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5772 - acc: 0.7463 - val_loss: 0.5496 - val_acc: 0.7704\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5696 - acc: 0.7463 - val_loss: 0.5438 - val_acc: 0.7704\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5663 - acc: 0.7463 - val_loss: 0.5432 - val_acc: 0.7704\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5660 - acc: 0.7463 - val_loss: 0.5458 - val_acc: 0.7704\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5653 - acc: 0.7463 - val_loss: 0.5481 - val_acc: 0.7704\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5629 - acc: 0.7463 - val_loss: 0.5491 - val_acc: 0.7704\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5621 - acc: 0.7463 - val_loss: 0.5512 - val_acc: 0.7704\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5617 - acc: 0.7463 - val_loss: 0.5512 - val_acc: 0.7704\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5608 - acc: 0.7463 - val_loss: 0.5537 - val_acc: 0.7704\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5597 - acc: 0.7463 - val_loss: 0.5575 - val_acc: 0.7704\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5593 - acc: 0.7463 - val_loss: 0.5613 - val_acc: 0.7704\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5585 - acc: 0.7463 - val_loss: 0.5587 - val_acc: 0.7704\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5585 - acc: 0.7463 - val_loss: 0.5603 - val_acc: 0.7704\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5585 - acc: 0.7463 - val_loss: 0.5607 - val_acc: 0.7704\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5580 - acc: 0.7463 - val_loss: 0.5626 - val_acc: 0.7704\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 0s 3ms/step - loss: 0.5581 - acc: 0.7463 - val_loss: 0.5661 - val_acc: 0.7704\n",
      "Epoch 17: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4411 - acc: 0.8555 - val_loss: 0.4964 - val_acc: 0.8067\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4288 - acc: 0.8555 - val_loss: 0.5007 - val_acc: 0.8067\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4223 - acc: 0.8555 - val_loss: 0.5049 - val_acc: 0.8067\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4156 - acc: 0.8555 - val_loss: 0.5061 - val_acc: 0.8067\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4134 - acc: 0.8555 - val_loss: 0.5135 - val_acc: 0.8067\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4111 - acc: 0.8555 - val_loss: 0.5199 - val_acc: 0.8067\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4087 - acc: 0.8555 - val_loss: 0.5238 - val_acc: 0.8067\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4097 - acc: 0.8555 - val_loss: 0.5197 - val_acc: 0.8067\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4555 - acc: 0.8366 - val_loss: 0.4253 - val_acc: 0.8545\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4475 - acc: 0.8366 - val_loss: 0.4254 - val_acc: 0.8545\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4442 - acc: 0.8366 - val_loss: 0.4267 - val_acc: 0.8545\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4431 - acc: 0.8366 - val_loss: 0.4272 - val_acc: 0.8545\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4412 - acc: 0.8366 - val_loss: 0.4312 - val_acc: 0.8545\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4425 - acc: 0.8366 - val_loss: 0.4292 - val_acc: 0.8545\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3019 - acc: 0.9189 - val_loss: 0.3086 - val_acc: 0.9182\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2850 - acc: 0.9189 - val_loss: 0.2991 - val_acc: 0.9182\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2824 - acc: 0.9189 - val_loss: 0.2926 - val_acc: 0.9182\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2812 - acc: 0.9189 - val_loss: 0.2955 - val_acc: 0.9182\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2780 - acc: 0.9189 - val_loss: 0.2968 - val_acc: 0.9182\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2779 - acc: 0.9189 - val_loss: 0.2986 - val_acc: 0.9182\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2801 - acc: 0.9189 - val_loss: 0.2984 - val_acc: 0.9182\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4484 - acc: 0.8394 - val_loss: 0.5602 - val_acc: 0.7643\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4407 - acc: 0.8394 - val_loss: 0.5633 - val_acc: 0.7643\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4392 - acc: 0.8394 - val_loss: 0.5717 - val_acc: 0.7643\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4398 - acc: 0.8394 - val_loss: 0.5719 - val_acc: 0.7643\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.3759 - acc: 0.8816 - val_loss: 0.4604 - val_acc: 0.8253\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3713 - acc: 0.8816 - val_loss: 0.4690 - val_acc: 0.8253\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3701 - acc: 0.8816 - val_loss: 0.4697 - val_acc: 0.8253\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3684 - acc: 0.8816 - val_loss: 0.4711 - val_acc: 0.8253\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3665 - acc: 0.8816 - val_loss: 0.4686 - val_acc: 0.8253\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3667 - acc: 0.8816 - val_loss: 0.4699 - val_acc: 0.8253\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3719 - acc: 0.8788 - val_loss: 0.5793 - val_acc: 0.7695\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3706 - acc: 0.8788 - val_loss: 0.5806 - val_acc: 0.7695\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3699 - acc: 0.8788 - val_loss: 0.5832 - val_acc: 0.7695\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3675 - acc: 0.8788 - val_loss: 0.5892 - val_acc: 0.7695\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3672 - acc: 0.8788 - val_loss: 0.5948 - val_acc: 0.7695\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3661 - acc: 0.8788 - val_loss: 0.5934 - val_acc: 0.7695\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3668 - acc: 0.8788 - val_loss: 0.5805 - val_acc: 0.7695\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4261 - acc: 0.8527 - val_loss: 0.7172 - val_acc: 0.6766\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4219 - acc: 0.8527 - val_loss: 0.7149 - val_acc: 0.6766\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4209 - acc: 0.8527 - val_loss: 0.7172 - val_acc: 0.6766\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4202 - acc: 0.8527 - val_loss: 0.7114 - val_acc: 0.6766\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4193 - acc: 0.8527 - val_loss: 0.7117 - val_acc: 0.6766\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4193 - acc: 0.8527 - val_loss: 0.7074 - val_acc: 0.6766\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4191 - acc: 0.8527 - val_loss: 0.7086 - val_acc: 0.6766\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4181 - acc: 0.8527 - val_loss: 0.7049 - val_acc: 0.6766\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4172 - acc: 0.8527 - val_loss: 0.7142 - val_acc: 0.6766\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4154 - acc: 0.8527 - val_loss: 0.7156 - val_acc: 0.6766\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4159 - acc: 0.8527 - val_loss: 0.7186 - val_acc: 0.6766\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 2s 14ms/step - loss: 0.4348 - acc: 0.8462 - val_loss: 0.5652 - val_acc: 0.7653\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4285 - acc: 0.8462 - val_loss: 0.5641 - val_acc: 0.7653\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4255 - acc: 0.8462 - val_loss: 0.5642 - val_acc: 0.7653\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.8462 - val_loss: 0.5732 - val_acc: 0.7653\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.8462 - val_loss: 0.5702 - val_acc: 0.7653\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4209 - acc: 0.8462 - val_loss: 0.5686 - val_acc: 0.7653\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4207 - acc: 0.8462 - val_loss: 0.5713 - val_acc: 0.7653\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4183 - acc: 0.8462 - val_loss: 0.5710 - val_acc: 0.7653\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.4189 - acc: 0.8462 - val_loss: 0.5761 - val_acc: 0.7653\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4495 - acc: 0.8350 - val_loss: 0.4885 - val_acc: 0.8141\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4474 - acc: 0.8350 - val_loss: 0.4895 - val_acc: 0.8141\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4466 - acc: 0.8350 - val_loss: 0.4877 - val_acc: 0.8141\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4446 - acc: 0.8350 - val_loss: 0.4851 - val_acc: 0.8141\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4438 - acc: 0.8350 - val_loss: 0.4884 - val_acc: 0.8141\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4462 - acc: 0.8350 - val_loss: 0.4887 - val_acc: 0.8141\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4245 - acc: 0.8546 - val_loss: 0.5271 - val_acc: 0.7918\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4175 - acc: 0.8546 - val_loss: 0.5320 - val_acc: 0.7918\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4126 - acc: 0.8546 - val_loss: 0.5371 - val_acc: 0.7918\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4102 - acc: 0.8546 - val_loss: 0.5387 - val_acc: 0.7918\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4079 - acc: 0.8546 - val_loss: 0.5451 - val_acc: 0.7918\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4065 - acc: 0.8546 - val_loss: 0.5476 - val_acc: 0.7918\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4060 - acc: 0.8546 - val_loss: 0.5504 - val_acc: 0.7918\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4059 - acc: 0.8546 - val_loss: 0.5546 - val_acc: 0.7918\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4046 - acc: 0.8546 - val_loss: 0.5529 - val_acc: 0.7918\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4050 - acc: 0.8546 - val_loss: 0.5575 - val_acc: 0.7918\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4145 - acc: 0.8574 - val_loss: 0.5899 - val_acc: 0.7844\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4107 - acc: 0.8574 - val_loss: 0.5918 - val_acc: 0.7844\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4083 - acc: 0.8574 - val_loss: 0.5848 - val_acc: 0.7844\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4080 - acc: 0.8574 - val_loss: 0.5524 - val_acc: 0.7844\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4067 - acc: 0.8574 - val_loss: 0.5455 - val_acc: 0.7844\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4068 - acc: 0.8574 - val_loss: 0.5357 - val_acc: 0.7844\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 12ms/step - loss: 0.5022 - acc: 0.8035 - val_loss: 0.5569 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4968 - acc: 0.8035 - val_loss: 0.5617 - val_acc: 0.7500\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4944 - acc: 0.8035 - val_loss: 0.5589 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4924 - acc: 0.8035 - val_loss: 0.5607 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4926 - acc: 0.8035 - val_loss: 0.5606 - val_acc: 0.7500\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4775 - acc: 0.8183 - val_loss: 0.5225 - val_acc: 0.7807\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4745 - acc: 0.8183 - val_loss: 0.5252 - val_acc: 0.7807\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4722 - acc: 0.8183 - val_loss: 0.5238 - val_acc: 0.7807\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4705 - acc: 0.8183 - val_loss: 0.5249 - val_acc: 0.7807\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4694 - acc: 0.8183 - val_loss: 0.5234 - val_acc: 0.7807\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4688 - acc: 0.8183 - val_loss: 0.5264 - val_acc: 0.7807\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4689 - acc: 0.8183 - val_loss: 0.5285 - val_acc: 0.7807\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s 19ms/step - loss: 0.4984 - acc: 0.8056 - val_loss: 0.6040 - val_acc: 0.7413\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4994 - acc: 0.8056 - val_loss: 0.5855 - val_acc: 0.7413\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4725 - acc: 0.8239 - val_loss: 0.4272 - val_acc: 0.8513\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4686 - acc: 0.8239 - val_loss: 0.4295 - val_acc: 0.8513\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4668 - acc: 0.8239 - val_loss: 0.4291 - val_acc: 0.8513\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4655 - acc: 0.8239 - val_loss: 0.4309 - val_acc: 0.8513\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4661 - acc: 0.8239 - val_loss: 0.4310 - val_acc: 0.8513\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3289 - acc: 0.9059 - val_loss: 0.3951 - val_acc: 0.8736\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3176 - acc: 0.9059 - val_loss: 0.3876 - val_acc: 0.8736\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3147 - acc: 0.9059 - val_loss: 0.3832 - val_acc: 0.8736\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3125 - acc: 0.9059 - val_loss: 0.3962 - val_acc: 0.8736\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3129 - acc: 0.9059 - val_loss: 0.3986 - val_acc: 0.8736\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5204 - acc: 0.8062 - val_loss: 0.4557 - val_acc: 0.8327\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4954 - acc: 0.8062 - val_loss: 0.4559 - val_acc: 0.8327\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4931 - acc: 0.8062 - val_loss: 0.4548 - val_acc: 0.8327\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4919 - acc: 0.8062 - val_loss: 0.4540 - val_acc: 0.8327\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4903 - acc: 0.8062 - val_loss: 0.4543 - val_acc: 0.8327\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4899 - acc: 0.8062 - val_loss: 0.4537 - val_acc: 0.8327\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4904 - acc: 0.8062 - val_loss: 0.4534 - val_acc: 0.8327\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 12ms/step - loss: 0.3482 - acc: 0.8976 - val_loss: 0.3909 - val_acc: 0.8755\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3328 - acc: 0.8976 - val_loss: 0.3898 - val_acc: 0.8755\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3307 - acc: 0.8976 - val_loss: 0.3825 - val_acc: 0.8755\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3292 - acc: 0.8976 - val_loss: 0.3877 - val_acc: 0.8755\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3281 - acc: 0.8976 - val_loss: 0.3837 - val_acc: 0.8755\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3273 - acc: 0.8976 - val_loss: 0.3888 - val_acc: 0.8755\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.3273 - acc: 0.8976 - val_loss: 0.3855 - val_acc: 0.8755\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4669 - acc: 0.8313 - val_loss: 0.5533 - val_acc: 0.7621\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4538 - acc: 0.8313 - val_loss: 0.5546 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4504 - acc: 0.8313 - val_loss: 0.5569 - val_acc: 0.7621\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4477 - acc: 0.8313 - val_loss: 0.5561 - val_acc: 0.7621\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4474 - acc: 0.8313 - val_loss: 0.5560 - val_acc: 0.7621\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4462 - acc: 0.8313 - val_loss: 0.5571 - val_acc: 0.7621\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4459 - acc: 0.8313 - val_loss: 0.5641 - val_acc: 0.7621\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4447 - acc: 0.8313 - val_loss: 0.5578 - val_acc: 0.7621\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4450 - acc: 0.8313 - val_loss: 0.5538 - val_acc: 0.7621\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5390 - acc: 0.7773 - val_loss: 0.6539 - val_acc: 0.6856\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5340 - acc: 0.7773 - val_loss: 0.6505 - val_acc: 0.6856\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5321 - acc: 0.7773 - val_loss: 0.6492 - val_acc: 0.6856\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5291 - acc: 0.7773 - val_loss: 0.6490 - val_acc: 0.6856\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5285 - acc: 0.7773 - val_loss: 0.6483 - val_acc: 0.6856\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5273 - acc: 0.7773 - val_loss: 0.6455 - val_acc: 0.6856\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5273 - acc: 0.7773 - val_loss: 0.6531 - val_acc: 0.6856\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5253 - acc: 0.7773 - val_loss: 0.6441 - val_acc: 0.6856\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5250 - acc: 0.7773 - val_loss: 0.6472 - val_acc: 0.6856\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5261 - acc: 0.7773 - val_loss: 0.6500 - val_acc: 0.6856\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3238 - acc: 0.9158 - val_loss: 0.3593 - val_acc: 0.8881\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2933 - acc: 0.9158 - val_loss: 0.3564 - val_acc: 0.8881\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2864 - acc: 0.9158 - val_loss: 0.3578 - val_acc: 0.8881\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2823 - acc: 0.9158 - val_loss: 0.3677 - val_acc: 0.8881\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2798 - acc: 0.9158 - val_loss: 0.3682 - val_acc: 0.8881\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2782 - acc: 0.9158 - val_loss: 0.3694 - val_acc: 0.8881\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2735 - acc: 0.9158 - val_loss: 0.3795 - val_acc: 0.8881\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2679 - acc: 0.9158 - val_loss: 0.3826 - val_acc: 0.8881\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2656 - acc: 0.9158 - val_loss: 0.4032 - val_acc: 0.8881\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2645 - acc: 0.9158 - val_loss: 0.3876 - val_acc: 0.8881\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2587 - acc: 0.9158 - val_loss: 0.3921 - val_acc: 0.8881\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2591 - acc: 0.9158 - val_loss: 0.4663 - val_acc: 0.8881\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5196 - acc: 0.8239 - val_loss: 0.4596 - val_acc: 0.8364\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4716 - acc: 0.8239 - val_loss: 0.4468 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4667 - acc: 0.8239 - val_loss: 0.4512 - val_acc: 0.8364\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4669 - acc: 0.8239 - val_loss: 0.4520 - val_acc: 0.8364\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4452 - acc: 0.8425 - val_loss: 0.4766 - val_acc: 0.8178\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4387 - acc: 0.8425 - val_loss: 0.4763 - val_acc: 0.8178\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4367 - acc: 0.8425 - val_loss: 0.4793 - val_acc: 0.8178\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4346 - acc: 0.8425 - val_loss: 0.4782 - val_acc: 0.8178\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4354 - acc: 0.8425 - val_loss: 0.4823 - val_acc: 0.8178\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4609 - acc: 0.8295 - val_loss: 0.4125 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4551 - acc: 0.8295 - val_loss: 0.4120 - val_acc: 0.8587\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4523 - acc: 0.8295 - val_loss: 0.4126 - val_acc: 0.8587\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4515 - acc: 0.8295 - val_loss: 0.4170 - val_acc: 0.8587\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4494 - acc: 0.8295 - val_loss: 0.4180 - val_acc: 0.8587\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4517 - acc: 0.8295 - val_loss: 0.4190 - val_acc: 0.8587\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4144 - acc: 0.8590 - val_loss: 0.4166 - val_acc: 0.8731\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4074 - acc: 0.8590 - val_loss: 0.4168 - val_acc: 0.8731\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4028 - acc: 0.8590 - val_loss: 0.4137 - val_acc: 0.8731\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4016 - acc: 0.8590 - val_loss: 0.4016 - val_acc: 0.8731\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4003 - acc: 0.8590 - val_loss: 0.4089 - val_acc: 0.8731\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4004 - acc: 0.8590 - val_loss: 0.4056 - val_acc: 0.8731\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4935 - acc: 0.8099 - val_loss: 0.4212 - val_acc: 0.8662\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4828 - acc: 0.8099 - val_loss: 0.4206 - val_acc: 0.8662\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4810 - acc: 0.8099 - val_loss: 0.4224 - val_acc: 0.8662\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4788 - acc: 0.8099 - val_loss: 0.4232 - val_acc: 0.8662\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4768 - acc: 0.8099 - val_loss: 0.4306 - val_acc: 0.8662\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4756 - acc: 0.8099 - val_loss: 0.4342 - val_acc: 0.8662\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4745 - acc: 0.8099 - val_loss: 0.4419 - val_acc: 0.8662\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4743 - acc: 0.8099 - val_loss: 0.4629 - val_acc: 0.8662\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4723 - acc: 0.8099 - val_loss: 0.4472 - val_acc: 0.8662\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4716 - acc: 0.8099 - val_loss: 0.5390 - val_acc: 0.8662\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4718 - acc: 0.8099 - val_loss: 0.4624 - val_acc: 0.8662\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5376 - acc: 0.7819 - val_loss: 0.5527 - val_acc: 0.7546\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5282 - acc: 0.7819 - val_loss: 0.5563 - val_acc: 0.7546\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5272 - acc: 0.7819 - val_loss: 0.5505 - val_acc: 0.7546\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5257 - acc: 0.7819 - val_loss: 0.5526 - val_acc: 0.7546\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5244 - acc: 0.7819 - val_loss: 0.5536 - val_acc: 0.7546\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5236 - acc: 0.7819 - val_loss: 0.5535 - val_acc: 0.7546\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5231 - acc: 0.7819 - val_loss: 0.5507 - val_acc: 0.7546\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5227 - acc: 0.7819 - val_loss: 0.5516 - val_acc: 0.7546\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5215 - acc: 0.7819 - val_loss: 0.5512 - val_acc: 0.7546\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5235 - acc: 0.7819 - val_loss: 0.5515 - val_acc: 0.7546\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3878 - acc: 0.8844 - val_loss: 0.2663 - val_acc: 0.9405\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3610 - acc: 0.8844 - val_loss: 0.2670 - val_acc: 0.9405\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3585 - acc: 0.8844 - val_loss: 0.2632 - val_acc: 0.9405\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3600 - acc: 0.8844 - val_loss: 0.2673 - val_acc: 0.9405\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2807 - acc: 0.9241 - val_loss: 0.2717 - val_acc: 0.9242\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2707 - acc: 0.9241 - val_loss: 0.2781 - val_acc: 0.9242\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2664 - acc: 0.9241 - val_loss: 0.3629 - val_acc: 0.9242\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2682 - acc: 0.9241 - val_loss: 0.2837 - val_acc: 0.9242\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4662 - acc: 0.8295 - val_loss: 0.5435 - val_acc: 0.7770\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4533 - acc: 0.8295 - val_loss: 0.5493 - val_acc: 0.7770\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4530 - acc: 0.8295 - val_loss: 0.5456 - val_acc: 0.7770\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4522 - acc: 0.8295 - val_loss: 0.5469 - val_acc: 0.7770\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4506 - acc: 0.8295 - val_loss: 0.5471 - val_acc: 0.7770\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4495 - acc: 0.8295 - val_loss: 0.5490 - val_acc: 0.7770\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4497 - acc: 0.8295 - val_loss: 0.5467 - val_acc: 0.7770\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 13ms/step - loss: 0.5816 - acc: 0.7557 - val_loss: 0.5844 - val_acc: 0.7306\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5575 - acc: 0.7557 - val_loss: 0.5853 - val_acc: 0.7306\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5571 - acc: 0.7557 - val_loss: 0.5869 - val_acc: 0.7306\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5553 - acc: 0.7557 - val_loss: 0.5821 - val_acc: 0.7306\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.5567 - acc: 0.7557 - val_loss: 0.5848 - val_acc: 0.7306\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4495 - acc: 0.8509 - val_loss: 0.3503 - val_acc: 0.8996\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4243 - acc: 0.8509 - val_loss: 0.3377 - val_acc: 0.8996\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4210 - acc: 0.8509 - val_loss: 0.3429 - val_acc: 0.8996\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4200 - acc: 0.8509 - val_loss: 0.3430 - val_acc: 0.8996\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4167 - acc: 0.8509 - val_loss: 0.3406 - val_acc: 0.8996\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4155 - acc: 0.8509 - val_loss: 0.3399 - val_acc: 0.8996\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4148 - acc: 0.8509 - val_loss: 0.3383 - val_acc: 0.8996\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4148 - acc: 0.8509 - val_loss: 0.3368 - val_acc: 0.8996\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4148 - acc: 0.8509 - val_loss: 0.3388 - val_acc: 0.8996\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4355 - acc: 0.8462 - val_loss: 0.4850 - val_acc: 0.8141\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4302 - acc: 0.8462 - val_loss: 0.4803 - val_acc: 0.8141\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4261 - acc: 0.8462 - val_loss: 0.4826 - val_acc: 0.8141\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4241 - acc: 0.8462 - val_loss: 0.4815 - val_acc: 0.8141\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4222 - acc: 0.8462 - val_loss: 0.4832 - val_acc: 0.8141\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4206 - acc: 0.8462 - val_loss: 0.4793 - val_acc: 0.8141\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4183 - acc: 0.8462 - val_loss: 0.4814 - val_acc: 0.8141\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4186 - acc: 0.8462 - val_loss: 0.4851 - val_acc: 0.8141\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3693 - acc: 0.8928 - val_loss: 0.4880 - val_acc: 0.8104\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3490 - acc: 0.8928 - val_loss: 0.5117 - val_acc: 0.8104\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3473 - acc: 0.8928 - val_loss: 0.4998 - val_acc: 0.8104\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3456 - acc: 0.8928 - val_loss: 0.5046 - val_acc: 0.8104\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3447 - acc: 0.8928 - val_loss: 0.5038 - val_acc: 0.8104\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3411 - acc: 0.8928 - val_loss: 0.5249 - val_acc: 0.8104\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3417 - acc: 0.8928 - val_loss: 0.5106 - val_acc: 0.8104\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2181 - acc: 0.9506 - val_loss: 0.1461 - val_acc: 0.9888\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2386 - acc: 0.9506 - val_loss: 0.1017 - val_acc: 0.9888\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2502 - acc: 0.9450 - val_loss: 0.2033 - val_acc: 0.9554\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2203 - acc: 0.9450 - val_loss: 0.1915 - val_acc: 0.9554\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2093 - acc: 0.9450 - val_loss: 0.1903 - val_acc: 0.9554\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2212 - acc: 0.9450 - val_loss: 0.1885 - val_acc: 0.9554\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3603 - acc: 0.8881 - val_loss: 0.2800 - val_acc: 0.9257\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3496 - acc: 0.8881 - val_loss: 0.2749 - val_acc: 0.9257\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3475 - acc: 0.8881 - val_loss: 0.2740 - val_acc: 0.9257\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3462 - acc: 0.8881 - val_loss: 0.2722 - val_acc: 0.9257\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3459 - acc: 0.8881 - val_loss: 0.2710 - val_acc: 0.9257\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3438 - acc: 0.8881 - val_loss: 0.2666 - val_acc: 0.9257\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3427 - acc: 0.8881 - val_loss: 0.2674 - val_acc: 0.9257\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3422 - acc: 0.8881 - val_loss: 0.2697 - val_acc: 0.9257\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3443 - acc: 0.8881 - val_loss: 0.2727 - val_acc: 0.9257\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4976 - acc: 0.8099 - val_loss: 0.4946 - val_acc: 0.8104\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4876 - acc: 0.8099 - val_loss: 0.4960 - val_acc: 0.8104\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4865 - acc: 0.8099 - val_loss: 0.4943 - val_acc: 0.8104\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4852 - acc: 0.8099 - val_loss: 0.4938 - val_acc: 0.8104\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4848 - acc: 0.8099 - val_loss: 0.4904 - val_acc: 0.8104\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4848 - acc: 0.8099 - val_loss: 0.4900 - val_acc: 0.8104\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4832 - acc: 0.8099 - val_loss: 0.4897 - val_acc: 0.8104\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4827 - acc: 0.8099 - val_loss: 0.4906 - val_acc: 0.8104\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4821 - acc: 0.8099 - val_loss: 0.4902 - val_acc: 0.8104\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4820 - acc: 0.8099 - val_loss: 0.4910 - val_acc: 0.8104\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4809 - acc: 0.8099 - val_loss: 0.4907 - val_acc: 0.8104\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4816 - acc: 0.8099 - val_loss: 0.4909 - val_acc: 0.8104\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4742 - acc: 0.8201 - val_loss: 0.5261 - val_acc: 0.7807\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4728 - acc: 0.8201 - val_loss: 0.5275 - val_acc: 0.7807\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4737 - acc: 0.8201 - val_loss: 0.5265 - val_acc: 0.7807\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4888 - acc: 0.8108 - val_loss: 0.4622 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4851 - acc: 0.8108 - val_loss: 0.4622 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4846 - acc: 0.8108 - val_loss: 0.4604 - val_acc: 0.8290\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4840 - acc: 0.8108 - val_loss: 0.4610 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4834 - acc: 0.8108 - val_loss: 0.4625 - val_acc: 0.8290\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4844 - acc: 0.8108 - val_loss: 0.4613 - val_acc: 0.8290\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3007 - acc: 0.9408 - val_loss: 0.2620 - val_acc: 0.9474\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2372 - acc: 0.9408 - val_loss: 0.2407 - val_acc: 0.9474\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2258 - acc: 0.9408 - val_loss: 0.2385 - val_acc: 0.9474\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2182 - acc: 0.9408 - val_loss: 0.2377 - val_acc: 0.9474\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2154 - acc: 0.9408 - val_loss: 0.2369 - val_acc: 0.9474\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2118 - acc: 0.9408 - val_loss: 0.2763 - val_acc: 0.9474\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2096 - acc: 0.9408 - val_loss: 0.2309 - val_acc: 0.9474\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2083 - acc: 0.9408 - val_loss: 0.2286 - val_acc: 0.9474\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2043 - acc: 0.9408 - val_loss: 0.2308 - val_acc: 0.9474\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2115 - acc: 0.9408 - val_loss: 0.2807 - val_acc: 0.9474\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4974 - acc: 0.8416 - val_loss: 0.4926 - val_acc: 0.8067\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4405 - acc: 0.8416 - val_loss: 0.4995 - val_acc: 0.8067\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4386 - acc: 0.8416 - val_loss: 0.4982 - val_acc: 0.8067\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4375 - acc: 0.8416 - val_loss: 0.4994 - val_acc: 0.8067\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4379 - acc: 0.8416 - val_loss: 0.4939 - val_acc: 0.8067\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4484 - acc: 0.8378 - val_loss: 0.5024 - val_acc: 0.8030\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4423 - acc: 0.8378 - val_loss: 0.5087 - val_acc: 0.8030\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4414 - acc: 0.8378 - val_loss: 0.5099 - val_acc: 0.8030\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4408 - acc: 0.8378 - val_loss: 0.5132 - val_acc: 0.8030\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4411 - acc: 0.8378 - val_loss: 0.5135 - val_acc: 0.8030\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2638 - acc: 0.9301 - val_loss: 0.1965 - val_acc: 0.9554\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2769 - acc: 0.9301 - val_loss: 0.2338 - val_acc: 0.9554\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4242 - acc: 0.8639 - val_loss: 0.4844 - val_acc: 0.8104\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4138 - acc: 0.8639 - val_loss: 0.4892 - val_acc: 0.8104\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4013 - acc: 0.8639 - val_loss: 0.4939 - val_acc: 0.8104\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3958 - acc: 0.8639 - val_loss: 0.4988 - val_acc: 0.8104\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3925 - acc: 0.8639 - val_loss: 0.5069 - val_acc: 0.8104\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3895 - acc: 0.8639 - val_loss: 0.5067 - val_acc: 0.8104\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3877 - acc: 0.8639 - val_loss: 0.5088 - val_acc: 0.8104\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3871 - acc: 0.8639 - val_loss: 0.5094 - val_acc: 0.8104\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3872 - acc: 0.8639 - val_loss: 0.5150 - val_acc: 0.8104\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4180 - acc: 0.8555 - val_loss: 0.5529 - val_acc: 0.7757\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4157 - acc: 0.8555 - val_loss: 0.5570 - val_acc: 0.7757\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4139 - acc: 0.8555 - val_loss: 0.5541 - val_acc: 0.7757\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4120 - acc: 0.8555 - val_loss: 0.5615 - val_acc: 0.7757\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4116 - acc: 0.8555 - val_loss: 0.5579 - val_acc: 0.7757\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4114 - acc: 0.8555 - val_loss: 0.5599 - val_acc: 0.7757\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4107 - acc: 0.8555 - val_loss: 0.5620 - val_acc: 0.7757\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4105 - acc: 0.8555 - val_loss: 0.5615 - val_acc: 0.7757\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4104 - acc: 0.8555 - val_loss: 0.5604 - val_acc: 0.7757\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4123 - acc: 0.8555 - val_loss: 0.5592 - val_acc: 0.7757\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4147 - acc: 0.8574 - val_loss: 0.3914 - val_acc: 0.8699\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4103 - acc: 0.8574 - val_loss: 0.3902 - val_acc: 0.8699\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4076 - acc: 0.8574 - val_loss: 0.3912 - val_acc: 0.8699\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4075 - acc: 0.8574 - val_loss: 0.3923 - val_acc: 0.8699\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4058 - acc: 0.8574 - val_loss: 0.3943 - val_acc: 0.8699\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4066 - acc: 0.8574 - val_loss: 0.3931 - val_acc: 0.8699\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4149 - acc: 0.8546 - val_loss: 0.5641 - val_acc: 0.7732\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4139 - acc: 0.8546 - val_loss: 0.5625 - val_acc: 0.7732\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4140 - acc: 0.8546 - val_loss: 0.5571 - val_acc: 0.7732\n",
      "Epoch 3: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3252 - acc: 0.9031 - val_loss: 0.5547 - val_acc: 0.7993\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3193 - acc: 0.9031 - val_loss: 0.5653 - val_acc: 0.7993\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3181 - acc: 0.9031 - val_loss: 0.5527 - val_acc: 0.7993\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3169 - acc: 0.9031 - val_loss: 0.5592 - val_acc: 0.7993\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3151 - acc: 0.9031 - val_loss: 0.5568 - val_acc: 0.7993\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3155 - acc: 0.9031 - val_loss: 0.5321 - val_acc: 0.7993\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4405 - acc: 0.8434 - val_loss: 0.5956 - val_acc: 0.7472\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4334 - acc: 0.8434 - val_loss: 0.5953 - val_acc: 0.7472\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4314 - acc: 0.8434 - val_loss: 0.5957 - val_acc: 0.7472\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4303 - acc: 0.8434 - val_loss: 0.5995 - val_acc: 0.7472\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4302 - acc: 0.8434 - val_loss: 0.5953 - val_acc: 0.7472\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4290 - acc: 0.8434 - val_loss: 0.5948 - val_acc: 0.7472\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4290 - acc: 0.8434 - val_loss: 0.5931 - val_acc: 0.7472\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4297 - acc: 0.8434 - val_loss: 0.5930 - val_acc: 0.7472\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5348 - acc: 0.7817 - val_loss: 0.4792 - val_acc: 0.8134\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5260 - acc: 0.7817 - val_loss: 0.4758 - val_acc: 0.8134\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5234 - acc: 0.7817 - val_loss: 0.4774 - val_acc: 0.8134\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5220 - acc: 0.7817 - val_loss: 0.4749 - val_acc: 0.8134\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5203 - acc: 0.7817 - val_loss: 0.4745 - val_acc: 0.8134\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5188 - acc: 0.7817 - val_loss: 0.4752 - val_acc: 0.8134\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5174 - acc: 0.7817 - val_loss: 0.4820 - val_acc: 0.8134\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5189 - acc: 0.7817 - val_loss: 0.4788 - val_acc: 0.8134\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5345 - acc: 0.7754 - val_loss: 0.5327 - val_acc: 0.7695\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5348 - acc: 0.7754 - val_loss: 0.5313 - val_acc: 0.7695\n",
      "Epoch 2: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4447 - acc: 0.8462 - val_loss: 0.3871 - val_acc: 0.8773\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4300 - acc: 0.8462 - val_loss: 0.3813 - val_acc: 0.8773\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4287 - acc: 0.8462 - val_loss: 0.3809 - val_acc: 0.8773\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4277 - acc: 0.8462 - val_loss: 0.3829 - val_acc: 0.8773\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4259 - acc: 0.8462 - val_loss: 0.3825 - val_acc: 0.8773\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4257 - acc: 0.8462 - val_loss: 0.3850 - val_acc: 0.8773\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4243 - acc: 0.8462 - val_loss: 0.3850 - val_acc: 0.8773\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4231 - acc: 0.8462 - val_loss: 0.3879 - val_acc: 0.8773\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4237 - acc: 0.8462 - val_loss: 0.3890 - val_acc: 0.8773\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4180 - acc: 0.8546 - val_loss: 0.4114 - val_acc: 0.8625\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4161 - acc: 0.8546 - val_loss: 0.4124 - val_acc: 0.8625\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4135 - acc: 0.8546 - val_loss: 0.4079 - val_acc: 0.8625\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4125 - acc: 0.8546 - val_loss: 0.4086 - val_acc: 0.8625\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4112 - acc: 0.8546 - val_loss: 0.4087 - val_acc: 0.8625\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4099 - acc: 0.8546 - val_loss: 0.4078 - val_acc: 0.8625\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4091 - acc: 0.8546 - val_loss: 0.4074 - val_acc: 0.8625\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4085 - acc: 0.8546 - val_loss: 0.4073 - val_acc: 0.8625\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4086 - acc: 0.8546 - val_loss: 0.4061 - val_acc: 0.8625\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4071 - acc: 0.8630 - val_loss: 0.4036 - val_acc: 0.8736\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3995 - acc: 0.8630 - val_loss: 0.4071 - val_acc: 0.8736\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3955 - acc: 0.8630 - val_loss: 0.4123 - val_acc: 0.8736\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3953 - acc: 0.8630 - val_loss: 0.4011 - val_acc: 0.8736\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3912 - acc: 0.8630 - val_loss: 0.4050 - val_acc: 0.8736\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3896 - acc: 0.8630 - val_loss: 0.4058 - val_acc: 0.8736\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3890 - acc: 0.8630 - val_loss: 0.4057 - val_acc: 0.8736\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3877 - acc: 0.8630 - val_loss: 0.4163 - val_acc: 0.8736\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3879 - acc: 0.8630 - val_loss: 0.4539 - val_acc: 0.8736\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 22ms/step - loss: 0.4944 - acc: 0.8211 - val_loss: 0.3734 - val_acc: 0.8927\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4707 - acc: 0.8211 - val_loss: 0.3641 - val_acc: 0.8927\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4688 - acc: 0.8211 - val_loss: 0.3615 - val_acc: 0.8927\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4675 - acc: 0.8211 - val_loss: 0.3618 - val_acc: 0.8927\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4671 - acc: 0.8211 - val_loss: 0.3694 - val_acc: 0.8927\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4660 - acc: 0.8211 - val_loss: 0.3628 - val_acc: 0.8927\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4667 - acc: 0.8211 - val_loss: 0.3635 - val_acc: 0.8927\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4310 - acc: 0.8462 - val_loss: 0.3994 - val_acc: 0.8647\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4284 - acc: 0.8462 - val_loss: 0.3990 - val_acc: 0.8647\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4269 - acc: 0.8462 - val_loss: 0.4036 - val_acc: 0.8647\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4259 - acc: 0.8462 - val_loss: 0.4023 - val_acc: 0.8647\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4261 - acc: 0.8462 - val_loss: 0.4027 - val_acc: 0.8647\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5057 - acc: 0.8015 - val_loss: 0.5652 - val_acc: 0.7546\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4999 - acc: 0.8015 - val_loss: 0.5656 - val_acc: 0.7546\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4983 - acc: 0.8015 - val_loss: 0.5632 - val_acc: 0.7546\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4977 - acc: 0.8015 - val_loss: 0.5615 - val_acc: 0.7546\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4972 - acc: 0.8015 - val_loss: 0.5609 - val_acc: 0.7546\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4977 - acc: 0.8015 - val_loss: 0.5599 - val_acc: 0.7546\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 12ms/step - loss: 0.4803 - acc: 0.8168 - val_loss: 0.4739 - val_acc: 0.8197\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4753 - acc: 0.8168 - val_loss: 0.4718 - val_acc: 0.8197\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4733 - acc: 0.8168 - val_loss: 0.4724 - val_acc: 0.8197\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4700 - acc: 0.8168 - val_loss: 0.4721 - val_acc: 0.8197\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4687 - acc: 0.8168 - val_loss: 0.4727 - val_acc: 0.8197\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4677 - acc: 0.8168 - val_loss: 0.4739 - val_acc: 0.8197\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4673 - acc: 0.8168 - val_loss: 0.4753 - val_acc: 0.8197\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4670 - acc: 0.8168 - val_loss: 0.4767 - val_acc: 0.8197\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4645 - acc: 0.8168 - val_loss: 0.4790 - val_acc: 0.8197\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4630 - acc: 0.8168 - val_loss: 0.4764 - val_acc: 0.8197\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4628 - acc: 0.8168 - val_loss: 0.4795 - val_acc: 0.8197\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4625 - acc: 0.8168 - val_loss: 0.4825 - val_acc: 0.8197\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4610 - acc: 0.8168 - val_loss: 0.4837 - val_acc: 0.8197\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 0.4618 - acc: 0.8168 - val_loss: 0.4872 - val_acc: 0.8197\n",
      "Epoch 14: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4630 - acc: 0.8369 - val_loss: 0.5620 - val_acc: 0.7658\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4440 - acc: 0.8369 - val_loss: 0.5619 - val_acc: 0.7658\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4387 - acc: 0.8369 - val_loss: 0.5700 - val_acc: 0.7658\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4351 - acc: 0.8369 - val_loss: 0.5701 - val_acc: 0.7658\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4328 - acc: 0.8369 - val_loss: 0.5787 - val_acc: 0.7658\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4317 - acc: 0.8369 - val_loss: 0.5733 - val_acc: 0.7658\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4307 - acc: 0.8369 - val_loss: 0.5723 - val_acc: 0.7658\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4301 - acc: 0.8369 - val_loss: 0.5702 - val_acc: 0.7658\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4345 - acc: 0.8369 - val_loss: 0.5548 - val_acc: 0.7658\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5093 - acc: 0.7996 - val_loss: 0.6035 - val_acc: 0.7286\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5043 - acc: 0.7996 - val_loss: 0.6051 - val_acc: 0.7286\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5024 - acc: 0.7996 - val_loss: 0.6098 - val_acc: 0.7286\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5010 - acc: 0.7996 - val_loss: 0.6126 - val_acc: 0.7286\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5005 - acc: 0.7996 - val_loss: 0.6095 - val_acc: 0.7286\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4998 - acc: 0.7996 - val_loss: 0.6134 - val_acc: 0.7286\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4996 - acc: 0.7996 - val_loss: 0.6113 - val_acc: 0.7286\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4989 - acc: 0.7996 - val_loss: 0.6082 - val_acc: 0.7286\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4984 - acc: 0.7996 - val_loss: 0.6082 - val_acc: 0.7286\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4988 - acc: 0.7996 - val_loss: 0.6147 - val_acc: 0.7286\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3058 - acc: 0.9200 - val_loss: 0.2648 - val_acc: 0.9248\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2774 - acc: 0.9200 - val_loss: 0.2781 - val_acc: 0.9248\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2731 - acc: 0.9200 - val_loss: 0.3195 - val_acc: 0.9248\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2834 - acc: 0.9200 - val_loss: 0.3160 - val_acc: 0.9248\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4906 - acc: 0.8164 - val_loss: 0.4994 - val_acc: 0.8067\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4822 - acc: 0.8164 - val_loss: 0.4950 - val_acc: 0.8067\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4799 - acc: 0.8164 - val_loss: 0.4982 - val_acc: 0.8067\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4775 - acc: 0.8164 - val_loss: 0.4962 - val_acc: 0.8067\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4763 - acc: 0.8164 - val_loss: 0.4956 - val_acc: 0.8067\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4767 - acc: 0.8164 - val_loss: 0.4930 - val_acc: 0.8067\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2635 - acc: 0.9478 - val_loss: 0.2183 - val_acc: 0.9480\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2327 - acc: 0.9478 - val_loss: 0.2270 - val_acc: 0.9480\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2222 - acc: 0.9478 - val_loss: 0.2108 - val_acc: 0.9480\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2184 - acc: 0.9478 - val_loss: 0.2109 - val_acc: 0.9480\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2115 - acc: 0.9478 - val_loss: 0.2504 - val_acc: 0.9480\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2180 - acc: 0.9478 - val_loss: 0.2258 - val_acc: 0.9480\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5522 - acc: 0.7773 - val_loss: 0.5379 - val_acc: 0.7695\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5329 - acc: 0.7773 - val_loss: 0.5369 - val_acc: 0.7695\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5316 - acc: 0.7773 - val_loss: 0.5389 - val_acc: 0.7695\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5315 - acc: 0.7773 - val_loss: 0.5391 - val_acc: 0.7695\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5307 - acc: 0.7773 - val_loss: 0.5389 - val_acc: 0.7695\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5294 - acc: 0.7773 - val_loss: 0.5386 - val_acc: 0.7695\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5293 - acc: 0.7773 - val_loss: 0.5405 - val_acc: 0.7695\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5290 - acc: 0.7773 - val_loss: 0.5408 - val_acc: 0.7695\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5288 - acc: 0.7773 - val_loss: 0.5410 - val_acc: 0.7695\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5284 - acc: 0.7773 - val_loss: 0.5420 - val_acc: 0.7695\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5283 - acc: 0.7773 - val_loss: 0.5419 - val_acc: 0.7695\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5284 - acc: 0.7773 - val_loss: 0.5421 - val_acc: 0.7695\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 22ms/step - loss: 0.4475 - acc: 0.8522 - val_loss: 0.4951 - val_acc: 0.8033\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4355 - acc: 0.8522 - val_loss: 0.4882 - val_acc: 0.8033\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4248 - acc: 0.8522 - val_loss: 0.4883 - val_acc: 0.8033\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4216 - acc: 0.8522 - val_loss: 0.4887 - val_acc: 0.8033\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4197 - acc: 0.8522 - val_loss: 0.4940 - val_acc: 0.8033\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4184 - acc: 0.8522 - val_loss: 0.4961 - val_acc: 0.8033\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4179 - acc: 0.8522 - val_loss: 0.4976 - val_acc: 0.8033\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4165 - acc: 0.8522 - val_loss: 0.4971 - val_acc: 0.8033\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4153 - acc: 0.8522 - val_loss: 0.4986 - val_acc: 0.8033\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4146 - acc: 0.8522 - val_loss: 0.4999 - val_acc: 0.8033\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4143 - acc: 0.8522 - val_loss: 0.5002 - val_acc: 0.8033\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4133 - acc: 0.8522 - val_loss: 0.5019 - val_acc: 0.8033\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4128 - acc: 0.8522 - val_loss: 0.5017 - val_acc: 0.8033\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4123 - acc: 0.8522 - val_loss: 0.5024 - val_acc: 0.8033\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4119 - acc: 0.8522 - val_loss: 0.5022 - val_acc: 0.8033\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4111 - acc: 0.8522 - val_loss: 0.5029 - val_acc: 0.8033\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4110 - acc: 0.8522 - val_loss: 0.5056 - val_acc: 0.8033\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4111 - acc: 0.8522 - val_loss: 0.5080 - val_acc: 0.8033\n",
      "Epoch 18: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 2s 29ms/step - loss: 0.5769 - acc: 0.7692 - val_loss: 0.7188 - val_acc: 0.6706\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5578 - acc: 0.7692 - val_loss: 0.6890 - val_acc: 0.6706\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5480 - acc: 0.7692 - val_loss: 0.6704 - val_acc: 0.6706\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5452 - acc: 0.7692 - val_loss: 0.6617 - val_acc: 0.6706\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5426 - acc: 0.7692 - val_loss: 0.6602 - val_acc: 0.6706\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5419 - acc: 0.7692 - val_loss: 0.6567 - val_acc: 0.6706\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5405 - acc: 0.7692 - val_loss: 0.6547 - val_acc: 0.6706\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5396 - acc: 0.7692 - val_loss: 0.6547 - val_acc: 0.6706\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5390 - acc: 0.7692 - val_loss: 0.6539 - val_acc: 0.6706\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5381 - acc: 0.7692 - val_loss: 0.6531 - val_acc: 0.6706\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5379 - acc: 0.7692 - val_loss: 0.6539 - val_acc: 0.6706\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5371 - acc: 0.7692 - val_loss: 0.6532 - val_acc: 0.6706\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5361 - acc: 0.7692 - val_loss: 0.6522 - val_acc: 0.6706\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5352 - acc: 0.7692 - val_loss: 0.6506 - val_acc: 0.6706\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5349 - acc: 0.7692 - val_loss: 0.6467 - val_acc: 0.6706\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5341 - acc: 0.7692 - val_loss: 0.6460 - val_acc: 0.6706\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5338 - acc: 0.7692 - val_loss: 0.6464 - val_acc: 0.6706\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5332 - acc: 0.7692 - val_loss: 0.6455 - val_acc: 0.6706\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5329 - acc: 0.7692 - val_loss: 0.6441 - val_acc: 0.6706\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5320 - acc: 0.7692 - val_loss: 0.6427 - val_acc: 0.6706\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5316 - acc: 0.7692 - val_loss: 0.6430 - val_acc: 0.6706\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5311 - acc: 0.7692 - val_loss: 0.6412 - val_acc: 0.6706\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5315 - acc: 0.7692 - val_loss: 0.6419 - val_acc: 0.6706\n",
      "Epoch 23: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4323 - acc: 0.8534 - val_loss: 0.4101 - val_acc: 0.8657\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4232 - acc: 0.8534 - val_loss: 0.4039 - val_acc: 0.8657\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4207 - acc: 0.8534 - val_loss: 0.4023 - val_acc: 0.8657\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4187 - acc: 0.8534 - val_loss: 0.4033 - val_acc: 0.8657\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4175 - acc: 0.8534 - val_loss: 0.4023 - val_acc: 0.8657\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4176 - acc: 0.8534 - val_loss: 0.4013 - val_acc: 0.8657\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4818 - acc: 0.8161 - val_loss: 0.6613 - val_acc: 0.7015\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4761 - acc: 0.8161 - val_loss: 0.6582 - val_acc: 0.7015\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4750 - acc: 0.8161 - val_loss: 0.6619 - val_acc: 0.7015\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4732 - acc: 0.8161 - val_loss: 0.6488 - val_acc: 0.7015\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4705 - acc: 0.8161 - val_loss: 0.6552 - val_acc: 0.7015\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4741 - acc: 0.8161 - val_loss: 0.6753 - val_acc: 0.7015\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.6103 - acc: 0.7176 - val_loss: 0.5574 - val_acc: 0.7621\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5955 - acc: 0.7176 - val_loss: 0.5591 - val_acc: 0.7621\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5948 - acc: 0.7176 - val_loss: 0.5601 - val_acc: 0.7621\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5938 - acc: 0.7176 - val_loss: 0.5583 - val_acc: 0.7621\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5936 - acc: 0.7176 - val_loss: 0.5591 - val_acc: 0.7621\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5927 - acc: 0.7176 - val_loss: 0.5582 - val_acc: 0.7621\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5928 - acc: 0.7176 - val_loss: 0.5580 - val_acc: 0.7621\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5289 - acc: 0.7847 - val_loss: 0.6306 - val_acc: 0.6952\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5241 - acc: 0.7847 - val_loss: 0.6366 - val_acc: 0.6952\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5230 - acc: 0.7847 - val_loss: 0.6313 - val_acc: 0.6952\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5220 - acc: 0.7847 - val_loss: 0.6325 - val_acc: 0.6952\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5218 - acc: 0.7847 - val_loss: 0.6328 - val_acc: 0.6952\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5207 - acc: 0.7847 - val_loss: 0.6374 - val_acc: 0.6952\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5204 - acc: 0.7847 - val_loss: 0.6345 - val_acc: 0.6952\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5198 - acc: 0.7847 - val_loss: 0.6359 - val_acc: 0.6952\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5194 - acc: 0.7847 - val_loss: 0.6364 - val_acc: 0.6952\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5191 - acc: 0.7847 - val_loss: 0.6344 - val_acc: 0.6952\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5189 - acc: 0.7847 - val_loss: 0.6363 - val_acc: 0.6952\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5187 - acc: 0.7847 - val_loss: 0.6332 - val_acc: 0.6952\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5181 - acc: 0.7847 - val_loss: 0.6355 - val_acc: 0.6952\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5176 - acc: 0.7847 - val_loss: 0.6386 - val_acc: 0.6952\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5178 - acc: 0.7847 - val_loss: 0.6353 - val_acc: 0.6952\n",
      "Epoch 15: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 21ms/step - loss: 0.4256 - acc: 0.8582 - val_loss: 0.4719 - val_acc: 0.8222\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4163 - acc: 0.8582 - val_loss: 0.4729 - val_acc: 0.8222\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4125 - acc: 0.8582 - val_loss: 0.4743 - val_acc: 0.8222\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4111 - acc: 0.8582 - val_loss: 0.4727 - val_acc: 0.8222\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4087 - acc: 0.8582 - val_loss: 0.4747 - val_acc: 0.8222\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4076 - acc: 0.8582 - val_loss: 0.4743 - val_acc: 0.8222\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4066 - acc: 0.8582 - val_loss: 0.4723 - val_acc: 0.8222\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4058 - acc: 0.8582 - val_loss: 0.4721 - val_acc: 0.8222\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4050 - acc: 0.8582 - val_loss: 0.4709 - val_acc: 0.8222\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4039 - acc: 0.8582 - val_loss: 0.4720 - val_acc: 0.8222\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4037 - acc: 0.8582 - val_loss: 0.4719 - val_acc: 0.8222\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4029 - acc: 0.8582 - val_loss: 0.4732 - val_acc: 0.8222\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4036 - acc: 0.8582 - val_loss: 0.4744 - val_acc: 0.8222\n",
      "Epoch 13: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.5540 - acc: 0.7670 - val_loss: 0.4687 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5446 - acc: 0.7670 - val_loss: 0.4735 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5429 - acc: 0.7670 - val_loss: 0.4702 - val_acc: 0.8290\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5428 - acc: 0.7670 - val_loss: 0.4697 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5421 - acc: 0.7670 - val_loss: 0.4694 - val_acc: 0.8290\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5420 - acc: 0.7670 - val_loss: 0.4692 - val_acc: 0.8290\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5411 - acc: 0.7670 - val_loss: 0.4663 - val_acc: 0.8290\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5407 - acc: 0.7670 - val_loss: 0.4701 - val_acc: 0.8290\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5409 - acc: 0.7670 - val_loss: 0.4670 - val_acc: 0.8290\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5111 - acc: 0.7950 - val_loss: 0.4237 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5091 - acc: 0.7950 - val_loss: 0.4263 - val_acc: 0.8587\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5080 - acc: 0.7950 - val_loss: 0.4210 - val_acc: 0.8587\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5076 - acc: 0.7950 - val_loss: 0.4217 - val_acc: 0.8587\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5073 - acc: 0.7950 - val_loss: 0.4203 - val_acc: 0.8587\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5057 - acc: 0.7950 - val_loss: 0.4236 - val_acc: 0.8587\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5058 - acc: 0.7950 - val_loss: 0.4226 - val_acc: 0.8587\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4230 - acc: 0.8537 - val_loss: 0.4451 - val_acc: 0.8364\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4193 - acc: 0.8537 - val_loss: 0.4474 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4175 - acc: 0.8537 - val_loss: 0.4482 - val_acc: 0.8364\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4171 - acc: 0.8537 - val_loss: 0.4484 - val_acc: 0.8364\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4173 - acc: 0.8537 - val_loss: 0.4486 - val_acc: 0.8364\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.2978 - acc: 0.9171 - val_loss: 0.3770 - val_acc: 0.8773\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2874 - acc: 0.9171 - val_loss: 0.3742 - val_acc: 0.8773\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2865 - acc: 0.9171 - val_loss: 0.3828 - val_acc: 0.8773\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2843 - acc: 0.9171 - val_loss: 0.3809 - val_acc: 0.8773\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2824 - acc: 0.9171 - val_loss: 0.3781 - val_acc: 0.8773\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2849 - acc: 0.9171 - val_loss: 0.3726 - val_acc: 0.8773\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 12ms/step - loss: 0.5576 - acc: 0.7749 - val_loss: 0.5759 - val_acc: 0.7479\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5380 - acc: 0.7749 - val_loss: 0.5730 - val_acc: 0.7479\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5376 - acc: 0.7749 - val_loss: 0.5729 - val_acc: 0.7479\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5358 - acc: 0.7749 - val_loss: 0.5709 - val_acc: 0.7479\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5353 - acc: 0.7749 - val_loss: 0.5725 - val_acc: 0.7479\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5347 - acc: 0.7749 - val_loss: 0.5721 - val_acc: 0.7479\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5336 - acc: 0.7749 - val_loss: 0.5699 - val_acc: 0.7479\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5340 - acc: 0.7749 - val_loss: 0.5693 - val_acc: 0.7479\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3480 - acc: 0.9021 - val_loss: 0.3460 - val_acc: 0.8922\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3204 - acc: 0.9021 - val_loss: 0.3473 - val_acc: 0.8922\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3177 - acc: 0.9021 - val_loss: 0.3472 - val_acc: 0.8922\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3187 - acc: 0.9021 - val_loss: 0.3489 - val_acc: 0.8922\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4111 - acc: 0.8583 - val_loss: 0.2748 - val_acc: 0.9294\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4071 - acc: 0.8583 - val_loss: 0.2734 - val_acc: 0.9294\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4070 - acc: 0.8583 - val_loss: 0.2789 - val_acc: 0.9294\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4062 - acc: 0.8583 - val_loss: 0.2731 - val_acc: 0.9294\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4055 - acc: 0.8583 - val_loss: 0.2745 - val_acc: 0.9294\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4061 - acc: 0.8583 - val_loss: 0.2701 - val_acc: 0.9294\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5125 - acc: 0.7940 - val_loss: 0.4608 - val_acc: 0.8364\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5115 - acc: 0.7940 - val_loss: 0.4536 - val_acc: 0.8364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5097 - acc: 0.7940 - val_loss: 0.4601 - val_acc: 0.8364\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5088 - acc: 0.7940 - val_loss: 0.4597 - val_acc: 0.8364\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5085 - acc: 0.7940 - val_loss: 0.4607 - val_acc: 0.8364\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5085 - acc: 0.7940 - val_loss: 0.4582 - val_acc: 0.8364\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5078 - acc: 0.7940 - val_loss: 0.4591 - val_acc: 0.8364\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5085 - acc: 0.7940 - val_loss: 0.4593 - val_acc: 0.8364\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5520 - acc: 0.7608 - val_loss: 0.7053 - val_acc: 0.6391\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5499 - acc: 0.7608 - val_loss: 0.6975 - val_acc: 0.6391\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5497 - acc: 0.7608 - val_loss: 0.6868 - val_acc: 0.6391\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5486 - acc: 0.7608 - val_loss: 0.6950 - val_acc: 0.6391\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5480 - acc: 0.7608 - val_loss: 0.6941 - val_acc: 0.6391\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5469 - acc: 0.7608 - val_loss: 0.6953 - val_acc: 0.6391\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5464 - acc: 0.7608 - val_loss: 0.6934 - val_acc: 0.6391\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5473 - acc: 0.7608 - val_loss: 0.6887 - val_acc: 0.6391\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4257 - acc: 0.8565 - val_loss: 0.6006 - val_acc: 0.7584\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4087 - acc: 0.8565 - val_loss: 0.5921 - val_acc: 0.7584\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4069 - acc: 0.8565 - val_loss: 0.5935 - val_acc: 0.7584\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4072 - acc: 0.8565 - val_loss: 0.5746 - val_acc: 0.7584\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3897 - acc: 0.8714 - val_loss: 0.3682 - val_acc: 0.8773\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3852 - acc: 0.8714 - val_loss: 0.3691 - val_acc: 0.8773\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3826 - acc: 0.8714 - val_loss: 0.3674 - val_acc: 0.8773\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3826 - acc: 0.8714 - val_loss: 0.3676 - val_acc: 0.8773\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3824 - acc: 0.8714 - val_loss: 0.3664 - val_acc: 0.8773\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3812 - acc: 0.8714 - val_loss: 0.3663 - val_acc: 0.8773\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3813 - acc: 0.8714 - val_loss: 0.3653 - val_acc: 0.8773\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4135 - acc: 0.8583 - val_loss: 0.5199 - val_acc: 0.7993\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4103 - acc: 0.8583 - val_loss: 0.5264 - val_acc: 0.7993\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4097 - acc: 0.8583 - val_loss: 0.5214 - val_acc: 0.7993\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4086 - acc: 0.8583 - val_loss: 0.5192 - val_acc: 0.7993\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4068 - acc: 0.8583 - val_loss: 0.5195 - val_acc: 0.7993\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4081 - acc: 0.8583 - val_loss: 0.5186 - val_acc: 0.7993\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4739 - acc: 0.8203 - val_loss: 0.4205 - val_acc: 0.8571\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4715 - acc: 0.8203 - val_loss: 0.4204 - val_acc: 0.8571\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4703 - acc: 0.8203 - val_loss: 0.4178 - val_acc: 0.8571\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4706 - acc: 0.8203 - val_loss: 0.4180 - val_acc: 0.8571\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 14ms/step - loss: 0.4986 - acc: 0.8022 - val_loss: 0.6192 - val_acc: 0.7198\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4948 - acc: 0.8022 - val_loss: 0.6193 - val_acc: 0.7198\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4928 - acc: 0.8022 - val_loss: 0.6135 - val_acc: 0.7198\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4918 - acc: 0.8022 - val_loss: 0.6192 - val_acc: 0.7198\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4904 - acc: 0.8022 - val_loss: 0.6222 - val_acc: 0.7198\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4896 - acc: 0.8022 - val_loss: 0.6252 - val_acc: 0.7198\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4889 - acc: 0.8022 - val_loss: 0.6253 - val_acc: 0.7198\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4906 - acc: 0.8022 - val_loss: 0.6208 - val_acc: 0.7198\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.5743 - acc: 0.7472 - val_loss: 0.5475 - val_acc: 0.7649\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5690 - acc: 0.7472 - val_loss: 0.5461 - val_acc: 0.7649\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5662 - acc: 0.7472 - val_loss: 0.5444 - val_acc: 0.7649\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5655 - acc: 0.7472 - val_loss: 0.5445 - val_acc: 0.7649\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5639 - acc: 0.7472 - val_loss: 0.5444 - val_acc: 0.7649\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5633 - acc: 0.7472 - val_loss: 0.5438 - val_acc: 0.7649\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5626 - acc: 0.7472 - val_loss: 0.5435 - val_acc: 0.7649\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5626 - acc: 0.7472 - val_loss: 0.5433 - val_acc: 0.7649\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5614 - acc: 0.7472 - val_loss: 0.5440 - val_acc: 0.7649\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5613 - acc: 0.7472 - val_loss: 0.5431 - val_acc: 0.7649\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5617 - acc: 0.7472 - val_loss: 0.5439 - val_acc: 0.7649\n",
      "Epoch 11: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5152 - acc: 0.7940 - val_loss: 0.5443 - val_acc: 0.7658\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5114 - acc: 0.7940 - val_loss: 0.5446 - val_acc: 0.7658\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5083 - acc: 0.7940 - val_loss: 0.5456 - val_acc: 0.7658\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5074 - acc: 0.7940 - val_loss: 0.5479 - val_acc: 0.7658\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5067 - acc: 0.7940 - val_loss: 0.5487 - val_acc: 0.7658\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5077 - acc: 0.7940 - val_loss: 0.5489 - val_acc: 0.7658\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4083 - acc: 0.8667 - val_loss: 0.4078 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3940 - acc: 0.8667 - val_loss: 0.4111 - val_acc: 0.8587\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3919 - acc: 0.8667 - val_loss: 0.4131 - val_acc: 0.8587\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3889 - acc: 0.8667 - val_loss: 0.4124 - val_acc: 0.8587\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3872 - acc: 0.8667 - val_loss: 0.4156 - val_acc: 0.8587\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3879 - acc: 0.8667 - val_loss: 0.4159 - val_acc: 0.8587\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3782 - acc: 0.8751 - val_loss: 0.3849 - val_acc: 0.8736\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3775 - acc: 0.8751 - val_loss: 0.3839 - val_acc: 0.8736\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3751 - acc: 0.8751 - val_loss: 0.3851 - val_acc: 0.8736\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3744 - acc: 0.8751 - val_loss: 0.3872 - val_acc: 0.8736\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3737 - acc: 0.8751 - val_loss: 0.3863 - val_acc: 0.8736\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3731 - acc: 0.8751 - val_loss: 0.3863 - val_acc: 0.8736\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3740 - acc: 0.8751 - val_loss: 0.3866 - val_acc: 0.8736\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 13ms/step - loss: 0.4842 - acc: 0.8181 - val_loss: 0.4799 - val_acc: 0.8153\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4755 - acc: 0.8181 - val_loss: 0.4797 - val_acc: 0.8153\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4736 - acc: 0.8181 - val_loss: 0.4800 - val_acc: 0.8153\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4727 - acc: 0.8181 - val_loss: 0.4803 - val_acc: 0.8153\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4723 - acc: 0.8181 - val_loss: 0.4814 - val_acc: 0.8153\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4717 - acc: 0.8181 - val_loss: 0.4820 - val_acc: 0.8153\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4710 - acc: 0.8181 - val_loss: 0.4818 - val_acc: 0.8153\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 0.4715 - acc: 0.8181 - val_loss: 0.4823 - val_acc: 0.8153\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3281 - acc: 0.9059 - val_loss: 0.3678 - val_acc: 0.8773\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3132 - acc: 0.9059 - val_loss: 0.3801 - val_acc: 0.8773\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3120 - acc: 0.9059 - val_loss: 0.4275 - val_acc: 0.8773\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3110 - acc: 0.9059 - val_loss: 0.3769 - val_acc: 0.8773\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3114 - acc: 0.9059 - val_loss: 0.3762 - val_acc: 0.8773\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5415 - acc: 0.7856 - val_loss: 0.5190 - val_acc: 0.7881\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5194 - acc: 0.7856 - val_loss: 0.5189 - val_acc: 0.7881\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5178 - acc: 0.7856 - val_loss: 0.5190 - val_acc: 0.7881\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5169 - acc: 0.7856 - val_loss: 0.5189 - val_acc: 0.7881\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5159 - acc: 0.7856 - val_loss: 0.5186 - val_acc: 0.7881\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5157 - acc: 0.7856 - val_loss: 0.5187 - val_acc: 0.7881\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5152 - acc: 0.7856 - val_loss: 0.5179 - val_acc: 0.7881\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5143 - acc: 0.7856 - val_loss: 0.5163 - val_acc: 0.7881\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5143 - acc: 0.7856 - val_loss: 0.5170 - val_acc: 0.7881\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4991 - acc: 0.8034 - val_loss: 0.4494 - val_acc: 0.8476\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4968 - acc: 0.8034 - val_loss: 0.4456 - val_acc: 0.8476\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4957 - acc: 0.8034 - val_loss: 0.4437 - val_acc: 0.8476\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4947 - acc: 0.8034 - val_loss: 0.4433 - val_acc: 0.8476\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4938 - acc: 0.8034 - val_loss: 0.4443 - val_acc: 0.8476\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4945 - acc: 0.8034 - val_loss: 0.4428 - val_acc: 0.8476\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5462 - acc: 0.7670 - val_loss: 0.6471 - val_acc: 0.6766\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5430 - acc: 0.7670 - val_loss: 0.6495 - val_acc: 0.6766\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5414 - acc: 0.7670 - val_loss: 0.6568 - val_acc: 0.6766\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5407 - acc: 0.7670 - val_loss: 0.6575 - val_acc: 0.6766\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5408 - acc: 0.7670 - val_loss: 0.6557 - val_acc: 0.6766\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4262 - acc: 0.8574 - val_loss: 0.6179 - val_acc: 0.7398\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4085 - acc: 0.8574 - val_loss: 0.6355 - val_acc: 0.7398\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4080 - acc: 0.8574 - val_loss: 0.6365 - val_acc: 0.7398\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4069 - acc: 0.8574 - val_loss: 0.6366 - val_acc: 0.7398\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4066 - acc: 0.8574 - val_loss: 0.6372 - val_acc: 0.7398\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4060 - acc: 0.8574 - val_loss: 0.6388 - val_acc: 0.7398\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4054 - acc: 0.8574 - val_loss: 0.6358 - val_acc: 0.7398\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4052 - acc: 0.8574 - val_loss: 0.6377 - val_acc: 0.7398\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4049 - acc: 0.8574 - val_loss: 0.6353 - val_acc: 0.7398\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4058 - acc: 0.8574 - val_loss: 0.6324 - val_acc: 0.7398\n",
      "Epoch 10: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5008 - acc: 0.8043 - val_loss: 0.5405 - val_acc: 0.7732\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4938 - acc: 0.8043 - val_loss: 0.5366 - val_acc: 0.7732\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4920 - acc: 0.8043 - val_loss: 0.5347 - val_acc: 0.7732\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4905 - acc: 0.8043 - val_loss: 0.5320 - val_acc: 0.7732\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4908 - acc: 0.8043 - val_loss: 0.5326 - val_acc: 0.7732\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 13ms/step - loss: 0.3539 - acc: 0.8926 - val_loss: 0.2438 - val_acc: 0.9440\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3432 - acc: 0.8926 - val_loss: 0.2369 - val_acc: 0.9440\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3423 - acc: 0.8926 - val_loss: 0.2404 - val_acc: 0.9440\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3411 - acc: 0.8926 - val_loss: 0.2417 - val_acc: 0.9440\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3403 - acc: 0.8926 - val_loss: 0.2403 - val_acc: 0.9440\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3400 - acc: 0.8926 - val_loss: 0.2415 - val_acc: 0.9440\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3388 - acc: 0.8926 - val_loss: 0.2377 - val_acc: 0.9440\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3395 - acc: 0.8926 - val_loss: 0.2381 - val_acc: 0.9440\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4480 - acc: 0.8378 - val_loss: 0.4061 - val_acc: 0.8587\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4404 - acc: 0.8378 - val_loss: 0.4039 - val_acc: 0.8587\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4386 - acc: 0.8378 - val_loss: 0.4028 - val_acc: 0.8587\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4380 - acc: 0.8378 - val_loss: 0.4029 - val_acc: 0.8587\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4383 - acc: 0.8378 - val_loss: 0.4045 - val_acc: 0.8587\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "27/27 [==============================] - 2s 14ms/step - loss: 0.3500 - acc: 0.8939 - val_loss: 0.3480 - val_acc: 0.8929\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.3383 - acc: 0.8939 - val_loss: 0.3477 - val_acc: 0.8929\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.3358 - acc: 0.8939 - val_loss: 0.3504 - val_acc: 0.8929\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.3335 - acc: 0.8939 - val_loss: 0.3463 - val_acc: 0.8929\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.3325 - acc: 0.8939 - val_loss: 0.3510 - val_acc: 0.8929\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.3304 - acc: 0.8939 - val_loss: 0.3559 - val_acc: 0.8929\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.3312 - acc: 0.8939 - val_loss: 0.3520 - val_acc: 0.8929\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4778 - acc: 0.8248 - val_loss: 0.5062 - val_acc: 0.7955\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4633 - acc: 0.8248 - val_loss: 0.5071 - val_acc: 0.7955\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4625 - acc: 0.8248 - val_loss: 0.5093 - val_acc: 0.7955\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4621 - acc: 0.8248 - val_loss: 0.5081 - val_acc: 0.7955\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4614 - acc: 0.8248 - val_loss: 0.5077 - val_acc: 0.7955\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4600 - acc: 0.8248 - val_loss: 0.5063 - val_acc: 0.7955\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4597 - acc: 0.8248 - val_loss: 0.5066 - val_acc: 0.7955\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4599 - acc: 0.8248 - val_loss: 0.5079 - val_acc: 0.7955\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.5531 - acc: 0.7764 - val_loss: 0.6045 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5452 - acc: 0.7764 - val_loss: 0.5945 - val_acc: 0.7500\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5386 - acc: 0.7764 - val_loss: 0.5863 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5328 - acc: 0.7764 - val_loss: 0.5804 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5299 - acc: 0.7764 - val_loss: 0.5767 - val_acc: 0.7500\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5281 - acc: 0.7764 - val_loss: 0.5746 - val_acc: 0.7500\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5265 - acc: 0.7764 - val_loss: 0.5741 - val_acc: 0.7500\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5255 - acc: 0.7764 - val_loss: 0.5727 - val_acc: 0.7500\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5240 - acc: 0.7764 - val_loss: 0.5722 - val_acc: 0.7500\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5227 - acc: 0.7764 - val_loss: 0.5717 - val_acc: 0.7500\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5217 - acc: 0.7764 - val_loss: 0.5719 - val_acc: 0.7500\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5208 - acc: 0.7764 - val_loss: 0.5722 - val_acc: 0.7500\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5203 - acc: 0.7764 - val_loss: 0.5730 - val_acc: 0.7500\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5194 - acc: 0.7764 - val_loss: 0.5733 - val_acc: 0.7500\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5187 - acc: 0.7764 - val_loss: 0.5723 - val_acc: 0.7500\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5179 - acc: 0.7764 - val_loss: 0.5725 - val_acc: 0.7500\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5170 - acc: 0.7764 - val_loss: 0.5727 - val_acc: 0.7500\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5166 - acc: 0.7764 - val_loss: 0.5729 - val_acc: 0.7500\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5157 - acc: 0.7764 - val_loss: 0.5728 - val_acc: 0.7500\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5153 - acc: 0.7764 - val_loss: 0.5747 - val_acc: 0.7500\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5145 - acc: 0.7764 - val_loss: 0.5754 - val_acc: 0.7500\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5134 - acc: 0.7764 - val_loss: 0.5749 - val_acc: 0.7500\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5123 - acc: 0.7764 - val_loss: 0.5756 - val_acc: 0.7500\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5133 - acc: 0.7764 - val_loss: 0.5753 - val_acc: 0.7500\n",
      "Epoch 24: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4197 - acc: 0.8630 - val_loss: 0.6244 - val_acc: 0.7435\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4052 - acc: 0.8630 - val_loss: 0.6197 - val_acc: 0.7435\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4011 - acc: 0.8630 - val_loss: 0.6240 - val_acc: 0.7435\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4032 - acc: 0.8630 - val_loss: 0.6108 - val_acc: 0.7435\n",
      "Epoch 4: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 19ms/step - loss: 0.4583 - acc: 0.8325 - val_loss: 0.2925 - val_acc: 0.9278\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4539 - acc: 0.8325 - val_loss: 0.2969 - val_acc: 0.9278\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4519 - acc: 0.8325 - val_loss: 0.2984 - val_acc: 0.9278\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4513 - acc: 0.8325 - val_loss: 0.2966 - val_acc: 0.9278\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4525 - acc: 0.8325 - val_loss: 0.3019 - val_acc: 0.9278\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4817 - acc: 0.8136 - val_loss: 0.5938 - val_acc: 0.7398\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4797 - acc: 0.8136 - val_loss: 0.5916 - val_acc: 0.7398\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4781 - acc: 0.8136 - val_loss: 0.5910 - val_acc: 0.7398\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4779 - acc: 0.8136 - val_loss: 0.5983 - val_acc: 0.7398\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4767 - acc: 0.8136 - val_loss: 0.5970 - val_acc: 0.7398\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4766 - acc: 0.8136 - val_loss: 0.5944 - val_acc: 0.7398\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4760 - acc: 0.8136 - val_loss: 0.5986 - val_acc: 0.7398\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4758 - acc: 0.8136 - val_loss: 0.6019 - val_acc: 0.7398\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4763 - acc: 0.8136 - val_loss: 0.6017 - val_acc: 0.7398\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 12ms/step - loss: 0.5457 - acc: 0.7704 - val_loss: 0.5419 - val_acc: 0.7615\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5388 - acc: 0.7704 - val_loss: 0.5421 - val_acc: 0.7615\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5383 - acc: 0.7704 - val_loss: 0.5408 - val_acc: 0.7615\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5382 - acc: 0.7704 - val_loss: 0.5401 - val_acc: 0.7615\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5378 - acc: 0.7704 - val_loss: 0.5404 - val_acc: 0.7615\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5378 - acc: 0.7704 - val_loss: 0.5404 - val_acc: 0.7615\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5383 - acc: 0.7704 - val_loss: 0.5402 - val_acc: 0.7615\n",
      "Epoch 7: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4877 - acc: 0.8127 - val_loss: 0.5212 - val_acc: 0.7844\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4832 - acc: 0.8127 - val_loss: 0.5233 - val_acc: 0.7844\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4813 - acc: 0.8127 - val_loss: 0.5264 - val_acc: 0.7844\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4800 - acc: 0.8127 - val_loss: 0.5265 - val_acc: 0.7844\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4804 - acc: 0.8127 - val_loss: 0.5277 - val_acc: 0.7844\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5132 - acc: 0.7938 - val_loss: 0.4606 - val_acc: 0.8290\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5078 - acc: 0.7938 - val_loss: 0.4610 - val_acc: 0.8290\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5064 - acc: 0.7938 - val_loss: 0.4626 - val_acc: 0.8290\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5059 - acc: 0.7938 - val_loss: 0.4622 - val_acc: 0.8290\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5053 - acc: 0.7938 - val_loss: 0.4622 - val_acc: 0.8290\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5046 - acc: 0.7938 - val_loss: 0.4628 - val_acc: 0.8290\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5039 - acc: 0.7938 - val_loss: 0.4623 - val_acc: 0.8290\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5035 - acc: 0.7938 - val_loss: 0.4595 - val_acc: 0.8290\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5026 - acc: 0.7938 - val_loss: 0.4598 - val_acc: 0.8290\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5023 - acc: 0.7938 - val_loss: 0.4597 - val_acc: 0.8290\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5021 - acc: 0.7938 - val_loss: 0.4600 - val_acc: 0.8290\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5012 - acc: 0.7938 - val_loss: 0.4609 - val_acc: 0.8290\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5022 - acc: 0.7938 - val_loss: 0.4598 - val_acc: 0.8290\n",
      "Epoch 13: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.5334 - acc: 0.7735 - val_loss: 0.6148 - val_acc: 0.7138\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5284 - acc: 0.7735 - val_loss: 0.6121 - val_acc: 0.7138\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5272 - acc: 0.7735 - val_loss: 0.6116 - val_acc: 0.7138\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5256 - acc: 0.7735 - val_loss: 0.6132 - val_acc: 0.7138\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5244 - acc: 0.7735 - val_loss: 0.6136 - val_acc: 0.7138\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5243 - acc: 0.7735 - val_loss: 0.6123 - val_acc: 0.7138\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5222 - acc: 0.7735 - val_loss: 0.6205 - val_acc: 0.7138\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5218 - acc: 0.7735 - val_loss: 0.6214 - val_acc: 0.7138\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5232 - acc: 0.7735 - val_loss: 0.6167 - val_acc: 0.7138\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4967 - acc: 0.8136 - val_loss: 0.5646 - val_acc: 0.7509\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4833 - acc: 0.8136 - val_loss: 0.5719 - val_acc: 0.7509\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4829 - acc: 0.8136 - val_loss: 0.5735 - val_acc: 0.7509\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4807 - acc: 0.8136 - val_loss: 0.5744 - val_acc: 0.7509\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4810 - acc: 0.8136 - val_loss: 0.5692 - val_acc: 0.7509\n",
      "Epoch 5: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4653 - acc: 0.8248 - val_loss: 0.5322 - val_acc: 0.7807\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4630 - acc: 0.8248 - val_loss: 0.5328 - val_acc: 0.7807\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4616 - acc: 0.8248 - val_loss: 0.5335 - val_acc: 0.7807\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4607 - acc: 0.8248 - val_loss: 0.5330 - val_acc: 0.7807\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4596 - acc: 0.8248 - val_loss: 0.5318 - val_acc: 0.7807\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4587 - acc: 0.8248 - val_loss: 0.5329 - val_acc: 0.7807\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4572 - acc: 0.8248 - val_loss: 0.5307 - val_acc: 0.7807\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4575 - acc: 0.8248 - val_loss: 0.5298 - val_acc: 0.7807\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 2s 25ms/step - loss: 0.5046 - acc: 0.8037 - val_loss: 0.5757 - val_acc: 0.7545\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4992 - acc: 0.8037 - val_loss: 0.5691 - val_acc: 0.7545\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4958 - acc: 0.8037 - val_loss: 0.5666 - val_acc: 0.7545\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4943 - acc: 0.8037 - val_loss: 0.5653 - val_acc: 0.7545\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4927 - acc: 0.8037 - val_loss: 0.5634 - val_acc: 0.7545\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4919 - acc: 0.8037 - val_loss: 0.5631 - val_acc: 0.7545\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4918 - acc: 0.8037 - val_loss: 0.5654 - val_acc: 0.7545\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4906 - acc: 0.8037 - val_loss: 0.5642 - val_acc: 0.7545\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4900 - acc: 0.8037 - val_loss: 0.5655 - val_acc: 0.7545\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4898 - acc: 0.8037 - val_loss: 0.5649 - val_acc: 0.7545\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4896 - acc: 0.8037 - val_loss: 0.5666 - val_acc: 0.7545\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4888 - acc: 0.8037 - val_loss: 0.5662 - val_acc: 0.7545\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4885 - acc: 0.8037 - val_loss: 0.5678 - val_acc: 0.7545\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4884 - acc: 0.8037 - val_loss: 0.5673 - val_acc: 0.7545\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4878 - acc: 0.8037 - val_loss: 0.5676 - val_acc: 0.7545\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4872 - acc: 0.8037 - val_loss: 0.5676 - val_acc: 0.7545\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4869 - acc: 0.8037 - val_loss: 0.5683 - val_acc: 0.7545\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4870 - acc: 0.8037 - val_loss: 0.5714 - val_acc: 0.7545\n",
      "Epoch 18: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4331 - acc: 0.8537 - val_loss: 0.5462 - val_acc: 0.7732\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4212 - acc: 0.8537 - val_loss: 0.5536 - val_acc: 0.7732\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4180 - acc: 0.8537 - val_loss: 0.5585 - val_acc: 0.7732\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4165 - acc: 0.8537 - val_loss: 0.5607 - val_acc: 0.7732\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4159 - acc: 0.8537 - val_loss: 0.5641 - val_acc: 0.7732\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4149 - acc: 0.8537 - val_loss: 0.5627 - val_acc: 0.7732\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4141 - acc: 0.8537 - val_loss: 0.5635 - val_acc: 0.7732\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4142 - acc: 0.8537 - val_loss: 0.5617 - val_acc: 0.7732\n",
      "Epoch 8: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3708 - acc: 0.8798 - val_loss: 0.4864 - val_acc: 0.8216\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3669 - acc: 0.8798 - val_loss: 0.4890 - val_acc: 0.8216\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3641 - acc: 0.8798 - val_loss: 0.4910 - val_acc: 0.8216\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3625 - acc: 0.8798 - val_loss: 0.4929 - val_acc: 0.8216\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3610 - acc: 0.8798 - val_loss: 0.4918 - val_acc: 0.8216\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3606 - acc: 0.8798 - val_loss: 0.4954 - val_acc: 0.8216\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3603 - acc: 0.8798 - val_loss: 0.4971 - val_acc: 0.8216\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3589 - acc: 0.8798 - val_loss: 0.5008 - val_acc: 0.8216\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3579 - acc: 0.8798 - val_loss: 0.5027 - val_acc: 0.8216\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3574 - acc: 0.8798 - val_loss: 0.5075 - val_acc: 0.8216\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3567 - acc: 0.8798 - val_loss: 0.5128 - val_acc: 0.8216\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3575 - acc: 0.8798 - val_loss: 0.5064 - val_acc: 0.8216\n",
      "Epoch 12: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 12ms/step - loss: 0.5627 - acc: 0.7693 - val_loss: 0.6081 - val_acc: 0.7293\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5404 - acc: 0.7693 - val_loss: 0.6013 - val_acc: 0.7293\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5392 - acc: 0.7693 - val_loss: 0.5971 - val_acc: 0.7293\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5370 - acc: 0.7693 - val_loss: 0.5944 - val_acc: 0.7293\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5363 - acc: 0.7693 - val_loss: 0.5933 - val_acc: 0.7293\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5357 - acc: 0.7693 - val_loss: 0.5970 - val_acc: 0.7293\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5353 - acc: 0.7693 - val_loss: 0.5943 - val_acc: 0.7293\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5340 - acc: 0.7693 - val_loss: 0.5936 - val_acc: 0.7293\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.5348 - acc: 0.7693 - val_loss: 0.5943 - val_acc: 0.7293\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 2s 35ms/step - loss: 0.4291 - acc: 0.8690 - val_loss: 0.5714 - val_acc: 0.7397\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4155 - acc: 0.8690 - val_loss: 0.5797 - val_acc: 0.7397\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4032 - acc: 0.8690 - val_loss: 0.5896 - val_acc: 0.7397\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3943 - acc: 0.8690 - val_loss: 0.6017 - val_acc: 0.7397\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3871 - acc: 0.8690 - val_loss: 0.6134 - val_acc: 0.7397\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3822 - acc: 0.8690 - val_loss: 0.6230 - val_acc: 0.7397\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3774 - acc: 0.8690 - val_loss: 0.6386 - val_acc: 0.7397\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3736 - acc: 0.8690 - val_loss: 0.6492 - val_acc: 0.7397\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3707 - acc: 0.8690 - val_loss: 0.6667 - val_acc: 0.7397\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3695 - acc: 0.8690 - val_loss: 0.8222 - val_acc: 0.7397\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3670 - acc: 0.8690 - val_loss: 0.8133 - val_acc: 0.7397\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3653 - acc: 0.8690 - val_loss: 0.8180 - val_acc: 0.7397\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3633 - acc: 0.8690 - val_loss: 0.8210 - val_acc: 0.7397\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3630 - acc: 0.8690 - val_loss: 0.8380 - val_acc: 0.7397\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3603 - acc: 0.8690 - val_loss: 0.8417 - val_acc: 0.7397\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3580 - acc: 0.8690 - val_loss: 0.8336 - val_acc: 0.7397\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3570 - acc: 0.8690 - val_loss: 0.8406 - val_acc: 0.7397\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3550 - acc: 0.8690 - val_loss: 0.8528 - val_acc: 0.7397\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3545 - acc: 0.8690 - val_loss: 0.8837 - val_acc: 0.7397\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3527 - acc: 0.8690 - val_loss: 0.8748 - val_acc: 0.7397\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3515 - acc: 0.8690 - val_loss: 0.8738 - val_acc: 0.7397\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3501 - acc: 0.8690 - val_loss: 0.9098 - val_acc: 0.7397\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3484 - acc: 0.8690 - val_loss: 1.1872 - val_acc: 0.7397\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3474 - acc: 0.8690 - val_loss: 1.1931 - val_acc: 0.7397\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3465 - acc: 0.8690 - val_loss: 1.1954 - val_acc: 0.7397\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3454 - acc: 0.8690 - val_loss: 1.2048 - val_acc: 0.7397\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3446 - acc: 0.8690 - val_loss: 1.2040 - val_acc: 0.7397\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3453 - acc: 0.8690 - val_loss: 1.2979 - val_acc: 0.7397\n",
      "Epoch 28: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.4054 - acc: 0.8910 - val_loss: 0.3395 - val_acc: 0.9108\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3517 - acc: 0.8910 - val_loss: 0.3253 - val_acc: 0.9108\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3459 - acc: 0.8910 - val_loss: 0.3231 - val_acc: 0.9108\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3443 - acc: 0.8910 - val_loss: 0.3216 - val_acc: 0.9108\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3433 - acc: 0.8910 - val_loss: 0.3209 - val_acc: 0.9108\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3429 - acc: 0.8910 - val_loss: 0.3187 - val_acc: 0.9108\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3419 - acc: 0.8910 - val_loss: 0.3198 - val_acc: 0.9108\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3407 - acc: 0.8910 - val_loss: 0.3192 - val_acc: 0.9108\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3407 - acc: 0.8910 - val_loss: 0.3183 - val_acc: 0.9108\n",
      "Epoch 9: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "27/27 [==============================] - 2s 14ms/step - loss: 0.4175 - acc: 0.8595 - val_loss: 0.3860 - val_acc: 0.8673\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.4092 - acc: 0.8595 - val_loss: 0.3901 - val_acc: 0.8673\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.4070 - acc: 0.8595 - val_loss: 0.3863 - val_acc: 0.8673\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.4053 - acc: 0.8595 - val_loss: 0.3861 - val_acc: 0.8673\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.4040 - acc: 0.8595 - val_loss: 0.3870 - val_acc: 0.8673\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.4044 - acc: 0.8595 - val_loss: 0.3865 - val_acc: 0.8673\n",
      "Epoch 6: early stopping\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,301\n",
      "Trainable params: 2,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 11ms/step - loss: 0.3617 - acc: 0.8835 - val_loss: 0.5421 - val_acc: 0.7844\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3504 - acc: 0.8835 - val_loss: 0.5461 - val_acc: 0.7844\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3431 - acc: 0.8835 - val_loss: 0.5469 - val_acc: 0.7844\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.3540 - acc: 0.8835 - val_loss: 0.5385 - val_acc: 0.7844\n",
      "Epoch 4: early stopping\n",
      "        loss       acc  val_loss   val_acc\n",
      "0   1.750967  0.809879  1.432946  0.862454\n",
      "1   1.841095  0.819198  1.575448  0.855019\n",
      "0   1.410256  0.844362  1.071762  0.895911\n",
      "1   1.445942  0.859273  1.011690  0.895911\n",
      "0   0.890241  0.898416  0.463553  0.955390\n",
      "..       ...       ...       ...       ...\n",
      "5   0.404423  0.859515  0.386517  0.867347\n",
      "0   0.361718  0.883504  0.542101  0.784387\n",
      "1   0.350425  0.883504  0.546106  0.784387\n",
      "2   0.343064  0.883504  0.546883  0.784387\n",
      "3   0.353987  0.883504  0.538461  0.784387\n",
      "\n",
      "[2385 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "column = ['기관합계', '기타법인', '개인', '외국인합계', '전체']\n",
    "data = pd.read_csv(\"modified_stock_trade_volume.csv\")\n",
    "codes = set(data['code'].apply(lambda x : \"0\" *(6-len(str(x))) + str(x)))\n",
    "log_df = pd.DataFrame(columns=['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)\n",
    "\n",
    "for code in codes:\n",
    "  info = list()         # feature들의 임시 저장 공간\n",
    "  y = list()            # y값의 임시 저장 공간\n",
    "  for col in column:\n",
    "    df = pd.read_csv(\"lstm/\" + code + str(col) + '_info.csv')   # 위에서 저장한 데이터 로드\n",
    "    df['y'] = df['y'] < -2\n",
    "    y.append(np.array(df['y']))                                 # y값 저장\n",
    "    df.drop(['날짜', 'code', 'y'], inplace=True, axis=1)         # 데이터에서 y값 및 인덱스값 분리\n",
    "    info.append(df)                                             # x값 저장\n",
    "\n",
    "  x = [[[info[i].iloc[k][j] for i in range(len(info))] for j in range(10)] for k in range(len(info[0]))]    # 데이터의 형태를 학습을 위해 변형\n",
    "  x = np.array(x)                                                                                           # (학습데이터수 * window * feature수)\n",
    "  x_train = x[:int(len(x)*0.8)]         # 전체 기간 중 앞부분 80%를 train에 활용, 나머지는 test\n",
    "  x_test = x[int(len(x)*0.8):]\n",
    "  y_train = y[0][:int(len(y[0])*0.8)]\n",
    "  y_test = y[0][int(len(y[0])*0.8):]\n",
    "\n",
    "  K.clear_session()\n",
    "  model = Sequential()                  # 간단한 LSTM 모델 형성\n",
    "  model.add(LSTM(20, input_shape=(10, 5)))\n",
    "  model.add(Dense(10, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "  model.summary()\n",
    "  if code != codes[0]:\n",
    "    model.load_weights('v')             # 반복적인 학습을 위해 학습된 모델의 weight 저장 및 가져오기\n",
    "\n",
    "  history = model.fit(x_train, y_train, epochs=50, batch_size=30, verbose=1, callbacks=early_stop, validation_data = (x_test, y_test))\n",
    "  log_df = pd.concat([log_df, pd.DataFrame(history.history)]) # log_df에 학습 과정 (acc, loss, val_acc, val_loss 저장)\n",
    "  model.save_weights('v')               # 학습한 모델의 weight 저장\n",
    "\n",
    "print(log_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 5-3 Result\n",
    "log_df에 저장한 값들을 바탕으로 학습 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(list(log_df['loss']), label='loss')\n",
    "plt.plot(list(log_df['val_loss']), label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(list(log_df['acc']), label='acc')\n",
    "plt.plot(list(log_df['val_acc']), label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. LSTM for stock volume with financial statement embeddings\n",
    "주가지표, 재무제표의 임베딩을 거래정보와 함께 feature로 사용해 LSTM 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3-1. Convert data to 3D time series data\n",
    "데이터를 시계열 데이터 형태로 변환 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trade_data = pd.read_csv(\"modified_stock_trade_volume.csv\")\n",
    "train = pd.read_csv(\"embedding/train_embedding.csv\")\n",
    "test = pd.read_csv(\"embedding/test_embedding.csv\")\n",
    "valid = pd.read_csv(\"embedding/validation_embedding.csv\")\n",
    "\n",
    "trade_data['code'] = trade_data['code'].apply(lambda x : \"0\" *(6-len(str(x))) + str(x))\n",
    "data = pd.concat([train, test, valid])                          # 임베딩 전체 데이터 병합\n",
    "data['code'] = data['CODE'].apply(lambda x : str(x)[-6:])\n",
    "data.drop(['Unnamed: 0', 'CODE'], axis=1, inplace=True)\n",
    "\n",
    "data = pd.merge(trade_data, data)                               # 병합된 임베딩 데이터와 기존 거래정보 데이터 병합\n",
    "data.set_index('날짜', inplace=True)\n",
    "data.sort_index(inplace=True)                                   # index를 날짜로 설정 후 날짜순으로 정렬해 시계열 데이터 생성 준비\n",
    "\n",
    "codes = set(data['code'])                                       # 데이터의 기업 코드 추출\n",
    "column = ['기관합계', '기타법인', '개인', '외국인합계', '전체', 'embedding1', 'embedding2', 'embedding3', 'embedding4', 'embedding5', 'embedding6', 'embedding7', 'embedding8', 'embedding9', 'embedding10', 'embedding11', 'embedding12', 'embedding13', 'embedding14', 'embedding15']      # 현재 20개 feature\n",
    "\n",
    "for code in codes:\n",
    "  for col in column:\n",
    "    a = pd.DataFrame()\n",
    "    for i in range(10):\n",
    "      a['shift'+str(i)] = data[data['code']==code][col].shift(i)    # 2-1에서 했던 같은 방식으로 시계열 데이터 생성\n",
    "    a['code'] = data[data['code']==code]['code']\n",
    "    a['y'] = data[data['code']==code]['Y']\n",
    "    a = a.dropna()\n",
    "    a.to_csv(\"lstm2/\" + str(code) + str(col) + '_info.csv')         # 기업 별, 특성 별로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "codes = list(set(data['code']))\n",
    "column = ['기관합계', '기타법인', '개인', '외국인합계', '전체', 'embedding1', 'embedding2', 'embedding3', 'embedding4', 'embedding5',\n",
    "          'embedding6', 'embedding7', 'embedding8', 'embedding9', 'embedding10', 'embedding11', 'embedding12',\n",
    "          'embedding13', 'embedding14', 'embedding15']      # 학습에 쓰일 feature 리스트\n",
    "train_data = list()\n",
    "\n",
    "for code in codes:          # train_data 생성 부분\n",
    "    info = list()\n",
    "    y = list()\n",
    "    for col in column:      # 기업별로 각 feature에 대해 info에 x값(feature), y에 y값을 모음\n",
    "        df = pd.read_csv(\"lstm2/\" + code + str(col) + '_info.csv')  # 위에서 저장한 데이터를 불러옴\n",
    "        df['y'] = df['y'] < -2\n",
    "        y.append(np.array(df['y']))                                 # y값 추출\n",
    "        df.drop(['날짜', 'code', 'y'], inplace=True, axis=1)         # x값 분리\n",
    "        info.append(df)\n",
    "\n",
    "    x = [[[info[i].iloc[k][j] for i in range(len(info))] for j in range(10)] for k in range(len(info[0]))]# 학습에 필요한(데이터수)*(window)*(feature수)형태로 변환\n",
    "    x = np.array(x)\n",
    "    train_data.append((x, y[0]))        #기업별로 train_data에 학습용 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_valid = list()\n",
    "y_valid = list()\n",
    "\n",
    "for x, y in train_data:\n",
    "  x_valid.append(x[int(len(x)*0.75):])  # valid용 데이터 우선분리, valid는 전체 기간 중 마지막 25%.\n",
    "  y_valid.append(y[int(len(y)*0.75):])  # train : test : valid = 50 : 25 : 25\n",
    "\n",
    "x = np.array(x_valid)\n",
    "y = np.array(y_valid)\n",
    "\n",
    "np.save('x_valid', x)       # valid용 데이터는 3차원 넘파이배열 형태로 따로 저장\n",
    "np.save('y_valid', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3-2-1. LSTM with huge weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 48ms/step - loss: 1.4146 - acc: 0.8341 - val_loss: 0.5974 - val_acc: 0.9057\n",
      "8/8 [==============================] - 3s 56ms/step - loss: 2.0942 - acc: 0.7630 - val_loss: 0.6329 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.8603 - acc: 0.7204 - val_loss: 0.6552 - val_acc: 0.7075\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.3967 - acc: 0.6303 - val_loss: 0.6281 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4458 - acc: 0.7867 - val_loss: 0.6611 - val_acc: 0.6226\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.7207 - acc: 0.7630 - val_loss: 0.6598 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.1866 - acc: 0.7156 - val_loss: 0.6382 - val_acc: 0.6604\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.1706 - acc: 0.7583 - val_loss: 0.6556 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.6411 - acc: 0.6493 - val_loss: 0.6675 - val_acc: 0.6226\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 1.3912 - acc: 0.7062 - val_loss: 0.6604 - val_acc: 0.6981\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 2.8930 - acc: 0.6019 - val_loss: 0.6904 - val_acc: 0.5566\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.2626 - acc: 0.5498 - val_loss: 0.6330 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 2.1316 - acc: 0.5924 - val_loss: 0.6923 - val_acc: 0.4811\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 1.4352 - acc: 0.5308 - val_loss: 0.6857 - val_acc: 0.4340\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 1.9133 - acc: 0.4076 - val_loss: 0.7084 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 1.3068 - acc: 0.3507 - val_loss: 0.7166 - val_acc: 0.2381\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.2661 - acc: 0.3033 - val_loss: 0.7026 - val_acc: 0.3396\n",
      "7/7 [==============================] - 3s 66ms/step - loss: 1.4031 - acc: 0.3260 - val_loss: 0.6901 - val_acc: 0.4176\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.9102 - acc: 0.2559 - val_loss: 0.7119 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.3768 - acc: 0.2559 - val_loss: 0.7142 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 1.0558 - acc: 0.2227 - val_loss: 0.7392 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 1.9884 - acc: 0.3033 - val_loss: 0.7081 - val_acc: 0.3396\n",
      "8/8 [==============================] - 2s 53ms/step - loss: 2.0370 - acc: 0.3175 - val_loss: 0.7078 - val_acc: 0.3962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.6062 - acc: 0.1754 - val_loss: 0.7382 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.8291 - acc: 0.2133 - val_loss: 0.7296 - val_acc: 0.2453\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.9723 - acc: 0.2559 - val_loss: 0.7351 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.8586 - acc: 0.2275 - val_loss: 0.7462 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6491 - acc: 0.1991 - val_loss: 0.7625 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7550 - acc: 0.1991 - val_loss: 0.7585 - val_acc: 0.1698\n",
      "7/7 [==============================] - 2s 60ms/step - loss: 1.2424 - acc: 0.1292 - val_loss: 0.7560 - val_acc: 0.1250\n",
      "8/8 [==============================] - 2s 57ms/step - loss: 1.2791 - acc: 0.1327 - val_loss: 0.7645 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8364 - acc: 0.2275 - val_loss: 0.7661 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 2.3875 - acc: 0.3128 - val_loss: 0.7731 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 1.6741 - acc: 0.1754 - val_loss: 0.7934 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 1.6424 - acc: 0.1659 - val_loss: 0.7876 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 2.0969 - acc: 0.2654 - val_loss: 0.7973 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2748 - acc: 0.0995 - val_loss: 0.7911 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.4269 - acc: 0.1327 - val_loss: 0.7935 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 54ms/step - loss: 1.8969 - acc: 0.1991 - val_loss: 0.7632 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6991 - acc: 0.1754 - val_loss: 0.7923 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6161 - acc: 0.1611 - val_loss: 0.8113 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.9105 - acc: 0.2227 - val_loss: 0.7791 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8584 - acc: 0.2180 - val_loss: 0.8091 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.0878 - acc: 0.0521 - val_loss: 0.8043 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5530 - acc: 0.1469 - val_loss: 0.8222 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4724 - acc: 0.1327 - val_loss: 0.7839 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8725 - acc: 0.2085 - val_loss: 0.8086 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3183 - acc: 0.1043 - val_loss: 0.8273 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 2.0769 - acc: 0.2607 - val_loss: 0.8108 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 1.4402 - acc: 0.1280 - val_loss: 0.8338 - val_acc: 0.0755\n",
      "6/6 [==============================] - 2s 63ms/step - loss: 1.2916 - acc: 0.0903 - val_loss: 0.8195 - val_acc: 0.1410\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6571 - acc: 0.1706 - val_loss: 0.8206 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3791 - acc: 0.1090 - val_loss: 0.8378 - val_acc: 0.1415\n",
      "4/4 [==============================] - 2s 93ms/step - loss: 1.6725 - acc: 0.1765 - val_loss: 0.7901 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5506 - acc: 0.1517 - val_loss: 0.8595 - val_acc: 0.1038\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.6643 - acc: 0.1750 - val_loss: 0.8016 - val_acc: 0.1900\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7053 - acc: 0.1848 - val_loss: 0.8461 - val_acc: 0.1604\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.5653 - acc: 0.1564 - val_loss: 0.8065 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 2.0356 - acc: 0.2701 - val_loss: 0.8248 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7071 - acc: 0.1896 - val_loss: 0.8689 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8545 - acc: 0.2227 - val_loss: 0.8783 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4621 - acc: 0.1280 - val_loss: 0.9104 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.1778 - acc: 0.0569 - val_loss: 0.9114 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5465 - acc: 0.1517 - val_loss: 0.8589 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7804 - acc: 0.2085 - val_loss: 0.8441 - val_acc: 0.2075\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.3440 - acc: 0.0955 - val_loss: 0.9078 - val_acc: 0.0600\n",
      "4/4 [==============================] - 2s 101ms/step - loss: 2.1650 - acc: 0.3025 - val_loss: 0.7999 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7644 - acc: 0.1943 - val_loss: 0.7980 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5314 - acc: 0.1422 - val_loss: 0.8783 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6590 - acc: 0.1754 - val_loss: 0.8278 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4612 - acc: 0.1280 - val_loss: 0.9006 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8378 - acc: 0.2227 - val_loss: 0.8954 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6143 - acc: 0.1754 - val_loss: 0.8985 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.9832 - acc: 0.2749 - val_loss: 0.8521 - val_acc: 0.2736\n",
      "5/5 [==============================] - 2s 71ms/step - loss: 2.1147 - acc: 0.3041 - val_loss: 0.9058 - val_acc: 0.2432\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4660 - acc: 0.1374 - val_loss: 0.9321 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 2.0898 - acc: 0.3223 - val_loss: 0.9041 - val_acc: 0.2925\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.9200 - acc: 0.2701 - val_loss: 0.8808 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6512 - acc: 0.1801 - val_loss: 0.9723 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7129 - acc: 0.1943 - val_loss: 1.0350 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2864 - acc: 0.0474 - val_loss: 1.1225 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2084 - acc: 0.0379 - val_loss: 1.0388 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7900 - acc: 0.2417 - val_loss: 0.9542 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7634 - acc: 0.2275 - val_loss: 0.9311 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5423 - acc: 0.1517 - val_loss: 0.9233 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.8249 - acc: 0.2322 - val_loss: 0.9217 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4453 - acc: 0.1137 - val_loss: 1.0650 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5553 - acc: 0.1469 - val_loss: 1.0538 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4526 - acc: 0.1090 - val_loss: 1.0644 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 1.7877 - acc: 0.2275 - val_loss: 1.0093 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6442 - acc: 0.1848 - val_loss: 0.9703 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4348 - acc: 0.1137 - val_loss: 0.9703 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4834 - acc: 0.1185 - val_loss: 1.0695 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5964 - acc: 0.1706 - val_loss: 0.8969 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7352 - acc: 0.2275 - val_loss: 0.9812 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3284 - acc: 0.0806 - val_loss: 1.0275 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2723 - acc: 0.0616 - val_loss: 1.0609 - val_acc: 0.0566\n",
      "6/6 [==============================] - 3s 59ms/step - loss: 2.1573 - acc: 0.3659 - val_loss: 0.8437 - val_acc: 0.4024\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6264 - acc: 0.1706 - val_loss: 1.0318 - val_acc: 0.1132\n",
      "5/5 [==============================] - 1s 72ms/step - loss: 1.3483 - acc: 0.0867 - val_loss: 1.0449 - val_acc: 0.0267\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 2.0216 - acc: 0.3223 - val_loss: 0.9889 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5882 - acc: 0.1517 - val_loss: 1.0581 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6435 - acc: 0.1754 - val_loss: 1.0058 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6668 - acc: 0.1801 - val_loss: 1.0348 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5439 - acc: 0.1611 - val_loss: 1.0061 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6481 - acc: 0.1848 - val_loss: 1.0281 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6788 - acc: 0.1943 - val_loss: 0.9142 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4945 - acc: 0.1232 - val_loss: 1.0734 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1416 - acc: 0.0190 - val_loss: 1.0361 - val_acc: 0.0189\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.6840 - acc: 0.1878 - val_loss: 0.8979 - val_acc: 0.2418\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7303 - acc: 0.2085 - val_loss: 0.9089 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6864 - acc: 0.1896 - val_loss: 1.0054 - val_acc: 0.2075\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.1744 - acc: 0.0190 - val_loss: 1.0755 - val_acc: 0.0094\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8280 - acc: 0.2227 - val_loss: 0.8724 - val_acc: 0.2170\n",
      "7/7 [==============================] - 3s 50ms/step - loss: 1.8126 - acc: 0.2308 - val_loss: 0.9424 - val_acc: 0.2115\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7313 - acc: 0.2133 - val_loss: 0.9626 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9085 - acc: 0.2701 - val_loss: 0.9948 - val_acc: 0.1981\n",
      "5/5 [==============================] - 2s 74ms/step - loss: 1.7466 - acc: 0.2027 - val_loss: 0.8843 - val_acc: 0.2703\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3868 - acc: 0.4583 - val_loss: 0.9539 - val_acc: 0.2500\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.4442 - acc: 0.0976 - val_loss: 1.0444 - val_acc: 0.1456\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2376 - acc: 0.0474 - val_loss: 1.0562 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3871 - acc: 0.1043 - val_loss: 0.9876 - val_acc: 0.1238\n",
      "7/7 [==============================] - 3s 49ms/step - loss: 1.8221 - acc: 0.2341 - val_loss: 0.9249 - val_acc: 0.2255\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5971 - acc: 0.1659 - val_loss: 0.9468 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4595 - acc: 0.1185 - val_loss: 0.9093 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 70ms/step - loss: 1.5957 - acc: 0.1678 - val_loss: 0.9740 - val_acc: 0.1892\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4746 - acc: 0.1280 - val_loss: 0.9107 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.9816 - acc: 0.2938 - val_loss: 0.8989 - val_acc: 0.3491\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8015 - acc: 0.2370 - val_loss: 0.9342 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4911 - acc: 0.1327 - val_loss: 0.9691 - val_acc: 0.2075\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.6805 - acc: 0.1896 - val_loss: 0.9323 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4772 - acc: 0.1280 - val_loss: 1.0330 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5665 - acc: 0.1611 - val_loss: 1.0524 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5548 - acc: 0.1469 - val_loss: 0.9695 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6068 - acc: 0.1706 - val_loss: 0.8875 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3835 - acc: 0.1043 - val_loss: 0.9830 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3706 - acc: 0.1043 - val_loss: 0.9864 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3839 - acc: 0.1090 - val_loss: 0.9351 - val_acc: 0.1509\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.5207 - acc: 0.1469 - val_loss: 0.9295 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6943 - acc: 0.1991 - val_loss: 0.8815 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.1450 - acc: 0.0427 - val_loss: 0.9181 - val_acc: 0.0377\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4126 - acc: 0.1185 - val_loss: 0.9351 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.4741 - acc: 0.1268 - val_loss: 0.8956 - val_acc: 0.2136\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5876 - acc: 0.1611 - val_loss: 0.9342 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.3833 - acc: 0.1090 - val_loss: 0.9568 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4148 - acc: 0.1185 - val_loss: 0.9000 - val_acc: 0.1698\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 1.6606 - acc: 0.1848 - val_loss: 0.9139 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.0605 - acc: 0.0190 - val_loss: 0.9634 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2706 - acc: 0.0711 - val_loss: 0.9481 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8769 - acc: 0.2417 - val_loss: 0.8552 - val_acc: 0.2642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.2907 - acc: 0.0853 - val_loss: 0.9808 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4677 - acc: 0.1280 - val_loss: 0.9117 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6633 - acc: 0.1801 - val_loss: 0.9050 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3829 - acc: 0.1043 - val_loss: 0.9378 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3604 - acc: 0.1043 - val_loss: 0.9693 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 1.7387 - acc: 0.2098 - val_loss: 0.9123 - val_acc: 0.1863\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8003 - acc: 0.2227 - val_loss: 0.9012 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6823 - acc: 0.1896 - val_loss: 0.9582 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6569 - acc: 0.1801 - val_loss: 0.9204 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3156 - acc: 0.0853 - val_loss: 0.9594 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7384 - acc: 0.2085 - val_loss: 0.9095 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7384 - acc: 0.2180 - val_loss: 0.9387 - val_acc: 0.1604\n",
      "5/5 [==============================] - 2s 72ms/step - loss: 2.0986 - acc: 0.3333 - val_loss: 0.8526 - val_acc: 0.3226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2900 - acc: 0.0758 - val_loss: 0.9739 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.0827 - acc: 0.0190 - val_loss: 1.0064 - val_acc: 0.0283\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.6979 - acc: 0.1970 - val_loss: 0.9137 - val_acc: 0.1717\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7691 - acc: 0.2180 - val_loss: 0.9490 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8080 - acc: 0.2275 - val_loss: 0.8960 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6540 - acc: 0.1801 - val_loss: 0.9529 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7292 - acc: 0.2085 - val_loss: 1.0190 - val_acc: 0.1604\n",
      "8/8 [==============================] - 3s 184ms/step - loss: 1.6380 - acc: 0.1706 - val_loss: 0.9820 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4592 - acc: 0.1232 - val_loss: 0.9457 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5317 - acc: 0.1469 - val_loss: 0.9093 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4723 - acc: 0.1327 - val_loss: 0.9255 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6145 - acc: 0.1754 - val_loss: 0.9572 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5311 - acc: 0.1469 - val_loss: 0.9103 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7355 - acc: 0.2133 - val_loss: 0.8866 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7485 - acc: 0.2085 - val_loss: 0.9883 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4471 - acc: 0.1280 - val_loss: 0.9683 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.7105 - acc: 0.2038 - val_loss: 0.9035 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9169 - acc: 0.2654 - val_loss: 0.9805 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1473 - acc: 0.0521 - val_loss: 0.9293 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4546 - acc: 0.1232 - val_loss: 1.0156 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8756 - acc: 0.2464 - val_loss: 0.9189 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2524 - acc: 0.0664 - val_loss: 0.9623 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2506 - acc: 0.0616 - val_loss: 0.9195 - val_acc: 0.1792\n",
      "6/6 [==============================] - 1s 57ms/step - loss: 1.6787 - acc: 0.1889 - val_loss: 0.9566 - val_acc: 0.1333\n",
      "8/8 [==============================] - 3s 44ms/step - loss: 1.5534 - acc: 0.1564 - val_loss: 0.9170 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5735 - acc: 0.1517 - val_loss: 0.8923 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5895 - acc: 0.1659 - val_loss: 0.8908 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.0032 - acc: 0.0190 - val_loss: 0.9473 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7011 - acc: 0.1991 - val_loss: 0.9539 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5806 - acc: 0.1611 - val_loss: 0.9029 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.4385 - acc: 0.1185 - val_loss: 0.9580 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3882 - acc: 0.1090 - val_loss: 0.9861 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 189ms/step - loss: 1.4630 - acc: 0.1280 - val_loss: 0.9615 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6822 - acc: 0.1943 - val_loss: 0.9084 - val_acc: 0.2075\n",
      "5/5 [==============================] - 1s 73ms/step - loss: 1.8130 - acc: 0.2267 - val_loss: 0.9171 - val_acc: 0.1733\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.3557 - acc: 0.0948 - val_loss: 0.9678 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4882 - acc: 0.1327 - val_loss: 0.9442 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.5979 - acc: 0.1602 - val_loss: 0.9370 - val_acc: 0.1429\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6231 - acc: 0.1706 - val_loss: 0.8184 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.5706 - acc: 0.1564 - val_loss: 0.9525 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6176 - acc: 0.1706 - val_loss: 0.9265 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.5412 - acc: 0.1469 - val_loss: 0.9388 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2133 - acc: 0.0474 - val_loss: 0.9820 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1466 - acc: 0.0427 - val_loss: 0.9472 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4115 - acc: 0.1185 - val_loss: 0.8721 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7274 - acc: 0.1943 - val_loss: 0.8917 - val_acc: 0.1792\n",
      "7/7 [==============================] - 2s 47ms/step - loss: 1.6789 - acc: 0.1865 - val_loss: 0.9366 - val_acc: 0.1959\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8044 - acc: 0.2370 - val_loss: 0.8847 - val_acc: 0.2736\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.8984 - acc: 0.2353 - val_loss: 0.8958 - val_acc: 0.2059\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3540 - acc: 0.0995 - val_loss: 0.9175 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9259 - acc: 0.2701 - val_loss: 0.9427 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5279 - acc: 0.1517 - val_loss: 0.8759 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3070 - acc: 0.0900 - val_loss: 0.9093 - val_acc: 0.1226\n",
      "7/7 [==============================] - 1s 48ms/step - loss: 1.8204 - acc: 0.2429 - val_loss: 0.9177 - val_acc: 0.2667\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8958 - acc: 0.2559 - val_loss: 0.9664 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9070 - acc: 0.2654 - val_loss: 0.9367 - val_acc: 0.2264\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.1091 - acc: 0.6667 - val_loss: 1.0397 - val_acc: 0.0000e+00\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.9145 - acc: 0.2559 - val_loss: 0.8928 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5955 - acc: 0.1659 - val_loss: 0.8817 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4338 - acc: 0.1185 - val_loss: 1.0031 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6456 - acc: 0.1754 - val_loss: 0.9779 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2138 - acc: 0.0569 - val_loss: 0.9981 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 51ms/step - loss: 1.9803 - acc: 0.2818 - val_loss: 0.8853 - val_acc: 0.3187\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2486 - acc: 0.0521 - val_loss: 0.9959 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7217 - acc: 0.2038 - val_loss: 0.9147 - val_acc: 0.1887\n",
      "8/8 [==============================] - 3s 44ms/step - loss: 1.3596 - acc: 0.0948 - val_loss: 0.9612 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5510 - acc: 0.1469 - val_loss: 0.9560 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2288 - acc: 0.0616 - val_loss: 0.9826 - val_acc: 0.0472\n",
      "6/6 [==============================] - 1s 61ms/step - loss: 1.7462 - acc: 0.2056 - val_loss: 0.8547 - val_acc: 0.3111\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7156 - acc: 0.2038 - val_loss: 0.9268 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6725 - acc: 0.1896 - val_loss: 0.9361 - val_acc: 0.1509\n",
      "4/4 [==============================] - 2s 103ms/step - loss: 1.8008 - acc: 0.2101 - val_loss: 0.8651 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4627 - acc: 0.1232 - val_loss: 0.9366 - val_acc: 0.1619\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.3403 - acc: 0.0900 - val_loss: 0.9429 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1977 - acc: 0.0521 - val_loss: 0.9872 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3685 - acc: 0.0995 - val_loss: 0.9677 - val_acc: 0.0755\n",
      "6/6 [==============================] - 1s 55ms/step - loss: 1.6765 - acc: 0.1833 - val_loss: 0.8489 - val_acc: 0.2889\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1318 - acc: 0.0427 - val_loss: 0.9554 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4685 - acc: 0.1280 - val_loss: 0.9291 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8090 - acc: 0.2275 - val_loss: 0.9112 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7988 - acc: 0.2180 - val_loss: 0.8953 - val_acc: 0.2075\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.2275 - acc: 0.0616 - val_loss: 0.9113 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 2.1208 - acc: 0.3081 - val_loss: 0.8100 - val_acc: 0.3679\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.9163 - acc: 0.2512 - val_loss: 0.8877 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4895 - acc: 0.1327 - val_loss: 0.9315 - val_acc: 0.1321\n",
      "3/3 [==============================] - 2s 148ms/step - loss: 1.3984 - acc: 0.1136 - val_loss: 0.9560 - val_acc: 0.0682\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8553 - acc: 0.2417 - val_loss: 0.8678 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.8384 - acc: 0.2370 - val_loss: 0.9043 - val_acc: 0.2736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3437 - acc: 0.3704 - val_loss: 0.9119 - val_acc: 0.2308\n",
      "7/7 [==============================] - 3s 54ms/step - loss: 2.0707 - acc: 0.3058 - val_loss: 0.9564 - val_acc: 0.1748\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7634 - acc: 0.2227 - val_loss: 0.9023 - val_acc: 0.2642\n",
      "6/6 [==============================] - 1s 56ms/step - loss: 1.5237 - acc: 0.1389 - val_loss: 0.9235 - val_acc: 0.1778\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5034 - acc: 0.1327 - val_loss: 0.8744 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7173 - acc: 0.2085 - val_loss: 0.9053 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6800 - acc: 0.1943 - val_loss: 0.9155 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.3126 - acc: 0.0758 - val_loss: 0.9944 - val_acc: 0.0755\n",
      "5/5 [==============================] - 2s 74ms/step - loss: 1.9145 - acc: 0.2752 - val_loss: 0.9055 - val_acc: 0.2703\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.7030 - acc: 0.2038 - val_loss: 0.9620 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6998 - acc: 0.1991 - val_loss: 0.9605 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.5578 - acc: 0.1564 - val_loss: 0.9948 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4117 - acc: 0.0995 - val_loss: 0.9641 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5425 - acc: 0.1517 - val_loss: 0.9928 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6911 - acc: 0.1896 - val_loss: 0.9266 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6177 - acc: 0.1754 - val_loss: 0.9288 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5242 - acc: 0.1374 - val_loss: 0.9791 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 184ms/step - loss: 1.7482 - acc: 0.2180 - val_loss: 0.9572 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6251 - acc: 0.1754 - val_loss: 0.9890 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.0588 - acc: 0.0047 - val_loss: 1.0377 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7196 - acc: 0.2038 - val_loss: 0.9044 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8156 - acc: 0.2275 - val_loss: 0.9054 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2552 - acc: 0.0664 - val_loss: 1.0098 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7054 - acc: 0.1943 - val_loss: 0.9302 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5424 - acc: 0.1517 - val_loss: 0.9320 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4178 - acc: 0.1137 - val_loss: 0.9666 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2142 - acc: 0.0521 - val_loss: 1.0254 - val_acc: 0.0377\n",
      "5/5 [==============================] - 2s 74ms/step - loss: 1.5827 - acc: 0.1644 - val_loss: 0.9298 - val_acc: 0.1918\n",
      "5/5 [==============================] - 2s 70ms/step - loss: 2.0927 - acc: 0.3243 - val_loss: 0.8124 - val_acc: 0.3919\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6209 - acc: 0.1706 - val_loss: 0.9715 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4818 - acc: 0.1327 - val_loss: 0.9365 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 69ms/step - loss: 1.5085 - acc: 0.1409 - val_loss: 0.9566 - val_acc: 0.1351\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6858 - acc: 0.1991 - val_loss: 0.9233 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2768 - acc: 0.0569 - val_loss: 1.0406 - val_acc: 0.0472\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4258 - acc: 0.1137 - val_loss: 0.9883 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4539 - acc: 0.1232 - val_loss: 0.9529 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6248 - acc: 0.1754 - val_loss: 0.9102 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1839 - acc: 0.0427 - val_loss: 0.9975 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 1.5316 - acc: 0.1422 - val_loss: 0.9957 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9864 - acc: 0.2749 - val_loss: 0.9083 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5627 - acc: 0.1564 - val_loss: 0.9141 - val_acc: 0.2075\n",
      "3/3 [==============================] - 2s 140ms/step - loss: 1.7039 - acc: 0.1954 - val_loss: 0.9323 - val_acc: 0.1818\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9005 - acc: 0.2559 - val_loss: 0.9340 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8735 - acc: 0.2512 - val_loss: 0.9524 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.5101 - acc: 0.1374 - val_loss: 1.0118 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8461 - acc: 0.2464 - val_loss: 0.9230 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.9658 - acc: 0.2891 - val_loss: 0.8757 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6999 - acc: 0.2085 - val_loss: 0.9716 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4660 - acc: 0.1137 - val_loss: 1.0571 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7214 - acc: 0.2038 - val_loss: 0.9894 - val_acc: 0.1698\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.6150 - acc: 0.1706 - val_loss: 0.9570 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4217 - acc: 0.1043 - val_loss: 1.0352 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4594 - acc: 0.1185 - val_loss: 0.9976 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5704 - acc: 0.1564 - val_loss: 0.9787 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.3171 - acc: 0.0758 - val_loss: 1.0270 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3116 - acc: 0.0758 - val_loss: 0.9924 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4955 - acc: 0.1374 - val_loss: 0.9929 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4080 - acc: 0.1090 - val_loss: 0.9765 - val_acc: 0.1226\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 2.1007 - acc: 0.3318 - val_loss: 0.8404 - val_acc: 0.3396\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3432 - acc: 0.0900 - val_loss: 1.0265 - val_acc: 0.0094\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7646 - acc: 0.2275 - val_loss: 0.8837 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4072 - acc: 0.1090 - val_loss: 0.9621 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6616 - acc: 0.1896 - val_loss: 0.9569 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3706 - acc: 0.0995 - val_loss: 0.9873 - val_acc: 0.0857\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.3574 - acc: 0.0900 - val_loss: 0.9540 - val_acc: 0.1509\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.4111 - acc: 0.1105 - val_loss: 0.9178 - val_acc: 0.1648\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.7310 - acc: 0.2038 - val_loss: 0.9078 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3832 - acc: 0.1090 - val_loss: 0.9984 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2375 - acc: 0.0569 - val_loss: 1.0031 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7528 - acc: 0.2085 - val_loss: 0.8470 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7976 - acc: 0.2227 - val_loss: 0.8393 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5423 - acc: 0.1517 - val_loss: 0.9564 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6776 - acc: 0.1943 - val_loss: 0.9149 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7611 - acc: 0.2180 - val_loss: 0.9320 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7019 - acc: 0.1991 - val_loss: 0.9460 - val_acc: 0.2075\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.5671 - acc: 0.1564 - val_loss: 0.9882 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6510 - acc: 0.1848 - val_loss: 0.9458 - val_acc: 0.1604\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.3025 - acc: 0.0861 - val_loss: 0.9663 - val_acc: 0.0577\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3668 - acc: 0.0948 - val_loss: 0.9654 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7024 - acc: 0.1991 - val_loss: 0.9288 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 2.0467 - acc: 0.3033 - val_loss: 0.9507 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6000 - acc: 0.1706 - val_loss: 1.0258 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6018 - acc: 0.1659 - val_loss: 0.9523 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.8630 - acc: 0.2607 - val_loss: 0.9784 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3505 - acc: 0.0900 - val_loss: 0.9738 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4353 - acc: 0.1185 - val_loss: 1.0014 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7383 - acc: 0.2038 - val_loss: 0.9270 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6109 - acc: 0.1754 - val_loss: 0.9558 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5764 - acc: 0.1611 - val_loss: 0.9784 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7779 - acc: 0.2227 - val_loss: 0.9122 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7293 - acc: 0.2085 - val_loss: 0.9853 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.2138 - acc: 0.0521 - val_loss: 0.9683 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5428 - acc: 0.1469 - val_loss: 0.9926 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4935 - acc: 0.1327 - val_loss: 0.9081 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7564 - acc: 0.2133 - val_loss: 0.9461 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3628 - acc: 0.0995 - val_loss: 0.9518 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.9024 - acc: 0.2607 - val_loss: 0.9363 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4548 - acc: 0.1232 - val_loss: 0.9706 - val_acc: 0.0755\n",
      "6/6 [==============================] - 2s 59ms/step - loss: 1.3464 - acc: 0.0903 - val_loss: 0.9599 - val_acc: 0.1410\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6170 - acc: 0.1706 - val_loss: 0.9600 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.4062 - acc: 0.1090 - val_loss: 0.9821 - val_acc: 0.1415\n",
      "4/4 [==============================] - 2s 94ms/step - loss: 1.6278 - acc: 0.1765 - val_loss: 0.8809 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5441 - acc: 0.1517 - val_loss: 0.9660 - val_acc: 0.1038\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 1.6331 - acc: 0.1750 - val_loss: 0.9183 - val_acc: 0.1900\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6637 - acc: 0.1848 - val_loss: 0.9633 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5723 - acc: 0.1564 - val_loss: 0.9200 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.9259 - acc: 0.2701 - val_loss: 0.9162 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6636 - acc: 0.1896 - val_loss: 0.9563 - val_acc: 0.1887\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.7641 - acc: 0.2227 - val_loss: 0.9849 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4977 - acc: 0.1280 - val_loss: 1.0404 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2761 - acc: 0.0569 - val_loss: 1.0628 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5506 - acc: 0.1517 - val_loss: 0.9997 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7033 - acc: 0.2085 - val_loss: 0.9391 - val_acc: 0.2075\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.3760 - acc: 0.0955 - val_loss: 1.0339 - val_acc: 0.0600\n",
      "4/4 [==============================] - 2s 94ms/step - loss: 2.0289 - acc: 0.3025 - val_loss: 0.8763 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6843 - acc: 0.1943 - val_loss: 0.8931 - val_acc: 0.2736\n",
      "8/8 [==============================] - 3s 194ms/step - loss: 1.5201 - acc: 0.1422 - val_loss: 0.9998 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6267 - acc: 0.1754 - val_loss: 0.9188 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4715 - acc: 0.1280 - val_loss: 0.9880 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7814 - acc: 0.2227 - val_loss: 0.9581 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6038 - acc: 0.1754 - val_loss: 0.9673 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8986 - acc: 0.2749 - val_loss: 0.9223 - val_acc: 0.2736\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 1.9998 - acc: 0.3041 - val_loss: 0.9640 - val_acc: 0.2432\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5141 - acc: 0.1374 - val_loss: 1.0211 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 2.0045 - acc: 0.3223 - val_loss: 0.9318 - val_acc: 0.2925\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8749 - acc: 0.2701 - val_loss: 0.9543 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6315 - acc: 0.1801 - val_loss: 1.0185 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6748 - acc: 0.1943 - val_loss: 1.0679 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3006 - acc: 0.0474 - val_loss: 1.1541 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2469 - acc: 0.0379 - val_loss: 1.0821 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7728 - acc: 0.2417 - val_loss: 0.9867 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7591 - acc: 0.2275 - val_loss: 0.9938 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5551 - acc: 0.1517 - val_loss: 0.9704 - val_acc: 0.2170\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.7648 - acc: 0.2322 - val_loss: 0.9813 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4598 - acc: 0.1137 - val_loss: 1.1188 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5459 - acc: 0.1469 - val_loss: 1.0732 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4441 - acc: 0.1090 - val_loss: 1.0884 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7562 - acc: 0.2275 - val_loss: 1.0266 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6507 - acc: 0.1848 - val_loss: 0.9806 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4483 - acc: 0.1137 - val_loss: 1.0077 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4508 - acc: 0.1185 - val_loss: 1.0495 - val_acc: 0.0566\n",
      "8/8 [==============================] - 3s 190ms/step - loss: 1.6121 - acc: 0.1706 - val_loss: 0.9195 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7276 - acc: 0.2275 - val_loss: 1.0103 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3673 - acc: 0.0806 - val_loss: 1.0819 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2893 - acc: 0.0616 - val_loss: 1.0940 - val_acc: 0.0566\n",
      "6/6 [==============================] - 2s 58ms/step - loss: 2.1544 - acc: 0.3659 - val_loss: 0.8386 - val_acc: 0.4024\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6340 - acc: 0.1706 - val_loss: 1.0523 - val_acc: 0.1132\n",
      "5/5 [==============================] - 1s 70ms/step - loss: 1.3772 - acc: 0.0867 - val_loss: 1.1253 - val_acc: 0.0267\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.9920 - acc: 0.3223 - val_loss: 0.9516 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5773 - acc: 0.1517 - val_loss: 1.0520 - val_acc: 0.1604\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.6192 - acc: 0.1754 - val_loss: 0.9953 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6561 - acc: 0.1801 - val_loss: 1.0459 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.5570 - acc: 0.1611 - val_loss: 1.0544 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6443 - acc: 0.1848 - val_loss: 1.0321 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6831 - acc: 0.1943 - val_loss: 0.9687 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4819 - acc: 0.1232 - val_loss: 1.0647 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.1690 - acc: 0.0190 - val_loss: 1.0822 - val_acc: 0.0189\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 1.6697 - acc: 0.1878 - val_loss: 0.9231 - val_acc: 0.2418\n",
      "8/8 [==============================] - 3s 194ms/step - loss: 1.7421 - acc: 0.2085 - val_loss: 0.9410 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6616 - acc: 0.1896 - val_loss: 0.9593 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.1487 - acc: 0.0190 - val_loss: 1.0726 - val_acc: 0.0094\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.7746 - acc: 0.2227 - val_loss: 0.9165 - val_acc: 0.2170\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.8005 - acc: 0.2308 - val_loss: 0.9431 - val_acc: 0.2115\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7309 - acc: 0.2133 - val_loss: 0.9530 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8953 - acc: 0.2701 - val_loss: 0.9685 - val_acc: 0.1981\n",
      "5/5 [==============================] - 2s 77ms/step - loss: 1.7006 - acc: 0.2027 - val_loss: 0.9046 - val_acc: 0.2703\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4162 - acc: 0.4583 - val_loss: 0.9301 - val_acc: 0.2500\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.4165 - acc: 0.0976 - val_loss: 1.0098 - val_acc: 0.1456\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.2323 - acc: 0.0474 - val_loss: 1.0532 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4076 - acc: 0.1043 - val_loss: 0.9775 - val_acc: 0.1238\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 1.8004 - acc: 0.2341 - val_loss: 0.9331 - val_acc: 0.2255\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5822 - acc: 0.1659 - val_loss: 0.9349 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4493 - acc: 0.1185 - val_loss: 0.9353 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 71ms/step - loss: 1.5851 - acc: 0.1678 - val_loss: 0.9557 - val_acc: 0.1892\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4670 - acc: 0.1280 - val_loss: 0.9191 - val_acc: 0.2358\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.9763 - acc: 0.2938 - val_loss: 0.8638 - val_acc: 0.3491\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8050 - acc: 0.2370 - val_loss: 0.9025 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.4850 - acc: 0.1327 - val_loss: 0.9584 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6701 - acc: 0.1896 - val_loss: 0.9501 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4782 - acc: 0.1280 - val_loss: 1.0544 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5789 - acc: 0.1611 - val_loss: 1.0411 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5454 - acc: 0.1469 - val_loss: 0.9854 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5888 - acc: 0.1706 - val_loss: 0.9162 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.4099 - acc: 0.1043 - val_loss: 1.0228 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3930 - acc: 0.1043 - val_loss: 1.0202 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3978 - acc: 0.1090 - val_loss: 0.9664 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5282 - acc: 0.1469 - val_loss: 0.9427 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6894 - acc: 0.1991 - val_loss: 0.9011 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.1710 - acc: 0.0427 - val_loss: 0.8902 - val_acc: 0.0377\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4261 - acc: 0.1185 - val_loss: 0.9705 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.4669 - acc: 0.1268 - val_loss: 0.9136 - val_acc: 0.2136\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5762 - acc: 0.1611 - val_loss: 0.9651 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 1.3909 - acc: 0.1090 - val_loss: 0.9843 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4208 - acc: 0.1185 - val_loss: 0.9162 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6580 - acc: 0.1848 - val_loss: 0.9130 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.0639 - acc: 0.0190 - val_loss: 0.9424 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2563 - acc: 0.0711 - val_loss: 0.9508 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.9039 - acc: 0.2417 - val_loss: 0.8435 - val_acc: 0.2642\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3127 - acc: 0.0853 - val_loss: 0.9715 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4590 - acc: 0.1280 - val_loss: 0.9195 - val_acc: 0.1321\n",
      "8/8 [==============================] - 3s 44ms/step - loss: 1.6700 - acc: 0.1801 - val_loss: 0.8703 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.3659 - acc: 0.1043 - val_loss: 0.9326 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3617 - acc: 0.1043 - val_loss: 0.9311 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.7583 - acc: 0.2098 - val_loss: 0.8629 - val_acc: 0.1863\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.8454 - acc: 0.2227 - val_loss: 0.8801 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6725 - acc: 0.1896 - val_loss: 0.9554 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6652 - acc: 0.1801 - val_loss: 0.9042 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.2996 - acc: 0.0853 - val_loss: 0.9612 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 1.7475 - acc: 0.2085 - val_loss: 0.9019 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7716 - acc: 0.2180 - val_loss: 0.9336 - val_acc: 0.1604\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 2.1761 - acc: 0.3333 - val_loss: 0.8483 - val_acc: 0.3226\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.2745 - acc: 0.0758 - val_loss: 0.9735 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.0661 - acc: 0.0190 - val_loss: 0.9807 - val_acc: 0.0283\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 1.7046 - acc: 0.1970 - val_loss: 0.9074 - val_acc: 0.1717\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7669 - acc: 0.2180 - val_loss: 0.9445 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8129 - acc: 0.2275 - val_loss: 0.8948 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6498 - acc: 0.1801 - val_loss: 0.9603 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 1.7373 - acc: 0.2085 - val_loss: 0.9653 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6036 - acc: 0.1706 - val_loss: 0.9665 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4446 - acc: 0.1232 - val_loss: 0.9368 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5294 - acc: 0.1469 - val_loss: 0.9127 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4708 - acc: 0.1327 - val_loss: 0.9190 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6042 - acc: 0.1754 - val_loss: 0.9279 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 1.5261 - acc: 0.1469 - val_loss: 0.9065 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7593 - acc: 0.2133 - val_loss: 0.8816 - val_acc: 0.2547\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.7432 - acc: 0.2085 - val_loss: 0.9637 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.4562 - acc: 0.1280 - val_loss: 0.9605 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.7271 - acc: 0.2038 - val_loss: 0.8733 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 1.9286 - acc: 0.2654 - val_loss: 0.9406 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1143 - acc: 0.0521 - val_loss: 0.8677 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4467 - acc: 0.1232 - val_loss: 0.9819 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8588 - acc: 0.2464 - val_loss: 0.9045 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2537 - acc: 0.0664 - val_loss: 0.9685 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 181ms/step - loss: 1.2398 - acc: 0.0616 - val_loss: 0.9198 - val_acc: 0.1792\n",
      "6/6 [==============================] - 1s 60ms/step - loss: 1.6588 - acc: 0.1889 - val_loss: 0.9654 - val_acc: 0.1333\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5436 - acc: 0.1564 - val_loss: 0.9061 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5446 - acc: 0.1517 - val_loss: 0.8890 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6006 - acc: 0.1659 - val_loss: 0.8957 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.9611 - acc: 0.1801 - val_loss: 0.9050 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7168 - acc: 0.1991 - val_loss: 0.9151 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5878 - acc: 0.1611 - val_loss: 0.8816 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4353 - acc: 0.1185 - val_loss: 0.9357 - val_acc: 0.0943\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.3742 - acc: 0.1090 - val_loss: 0.9245 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4576 - acc: 0.1280 - val_loss: 0.9257 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6845 - acc: 0.1943 - val_loss: 0.8790 - val_acc: 0.2075\n",
      "5/5 [==============================] - 1s 74ms/step - loss: 1.8245 - acc: 0.2267 - val_loss: 0.8670 - val_acc: 0.1733\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3263 - acc: 0.0948 - val_loss: 0.9204 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4837 - acc: 0.1327 - val_loss: 0.9119 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.6036 - acc: 0.1602 - val_loss: 0.8936 - val_acc: 0.1429\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6139 - acc: 0.1706 - val_loss: 0.8130 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5699 - acc: 0.1564 - val_loss: 0.9077 - val_acc: 0.1509\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.6286 - acc: 0.1706 - val_loss: 0.9054 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5583 - acc: 0.1469 - val_loss: 0.9039 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1627 - acc: 0.0474 - val_loss: 0.9229 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1333 - acc: 0.0427 - val_loss: 0.9221 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4244 - acc: 0.1185 - val_loss: 0.8474 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7266 - acc: 0.1943 - val_loss: 0.8602 - val_acc: 0.1792\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.6863 - acc: 0.1865 - val_loss: 0.8803 - val_acc: 0.1959\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8427 - acc: 0.2370 - val_loss: 0.8492 - val_acc: 0.2736\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.9019 - acc: 0.2353 - val_loss: 0.8650 - val_acc: 0.2059\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.3661 - acc: 0.0995 - val_loss: 0.8896 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9615 - acc: 0.2701 - val_loss: 0.9017 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5298 - acc: 0.1517 - val_loss: 0.8661 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2973 - acc: 0.0900 - val_loss: 0.9014 - val_acc: 0.1226\n",
      "7/7 [==============================] - 1s 49ms/step - loss: 1.8484 - acc: 0.2429 - val_loss: 0.8745 - val_acc: 0.2667\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.9119 - acc: 0.2559 - val_loss: 0.9234 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.9592 - acc: 0.2654 - val_loss: 0.8999 - val_acc: 0.2264\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.4330 - acc: 0.6667 - val_loss: 0.9398 - val_acc: 0.0000e+00\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.9322 - acc: 0.2559 - val_loss: 0.8937 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5911 - acc: 0.1659 - val_loss: 0.8823 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4260 - acc: 0.1185 - val_loss: 0.9974 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6332 - acc: 0.1754 - val_loss: 0.9478 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2262 - acc: 0.0569 - val_loss: 1.0036 - val_acc: 0.0660\n",
      "7/7 [==============================] - 1s 47ms/step - loss: 1.9752 - acc: 0.2818 - val_loss: 0.8625 - val_acc: 0.3187\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2308 - acc: 0.0521 - val_loss: 0.9794 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7160 - acc: 0.2038 - val_loss: 0.9190 - val_acc: 0.1887\n",
      "8/8 [==============================] - 3s 189ms/step - loss: 1.3505 - acc: 0.0948 - val_loss: 0.9665 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5322 - acc: 0.1469 - val_loss: 0.9849 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2192 - acc: 0.0616 - val_loss: 0.9804 - val_acc: 0.0472\n",
      "6/6 [==============================] - 1s 55ms/step - loss: 1.7360 - acc: 0.2056 - val_loss: 0.8493 - val_acc: 0.3111\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7180 - acc: 0.2038 - val_loss: 0.9275 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6636 - acc: 0.1896 - val_loss: 0.9351 - val_acc: 0.1509\n",
      "4/4 [==============================] - 2s 96ms/step - loss: 1.7608 - acc: 0.2101 - val_loss: 0.8613 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4511 - acc: 0.1232 - val_loss: 0.9288 - val_acc: 0.1619\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3269 - acc: 0.0900 - val_loss: 0.9304 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.1841 - acc: 0.0521 - val_loss: 0.9691 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3693 - acc: 0.0995 - val_loss: 0.9569 - val_acc: 0.0755\n",
      "6/6 [==============================] - 1s 60ms/step - loss: 1.6715 - acc: 0.1833 - val_loss: 0.8414 - val_acc: 0.2889\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1357 - acc: 0.0427 - val_loss: 0.9387 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4610 - acc: 0.1280 - val_loss: 0.9166 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.8054 - acc: 0.2275 - val_loss: 0.9123 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8061 - acc: 0.2180 - val_loss: 0.8852 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2241 - acc: 0.0616 - val_loss: 0.9118 - val_acc: 0.1698\n",
      "8/8 [==============================] - 3s 197ms/step - loss: 2.0966 - acc: 0.3081 - val_loss: 0.8149 - val_acc: 0.3679\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.9081 - acc: 0.2512 - val_loss: 0.8939 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4792 - acc: 0.1327 - val_loss: 0.9587 - val_acc: 0.1321\n",
      "3/3 [==============================] - 2s 139ms/step - loss: 1.4188 - acc: 0.1136 - val_loss: 0.9779 - val_acc: 0.0682\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8443 - acc: 0.2417 - val_loss: 0.8650 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8349 - acc: 0.2370 - val_loss: 0.8982 - val_acc: 0.2736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3167 - acc: 0.3704 - val_loss: 0.9238 - val_acc: 0.2308\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 2.0564 - acc: 0.3058 - val_loss: 0.9525 - val_acc: 0.1748\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7714 - acc: 0.2227 - val_loss: 0.9011 - val_acc: 0.2642\n",
      "6/6 [==============================] - 2s 56ms/step - loss: 1.5181 - acc: 0.1389 - val_loss: 0.9315 - val_acc: 0.1778\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4839 - acc: 0.1327 - val_loss: 0.8792 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7337 - acc: 0.2085 - val_loss: 0.8987 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6752 - acc: 0.1943 - val_loss: 0.9050 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3141 - acc: 0.0758 - val_loss: 1.0035 - val_acc: 0.0755\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 1.9027 - acc: 0.2752 - val_loss: 0.8994 - val_acc: 0.2703\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7054 - acc: 0.2038 - val_loss: 0.9385 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7024 - acc: 0.1991 - val_loss: 0.9568 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.5517 - acc: 0.1564 - val_loss: 0.9880 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4053 - acc: 0.0995 - val_loss: 0.9657 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5416 - acc: 0.1517 - val_loss: 0.9940 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6842 - acc: 0.1896 - val_loss: 0.9443 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6095 - acc: 0.1754 - val_loss: 0.9282 - val_acc: 0.2358\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.5107 - acc: 0.1374 - val_loss: 0.9702 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7393 - acc: 0.2180 - val_loss: 0.9634 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.6271 - acc: 0.1754 - val_loss: 1.0097 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.0592 - acc: 0.0047 - val_loss: 1.0312 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7171 - acc: 0.2038 - val_loss: 0.9122 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7950 - acc: 0.2275 - val_loss: 0.8975 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2566 - acc: 0.0664 - val_loss: 1.0107 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6970 - acc: 0.1943 - val_loss: 0.9313 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5440 - acc: 0.1517 - val_loss: 0.9286 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4223 - acc: 0.1137 - val_loss: 0.9622 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.1916 - acc: 0.0521 - val_loss: 0.9905 - val_acc: 0.0377\n",
      "5/5 [==============================] - 2s 72ms/step - loss: 1.6095 - acc: 0.1644 - val_loss: 0.9006 - val_acc: 0.1918\n",
      "5/5 [==============================] - 3s 69ms/step - loss: 2.1126 - acc: 0.3243 - val_loss: 0.8068 - val_acc: 0.3919\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6104 - acc: 0.1706 - val_loss: 0.9362 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4613 - acc: 0.1327 - val_loss: 0.8950 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 70ms/step - loss: 1.5262 - acc: 0.1409 - val_loss: 0.9245 - val_acc: 0.1351\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.7058 - acc: 0.1991 - val_loss: 0.8746 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2216 - acc: 0.0569 - val_loss: 0.9811 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 1.3872 - acc: 0.1185 - val_loss: 0.9025 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4381 - acc: 0.1232 - val_loss: 0.9253 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6311 - acc: 0.1754 - val_loss: 0.8939 - val_acc: 0.2170\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.1544 - acc: 0.0427 - val_loss: 0.9686 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5119 - acc: 0.1422 - val_loss: 0.9441 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 2.0022 - acc: 0.2749 - val_loss: 0.8751 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5686 - acc: 0.1564 - val_loss: 0.8948 - val_acc: 0.2075\n",
      "3/3 [==============================] - 1s 137ms/step - loss: 1.7175 - acc: 0.1954 - val_loss: 0.9068 - val_acc: 0.1818\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.9308 - acc: 0.2559 - val_loss: 0.9166 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.8830 - acc: 0.2512 - val_loss: 0.9270 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4925 - acc: 0.1374 - val_loss: 0.9820 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.8636 - acc: 0.2464 - val_loss: 0.9128 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9831 - acc: 0.2891 - val_loss: 0.8764 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7112 - acc: 0.2085 - val_loss: 0.9599 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4412 - acc: 0.1137 - val_loss: 1.0191 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7222 - acc: 0.2038 - val_loss: 0.9596 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6133 - acc: 0.1706 - val_loss: 0.9176 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4086 - acc: 0.1043 - val_loss: 0.9920 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4339 - acc: 0.1185 - val_loss: 0.9712 - val_acc: 0.1321\n",
      "8/8 [==============================] - 3s 185ms/step - loss: 1.5675 - acc: 0.1564 - val_loss: 0.9341 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2953 - acc: 0.0758 - val_loss: 1.0081 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2868 - acc: 0.0758 - val_loss: 0.9665 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5004 - acc: 0.1374 - val_loss: 0.9700 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4043 - acc: 0.1090 - val_loss: 0.9723 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 2.1176 - acc: 0.3318 - val_loss: 0.8475 - val_acc: 0.3396\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.3420 - acc: 0.0900 - val_loss: 1.0474 - val_acc: 0.0094\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7539 - acc: 0.2275 - val_loss: 0.9079 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4055 - acc: 0.1090 - val_loss: 0.9962 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.6692 - acc: 0.1896 - val_loss: 0.9821 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.3870 - acc: 0.0995 - val_loss: 1.0389 - val_acc: 0.0857\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.3666 - acc: 0.0900 - val_loss: 0.9772 - val_acc: 0.1509\n",
      "7/7 [==============================] - 2s 47ms/step - loss: 1.4316 - acc: 0.1105 - val_loss: 0.9619 - val_acc: 0.1648\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7151 - acc: 0.2038 - val_loss: 0.9263 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4032 - acc: 0.1090 - val_loss: 1.0288 - val_acc: 0.0566\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.2598 - acc: 0.0569 - val_loss: 1.0415 - val_acc: 0.0472\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.7377 - acc: 0.2085 - val_loss: 0.8756 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7686 - acc: 0.2227 - val_loss: 0.8557 - val_acc: 0.3019\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.5463 - acc: 0.1517 - val_loss: 0.9572 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6882 - acc: 0.1943 - val_loss: 0.9180 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7623 - acc: 0.2180 - val_loss: 0.9259 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7029 - acc: 0.1991 - val_loss: 0.9373 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5727 - acc: 0.1564 - val_loss: 0.9944 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6504 - acc: 0.1848 - val_loss: 0.9514 - val_acc: 0.1604\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.2960 - acc: 0.0861 - val_loss: 0.9472 - val_acc: 0.0769\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3621 - acc: 0.0948 - val_loss: 0.9936 - val_acc: 0.0943\n",
      "8/8 [==============================] - 3s 44ms/step - loss: 1.6955 - acc: 0.1991 - val_loss: 0.9266 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 2.0297 - acc: 0.3033 - val_loss: 0.9633 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5964 - acc: 0.1706 - val_loss: 1.0349 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5810 - acc: 0.1659 - val_loss: 0.9641 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8535 - acc: 0.2607 - val_loss: 0.9973 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3625 - acc: 0.0900 - val_loss: 1.0010 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4399 - acc: 0.1185 - val_loss: 1.0325 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7154 - acc: 0.2038 - val_loss: 0.9494 - val_acc: 0.1509\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.6055 - acc: 0.1754 - val_loss: 0.9821 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5709 - acc: 0.1611 - val_loss: 1.0110 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7407 - acc: 0.2227 - val_loss: 0.9446 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7275 - acc: 0.2085 - val_loss: 1.0205 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2458 - acc: 0.0521 - val_loss: 0.9923 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5548 - acc: 0.1469 - val_loss: 1.0202 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4924 - acc: 0.1327 - val_loss: 0.9350 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7422 - acc: 0.2133 - val_loss: 0.9884 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3769 - acc: 0.0995 - val_loss: 0.9914 - val_acc: 0.0943\n",
      "8/8 [==============================] - 3s 201ms/step - loss: 1.8650 - acc: 0.2607 - val_loss: 0.9518 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4626 - acc: 0.1232 - val_loss: 1.0029 - val_acc: 0.0755\n",
      "6/6 [==============================] - 2s 57ms/step - loss: 1.3594 - acc: 0.0903 - val_loss: 0.9927 - val_acc: 0.1410\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6146 - acc: 0.1706 - val_loss: 0.9979 - val_acc: 0.1415\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.4231 - acc: 0.1090 - val_loss: 1.0269 - val_acc: 0.1415\n",
      "4/4 [==============================] - 2s 96ms/step - loss: 1.6379 - acc: 0.1765 - val_loss: 0.9087 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5393 - acc: 0.1517 - val_loss: 1.0040 - val_acc: 0.1038\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.6247 - acc: 0.1750 - val_loss: 0.9407 - val_acc: 0.1900\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6613 - acc: 0.1848 - val_loss: 0.9899 - val_acc: 0.1604\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.5622 - acc: 0.1564 - val_loss: 0.9307 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.9112 - acc: 0.2701 - val_loss: 0.9579 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.6866 - acc: 0.1896 - val_loss: 1.0194 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7602 - acc: 0.2227 - val_loss: 1.0029 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5234 - acc: 0.1280 - val_loss: 1.0686 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2775 - acc: 0.0569 - val_loss: 1.0659 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.5492 - acc: 0.1517 - val_loss: 1.0158 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7182 - acc: 0.2085 - val_loss: 0.9542 - val_acc: 0.2075\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.3826 - acc: 0.0955 - val_loss: 1.0676 - val_acc: 0.0600\n",
      "4/4 [==============================] - 2s 100ms/step - loss: 1.9899 - acc: 0.3025 - val_loss: 0.9044 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6797 - acc: 0.1943 - val_loss: 0.8957 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.5226 - acc: 0.1422 - val_loss: 1.0176 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6212 - acc: 0.1754 - val_loss: 0.9175 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4830 - acc: 0.1280 - val_loss: 1.0068 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7723 - acc: 0.2227 - val_loss: 0.9671 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6055 - acc: 0.1754 - val_loss: 0.9920 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8761 - acc: 0.2749 - val_loss: 0.9157 - val_acc: 0.2736\n",
      "5/5 [==============================] - 3s 73ms/step - loss: 2.0251 - acc: 0.3041 - val_loss: 0.9544 - val_acc: 0.2432\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4992 - acc: 0.1374 - val_loss: 1.0126 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 2.0159 - acc: 0.3223 - val_loss: 0.9303 - val_acc: 0.2925\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8911 - acc: 0.2701 - val_loss: 0.9408 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6213 - acc: 0.1801 - val_loss: 1.0048 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6969 - acc: 0.1943 - val_loss: 1.0104 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2583 - acc: 0.0474 - val_loss: 1.1035 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.2113 - acc: 0.0379 - val_loss: 1.0602 - val_acc: 0.0472\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.7961 - acc: 0.2417 - val_loss: 0.9742 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7671 - acc: 0.2275 - val_loss: 0.9536 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5420 - acc: 0.1517 - val_loss: 0.9532 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8006 - acc: 0.2322 - val_loss: 0.9459 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4457 - acc: 0.1137 - val_loss: 1.0678 - val_acc: 0.0755\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.5366 - acc: 0.1469 - val_loss: 1.0446 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4410 - acc: 0.1090 - val_loss: 1.0652 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7724 - acc: 0.2275 - val_loss: 1.0155 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6411 - acc: 0.1848 - val_loss: 0.9618 - val_acc: 0.2547\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4276 - acc: 0.1137 - val_loss: 0.9940 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4487 - acc: 0.1185 - val_loss: 1.0295 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6010 - acc: 0.1706 - val_loss: 0.8955 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7357 - acc: 0.2275 - val_loss: 0.9815 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3298 - acc: 0.0806 - val_loss: 1.0261 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.2731 - acc: 0.0616 - val_loss: 1.0531 - val_acc: 0.0566\n",
      "6/6 [==============================] - 2s 57ms/step - loss: 2.1851 - acc: 0.3659 - val_loss: 0.8298 - val_acc: 0.4024\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6221 - acc: 0.1706 - val_loss: 1.0318 - val_acc: 0.1132\n",
      "5/5 [==============================] - 1s 70ms/step - loss: 1.3609 - acc: 0.0867 - val_loss: 1.1028 - val_acc: 0.0267\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 2.0105 - acc: 0.3223 - val_loss: 0.9614 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5951 - acc: 0.1517 - val_loss: 1.0819 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 1.6413 - acc: 0.1754 - val_loss: 1.0169 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6599 - acc: 0.1801 - val_loss: 1.0747 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5717 - acc: 0.1611 - val_loss: 1.0573 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6549 - acc: 0.1848 - val_loss: 1.0453 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6767 - acc: 0.1943 - val_loss: 0.9613 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5157 - acc: 0.1232 - val_loss: 1.1020 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 195ms/step - loss: 1.1784 - acc: 0.0190 - val_loss: 1.0674 - val_acc: 0.0189\n",
      "7/7 [==============================] - 2s 47ms/step - loss: 1.6906 - acc: 0.1878 - val_loss: 0.9282 - val_acc: 0.2418\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7267 - acc: 0.2085 - val_loss: 0.9356 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6663 - acc: 0.1896 - val_loss: 0.9854 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1710 - acc: 0.0190 - val_loss: 1.0682 - val_acc: 0.0094\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7662 - acc: 0.2227 - val_loss: 0.9157 - val_acc: 0.2170\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.7876 - acc: 0.2308 - val_loss: 0.9421 - val_acc: 0.2115\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.7151 - acc: 0.2133 - val_loss: 0.9570 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9030 - acc: 0.2701 - val_loss: 0.9615 - val_acc: 0.1981\n",
      "5/5 [==============================] - 3s 72ms/step - loss: 1.7025 - acc: 0.2027 - val_loss: 0.8891 - val_acc: 0.2703\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4480 - acc: 0.4583 - val_loss: 0.9359 - val_acc: 0.2500\n",
      "7/7 [==============================] - 2s 51ms/step - loss: 1.4400 - acc: 0.0976 - val_loss: 1.0295 - val_acc: 0.1456\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2216 - acc: 0.0474 - val_loss: 1.0356 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4097 - acc: 0.1043 - val_loss: 0.9713 - val_acc: 0.1238\n",
      "7/7 [==============================] - 1s 48ms/step - loss: 1.8015 - acc: 0.2341 - val_loss: 0.9332 - val_acc: 0.2255\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6052 - acc: 0.1659 - val_loss: 0.9352 - val_acc: 0.2358\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.4622 - acc: 0.1185 - val_loss: 0.9359 - val_acc: 0.2075\n",
      "5/5 [==============================] - 3s 70ms/step - loss: 1.5949 - acc: 0.1678 - val_loss: 0.9674 - val_acc: 0.1892\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4729 - acc: 0.1280 - val_loss: 0.9225 - val_acc: 0.2358\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.9945 - acc: 0.2938 - val_loss: 0.8727 - val_acc: 0.3491\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.8191 - acc: 0.2370 - val_loss: 0.8853 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4814 - acc: 0.1327 - val_loss: 0.9320 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6772 - acc: 0.1896 - val_loss: 0.9387 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4674 - acc: 0.1280 - val_loss: 1.0152 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5737 - acc: 0.1611 - val_loss: 1.0018 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5248 - acc: 0.1469 - val_loss: 0.9586 - val_acc: 0.1604\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.6129 - acc: 0.1706 - val_loss: 0.9046 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4020 - acc: 0.1043 - val_loss: 0.9999 - val_acc: 0.0849\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.3890 - acc: 0.1043 - val_loss: 1.0088 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4140 - acc: 0.1090 - val_loss: 0.9528 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5298 - acc: 0.1469 - val_loss: 0.9335 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6950 - acc: 0.1991 - val_loss: 0.8780 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.1532 - acc: 0.0474 - val_loss: 0.7993 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4254 - acc: 0.1185 - val_loss: 0.9528 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 1.4501 - acc: 0.1268 - val_loss: 0.8909 - val_acc: 0.2136\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5782 - acc: 0.1611 - val_loss: 0.9333 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.3872 - acc: 0.1090 - val_loss: 0.9731 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4156 - acc: 0.1185 - val_loss: 0.9139 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6550 - acc: 0.1848 - val_loss: 0.9051 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.0580 - acc: 0.0190 - val_loss: 0.9394 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.2475 - acc: 0.0711 - val_loss: 0.9445 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.8813 - acc: 0.2417 - val_loss: 0.8398 - val_acc: 0.2642\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2966 - acc: 0.0853 - val_loss: 0.9494 - val_acc: 0.0472\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4570 - acc: 0.1280 - val_loss: 0.9217 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6664 - acc: 0.1801 - val_loss: 0.8676 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3601 - acc: 0.1043 - val_loss: 0.9179 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3605 - acc: 0.1043 - val_loss: 0.9336 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 1.7619 - acc: 0.2098 - val_loss: 0.8673 - val_acc: 0.1863\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8423 - acc: 0.2227 - val_loss: 0.8713 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6879 - acc: 0.1896 - val_loss: 0.9392 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6538 - acc: 0.1801 - val_loss: 0.9095 - val_acc: 0.1415\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.2924 - acc: 0.0853 - val_loss: 0.9433 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.7576 - acc: 0.2085 - val_loss: 0.8889 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7776 - acc: 0.2180 - val_loss: 0.9083 - val_acc: 0.1604\n",
      "5/5 [==============================] - 2s 71ms/step - loss: 2.2075 - acc: 0.3333 - val_loss: 0.8346 - val_acc: 0.3226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2667 - acc: 0.0758 - val_loss: 0.9476 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.0618 - acc: 0.0190 - val_loss: 0.9856 - val_acc: 0.0283\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.7151 - acc: 0.1970 - val_loss: 0.9061 - val_acc: 0.1717\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.7661 - acc: 0.2180 - val_loss: 0.9187 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8154 - acc: 0.2275 - val_loss: 0.8895 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 205ms/step - loss: 1.6475 - acc: 0.1801 - val_loss: 0.9511 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7447 - acc: 0.2085 - val_loss: 0.9186 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.6015 - acc: 0.1706 - val_loss: 0.9416 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4388 - acc: 0.1232 - val_loss: 0.9188 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5275 - acc: 0.1469 - val_loss: 0.8963 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4781 - acc: 0.1327 - val_loss: 0.9009 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6175 - acc: 0.1754 - val_loss: 0.8985 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5272 - acc: 0.1469 - val_loss: 0.9024 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7701 - acc: 0.2133 - val_loss: 0.8732 - val_acc: 0.2547\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.7417 - acc: 0.2085 - val_loss: 0.9329 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4506 - acc: 0.1280 - val_loss: 0.9663 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7193 - acc: 0.2038 - val_loss: 0.8574 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.9274 - acc: 0.2654 - val_loss: 0.9097 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.0744 - acc: 0.6730 - val_loss: 0.8110 - val_acc: 0.3868\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4419 - acc: 0.1232 - val_loss: 0.9820 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8493 - acc: 0.2464 - val_loss: 0.9054 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2528 - acc: 0.0664 - val_loss: 0.9666 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2377 - acc: 0.0616 - val_loss: 0.9182 - val_acc: 0.1792\n",
      "6/6 [==============================] - 2s 56ms/step - loss: 1.6651 - acc: 0.1889 - val_loss: 0.9458 - val_acc: 0.1333\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5401 - acc: 0.1564 - val_loss: 0.9091 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5498 - acc: 0.1517 - val_loss: 0.9048 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.5845 - acc: 0.1659 - val_loss: 0.9024 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 0.9200 - acc: 0.3318 - val_loss: 0.8646 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7114 - acc: 0.1991 - val_loss: 0.9201 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6005 - acc: 0.1611 - val_loss: 0.8883 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4353 - acc: 0.1185 - val_loss: 0.9532 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3815 - acc: 0.1090 - val_loss: 0.9586 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.4575 - acc: 0.1280 - val_loss: 0.9492 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6847 - acc: 0.1943 - val_loss: 0.8948 - val_acc: 0.2075\n",
      "5/5 [==============================] - 1s 70ms/step - loss: 1.8117 - acc: 0.2267 - val_loss: 0.9052 - val_acc: 0.1733\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3456 - acc: 0.0948 - val_loss: 0.9613 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4868 - acc: 0.1327 - val_loss: 0.9403 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.5821 - acc: 0.1602 - val_loss: 0.9330 - val_acc: 0.1429\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5983 - acc: 0.1706 - val_loss: 0.8314 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5683 - acc: 0.1564 - val_loss: 0.9348 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6093 - acc: 0.1706 - val_loss: 0.9325 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5464 - acc: 0.1469 - val_loss: 0.9473 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1823 - acc: 0.0474 - val_loss: 0.9588 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1602 - acc: 0.0427 - val_loss: 0.9369 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4227 - acc: 0.1185 - val_loss: 0.8735 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7084 - acc: 0.1943 - val_loss: 0.8969 - val_acc: 0.1792\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.6662 - acc: 0.1865 - val_loss: 0.9060 - val_acc: 0.1959\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8274 - acc: 0.2370 - val_loss: 0.8648 - val_acc: 0.2736\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.8526 - acc: 0.2353 - val_loss: 0.9018 - val_acc: 0.2059\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.3679 - acc: 0.0995 - val_loss: 0.9363 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9370 - acc: 0.2701 - val_loss: 0.9154 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5202 - acc: 0.1517 - val_loss: 0.8882 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3291 - acc: 0.0900 - val_loss: 0.9303 - val_acc: 0.1226\n",
      "7/7 [==============================] - 1s 48ms/step - loss: 1.8427 - acc: 0.2429 - val_loss: 0.8873 - val_acc: 0.2667\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8935 - acc: 0.2559 - val_loss: 0.9350 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9206 - acc: 0.2654 - val_loss: 0.9183 - val_acc: 0.2264\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.2855 - acc: 0.6667 - val_loss: 1.0364 - val_acc: 0.0000e+00\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8871 - acc: 0.2559 - val_loss: 0.9206 - val_acc: 0.2264\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.5951 - acc: 0.1659 - val_loss: 0.8963 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4358 - acc: 0.1185 - val_loss: 1.0080 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6230 - acc: 0.1754 - val_loss: 0.9564 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2321 - acc: 0.0569 - val_loss: 1.0157 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 47ms/step - loss: 1.9657 - acc: 0.2818 - val_loss: 0.8720 - val_acc: 0.3187\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.2430 - acc: 0.0521 - val_loss: 0.9988 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7079 - acc: 0.2038 - val_loss: 0.9374 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3648 - acc: 0.0948 - val_loss: 0.9850 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 1.5387 - acc: 0.1469 - val_loss: 0.9967 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2433 - acc: 0.0616 - val_loss: 1.0055 - val_acc: 0.0472\n",
      "6/6 [==============================] - 1s 62ms/step - loss: 1.7217 - acc: 0.2056 - val_loss: 0.8690 - val_acc: 0.3111\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7027 - acc: 0.2038 - val_loss: 0.9441 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6617 - acc: 0.1896 - val_loss: 0.9526 - val_acc: 0.1509\n",
      "4/4 [==============================] - 2s 94ms/step - loss: 1.7347 - acc: 0.2101 - val_loss: 0.8782 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4667 - acc: 0.1232 - val_loss: 0.9550 - val_acc: 0.1619\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.3365 - acc: 0.0900 - val_loss: 0.9564 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1963 - acc: 0.0521 - val_loss: 0.9958 - val_acc: 0.0472\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.3785 - acc: 0.0995 - val_loss: 0.9911 - val_acc: 0.0755\n",
      "6/6 [==============================] - 1s 56ms/step - loss: 1.6597 - acc: 0.1833 - val_loss: 0.8738 - val_acc: 0.2889\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1650 - acc: 0.0427 - val_loss: 0.9731 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4667 - acc: 0.1280 - val_loss: 0.9534 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7900 - acc: 0.2275 - val_loss: 0.9381 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7777 - acc: 0.2180 - val_loss: 0.9182 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.2380 - acc: 0.0616 - val_loss: 0.9363 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 2.0632 - acc: 0.3081 - val_loss: 0.8265 - val_acc: 0.3679\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8844 - acc: 0.2512 - val_loss: 0.9183 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.4952 - acc: 0.1327 - val_loss: 0.9735 - val_acc: 0.1321\n",
      "3/3 [==============================] - 2s 146ms/step - loss: 1.4255 - acc: 0.1136 - val_loss: 0.9839 - val_acc: 0.0682\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8335 - acc: 0.2417 - val_loss: 0.8879 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8027 - acc: 0.2370 - val_loss: 0.9163 - val_acc: 0.2736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2540 - acc: 0.3704 - val_loss: 0.9432 - val_acc: 0.2308\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 2.0223 - acc: 0.3058 - val_loss: 0.9864 - val_acc: 0.1748\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7519 - acc: 0.2227 - val_loss: 0.9290 - val_acc: 0.2642\n",
      "6/6 [==============================] - 1s 58ms/step - loss: 1.5203 - acc: 0.1389 - val_loss: 0.9664 - val_acc: 0.1778\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4934 - acc: 0.1327 - val_loss: 0.8997 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7272 - acc: 0.2085 - val_loss: 0.9278 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6808 - acc: 0.1943 - val_loss: 0.9332 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3300 - acc: 0.0758 - val_loss: 1.0351 - val_acc: 0.0755\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 1.8821 - acc: 0.2752 - val_loss: 0.9231 - val_acc: 0.2703\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6917 - acc: 0.2038 - val_loss: 0.9749 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6936 - acc: 0.1991 - val_loss: 0.9755 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5683 - acc: 0.1564 - val_loss: 1.0125 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4219 - acc: 0.0995 - val_loss: 0.9873 - val_acc: 0.1792\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.5395 - acc: 0.1517 - val_loss: 1.0065 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6774 - acc: 0.1896 - val_loss: 0.9561 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6062 - acc: 0.1754 - val_loss: 0.9406 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4929 - acc: 0.1374 - val_loss: 0.9880 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7375 - acc: 0.2180 - val_loss: 0.9806 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6228 - acc: 0.1754 - val_loss: 1.0279 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.0896 - acc: 0.0047 - val_loss: 1.0663 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7038 - acc: 0.2038 - val_loss: 0.9361 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7826 - acc: 0.2275 - val_loss: 0.9300 - val_acc: 0.2264\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.2816 - acc: 0.0664 - val_loss: 1.0359 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6769 - acc: 0.1943 - val_loss: 0.9543 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5442 - acc: 0.1517 - val_loss: 0.9527 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4302 - acc: 0.1137 - val_loss: 1.0066 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2154 - acc: 0.0521 - val_loss: 1.0286 - val_acc: 0.0377\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 1.5858 - acc: 0.1644 - val_loss: 0.9436 - val_acc: 0.1918\n",
      "5/5 [==============================] - 2s 78ms/step - loss: 2.0942 - acc: 0.3243 - val_loss: 0.8098 - val_acc: 0.3919\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6047 - acc: 0.1706 - val_loss: 0.9674 - val_acc: 0.1321\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4870 - acc: 0.1327 - val_loss: 0.9294 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 75ms/step - loss: 1.5222 - acc: 0.1409 - val_loss: 0.9778 - val_acc: 0.1351\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6849 - acc: 0.1991 - val_loss: 0.9168 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2525 - acc: 0.0569 - val_loss: 1.0297 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4097 - acc: 0.1327 - val_loss: 0.9988 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4632 - acc: 0.1232 - val_loss: 0.9379 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6341 - acc: 0.1754 - val_loss: 0.9049 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1658 - acc: 0.0427 - val_loss: 0.9817 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5053 - acc: 0.1422 - val_loss: 0.9631 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.9970 - acc: 0.2749 - val_loss: 0.9000 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5644 - acc: 0.1564 - val_loss: 0.9095 - val_acc: 0.2075\n",
      "3/3 [==============================] - 2s 137ms/step - loss: 1.6897 - acc: 0.1954 - val_loss: 0.9332 - val_acc: 0.1818\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8816 - acc: 0.2559 - val_loss: 0.9260 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8704 - acc: 0.2512 - val_loss: 0.9436 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5073 - acc: 0.1374 - val_loss: 1.0035 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8323 - acc: 0.2464 - val_loss: 0.9313 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9577 - acc: 0.2891 - val_loss: 0.8862 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6978 - acc: 0.2085 - val_loss: 0.9850 - val_acc: 0.1887\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4438 - acc: 0.1137 - val_loss: 1.0581 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7148 - acc: 0.2038 - val_loss: 0.9859 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6079 - acc: 0.1706 - val_loss: 0.9514 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4073 - acc: 0.1043 - val_loss: 1.0111 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4521 - acc: 0.1185 - val_loss: 0.9896 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.5627 - acc: 0.1564 - val_loss: 0.9852 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3121 - acc: 0.0758 - val_loss: 1.0459 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.3146 - acc: 0.0758 - val_loss: 1.0076 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.5026 - acc: 0.1374 - val_loss: 1.0112 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4145 - acc: 0.1090 - val_loss: 1.0039 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 2.0750 - acc: 0.3318 - val_loss: 0.8637 - val_acc: 0.3396\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3448 - acc: 0.0900 - val_loss: 1.0274 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7659 - acc: 0.2275 - val_loss: 0.9100 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4155 - acc: 0.1090 - val_loss: 0.9840 - val_acc: 0.1415\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.6684 - acc: 0.1896 - val_loss: 0.9772 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3815 - acc: 0.0995 - val_loss: 1.0032 - val_acc: 0.0857\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.3561 - acc: 0.0900 - val_loss: 0.9770 - val_acc: 0.1509\n",
      "7/7 [==============================] - 3s 50ms/step - loss: 1.4211 - acc: 0.1105 - val_loss: 0.9427 - val_acc: 0.1648\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7159 - acc: 0.2038 - val_loss: 0.9171 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.3984 - acc: 0.1090 - val_loss: 0.9977 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2403 - acc: 0.0569 - val_loss: 1.0038 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7436 - acc: 0.2085 - val_loss: 0.8707 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.7875 - acc: 0.2227 - val_loss: 0.8503 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5490 - acc: 0.1517 - val_loss: 0.9542 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6861 - acc: 0.1943 - val_loss: 0.9149 - val_acc: 0.2170\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.7683 - acc: 0.2180 - val_loss: 0.9232 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.7089 - acc: 0.1991 - val_loss: 0.9249 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5739 - acc: 0.1564 - val_loss: 0.9631 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6498 - acc: 0.1848 - val_loss: 0.9387 - val_acc: 0.1604\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.2546 - acc: 0.1866 - val_loss: 0.8462 - val_acc: 0.3077\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3424 - acc: 0.0948 - val_loss: 0.9604 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7093 - acc: 0.1991 - val_loss: 0.9054 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 2.0579 - acc: 0.3033 - val_loss: 0.9306 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5936 - acc: 0.1706 - val_loss: 0.9886 - val_acc: 0.0660\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.5882 - acc: 0.1659 - val_loss: 0.9292 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8938 - acc: 0.2607 - val_loss: 0.9466 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3424 - acc: 0.0900 - val_loss: 0.9734 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4316 - acc: 0.1185 - val_loss: 0.9934 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.7264 - acc: 0.2038 - val_loss: 0.9306 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6182 - acc: 0.1754 - val_loss: 0.9633 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5704 - acc: 0.1611 - val_loss: 0.9862 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7661 - acc: 0.2227 - val_loss: 0.9177 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7227 - acc: 0.2085 - val_loss: 0.9807 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.2248 - acc: 0.0521 - val_loss: 0.9715 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.5324 - acc: 0.1469 - val_loss: 1.0020 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4879 - acc: 0.1327 - val_loss: 0.9279 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7494 - acc: 0.2133 - val_loss: 0.9569 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.3782 - acc: 0.0995 - val_loss: 0.9832 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8914 - acc: 0.2607 - val_loss: 0.9429 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4426 - acc: 0.1280 - val_loss: 0.9637 - val_acc: 0.1132\n",
      "6/6 [==============================] - 2s 56ms/step - loss: 1.3477 - acc: 0.0903 - val_loss: 0.9488 - val_acc: 0.1410\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6095 - acc: 0.1706 - val_loss: 0.9471 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 1.4122 - acc: 0.1090 - val_loss: 0.9661 - val_acc: 0.1415\n",
      "4/4 [==============================] - 2s 98ms/step - loss: 1.6299 - acc: 0.1765 - val_loss: 0.8722 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5472 - acc: 0.1517 - val_loss: 0.9750 - val_acc: 0.1038\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 1.6298 - acc: 0.1750 - val_loss: 0.9198 - val_acc: 0.1900\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6480 - acc: 0.1848 - val_loss: 0.9521 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5592 - acc: 0.1564 - val_loss: 0.9062 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.9227 - acc: 0.2701 - val_loss: 0.9100 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6740 - acc: 0.1896 - val_loss: 0.9529 - val_acc: 0.1887\n",
      "8/8 [==============================] - 3s 191ms/step - loss: 1.7763 - acc: 0.2227 - val_loss: 0.9665 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4838 - acc: 0.1280 - val_loss: 1.0065 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2420 - acc: 0.0569 - val_loss: 1.0133 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5498 - acc: 0.1517 - val_loss: 0.9945 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7209 - acc: 0.2085 - val_loss: 0.9268 - val_acc: 0.2075\n",
      "7/7 [==============================] - 2s 46ms/step - loss: 1.3584 - acc: 0.0955 - val_loss: 1.0220 - val_acc: 0.0600\n",
      "4/4 [==============================] - 2s 110ms/step - loss: 2.0284 - acc: 0.3025 - val_loss: 0.8733 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6895 - acc: 0.1943 - val_loss: 0.8746 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5148 - acc: 0.1422 - val_loss: 0.9991 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.6131 - acc: 0.1754 - val_loss: 0.8904 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4736 - acc: 0.1280 - val_loss: 0.9813 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7806 - acc: 0.2227 - val_loss: 0.9554 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6093 - acc: 0.1754 - val_loss: 0.9554 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.9123 - acc: 0.2749 - val_loss: 0.8976 - val_acc: 0.2736\n",
      "5/5 [==============================] - 2s 71ms/step - loss: 2.0222 - acc: 0.3041 - val_loss: 0.9277 - val_acc: 0.2432\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.4913 - acc: 0.1374 - val_loss: 0.9926 - val_acc: 0.1132\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 2.0508 - acc: 0.3223 - val_loss: 0.9019 - val_acc: 0.2925\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9136 - acc: 0.2701 - val_loss: 0.9147 - val_acc: 0.2264\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.6323 - acc: 0.1801 - val_loss: 0.9763 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6824 - acc: 0.1943 - val_loss: 1.0033 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2453 - acc: 0.0474 - val_loss: 1.0844 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.1990 - acc: 0.0379 - val_loss: 1.0476 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7977 - acc: 0.2417 - val_loss: 0.9563 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7749 - acc: 0.2275 - val_loss: 0.9479 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5549 - acc: 0.1517 - val_loss: 0.9315 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7969 - acc: 0.2322 - val_loss: 0.9115 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4282 - acc: 0.1137 - val_loss: 1.0405 - val_acc: 0.0755\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.5335 - acc: 0.1469 - val_loss: 1.0323 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4237 - acc: 0.1090 - val_loss: 1.0500 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.7707 - acc: 0.2275 - val_loss: 0.9782 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6540 - acc: 0.1801 - val_loss: 0.9397 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4335 - acc: 0.1137 - val_loss: 0.9760 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4509 - acc: 0.1185 - val_loss: 1.0435 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6039 - acc: 0.1706 - val_loss: 0.8855 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7596 - acc: 0.2275 - val_loss: 0.9513 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3201 - acc: 0.0806 - val_loss: 1.0138 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.2554 - acc: 0.0616 - val_loss: 1.0188 - val_acc: 0.0566\n",
      "6/6 [==============================] - 2s 59ms/step - loss: 2.2054 - acc: 0.3659 - val_loss: 0.8158 - val_acc: 0.4024\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6127 - acc: 0.1706 - val_loss: 0.9962 - val_acc: 0.1132\n",
      "5/5 [==============================] - 1s 71ms/step - loss: 1.3284 - acc: 0.0867 - val_loss: 1.0588 - val_acc: 0.0267\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 2.0522 - acc: 0.3223 - val_loss: 0.9087 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5512 - acc: 0.1517 - val_loss: 0.9916 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6206 - acc: 0.1754 - val_loss: 0.9543 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6396 - acc: 0.1801 - val_loss: 0.9964 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5666 - acc: 0.1611 - val_loss: 1.0032 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.6493 - acc: 0.1848 - val_loss: 0.9907 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6806 - acc: 0.1943 - val_loss: 0.9349 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4688 - acc: 0.1232 - val_loss: 1.0503 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.1511 - acc: 0.0237 - val_loss: 1.0601 - val_acc: 0.0189\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 1.6817 - acc: 0.1878 - val_loss: 0.9256 - val_acc: 0.2418\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7143 - acc: 0.2085 - val_loss: 0.9408 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6635 - acc: 0.1896 - val_loss: 0.9564 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1425 - acc: 0.0190 - val_loss: 1.0724 - val_acc: 0.0094\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7771 - acc: 0.2227 - val_loss: 0.9286 - val_acc: 0.2170\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.7835 - acc: 0.2308 - val_loss: 0.9512 - val_acc: 0.2115\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7217 - acc: 0.2133 - val_loss: 0.9481 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8931 - acc: 0.2701 - val_loss: 0.9742 - val_acc: 0.1981\n",
      "5/5 [==============================] - 2s 72ms/step - loss: 1.7019 - acc: 0.2027 - val_loss: 0.9074 - val_acc: 0.2703\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4164 - acc: 0.4583 - val_loss: 0.9371 - val_acc: 0.2500\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.4119 - acc: 0.0976 - val_loss: 1.0176 - val_acc: 0.1456\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2481 - acc: 0.0474 - val_loss: 1.0490 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4056 - acc: 0.1043 - val_loss: 1.0032 - val_acc: 0.1238\n",
      "7/7 [==============================] - 3s 49ms/step - loss: 1.7955 - acc: 0.2341 - val_loss: 0.9371 - val_acc: 0.2255\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5911 - acc: 0.1659 - val_loss: 0.9420 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4515 - acc: 0.1185 - val_loss: 0.9470 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 70ms/step - loss: 1.5942 - acc: 0.1678 - val_loss: 0.9621 - val_acc: 0.1892\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4752 - acc: 0.1280 - val_loss: 0.9283 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9806 - acc: 0.2938 - val_loss: 0.8646 - val_acc: 0.3491\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8005 - acc: 0.2370 - val_loss: 0.9015 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4890 - acc: 0.1327 - val_loss: 0.9532 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6690 - acc: 0.1896 - val_loss: 0.9469 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 189ms/step - loss: 1.4710 - acc: 0.1280 - val_loss: 1.0412 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5670 - acc: 0.1611 - val_loss: 1.0346 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5358 - acc: 0.1469 - val_loss: 0.9858 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6129 - acc: 0.1706 - val_loss: 0.9222 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4059 - acc: 0.1043 - val_loss: 1.0264 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.3972 - acc: 0.1043 - val_loss: 1.0209 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4071 - acc: 0.1090 - val_loss: 0.9669 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5305 - acc: 0.1469 - val_loss: 0.9437 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6779 - acc: 0.1991 - val_loss: 0.9006 - val_acc: 0.2453\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.1149 - acc: 0.1659 - val_loss: 0.5703 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4331 - acc: 0.1185 - val_loss: 0.9539 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.4591 - acc: 0.1268 - val_loss: 0.9121 - val_acc: 0.2136\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5892 - acc: 0.1611 - val_loss: 0.9514 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3987 - acc: 0.1090 - val_loss: 0.9975 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4360 - acc: 0.1185 - val_loss: 0.9283 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6638 - acc: 0.1848 - val_loss: 0.9276 - val_acc: 0.1698\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.0510 - acc: 0.0379 - val_loss: 0.9347 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.2486 - acc: 0.0711 - val_loss: 0.9472 - val_acc: 0.0660\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.8801 - acc: 0.2417 - val_loss: 0.8451 - val_acc: 0.2642\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2898 - acc: 0.0853 - val_loss: 0.9439 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4685 - acc: 0.1280 - val_loss: 0.8859 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6561 - acc: 0.1801 - val_loss: 0.9023 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3578 - acc: 0.1043 - val_loss: 0.8962 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3504 - acc: 0.1043 - val_loss: 0.9225 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.7480 - acc: 0.2098 - val_loss: 0.8774 - val_acc: 0.2059\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8190 - acc: 0.2227 - val_loss: 0.8891 - val_acc: 0.1981\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.6993 - acc: 0.1896 - val_loss: 0.9575 - val_acc: 0.0943\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.6468 - acc: 0.1801 - val_loss: 0.9391 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3057 - acc: 0.0853 - val_loss: 0.9694 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7413 - acc: 0.2085 - val_loss: 0.9204 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7516 - acc: 0.2180 - val_loss: 0.9481 - val_acc: 0.1604\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 2.1504 - acc: 0.3333 - val_loss: 0.8670 - val_acc: 0.3226\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2907 - acc: 0.0758 - val_loss: 0.9728 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.0919 - acc: 0.0190 - val_loss: 1.0103 - val_acc: 0.0283\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 1.7039 - acc: 0.1970 - val_loss: 0.9218 - val_acc: 0.1717\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.7585 - acc: 0.2180 - val_loss: 0.9533 - val_acc: 0.1509\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.8010 - acc: 0.2275 - val_loss: 0.9096 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6459 - acc: 0.1801 - val_loss: 0.9715 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7262 - acc: 0.2085 - val_loss: 0.9624 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6022 - acc: 0.1706 - val_loss: 0.9576 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.4628 - acc: 0.1232 - val_loss: 0.9557 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5334 - acc: 0.1469 - val_loss: 0.9090 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4768 - acc: 0.1327 - val_loss: 0.9259 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6282 - acc: 0.1754 - val_loss: 0.9256 - val_acc: 0.1981\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.5421 - acc: 0.1469 - val_loss: 0.9020 - val_acc: 0.2170\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.7559 - acc: 0.2133 - val_loss: 0.8944 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7367 - acc: 0.2085 - val_loss: 0.9452 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4660 - acc: 0.1280 - val_loss: 0.9753 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7138 - acc: 0.2038 - val_loss: 0.8714 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9008 - acc: 0.2654 - val_loss: 0.9330 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.0734 - acc: 0.7820 - val_loss: 0.6549 - val_acc: 0.5660\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4536 - acc: 0.1232 - val_loss: 1.0151 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8479 - acc: 0.2464 - val_loss: 0.9243 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2808 - acc: 0.0664 - val_loss: 0.9976 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.2436 - acc: 0.0616 - val_loss: 0.9329 - val_acc: 0.1792\n",
      "6/6 [==============================] - 1s 56ms/step - loss: 1.6606 - acc: 0.1889 - val_loss: 0.9757 - val_acc: 0.1333\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5443 - acc: 0.1564 - val_loss: 0.9278 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5553 - acc: 0.1517 - val_loss: 0.9339 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5881 - acc: 0.1659 - val_loss: 0.9252 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 0.8490 - acc: 0.4123 - val_loss: 0.8056 - val_acc: 0.3208\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7066 - acc: 0.1991 - val_loss: 0.9496 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6009 - acc: 0.1611 - val_loss: 0.9082 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4504 - acc: 0.1185 - val_loss: 0.9641 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.4024 - acc: 0.1090 - val_loss: 0.9890 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4687 - acc: 0.1280 - val_loss: 0.9777 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6740 - acc: 0.1943 - val_loss: 0.9282 - val_acc: 0.2075\n",
      "5/5 [==============================] - 1s 76ms/step - loss: 1.8082 - acc: 0.2267 - val_loss: 0.9116 - val_acc: 0.1733\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3503 - acc: 0.0948 - val_loss: 0.9883 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4686 - acc: 0.1327 - val_loss: 0.9665 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.5999 - acc: 0.1602 - val_loss: 0.9299 - val_acc: 0.1429\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6158 - acc: 0.1706 - val_loss: 0.8347 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5612 - acc: 0.1564 - val_loss: 0.9432 - val_acc: 0.1509\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.6286 - acc: 0.1706 - val_loss: 0.9332 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5378 - acc: 0.1517 - val_loss: 0.9305 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.2159 - acc: 0.0474 - val_loss: 0.9933 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1756 - acc: 0.0427 - val_loss: 0.9674 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4355 - acc: 0.1185 - val_loss: 0.9025 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6962 - acc: 0.1943 - val_loss: 0.9343 - val_acc: 0.1792\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.6606 - acc: 0.1865 - val_loss: 0.9358 - val_acc: 0.1959\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8070 - acc: 0.2370 - val_loss: 0.8970 - val_acc: 0.2736\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.8312 - acc: 0.2353 - val_loss: 0.9067 - val_acc: 0.2059\n",
      "8/8 [==============================] - 3s 198ms/step - loss: 1.3784 - acc: 0.0995 - val_loss: 0.9466 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9111 - acc: 0.2701 - val_loss: 0.9453 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5290 - acc: 0.1517 - val_loss: 0.9152 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3321 - acc: 0.0900 - val_loss: 0.9406 - val_acc: 0.1226\n",
      "7/7 [==============================] - 1s 48ms/step - loss: 1.8169 - acc: 0.2429 - val_loss: 0.9153 - val_acc: 0.2667\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8712 - acc: 0.2559 - val_loss: 0.9645 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8961 - acc: 0.2654 - val_loss: 0.9508 - val_acc: 0.2264\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.2480 - acc: 0.6667 - val_loss: 0.9461 - val_acc: 0.0000e+00\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8850 - acc: 0.2559 - val_loss: 0.9345 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5857 - acc: 0.1659 - val_loss: 0.9126 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4477 - acc: 0.1185 - val_loss: 1.0331 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6382 - acc: 0.1754 - val_loss: 0.9837 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.2485 - acc: 0.0569 - val_loss: 1.0354 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 47ms/step - loss: 1.9479 - acc: 0.2818 - val_loss: 0.8897 - val_acc: 0.3187\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.2603 - acc: 0.0521 - val_loss: 1.0096 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 1.7216 - acc: 0.2038 - val_loss: 0.9322 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.3573 - acc: 0.0948 - val_loss: 0.9959 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5318 - acc: 0.1469 - val_loss: 0.9974 - val_acc: 0.0849\n",
      "8/8 [==============================] - 3s 203ms/step - loss: 1.2443 - acc: 0.0616 - val_loss: 1.0051 - val_acc: 0.0472\n",
      "6/6 [==============================] - 1s 58ms/step - loss: 1.7193 - acc: 0.2056 - val_loss: 0.8710 - val_acc: 0.3111\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 1.7033 - acc: 0.2038 - val_loss: 0.9556 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6773 - acc: 0.1896 - val_loss: 0.9527 - val_acc: 0.1509\n",
      "4/4 [==============================] - 2s 91ms/step - loss: 1.7265 - acc: 0.2101 - val_loss: 0.8797 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4598 - acc: 0.1232 - val_loss: 0.9690 - val_acc: 0.1619\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3335 - acc: 0.0900 - val_loss: 0.9638 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.1982 - acc: 0.0521 - val_loss: 0.9908 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 1.3687 - acc: 0.0995 - val_loss: 0.9846 - val_acc: 0.0755\n",
      "6/6 [==============================] - 1s 59ms/step - loss: 1.6781 - acc: 0.1833 - val_loss: 0.8734 - val_acc: 0.2889\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.1400 - acc: 0.0569 - val_loss: 0.8964 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 1.4697 - acc: 0.1280 - val_loss: 0.9319 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7942 - acc: 0.2275 - val_loss: 0.9194 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7858 - acc: 0.2180 - val_loss: 0.8952 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2148 - acc: 0.0616 - val_loss: 0.9156 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 2.0882 - acc: 0.3081 - val_loss: 0.8088 - val_acc: 0.3679\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9204 - acc: 0.2464 - val_loss: 0.8941 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4969 - acc: 0.1327 - val_loss: 0.9634 - val_acc: 0.1321\n",
      "3/3 [==============================] - 3s 686ms/step - loss: 1.4182 - acc: 0.1136 - val_loss: 0.9049 - val_acc: 0.0682\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8568 - acc: 0.2417 - val_loss: 0.8566 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8133 - acc: 0.2370 - val_loss: 0.8941 - val_acc: 0.2736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2567 - acc: 0.3704 - val_loss: 0.9163 - val_acc: 0.2308\n",
      "7/7 [==============================] - 2s 51ms/step - loss: 2.0595 - acc: 0.3058 - val_loss: 0.9606 - val_acc: 0.1748\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7812 - acc: 0.2227 - val_loss: 0.9036 - val_acc: 0.2642\n",
      "6/6 [==============================] - 1s 57ms/step - loss: 1.5067 - acc: 0.1389 - val_loss: 0.9457 - val_acc: 0.1778\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4681 - acc: 0.1327 - val_loss: 0.8732 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7402 - acc: 0.2085 - val_loss: 0.8961 - val_acc: 0.2830\n",
      "8/8 [==============================] - 3s 197ms/step - loss: 1.6939 - acc: 0.1943 - val_loss: 0.9112 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3136 - acc: 0.0758 - val_loss: 1.0103 - val_acc: 0.0755\n",
      "5/5 [==============================] - 2s 72ms/step - loss: 1.9098 - acc: 0.2752 - val_loss: 0.9058 - val_acc: 0.2703\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7071 - acc: 0.2038 - val_loss: 0.9429 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6772 - acc: 0.1991 - val_loss: 0.9434 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5525 - acc: 0.1564 - val_loss: 0.9837 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4012 - acc: 0.0995 - val_loss: 0.9714 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5296 - acc: 0.1517 - val_loss: 1.0015 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6800 - acc: 0.1896 - val_loss: 0.9607 - val_acc: 0.2075\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.6081 - acc: 0.1754 - val_loss: 0.9434 - val_acc: 0.2358\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 1.5108 - acc: 0.1374 - val_loss: 1.0044 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7373 - acc: 0.2180 - val_loss: 0.9938 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6226 - acc: 0.1754 - val_loss: 1.0358 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.0905 - acc: 0.0047 - val_loss: 1.0740 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7038 - acc: 0.2038 - val_loss: 0.9414 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.7819 - acc: 0.2275 - val_loss: 0.9287 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2861 - acc: 0.0664 - val_loss: 1.0257 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6756 - acc: 0.1943 - val_loss: 0.9612 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5491 - acc: 0.1517 - val_loss: 0.9615 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4427 - acc: 0.1137 - val_loss: 1.0030 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2148 - acc: 0.0521 - val_loss: 1.0213 - val_acc: 0.0377\n",
      "5/5 [==============================] - 2s 75ms/step - loss: 1.5908 - acc: 0.1644 - val_loss: 0.9368 - val_acc: 0.1918\n",
      "5/5 [==============================] - 2s 75ms/step - loss: 2.0732 - acc: 0.3243 - val_loss: 0.8237 - val_acc: 0.3919\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5971 - acc: 0.1706 - val_loss: 0.9594 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4865 - acc: 0.1327 - val_loss: 0.9280 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 76ms/step - loss: 1.5242 - acc: 0.1409 - val_loss: 0.9902 - val_acc: 0.1351\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6836 - acc: 0.1991 - val_loss: 0.9039 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2383 - acc: 0.0569 - val_loss: 1.0238 - val_acc: 0.0472\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.3952 - acc: 0.1422 - val_loss: 0.9298 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4632 - acc: 0.1232 - val_loss: 0.9201 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6347 - acc: 0.1754 - val_loss: 0.9022 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1519 - acc: 0.0427 - val_loss: 0.9620 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5148 - acc: 0.1422 - val_loss: 0.9724 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 2.0055 - acc: 0.2749 - val_loss: 0.9072 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5614 - acc: 0.1564 - val_loss: 0.9292 - val_acc: 0.2075\n",
      "3/3 [==============================] - 1s 137ms/step - loss: 1.6845 - acc: 0.1954 - val_loss: 0.9525 - val_acc: 0.1818\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8743 - acc: 0.2559 - val_loss: 0.9425 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.8472 - acc: 0.2512 - val_loss: 0.9680 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5109 - acc: 0.1374 - val_loss: 1.0174 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8342 - acc: 0.2464 - val_loss: 0.9336 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9463 - acc: 0.2891 - val_loss: 0.8877 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6996 - acc: 0.2085 - val_loss: 0.9790 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4422 - acc: 0.1137 - val_loss: 1.0670 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6979 - acc: 0.2038 - val_loss: 1.0057 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5972 - acc: 0.1706 - val_loss: 0.9625 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4113 - acc: 0.1043 - val_loss: 1.0268 - val_acc: 0.1132\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.4436 - acc: 0.1185 - val_loss: 1.0012 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5636 - acc: 0.1564 - val_loss: 0.9750 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.3152 - acc: 0.0758 - val_loss: 1.0393 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 1.3121 - acc: 0.0758 - val_loss: 1.0023 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4995 - acc: 0.1374 - val_loss: 1.0020 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4036 - acc: 0.1090 - val_loss: 0.9849 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 2.1069 - acc: 0.3318 - val_loss: 0.8506 - val_acc: 0.3396\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3411 - acc: 0.0900 - val_loss: 1.0088 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7728 - acc: 0.2275 - val_loss: 0.8931 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4200 - acc: 0.1090 - val_loss: 0.9732 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6727 - acc: 0.1896 - val_loss: 0.9675 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.3706 - acc: 0.1043 - val_loss: 1.0068 - val_acc: 0.0857\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3496 - acc: 0.0900 - val_loss: 0.9742 - val_acc: 0.1509\n",
      "7/7 [==============================] - 2s 47ms/step - loss: 1.4142 - acc: 0.1105 - val_loss: 0.9308 - val_acc: 0.1648\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7156 - acc: 0.2038 - val_loss: 0.9196 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3984 - acc: 0.1090 - val_loss: 0.9913 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2257 - acc: 0.0569 - val_loss: 1.0144 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.7631 - acc: 0.2085 - val_loss: 0.8625 - val_acc: 0.2830\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 1.7855 - acc: 0.2227 - val_loss: 0.8494 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5439 - acc: 0.1517 - val_loss: 0.9607 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.6833 - acc: 0.1943 - val_loss: 0.9207 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7700 - acc: 0.2180 - val_loss: 0.9309 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6947 - acc: 0.1991 - val_loss: 0.9284 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5738 - acc: 0.1564 - val_loss: 0.9739 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6503 - acc: 0.1848 - val_loss: 0.9540 - val_acc: 0.1604\n",
      "7/7 [==============================] - 2s 49ms/step - loss: 1.2279 - acc: 0.3445 - val_loss: 0.8088 - val_acc: 0.3462\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.3543 - acc: 0.0948 - val_loss: 0.9782 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6975 - acc: 0.1991 - val_loss: 0.9320 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 2.0256 - acc: 0.3033 - val_loss: 0.9507 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5924 - acc: 0.1706 - val_loss: 1.0391 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.5863 - acc: 0.1659 - val_loss: 0.9695 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8580 - acc: 0.2607 - val_loss: 0.9979 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3633 - acc: 0.0900 - val_loss: 1.0178 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.4396 - acc: 0.1185 - val_loss: 1.0329 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7149 - acc: 0.2038 - val_loss: 0.9479 - val_acc: 0.1509\n",
      "8/8 [==============================] - 3s 191ms/step - loss: 1.6015 - acc: 0.1754 - val_loss: 0.9951 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5874 - acc: 0.1611 - val_loss: 1.0275 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7554 - acc: 0.2227 - val_loss: 0.9459 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7278 - acc: 0.2085 - val_loss: 1.0174 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.2535 - acc: 0.0521 - val_loss: 1.0015 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5393 - acc: 0.1469 - val_loss: 1.0339 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4904 - acc: 0.1327 - val_loss: 0.9490 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7307 - acc: 0.2133 - val_loss: 0.9876 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3899 - acc: 0.0995 - val_loss: 1.0059 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.8655 - acc: 0.2607 - val_loss: 0.9796 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4455 - acc: 0.1280 - val_loss: 0.9768 - val_acc: 0.1509\n",
      "6/6 [==============================] - 2s 56ms/step - loss: 1.3673 - acc: 0.0903 - val_loss: 1.0077 - val_acc: 0.1410\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6139 - acc: 0.1706 - val_loss: 1.0089 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4342 - acc: 0.1090 - val_loss: 1.0279 - val_acc: 0.1415\n",
      "4/4 [==============================] - 2s 111ms/step - loss: 1.6275 - acc: 0.1765 - val_loss: 0.9202 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5487 - acc: 0.1517 - val_loss: 1.0216 - val_acc: 0.1038\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 1.6197 - acc: 0.1750 - val_loss: 0.9547 - val_acc: 0.1900\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.6466 - acc: 0.1848 - val_loss: 0.9930 - val_acc: 0.1604\n",
      "8/8 [==============================] - 3s 226ms/step - loss: 1.5605 - acc: 0.1564 - val_loss: 0.9337 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.9021 - acc: 0.2701 - val_loss: 0.9320 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6622 - acc: 0.1896 - val_loss: 0.9793 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7627 - acc: 0.2227 - val_loss: 0.9911 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.4853 - acc: 0.1280 - val_loss: 1.0185 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2569 - acc: 0.0569 - val_loss: 1.0284 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5444 - acc: 0.1517 - val_loss: 1.0075 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.7224 - acc: 0.2085 - val_loss: 0.9342 - val_acc: 0.2075\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.3616 - acc: 0.0955 - val_loss: 1.0297 - val_acc: 0.0600\n",
      "4/4 [==============================] - 1s 92ms/step - loss: 2.0289 - acc: 0.3025 - val_loss: 0.8807 - val_acc: 0.2881\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.7109 - acc: 0.1943 - val_loss: 0.8682 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5281 - acc: 0.1422 - val_loss: 1.0008 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6216 - acc: 0.1754 - val_loss: 0.9069 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4752 - acc: 0.1280 - val_loss: 0.9923 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7691 - acc: 0.2227 - val_loss: 0.9719 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6086 - acc: 0.1754 - val_loss: 0.9734 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9027 - acc: 0.2749 - val_loss: 0.9107 - val_acc: 0.2736\n",
      "5/5 [==============================] - 2s 69ms/step - loss: 1.9932 - acc: 0.3041 - val_loss: 0.9464 - val_acc: 0.2432\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5050 - acc: 0.1374 - val_loss: 1.0292 - val_acc: 0.1132\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 2.0051 - acc: 0.3223 - val_loss: 0.9308 - val_acc: 0.2925\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8840 - acc: 0.2701 - val_loss: 0.9463 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6257 - acc: 0.1801 - val_loss: 1.0166 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 1.6825 - acc: 0.1943 - val_loss: 1.0327 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2788 - acc: 0.0474 - val_loss: 1.1243 - val_acc: 0.0189\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.2288 - acc: 0.0379 - val_loss: 1.0880 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7831 - acc: 0.2417 - val_loss: 0.9762 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.7602 - acc: 0.2275 - val_loss: 0.9646 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5576 - acc: 0.1517 - val_loss: 0.9552 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7814 - acc: 0.2322 - val_loss: 0.9381 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 53ms/step - loss: 1.4421 - acc: 0.1137 - val_loss: 1.0702 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5385 - acc: 0.1469 - val_loss: 1.0686 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4407 - acc: 0.1090 - val_loss: 1.0797 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7602 - acc: 0.2275 - val_loss: 1.0067 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6478 - acc: 0.1848 - val_loss: 0.9542 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4276 - acc: 0.1137 - val_loss: 0.9842 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4477 - acc: 0.1185 - val_loss: 1.0503 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5951 - acc: 0.1706 - val_loss: 0.8974 - val_acc: 0.3113\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 1.7552 - acc: 0.2275 - val_loss: 0.9652 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.3152 - acc: 0.0806 - val_loss: 1.0255 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.2593 - acc: 0.0616 - val_loss: 1.0342 - val_acc: 0.0566\n",
      "6/6 [==============================] - 2s 60ms/step - loss: 2.2010 - acc: 0.3659 - val_loss: 0.8207 - val_acc: 0.4024\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6009 - acc: 0.1706 - val_loss: 1.0132 - val_acc: 0.1132\n",
      "5/5 [==============================] - 1s 73ms/step - loss: 1.3373 - acc: 0.0867 - val_loss: 1.0662 - val_acc: 0.0267\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 2.0389 - acc: 0.3223 - val_loss: 0.9196 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5486 - acc: 0.1517 - val_loss: 1.0172 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6116 - acc: 0.1754 - val_loss: 0.9720 - val_acc: 0.2075\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.6382 - acc: 0.1801 - val_loss: 1.0238 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5670 - acc: 0.1611 - val_loss: 1.0198 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6517 - acc: 0.1848 - val_loss: 1.0151 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6752 - acc: 0.1943 - val_loss: 0.9418 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4874 - acc: 0.1232 - val_loss: 1.0613 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.1515 - acc: 0.0237 - val_loss: 1.0515 - val_acc: 0.0283\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.6656 - acc: 0.1878 - val_loss: 0.9257 - val_acc: 0.2418\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7306 - acc: 0.2085 - val_loss: 0.9340 - val_acc: 0.2075\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.6564 - acc: 0.1896 - val_loss: 0.9685 - val_acc: 0.2075\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.1553 - acc: 0.0190 - val_loss: 1.0779 - val_acc: 0.0094\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.7727 - acc: 0.2227 - val_loss: 0.9106 - val_acc: 0.2170\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.7993 - acc: 0.2308 - val_loss: 0.9441 - val_acc: 0.2115\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7261 - acc: 0.2133 - val_loss: 0.9502 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.9061 - acc: 0.2701 - val_loss: 0.9709 - val_acc: 0.1981\n",
      "5/5 [==============================] - 2s 77ms/step - loss: 1.7043 - acc: 0.2027 - val_loss: 0.9077 - val_acc: 0.2703\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4356 - acc: 0.4583 - val_loss: 0.9319 - val_acc: 0.2500\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.4060 - acc: 0.0976 - val_loss: 1.0163 - val_acc: 0.1456\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2239 - acc: 0.0474 - val_loss: 1.0329 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.3959 - acc: 0.1043 - val_loss: 0.9678 - val_acc: 0.1238\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.7976 - acc: 0.2341 - val_loss: 0.9306 - val_acc: 0.2255\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5857 - acc: 0.1659 - val_loss: 0.9364 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4325 - acc: 0.1185 - val_loss: 0.9280 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 78ms/step - loss: 1.5890 - acc: 0.1678 - val_loss: 0.9645 - val_acc: 0.1892\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4705 - acc: 0.1280 - val_loss: 0.9297 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.9791 - acc: 0.2938 - val_loss: 0.8653 - val_acc: 0.3491\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.7916 - acc: 0.2370 - val_loss: 0.9164 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4883 - acc: 0.1327 - val_loss: 0.9599 - val_acc: 0.2075\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.6764 - acc: 0.1896 - val_loss: 0.9352 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4633 - acc: 0.1280 - val_loss: 1.0308 - val_acc: 0.0566\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5624 - acc: 0.1611 - val_loss: 1.0422 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5428 - acc: 0.1469 - val_loss: 0.9787 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6184 - acc: 0.1706 - val_loss: 0.8975 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.3983 - acc: 0.1043 - val_loss: 0.9802 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3944 - acc: 0.1043 - val_loss: 0.9862 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4005 - acc: 0.1090 - val_loss: 0.9366 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.5289 - acc: 0.1469 - val_loss: 0.9170 - val_acc: 0.1698\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.6868 - acc: 0.2038 - val_loss: 0.9055 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 0.9908 - acc: 0.3981 - val_loss: 0.4311 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4232 - acc: 0.1280 - val_loss: 0.9351 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.4758 - acc: 0.1366 - val_loss: 0.9262 - val_acc: 0.2039\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5439 - acc: 0.1611 - val_loss: 0.9198 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.3687 - acc: 0.1090 - val_loss: 0.9433 - val_acc: 0.0660\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.4368 - acc: 0.1185 - val_loss: 0.8885 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6970 - acc: 0.1848 - val_loss: 0.9012 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 0.9670 - acc: 0.0995 - val_loss: 0.7762 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2402 - acc: 0.0758 - val_loss: 0.9116 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9123 - acc: 0.2417 - val_loss: 0.8148 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.2819 - acc: 0.1137 - val_loss: 0.9145 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4959 - acc: 0.1327 - val_loss: 0.8655 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6851 - acc: 0.1848 - val_loss: 0.8799 - val_acc: 0.1604\n",
      "8/8 [==============================] - 1s 40ms/step - loss: 1.3510 - acc: 0.1137 - val_loss: 0.9014 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3502 - acc: 0.1043 - val_loss: 0.8622 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.7174 - acc: 0.2098 - val_loss: 0.8926 - val_acc: 0.1863\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7977 - acc: 0.2227 - val_loss: 0.9144 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 42ms/step - loss: 1.6886 - acc: 0.1896 - val_loss: 0.9592 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6673 - acc: 0.1848 - val_loss: 0.8925 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2887 - acc: 0.0853 - val_loss: 0.9265 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7422 - acc: 0.2085 - val_loss: 0.8992 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.7490 - acc: 0.2180 - val_loss: 0.9648 - val_acc: 0.1604\n",
      "5/5 [==============================] - 1s 70ms/step - loss: 2.1242 - acc: 0.3333 - val_loss: 0.8618 - val_acc: 0.3226\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2845 - acc: 0.0758 - val_loss: 0.9738 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.0993 - acc: 0.0190 - val_loss: 0.9789 - val_acc: 0.0283\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 1.7026 - acc: 0.1970 - val_loss: 0.9261 - val_acc: 0.1717\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.7655 - acc: 0.2180 - val_loss: 0.9632 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.8133 - acc: 0.2275 - val_loss: 0.8923 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6473 - acc: 0.1801 - val_loss: 0.9719 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7373 - acc: 0.2085 - val_loss: 0.9747 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6035 - acc: 0.1706 - val_loss: 0.9511 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.4522 - acc: 0.1232 - val_loss: 0.9427 - val_acc: 0.1509\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.5322 - acc: 0.1469 - val_loss: 0.8851 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4826 - acc: 0.1327 - val_loss: 0.9415 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6122 - acc: 0.1754 - val_loss: 0.9515 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5366 - acc: 0.1469 - val_loss: 0.9233 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7400 - acc: 0.2133 - val_loss: 0.8969 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7491 - acc: 0.2085 - val_loss: 0.9752 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4554 - acc: 0.1280 - val_loss: 0.9765 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7266 - acc: 0.2038 - val_loss: 0.8875 - val_acc: 0.3019\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.8861 - acc: 0.2654 - val_loss: 0.9539 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.0361 - acc: 0.7867 - val_loss: 0.6211 - val_acc: 0.5849\n",
      "8/8 [==============================] - 1s 41ms/step - loss: 1.4542 - acc: 0.1232 - val_loss: 1.0030 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8489 - acc: 0.2464 - val_loss: 0.9194 - val_acc: 0.2170\n",
      "8/8 [==============================] - 3s 44ms/step - loss: 1.2672 - acc: 0.0664 - val_loss: 0.9898 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2402 - acc: 0.0616 - val_loss: 0.9293 - val_acc: 0.1792\n",
      "6/6 [==============================] - 1s 58ms/step - loss: 1.6732 - acc: 0.1889 - val_loss: 0.9818 - val_acc: 0.1333\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5486 - acc: 0.1564 - val_loss: 0.9227 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.5561 - acc: 0.1517 - val_loss: 0.9230 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5848 - acc: 0.1659 - val_loss: 0.8929 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 0.8022 - acc: 0.4692 - val_loss: 0.7792 - val_acc: 0.3585\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6971 - acc: 0.1991 - val_loss: 0.9373 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6063 - acc: 0.1564 - val_loss: 0.8969 - val_acc: 0.1981\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4406 - acc: 0.1232 - val_loss: 0.9364 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.4124 - acc: 0.1090 - val_loss: 0.9551 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4616 - acc: 0.1280 - val_loss: 0.9511 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6890 - acc: 0.1943 - val_loss: 0.9160 - val_acc: 0.2075\n",
      "5/5 [==============================] - 1s 71ms/step - loss: 1.8044 - acc: 0.2267 - val_loss: 0.8771 - val_acc: 0.1733\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3525 - acc: 0.0948 - val_loss: 0.9474 - val_acc: 0.1038\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4849 - acc: 0.1327 - val_loss: 0.9323 - val_acc: 0.1321\n",
      "7/7 [==============================] - 2s 48ms/step - loss: 1.6218 - acc: 0.1657 - val_loss: 0.8888 - val_acc: 0.1429\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.6187 - acc: 0.1754 - val_loss: 0.8171 - val_acc: 0.3113\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.5558 - acc: 0.1564 - val_loss: 0.9375 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6333 - acc: 0.1706 - val_loss: 0.8952 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5409 - acc: 0.1754 - val_loss: 0.8755 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1864 - acc: 0.0474 - val_loss: 0.9477 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.1210 - acc: 0.0427 - val_loss: 0.9014 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4048 - acc: 0.1185 - val_loss: 0.8296 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.7318 - acc: 0.1943 - val_loss: 0.8642 - val_acc: 0.1698\n",
      "7/7 [==============================] - 2s 50ms/step - loss: 1.6642 - acc: 0.1865 - val_loss: 0.9141 - val_acc: 0.1959\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.8213 - acc: 0.2370 - val_loss: 0.8799 - val_acc: 0.2736\n",
      "7/7 [==============================] - 3s 50ms/step - loss: 1.8977 - acc: 0.2353 - val_loss: 0.8680 - val_acc: 0.2059\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.3489 - acc: 0.0995 - val_loss: 0.9039 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9216 - acc: 0.2701 - val_loss: 0.9337 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 1.5121 - acc: 0.1517 - val_loss: 0.8847 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.2933 - acc: 0.0900 - val_loss: 0.8877 - val_acc: 0.1226\n",
      "7/7 [==============================] - 1s 53ms/step - loss: 1.8276 - acc: 0.2476 - val_loss: 0.8855 - val_acc: 0.2667\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9027 - acc: 0.2559 - val_loss: 0.9426 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.9515 - acc: 0.2654 - val_loss: 0.9281 - val_acc: 0.2264\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.9748 - acc: 0.6667 - val_loss: 0.8397 - val_acc: 0.0000e+00\n",
      "8/8 [==============================] - 3s 204ms/step - loss: 1.9394 - acc: 0.2559 - val_loss: 0.9148 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.6098 - acc: 0.1659 - val_loss: 0.9024 - val_acc: 0.2547\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4532 - acc: 0.1185 - val_loss: 1.0396 - val_acc: 0.1132\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6319 - acc: 0.1754 - val_loss: 0.9654 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2409 - acc: 0.0569 - val_loss: 1.0123 - val_acc: 0.0660\n",
      "7/7 [==============================] - 2s 47ms/step - loss: 1.9490 - acc: 0.2818 - val_loss: 0.8954 - val_acc: 0.3187\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.2625 - acc: 0.0521 - val_loss: 1.0136 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7275 - acc: 0.2038 - val_loss: 0.9236 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3580 - acc: 0.0948 - val_loss: 0.9923 - val_acc: 0.1038\n",
      "8/8 [==============================] - 3s 208ms/step - loss: 1.5424 - acc: 0.1469 - val_loss: 0.9977 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.2479 - acc: 0.0616 - val_loss: 1.0081 - val_acc: 0.0472\n",
      "6/6 [==============================] - 1s 55ms/step - loss: 1.7135 - acc: 0.2056 - val_loss: 0.8791 - val_acc: 0.3111\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7005 - acc: 0.2038 - val_loss: 0.9687 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.6826 - acc: 0.1896 - val_loss: 0.9575 - val_acc: 0.1509\n",
      "4/4 [==============================] - 2s 100ms/step - loss: 1.7307 - acc: 0.2101 - val_loss: 0.8901 - val_acc: 0.2881\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.4680 - acc: 0.1232 - val_loss: 0.9764 - val_acc: 0.1619\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.3396 - acc: 0.0900 - val_loss: 0.9839 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 1.2151 - acc: 0.0521 - val_loss: 1.0058 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.3779 - acc: 0.0995 - val_loss: 1.0211 - val_acc: 0.0755\n",
      "6/6 [==============================] - 2s 62ms/step - loss: 1.6552 - acc: 0.1833 - val_loss: 0.8973 - val_acc: 0.2889\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1712 - acc: 0.0521 - val_loss: 0.9465 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4755 - acc: 0.1280 - val_loss: 0.9834 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7726 - acc: 0.2275 - val_loss: 0.9535 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7622 - acc: 0.2180 - val_loss: 0.9319 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2475 - acc: 0.0616 - val_loss: 0.9521 - val_acc: 0.1698\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 2.0481 - acc: 0.3081 - val_loss: 0.8387 - val_acc: 0.3679\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.8671 - acc: 0.2512 - val_loss: 0.9247 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.4980 - acc: 0.1327 - val_loss: 0.9909 - val_acc: 0.1321\n",
      "3/3 [==============================] - 2s 141ms/step - loss: 1.4179 - acc: 0.1136 - val_loss: 0.9813 - val_acc: 0.0682\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.8355 - acc: 0.2417 - val_loss: 0.8951 - val_acc: 0.2736\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.7875 - acc: 0.2370 - val_loss: 0.9381 - val_acc: 0.2736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.1985 - acc: 0.3704 - val_loss: 0.9631 - val_acc: 0.2308\n",
      "7/7 [==============================] - 2s 51ms/step - loss: 2.0103 - acc: 0.3058 - val_loss: 0.9978 - val_acc: 0.1748\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7608 - acc: 0.2227 - val_loss: 0.9399 - val_acc: 0.2642\n",
      "6/6 [==============================] - 1s 63ms/step - loss: 1.5181 - acc: 0.1389 - val_loss: 0.9923 - val_acc: 0.1778\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.5046 - acc: 0.1327 - val_loss: 0.8959 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7183 - acc: 0.2085 - val_loss: 0.9433 - val_acc: 0.2830\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6841 - acc: 0.1943 - val_loss: 0.9367 - val_acc: 0.2358\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 1.3498 - acc: 0.0758 - val_loss: 1.0601 - val_acc: 0.0849\n",
      "5/5 [==============================] - 2s 84ms/step - loss: 1.8636 - acc: 0.2752 - val_loss: 0.9418 - val_acc: 0.2703\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6939 - acc: 0.2038 - val_loss: 0.9786 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.6783 - acc: 0.1991 - val_loss: 0.9886 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.5593 - acc: 0.1564 - val_loss: 1.0188 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.4298 - acc: 0.0995 - val_loss: 1.0122 - val_acc: 0.1792\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.5435 - acc: 0.1517 - val_loss: 1.0374 - val_acc: 0.0943\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 1.6700 - acc: 0.1896 - val_loss: 0.9966 - val_acc: 0.2075\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6149 - acc: 0.1754 - val_loss: 0.9691 - val_acc: 0.2358\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 1.5010 - acc: 0.1374 - val_loss: 1.0125 - val_acc: 0.1415\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7283 - acc: 0.2180 - val_loss: 1.0136 - val_acc: 0.1604\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6195 - acc: 0.1754 - val_loss: 1.0578 - val_acc: 0.1226\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.1089 - acc: 0.0047 - val_loss: 1.0735 - val_acc: 0.0283\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.7104 - acc: 0.2038 - val_loss: 0.9526 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.7874 - acc: 0.2275 - val_loss: 0.9238 - val_acc: 0.2264\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.2814 - acc: 0.0664 - val_loss: 1.0291 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.6717 - acc: 0.1943 - val_loss: 0.9694 - val_acc: 0.1887\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.5477 - acc: 0.1517 - val_loss: 0.9784 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.4326 - acc: 0.1137 - val_loss: 1.0107 - val_acc: 0.0943\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.2309 - acc: 0.0521 - val_loss: 1.0394 - val_acc: 0.0377\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 1.5826 - acc: 0.1644 - val_loss: 0.9387 - val_acc: 0.1918\n",
      "5/5 [==============================] - 2s 71ms/step - loss: 2.0461 - acc: 0.3243 - val_loss: 0.8312 - val_acc: 0.3919\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.6060 - acc: 0.1706 - val_loss: 0.9748 - val_acc: 0.1321\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.4896 - acc: 0.1327 - val_loss: 0.9339 - val_acc: 0.2075\n",
      "5/5 [==============================] - 2s 71ms/step - loss: 1.5188 - acc: 0.1409 - val_loss: 0.9907 - val_acc: 0.1351\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6895 - acc: 0.1991 - val_loss: 0.9117 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 1.2479 - acc: 0.0569 - val_loss: 1.0233 - val_acc: 0.0472\n",
      "8/8 [==============================] - 2s 40ms/step - loss: 1.3844 - acc: 0.1422 - val_loss: 0.8941 - val_acc: 0.1415\n",
      "8/8 [==============================] - 3s 41ms/step - loss: 1.4410 - acc: 0.1232 - val_loss: 0.9247 - val_acc: 0.1509\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.6264 - acc: 0.1754 - val_loss: 0.9049 - val_acc: 0.2170\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.1645 - acc: 0.0427 - val_loss: 0.9638 - val_acc: 0.0755\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5060 - acc: 0.1422 - val_loss: 0.9649 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 42ms/step - loss: 1.9850 - acc: 0.2749 - val_loss: 0.8985 - val_acc: 0.2453\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 1.5656 - acc: 0.1564 - val_loss: 0.9210 - val_acc: 0.2075\n",
      "3/3 [==============================] - 2s 142ms/step - loss: 1.6775 - acc: 0.1954 - val_loss: 0.9622 - val_acc: 0.1818\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8814 - acc: 0.2559 - val_loss: 0.9512 - val_acc: 0.1981\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8482 - acc: 0.2512 - val_loss: 0.9660 - val_acc: 0.1887\n",
      "8/8 [==============================] - 3s 43ms/step - loss: 1.5112 - acc: 0.1374 - val_loss: 0.9938 - val_acc: 0.0849\n",
      "8/8 [==============================] - 2s 41ms/step - loss: 1.8334 - acc: 0.2464 - val_loss: 0.9343 - val_acc: 0.2358\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 1.9547 - acc: 0.2891 - val_loss: 0.8859 - val_acc: 0.3113\n",
      "8/8 [==============================] - 2s 43ms/step - loss: 1.7095 - acc: 0.2085 - val_loss: 0.9910 - val_acc: 0.1887\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "\n",
    "log_df = pd.DataFrame(columns=['loss', 'acc', 'val_loss', 'val_acc'])   # 학습 진행과정을 담을 데이터프레임\n",
    "epoch = 5       # 총 실행횟수\n",
    "isFirst = True\n",
    "\n",
    "x_valid = list()\n",
    "y_valid = list()\n",
    "\n",
    "for i in range(epoch):\n",
    "  for x, y in train_data:           # 각 에포크마다 train_data안의 모든 기업에 대해 1회 학습을 진행함\n",
    "    x_train = x[:int(len(x)*0.5)]   # train : test = 50 : 25, valid는 미리 위에서 따로 저장\n",
    "    x_test = x[int(len(x)*0.5):int(len(x)*0.75)]\n",
    "    y_train = y[:int(len(y)*0.5)]\n",
    "    y_test = y[int(len(y)*0.5):int(len(y)*0.75)]\n",
    "\n",
    "    K.clear_session()\n",
    "    model = Sequential()                        # 모델은 20개의 output을 가지는 LSTM 위에 hidden layer로 2개의 dense층, 마지막으로 하나의 output을 가짐\n",
    "    model.add(LSTM(20, input_shape=(10, 20)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))   # 마지막 층은 classification값을 내놓아야 하므로 sigmoid를 활성함수로 지정\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) # optimizer로 adam활용, loss는 엔트로피 활용\n",
    "    if not isFirst:\n",
    "      model.load_weights('v2')                  # 모델의 가중치를 학습마다 저장 후 불러와 연속적인 학습을 가능하게 함\n",
    "    else:\n",
    "      isFirst = False\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=1, batch_size=30, verbose=1, validation_data = (x_test, y_test), class_weight={True:10, False:1})\n",
    "    log_df = pd.concat([log_df, pd.DataFrame(history.history)])  # class weight을 강하게 적용시켜 상대적으로 적은 인스턴스를 가진 true를 강화\n",
    "    model.save_weights('v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cd6cd163a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(221)                                        # 전체 학습에 대한 히스토리를 그래프로 그림\n",
    "plt.plot(list(log_df['loss']), label='loss')            # x축은 학습 진행정도, y축은 각각의 label\n",
    "plt.plot(list(log_df['val_loss']), label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(list(log_df['acc']), label='acc')\n",
    "plt.plot(list(log_df['val_acc']), label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "log_df.to_csv(\"lstm2/log_for_test.csv\", index=False)\n",
    "\n",
    "l1 = log_df[:299]       # 한 에포크는 299개 기업을 대상으로 돌아가므로 299개 실행마다 분할함\n",
    "l2 = log_df[299:598]\n",
    "l3 = log_df[598:897]\n",
    "l4 = log_df[897:1196]\n",
    "l5 = log_df[1196:]\n",
    "\n",
    "plt.subplot(223)                # 에포크 당 평균적인 지표를 환산하여 그래프로 나타냄\n",
    "plt.plot([1,2,3,4,5], [l1['loss'].mean(), l2['loss'].mean(), l3['loss'].mean(), l4['loss'].mean(), l5['loss'].mean()], label='loss')\n",
    "plt.plot([1,2,3,4,5], [l1['val_loss'].mean(), l2['val_loss'].mean(), l3['val_loss'].mean(), l4['val_loss'].mean(), l5['val_loss'].mean()], label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot([1,2,3,4,5], [l1['acc'].mean(), l2['acc'].mean(), l3['acc'].mean(), l4['acc'].mean(), l5['acc'].mean()], label='acc')\n",
    "plt.plot([1,2,3,4,5], [l1['val_acc'].mean(), l2['val_acc'].mean(), l3['val_acc'].mean(), l4['val_acc'].mean(), l5['val_acc'].mean()], label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3-2-2 LSTM for lower weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 53ms/step - loss: 0.2113 - acc: 0.8104 - val_loss: 0.6629 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2095 - acc: 0.8152 - val_loss: 0.6199 - val_acc: 0.8774\n",
      "8/8 [==============================] - 4s 50ms/step - loss: 0.1978 - acc: 0.8483 - val_loss: 0.5956 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2795 - acc: 0.6967 - val_loss: 0.6498 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2133 - acc: 0.7962 - val_loss: 0.6369 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1964 - acc: 0.8341 - val_loss: 0.6487 - val_acc: 0.7547\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2034 - acc: 0.8294 - val_loss: 0.5808 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1779 - acc: 0.8815 - val_loss: 0.5944 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2266 - acc: 0.8009 - val_loss: 0.6105 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2608 - acc: 0.7488 - val_loss: 0.6325 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1678 - acc: 0.9100 - val_loss: 0.6141 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2178 - acc: 0.8152 - val_loss: 0.6036 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1671 - acc: 0.9005 - val_loss: 0.5471 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2013 - acc: 0.8436 - val_loss: 0.5913 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2522 - acc: 0.7678 - val_loss: 0.6166 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2798 - acc: 0.7109 - val_loss: 0.6569 - val_acc: 0.6887\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2211 - acc: 0.8057 - val_loss: 0.6392 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1601 - acc: 0.9242 - val_loss: 0.6118 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2420 - acc: 0.7630 - val_loss: 0.6620 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 53ms/step - loss: 0.1515 - acc: 0.9431 - val_loss: 0.6061 - val_acc: 0.9340\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1608 - acc: 0.9147 - val_loss: 0.5615 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2674 - acc: 0.7299 - val_loss: 0.6257 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 62ms/step - loss: 0.1675 - acc: 0.9024 - val_loss: 0.5880 - val_acc: 0.8544\n",
      "7/7 [==============================] - 2s 59ms/step - loss: 0.2477 - acc: 0.7659 - val_loss: 0.6137 - val_acc: 0.7745\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1947 - acc: 0.8531 - val_loss: 0.5858 - val_acc: 0.9057\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1833 - acc: 0.8720 - val_loss: 0.5667 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2242 - acc: 0.8057 - val_loss: 0.6013 - val_acc: 0.8208\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.2128 - acc: 0.8250 - val_loss: 0.6069 - val_acc: 0.8100\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1345 - acc: 0.9573 - val_loss: 0.5635 - val_acc: 0.9151\n",
      "8/8 [==============================] - 3s 146ms/step - loss: 0.1770 - acc: 0.8815 - val_loss: 0.5028 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2240 - acc: 0.8152 - val_loss: 0.5642 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1861 - acc: 0.8673 - val_loss: 0.5816 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2613 - acc: 0.7536 - val_loss: 0.6239 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2295 - acc: 0.7915 - val_loss: 0.6423 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1477 - acc: 0.9479 - val_loss: 0.6121 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1439 - acc: 0.9526 - val_loss: 0.5993 - val_acc: 0.9245\n",
      "3/3 [==============================] - 2s 157ms/step - loss: 0.1744 - acc: 0.8864 - val_loss: 0.5618 - val_acc: 0.9318\n",
      "6/6 [==============================] - 2s 66ms/step - loss: 0.2161 - acc: 0.8111 - val_loss: 0.5983 - val_acc: 0.8667\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1966 - acc: 0.8531 - val_loss: 0.6044 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1874 - acc: 0.8673 - val_loss: 0.5856 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2075 - acc: 0.8341 - val_loss: 0.6163 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2449 - acc: 0.7725 - val_loss: 0.6065 - val_acc: 0.7736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3311 - acc: 0.6296 - val_loss: 0.6063 - val_acc: 0.7692\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1814 - acc: 0.8768 - val_loss: 0.5614 - val_acc: 0.8962\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 0.2578 - acc: 0.7571 - val_loss: 0.6323 - val_acc: 0.7333\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2837 - acc: 0.6919 - val_loss: 0.6713 - val_acc: 0.6321\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1785 - acc: 0.8910 - val_loss: 0.6164 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1446 - acc: 0.9573 - val_loss: 0.5993 - val_acc: 0.9623\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2020 - acc: 0.8436 - val_loss: 0.6019 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1433 - acc: 0.9479 - val_loss: 0.5881 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1678 - acc: 0.8957 - val_loss: 0.5251 - val_acc: 0.9245\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2085 - acc: 0.8322 - val_loss: 0.5855 - val_acc: 0.8108\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2306 - acc: 0.7962 - val_loss: 0.5930 - val_acc: 0.7736\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1213 - acc: 0.9621 - val_loss: 0.4788 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1734 - acc: 0.8910 - val_loss: 0.4573 - val_acc: 0.9717\n",
      "5/5 [==============================] - 2s 85ms/step - loss: 0.3270 - acc: 0.6667 - val_loss: 0.6516 - val_acc: 0.6774\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1605 - acc: 0.9100 - val_loss: 0.5915 - val_acc: 0.8585\n",
      "8/8 [==============================] - 3s 143ms/step - loss: 0.2406 - acc: 0.7773 - val_loss: 0.6131 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1417 - acc: 0.9479 - val_loss: 0.5656 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2480 - acc: 0.7583 - val_loss: 0.6210 - val_acc: 0.7736\n",
      "6/6 [==============================] - 2s 65ms/step - loss: 0.1628 - acc: 0.9097 - val_loss: 0.5919 - val_acc: 0.8590\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2341 - acc: 0.7820 - val_loss: 0.6058 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1782 - acc: 0.8768 - val_loss: 0.5541 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1937 - acc: 0.8578 - val_loss: 0.5529 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2092 - acc: 0.8294 - val_loss: 0.5393 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1843 - acc: 0.8720 - val_loss: 0.5703 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2651 - acc: 0.7488 - val_loss: 0.6013 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 51ms/step - loss: 0.2095 - acc: 0.8246 - val_loss: 0.6401 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2005 - acc: 0.8436 - val_loss: 0.6309 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1712 - acc: 0.8957 - val_loss: 0.5817 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1956 - acc: 0.8436 - val_loss: 0.6347 - val_acc: 0.7830\n",
      "7/7 [==============================] - 3s 163ms/step - loss: 0.2203 - acc: 0.8122 - val_loss: 0.6364 - val_acc: 0.7582\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1496 - acc: 0.9289 - val_loss: 0.5473 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2533 - acc: 0.7583 - val_loss: 0.6166 - val_acc: 0.7358\n",
      "5/5 [==============================] - 2s 89ms/step - loss: 0.1936 - acc: 0.8591 - val_loss: 0.5847 - val_acc: 0.8649\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1403 - acc: 0.9431 - val_loss: 0.5757 - val_acc: 0.9245\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1850 - acc: 0.8720 - val_loss: 0.5963 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 54ms/step - loss: 0.1688 - acc: 0.8910 - val_loss: 0.5656 - val_acc: 0.9434\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.1813 - acc: 0.8732 - val_loss: 0.6033 - val_acc: 0.7864\n",
      "5/5 [==============================] - 2s 83ms/step - loss: 0.1589 - acc: 0.9133 - val_loss: 0.5426 - val_acc: 0.9733\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1972 - acc: 0.8531 - val_loss: 0.5491 - val_acc: 0.9151\n",
      "7/7 [==============================] - 3s 55ms/step - loss: 0.2909 - acc: 0.6942 - val_loss: 0.6254 - val_acc: 0.8252\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2437 - acc: 0.7630 - val_loss: 0.6628 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2138 - acc: 0.8152 - val_loss: 0.6453 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2091 - acc: 0.8294 - val_loss: 0.6641 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.1932 - acc: 0.8626 - val_loss: 0.6588 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2218 - acc: 0.8009 - val_loss: 0.6718 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2382 - acc: 0.7630 - val_loss: 0.6851 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2517 - acc: 0.7346 - val_loss: 0.6790 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2243 - acc: 0.7915 - val_loss: 0.6639 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2112 - acc: 0.8199 - val_loss: 0.6771 - val_acc: 0.7547\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.1591 - acc: 0.9384 - val_loss: 0.6636 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2539 - acc: 0.7299 - val_loss: 0.6761 - val_acc: 0.8019\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2545 - acc: 0.7248 - val_loss: 0.6835 - val_acc: 0.7297\n",
      "5/5 [==============================] - 2s 79ms/step - loss: 0.2326 - acc: 0.7733 - val_loss: 0.6642 - val_acc: 0.8267\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3376 - acc: 0.5417 - val_loss: 0.6809 - val_acc: 0.7500\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1839 - acc: 0.8768 - val_loss: 0.6529 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2260 - acc: 0.7962 - val_loss: 0.6375 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2578 - acc: 0.7251 - val_loss: 0.6807 - val_acc: 0.6604\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1820 - acc: 0.8436 - val_loss: 0.6690 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1853 - acc: 0.8483 - val_loss: 0.6698 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2269 - acc: 0.7725 - val_loss: 0.6801 - val_acc: 0.7075\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2762 - acc: 0.6588 - val_loss: 0.6879 - val_acc: 0.6509\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1987 - acc: 0.6635 - val_loss: 0.6892 - val_acc: 0.5377\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2008 - acc: 0.6351 - val_loss: 0.6740 - val_acc: 0.8868\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1581 - acc: 0.9384 - val_loss: 0.6553 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1908 - acc: 0.8673 - val_loss: 0.6602 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1884 - acc: 0.8720 - val_loss: 0.6568 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2028 - acc: 0.8389 - val_loss: 0.6349 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1674 - acc: 0.9005 - val_loss: 0.6054 - val_acc: 0.9143\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1991 - acc: 0.8483 - val_loss: 0.6276 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1795 - acc: 0.8815 - val_loss: 0.6044 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1979 - acc: 0.8483 - val_loss: 0.6041 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2885 - acc: 0.6777 - val_loss: 0.6558 - val_acc: 0.7170\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2069 - acc: 0.8341 - val_loss: 0.6676 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1910 - acc: 0.8673 - val_loss: 0.6563 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1903 - acc: 0.8673 - val_loss: 0.6319 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2204 - acc: 0.8057 - val_loss: 0.6544 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2116 - acc: 0.8199 - val_loss: 0.6157 - val_acc: 0.8585\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1524 - acc: 0.9336 - val_loss: 0.6215 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1426 - acc: 0.9526 - val_loss: 0.5700 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2400 - acc: 0.7725 - val_loss: 0.6516 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1299 - acc: 0.9573 - val_loss: 0.4989 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2131 - acc: 0.8246 - val_loss: 0.6005 - val_acc: 0.7925\n",
      "7/7 [==============================] - 3s 54ms/step - loss: 0.2154 - acc: 0.8398 - val_loss: 0.5446 - val_acc: 0.8571\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1949 - acc: 0.8531 - val_loss: 0.5602 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1734 - acc: 0.8863 - val_loss: 0.5659 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2392 - acc: 0.7867 - val_loss: 0.5731 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1667 - acc: 0.8957 - val_loss: 0.5687 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1032 - acc: 0.9810 - val_loss: 0.4984 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2386 - acc: 0.7820 - val_loss: 0.5491 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.0847 - acc: 0.9953 - val_loss: 0.4588 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1966 - acc: 0.8531 - val_loss: 0.5496 - val_acc: 0.8113\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.1843 - acc: 0.8768 - val_loss: 0.5006 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1705 - acc: 0.8957 - val_loss: 0.4827 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1434 - acc: 0.9242 - val_loss: 0.4224 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1432 - acc: 0.9242 - val_loss: 0.4443 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2412 - acc: 0.8104 - val_loss: 0.5613 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1843 - acc: 0.8720 - val_loss: 0.5382 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1972 - acc: 0.8531 - val_loss: 0.5546 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2095 - acc: 0.8294 - val_loss: 0.6207 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2514 - acc: 0.7583 - val_loss: 0.6267 - val_acc: 0.7264\n",
      "8/8 [==============================] - 3s 144ms/step - loss: 0.1827 - acc: 0.8768 - val_loss: 0.5705 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2226 - acc: 0.8104 - val_loss: 0.6189 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2096 - acc: 0.8246 - val_loss: 0.6191 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2266 - acc: 0.8009 - val_loss: 0.6273 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1742 - acc: 0.8910 - val_loss: 0.5890 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.2601 - acc: 0.7346 - val_loss: 0.6429 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1531 - acc: 0.9336 - val_loss: 0.5745 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1999 - acc: 0.8483 - val_loss: 0.6157 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2184 - acc: 0.8104 - val_loss: 0.6086 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1777 - acc: 0.8863 - val_loss: 0.6000 - val_acc: 0.9245\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.2102 - acc: 0.8294 - val_loss: 0.5769 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2060 - acc: 0.8389 - val_loss: 0.5996 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1665 - acc: 0.9005 - val_loss: 0.5801 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1826 - acc: 0.8720 - val_loss: 0.5307 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2188 - acc: 0.8246 - val_loss: 0.5379 - val_acc: 0.8396\n",
      "7/7 [==============================] - 3s 55ms/step - loss: 0.2856 - acc: 0.7182 - val_loss: 0.6368 - val_acc: 0.6813\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2174 - acc: 0.8152 - val_loss: 0.6008 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1968 - acc: 0.8483 - val_loss: 0.6238 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2086 - acc: 0.8294 - val_loss: 0.6291 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1331 - acc: 0.9573 - val_loss: 0.5592 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 107ms/step - loss: 0.2341 - acc: 0.7899 - val_loss: 0.6247 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2693 - acc: 0.7393 - val_loss: 0.5939 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1276 - acc: 0.9479 - val_loss: 0.4323 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1945 - acc: 0.8531 - val_loss: 0.5347 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2536 - acc: 0.7536 - val_loss: 0.6300 - val_acc: 0.7642\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2324 - acc: 0.7867 - val_loss: 0.6440 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2017 - acc: 0.8389 - val_loss: 0.5928 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2279 - acc: 0.7962 - val_loss: 0.6633 - val_acc: 0.6981\n",
      "7/7 [==============================] - 2s 59ms/step - loss: 0.2273 - acc: 0.7902 - val_loss: 0.6540 - val_acc: 0.8137\n",
      "7/7 [==============================] - 2s 57ms/step - loss: 0.1693 - acc: 0.9045 - val_loss: 0.6377 - val_acc: 0.9400\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2078 - acc: 0.8294 - val_loss: 0.6347 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1869 - acc: 0.8673 - val_loss: 0.6585 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2233 - acc: 0.8009 - val_loss: 0.5994 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1770 - acc: 0.8910 - val_loss: 0.6344 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2159 - acc: 0.8199 - val_loss: 0.6228 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2576 - acc: 0.7441 - val_loss: 0.6425 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.2267 - acc: 0.7962 - val_loss: 0.6472 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2009 - acc: 0.8436 - val_loss: 0.6397 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1984 - acc: 0.8483 - val_loss: 0.6439 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1576 - acc: 0.9194 - val_loss: 0.5927 - val_acc: 0.9151\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1569 - acc: 0.9242 - val_loss: 0.5674 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 110ms/step - loss: 0.2892 - acc: 0.6975 - val_loss: 0.6387 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2146 - acc: 0.8152 - val_loss: 0.6394 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1471 - acc: 0.9384 - val_loss: 0.6212 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1438 - acc: 0.9431 - val_loss: 0.5773 - val_acc: 0.9528\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1629 - acc: 0.9052 - val_loss: 0.5241 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2006 - acc: 0.8436 - val_loss: 0.5935 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1988 - acc: 0.8483 - val_loss: 0.5793 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2079 - acc: 0.8341 - val_loss: 0.5861 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2174 - acc: 0.8246 - val_loss: 0.5780 - val_acc: 0.8113\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1258 - acc: 0.9526 - val_loss: 0.5134 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2271 - acc: 0.8009 - val_loss: 0.6049 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2224 - acc: 0.8104 - val_loss: 0.6165 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.2221 - acc: 0.8030 - val_loss: 0.5887 - val_acc: 0.8283\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5943 - acc: 0.3333 - val_loss: 0.4970 - val_acc: 1.0000\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2323 - acc: 0.7820 - val_loss: 0.6329 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2393 - acc: 0.7725 - val_loss: 0.6347 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2293 - acc: 0.7915 - val_loss: 0.6453 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1664 - acc: 0.9100 - val_loss: 0.6142 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1841 - acc: 0.8815 - val_loss: 0.6279 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1711 - acc: 0.8957 - val_loss: 0.5852 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2260 - acc: 0.8057 - val_loss: 0.6105 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1757 - acc: 0.8910 - val_loss: 0.6081 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1206 - acc: 0.9810 - val_loss: 0.5314 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1377 - acc: 0.9431 - val_loss: 0.4954 - val_acc: 0.9528\n",
      "6/6 [==============================] - 3s 68ms/step - loss: 0.3473 - acc: 0.6341 - val_loss: 0.6865 - val_acc: 0.5976\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2378 - acc: 0.7915 - val_loss: 0.6104 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1660 - acc: 0.9005 - val_loss: 0.6033 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2547 - acc: 0.7441 - val_loss: 0.6243 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 53ms/step - loss: 0.2306 - acc: 0.7915 - val_loss: 0.5871 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.2466 - acc: 0.7725 - val_loss: 0.6116 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2247 - acc: 0.7962 - val_loss: 0.6421 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2515 - acc: 0.7441 - val_loss: 0.6456 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1668 - acc: 0.9052 - val_loss: 0.5938 - val_acc: 0.8962\n",
      "6/6 [==============================] - 2s 65ms/step - loss: 0.2268 - acc: 0.7944 - val_loss: 0.6580 - val_acc: 0.6889\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1224 - acc: 0.9810 - val_loss: 0.4991 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2601 - acc: 0.7299 - val_loss: 0.6390 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2367 - acc: 0.7773 - val_loss: 0.6361 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1460 - acc: 0.9384 - val_loss: 0.5351 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2408 - acc: 0.7773 - val_loss: 0.6209 - val_acc: 0.7830\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1906 - acc: 0.8626 - val_loss: 0.5670 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2403 - acc: 0.7692 - val_loss: 0.6265 - val_acc: 0.7885\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2381 - acc: 0.7725 - val_loss: 0.6341 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2195 - acc: 0.8057 - val_loss: 0.6153 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2085 - acc: 0.8294 - val_loss: 0.6714 - val_acc: 0.6887\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2350 - acc: 0.7725 - val_loss: 0.6545 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1238 - acc: 0.9810 - val_loss: 0.5365 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1912 - acc: 0.8626 - val_loss: 0.6127 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2702 - acc: 0.7062 - val_loss: 0.6791 - val_acc: 0.6509\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1925 - acc: 0.8578 - val_loss: 0.6313 - val_acc: 0.9151\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2311 - acc: 0.7820 - val_loss: 0.6643 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1841 - acc: 0.8815 - val_loss: 0.6481 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1781 - acc: 0.8910 - val_loss: 0.6496 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1694 - acc: 0.9100 - val_loss: 0.6306 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2173 - acc: 0.8104 - val_loss: 0.6418 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1898 - acc: 0.8673 - val_loss: 0.6440 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2341 - acc: 0.7773 - val_loss: 0.6612 - val_acc: 0.6981\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2225 - acc: 0.8009 - val_loss: 0.6377 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1694 - acc: 0.9052 - val_loss: 0.6071 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2189 - acc: 0.8104 - val_loss: 0.6230 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 58ms/step - loss: 0.2169 - acc: 0.8135 - val_loss: 0.6206 - val_acc: 0.8041\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1847 - acc: 0.8720 - val_loss: 0.6003 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2229 - acc: 0.8057 - val_loss: 0.6235 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2112 - acc: 0.8246 - val_loss: 0.6080 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 157ms/step - loss: 0.1604 - acc: 0.9139 - val_loss: 0.5532 - val_acc: 0.9423\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1796 - acc: 0.8815 - val_loss: 0.5733 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2642 - acc: 0.7393 - val_loss: 0.5901 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2668 - acc: 0.7251 - val_loss: 0.6438 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2082 - acc: 0.8294 - val_loss: 0.6673 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1814 - acc: 0.8863 - val_loss: 0.6435 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1926 - acc: 0.8626 - val_loss: 0.6404 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2240 - acc: 0.7962 - val_loss: 0.6488 - val_acc: 0.8208\n",
      "5/5 [==============================] - 2s 76ms/step - loss: 0.2822 - acc: 0.6757 - val_loss: 0.6827 - val_acc: 0.6081\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2241 - acc: 0.7962 - val_loss: 0.6620 - val_acc: 0.8208\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1374 - acc: 0.9810 - val_loss: 0.6392 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1763 - acc: 0.8957 - val_loss: 0.6293 - val_acc: 0.8762\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1794 - acc: 0.8863 - val_loss: 0.6303 - val_acc: 0.8491\n",
      "5/5 [==============================] - 2s 78ms/step - loss: 0.2770 - acc: 0.6959 - val_loss: 0.6493 - val_acc: 0.7568\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2281 - acc: 0.7915 - val_loss: 0.6452 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2130 - acc: 0.8199 - val_loss: 0.6403 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1673 - acc: 0.9100 - val_loss: 0.5845 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2052 - acc: 0.8341 - val_loss: 0.6352 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2137 - acc: 0.8199 - val_loss: 0.6337 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1639 - acc: 0.9147 - val_loss: 0.5796 - val_acc: 0.9528\n",
      "5/5 [==============================] - 3s 81ms/step - loss: 0.2054 - acc: 0.8356 - val_loss: 0.6232 - val_acc: 0.8082\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2025 - acc: 0.8389 - val_loss: 0.5962 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2207 - acc: 0.8057 - val_loss: 0.6373 - val_acc: 0.7925\n",
      "6/6 [==============================] - 2s 64ms/step - loss: 0.2155 - acc: 0.8167 - val_loss: 0.6626 - val_acc: 0.7111\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2322 - acc: 0.7820 - val_loss: 0.6447 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2342 - acc: 0.7773 - val_loss: 0.6459 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1872 - acc: 0.8720 - val_loss: 0.6336 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2338 - acc: 0.7773 - val_loss: 0.6605 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2566 - acc: 0.7299 - val_loss: 0.6637 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2324 - acc: 0.7773 - val_loss: 0.6822 - val_acc: 0.7358\n",
      "4/4 [==============================] - 3s 104ms/step - loss: 0.2119 - acc: 0.8235 - val_loss: 0.6834 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1880 - acc: 0.8768 - val_loss: 0.6620 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2346 - acc: 0.7725 - val_loss: 0.6777 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1882 - acc: 0.8768 - val_loss: 0.6748 - val_acc: 0.8381\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2088 - acc: 0.8294 - val_loss: 0.6649 - val_acc: 0.8585\n",
      "7/7 [==============================] - 3s 58ms/step - loss: 0.2378 - acc: 0.7647 - val_loss: 0.6712 - val_acc: 0.7941\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2175 - acc: 0.8104 - val_loss: 0.6591 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2812 - acc: 0.6682 - val_loss: 0.6841 - val_acc: 0.6604\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2256 - acc: 0.7915 - val_loss: 0.6744 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2277 - acc: 0.7867 - val_loss: 0.6821 - val_acc: 0.7736\n",
      "7/7 [==============================] - 3s 56ms/step - loss: 0.1819 - acc: 0.8895 - val_loss: 0.6692 - val_acc: 0.8352\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2090 - acc: 0.8294 - val_loss: 0.6701 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2256 - acc: 0.7915 - val_loss: 0.6854 - val_acc: 0.7170\n",
      "6/6 [==============================] - 2s 64ms/step - loss: 0.1965 - acc: 0.8611 - val_loss: 0.6803 - val_acc: 0.8222\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1861 - acc: 0.8815 - val_loss: 0.6784 - val_acc: 0.8868\n",
      "5/5 [==============================] - 3s 83ms/step - loss: 0.2235 - acc: 0.7973 - val_loss: 0.6793 - val_acc: 0.7297\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2029 - acc: 0.8436 - val_loss: 0.6774 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1547 - acc: 0.9479 - val_loss: 0.6733 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1760 - acc: 0.9005 - val_loss: 0.6512 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1820 - acc: 0.8815 - val_loss: 0.6128 - val_acc: 0.9434\n",
      "3/3 [==============================] - 2s 159ms/step - loss: 0.2204 - acc: 0.8046 - val_loss: 0.6480 - val_acc: 0.8182\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2289 - acc: 0.7915 - val_loss: 0.6292 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2203 - acc: 0.8057 - val_loss: 0.6594 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2101 - acc: 0.8246 - val_loss: 0.6615 - val_acc: 0.7830\n",
      "8/8 [==============================] - 3s 153ms/step - loss: 0.2110 - acc: 0.8246 - val_loss: 0.6705 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1996 - acc: 0.8483 - val_loss: 0.6541 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2688 - acc: 0.6967 - val_loss: 0.6681 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 53ms/step - loss: 0.2132 - acc: 0.8199 - val_loss: 0.6637 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1993 - acc: 0.8483 - val_loss: 0.6781 - val_acc: 0.7642\n",
      "8/8 [==============================] - 3s 152ms/step - loss: 0.2049 - acc: 0.8294 - val_loss: 0.6661 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.1836 - acc: 0.8768 - val_loss: 0.6610 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2226 - acc: 0.8009 - val_loss: 0.6633 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2467 - acc: 0.7488 - val_loss: 0.6734 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1721 - acc: 0.9100 - val_loss: 0.6534 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2147 - acc: 0.8152 - val_loss: 0.6468 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1709 - acc: 0.9005 - val_loss: 0.5372 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2015 - acc: 0.8436 - val_loss: 0.6306 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2427 - acc: 0.7678 - val_loss: 0.6544 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2661 - acc: 0.7109 - val_loss: 0.6701 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2181 - acc: 0.8057 - val_loss: 0.6566 - val_acc: 0.8208\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1641 - acc: 0.9194 - val_loss: 0.6439 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2399 - acc: 0.7630 - val_loss: 0.6739 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1476 - acc: 0.9431 - val_loss: 0.6006 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1583 - acc: 0.9147 - val_loss: 0.5630 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2641 - acc: 0.7299 - val_loss: 0.6530 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 59ms/step - loss: 0.1777 - acc: 0.9024 - val_loss: 0.6411 - val_acc: 0.8544\n",
      "7/7 [==============================] - 2s 58ms/step - loss: 0.2539 - acc: 0.7659 - val_loss: 0.6547 - val_acc: 0.7745\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1961 - acc: 0.8531 - val_loss: 0.6255 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1818 - acc: 0.8720 - val_loss: 0.6046 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2234 - acc: 0.8057 - val_loss: 0.5928 - val_acc: 0.8208\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2171 - acc: 0.8250 - val_loss: 0.6176 - val_acc: 0.8100\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1365 - acc: 0.9573 - val_loss: 0.5815 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1819 - acc: 0.8815 - val_loss: 0.5352 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2179 - acc: 0.8152 - val_loss: 0.6162 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1882 - acc: 0.8673 - val_loss: 0.6137 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2531 - acc: 0.7536 - val_loss: 0.6174 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2278 - acc: 0.7915 - val_loss: 0.6552 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1372 - acc: 0.9479 - val_loss: 0.5542 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1301 - acc: 0.9526 - val_loss: 0.5150 - val_acc: 0.9245\n",
      "3/3 [==============================] - 2s 157ms/step - loss: 0.1656 - acc: 0.8864 - val_loss: 0.4370 - val_acc: 0.9318\n",
      "6/6 [==============================] - 2s 63ms/step - loss: 0.2209 - acc: 0.8111 - val_loss: 0.5354 - val_acc: 0.8667\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2016 - acc: 0.8531 - val_loss: 0.6076 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 54ms/step - loss: 0.1897 - acc: 0.8673 - val_loss: 0.5978 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2074 - acc: 0.8341 - val_loss: 0.6047 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2481 - acc: 0.7725 - val_loss: 0.6097 - val_acc: 0.7736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3546 - acc: 0.6296 - val_loss: 0.6596 - val_acc: 0.7692\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1843 - acc: 0.8768 - val_loss: 0.6317 - val_acc: 0.8962\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2458 - acc: 0.7571 - val_loss: 0.6586 - val_acc: 0.7333\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2778 - acc: 0.6919 - val_loss: 0.6793 - val_acc: 0.6321\n",
      "8/8 [==============================] - 3s 147ms/step - loss: 0.1780 - acc: 0.8910 - val_loss: 0.6306 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1350 - acc: 0.9573 - val_loss: 0.3791 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2030 - acc: 0.8436 - val_loss: 0.5944 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1437 - acc: 0.9479 - val_loss: 0.5989 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1667 - acc: 0.8957 - val_loss: 0.5208 - val_acc: 0.9245\n",
      "5/5 [==============================] - 3s 265ms/step - loss: 0.2078 - acc: 0.8322 - val_loss: 0.5821 - val_acc: 0.8108\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2353 - acc: 0.7962 - val_loss: 0.6150 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1205 - acc: 0.9621 - val_loss: 0.5213 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1738 - acc: 0.8910 - val_loss: 0.4999 - val_acc: 0.9717\n",
      "5/5 [==============================] - 2s 80ms/step - loss: 0.3038 - acc: 0.6667 - val_loss: 0.6576 - val_acc: 0.6774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1599 - acc: 0.9100 - val_loss: 0.6001 - val_acc: 0.8585\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2446 - acc: 0.7773 - val_loss: 0.5948 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1377 - acc: 0.9479 - val_loss: 0.5381 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2456 - acc: 0.7583 - val_loss: 0.6321 - val_acc: 0.7736\n",
      "6/6 [==============================] - 2s 68ms/step - loss: 0.1587 - acc: 0.9097 - val_loss: 0.5684 - val_acc: 0.8590\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2316 - acc: 0.7820 - val_loss: 0.6273 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1754 - acc: 0.8768 - val_loss: 0.5513 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1916 - acc: 0.8578 - val_loss: 0.5282 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2093 - acc: 0.8294 - val_loss: 0.5500 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1826 - acc: 0.8720 - val_loss: 0.6100 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2669 - acc: 0.7488 - val_loss: 0.6046 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2102 - acc: 0.8246 - val_loss: 0.6470 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2034 - acc: 0.8436 - val_loss: 0.5912 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1701 - acc: 0.8957 - val_loss: 0.5184 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1979 - acc: 0.8436 - val_loss: 0.6413 - val_acc: 0.7830\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2243 - acc: 0.8122 - val_loss: 0.6332 - val_acc: 0.7582\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.1477 - acc: 0.9289 - val_loss: 0.5766 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2492 - acc: 0.7583 - val_loss: 0.6166 - val_acc: 0.7358\n",
      "5/5 [==============================] - 2s 88ms/step - loss: 0.1909 - acc: 0.8591 - val_loss: 0.5555 - val_acc: 0.8649\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1396 - acc: 0.9431 - val_loss: 0.5414 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1840 - acc: 0.8720 - val_loss: 0.5672 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1678 - acc: 0.8910 - val_loss: 0.5056 - val_acc: 0.9434\n",
      "7/7 [==============================] - 2s 57ms/step - loss: 0.1851 - acc: 0.8732 - val_loss: 0.6262 - val_acc: 0.7864\n",
      "5/5 [==============================] - 2s 82ms/step - loss: 0.1521 - acc: 0.9133 - val_loss: 0.4721 - val_acc: 0.9733\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1998 - acc: 0.8531 - val_loss: 0.4923 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 58ms/step - loss: 0.3055 - acc: 0.6942 - val_loss: 0.6081 - val_acc: 0.8252\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2503 - acc: 0.7630 - val_loss: 0.6346 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2135 - acc: 0.8152 - val_loss: 0.6021 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2103 - acc: 0.8294 - val_loss: 0.6435 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1929 - acc: 0.8626 - val_loss: 0.5870 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2228 - acc: 0.8009 - val_loss: 0.6416 - val_acc: 0.7547\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2436 - acc: 0.7630 - val_loss: 0.6564 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2595 - acc: 0.7346 - val_loss: 0.6472 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2268 - acc: 0.7962 - val_loss: 0.6288 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 54ms/step - loss: 0.2114 - acc: 0.8246 - val_loss: 0.6505 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1514 - acc: 0.9384 - val_loss: 0.6306 - val_acc: 0.8208\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.2583 - acc: 0.7299 - val_loss: 0.6567 - val_acc: 0.8019\n",
      "5/5 [==============================] - 2s 79ms/step - loss: 0.2579 - acc: 0.7248 - val_loss: 0.6656 - val_acc: 0.7297\n",
      "5/5 [==============================] - 2s 78ms/step - loss: 0.2343 - acc: 0.7733 - val_loss: 0.6320 - val_acc: 0.8267\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3471 - acc: 0.5417 - val_loss: 0.6659 - val_acc: 0.7500\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1810 - acc: 0.8863 - val_loss: 0.6257 - val_acc: 0.9057\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2271 - acc: 0.7962 - val_loss: 0.6259 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2612 - acc: 0.7251 - val_loss: 0.6633 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1792 - acc: 0.8910 - val_loss: 0.6297 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1820 - acc: 0.8815 - val_loss: 0.6475 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2274 - acc: 0.7915 - val_loss: 0.6650 - val_acc: 0.7170\n",
      "8/8 [==============================] - 3s 156ms/step - loss: 0.2795 - acc: 0.6777 - val_loss: 0.6774 - val_acc: 0.7075\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1969 - acc: 0.8531 - val_loss: 0.6684 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1994 - acc: 0.8483 - val_loss: 0.6123 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1525 - acc: 0.9384 - val_loss: 0.6156 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1889 - acc: 0.8673 - val_loss: 0.6198 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1866 - acc: 0.8720 - val_loss: 0.6469 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2029 - acc: 0.8389 - val_loss: 0.5815 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1641 - acc: 0.9005 - val_loss: 0.5675 - val_acc: 0.9143\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1997 - acc: 0.8483 - val_loss: 0.6220 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1773 - acc: 0.8815 - val_loss: 0.5438 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2001 - acc: 0.8483 - val_loss: 0.5336 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2895 - acc: 0.6777 - val_loss: 0.6609 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2061 - acc: 0.8341 - val_loss: 0.6618 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1903 - acc: 0.8673 - val_loss: 0.6588 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1885 - acc: 0.8673 - val_loss: 0.6321 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2212 - acc: 0.8057 - val_loss: 0.6495 - val_acc: 0.7264\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2141 - acc: 0.8199 - val_loss: 0.6295 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1532 - acc: 0.9336 - val_loss: 0.6081 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1428 - acc: 0.9526 - val_loss: 0.5903 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2411 - acc: 0.7725 - val_loss: 0.6415 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1349 - acc: 0.9573 - val_loss: 0.5485 - val_acc: 0.9434\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2116 - acc: 0.8246 - val_loss: 0.5977 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.2032 - acc: 0.8398 - val_loss: 0.5724 - val_acc: 0.8571\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1948 - acc: 0.8531 - val_loss: 0.5289 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1753 - acc: 0.8863 - val_loss: 0.5234 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2422 - acc: 0.7867 - val_loss: 0.5677 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1700 - acc: 0.8957 - val_loss: 0.5785 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1114 - acc: 0.9810 - val_loss: 0.4872 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2392 - acc: 0.7820 - val_loss: 0.5665 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.0989 - acc: 0.9953 - val_loss: 0.4743 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1972 - acc: 0.8531 - val_loss: 0.5668 - val_acc: 0.8113\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1767 - acc: 0.8768 - val_loss: 0.5151 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1680 - acc: 0.8957 - val_loss: 0.4600 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1417 - acc: 0.9242 - val_loss: 0.4215 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1390 - acc: 0.9242 - val_loss: 0.4337 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2466 - acc: 0.8104 - val_loss: 0.5445 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.1852 - acc: 0.8720 - val_loss: 0.4625 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1971 - acc: 0.8531 - val_loss: 0.5333 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2107 - acc: 0.8294 - val_loss: 0.5577 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2577 - acc: 0.7583 - val_loss: 0.6135 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1810 - acc: 0.8768 - val_loss: 0.5641 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2225 - acc: 0.8104 - val_loss: 0.6024 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2112 - acc: 0.8246 - val_loss: 0.6063 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2278 - acc: 0.8009 - val_loss: 0.6097 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1733 - acc: 0.8910 - val_loss: 0.5837 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2642 - acc: 0.7346 - val_loss: 0.6198 - val_acc: 0.7736\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1462 - acc: 0.9336 - val_loss: 0.5175 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1978 - acc: 0.8483 - val_loss: 0.5930 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2203 - acc: 0.8104 - val_loss: 0.5630 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1724 - acc: 0.8863 - val_loss: 0.5341 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2127 - acc: 0.8294 - val_loss: 0.5469 - val_acc: 0.8868\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2067 - acc: 0.8389 - val_loss: 0.5865 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1631 - acc: 0.9005 - val_loss: 0.5846 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1839 - acc: 0.8720 - val_loss: 0.5729 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2138 - acc: 0.8246 - val_loss: 0.5781 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 58ms/step - loss: 0.2779 - acc: 0.7182 - val_loss: 0.6479 - val_acc: 0.6813\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2177 - acc: 0.8152 - val_loss: 0.5957 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1992 - acc: 0.8483 - val_loss: 0.6081 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2072 - acc: 0.8294 - val_loss: 0.6262 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1326 - acc: 0.9573 - val_loss: 0.5552 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 106ms/step - loss: 0.2315 - acc: 0.7899 - val_loss: 0.6334 - val_acc: 0.7119\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2653 - acc: 0.7393 - val_loss: 0.6019 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1241 - acc: 0.9479 - val_loss: 0.4406 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1942 - acc: 0.8531 - val_loss: 0.5915 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2475 - acc: 0.7536 - val_loss: 0.6507 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2297 - acc: 0.7867 - val_loss: 0.6556 - val_acc: 0.7453\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2037 - acc: 0.8389 - val_loss: 0.6221 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2245 - acc: 0.7962 - val_loss: 0.6602 - val_acc: 0.6981\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2280 - acc: 0.7902 - val_loss: 0.6455 - val_acc: 0.8137\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.1693 - acc: 0.9045 - val_loss: 0.6143 - val_acc: 0.9400\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2065 - acc: 0.8294 - val_loss: 0.6218 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1884 - acc: 0.8673 - val_loss: 0.6562 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2235 - acc: 0.8009 - val_loss: 0.6049 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1756 - acc: 0.8910 - val_loss: 0.6121 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2129 - acc: 0.8199 - val_loss: 0.6023 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2554 - acc: 0.7441 - val_loss: 0.6363 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2258 - acc: 0.7962 - val_loss: 0.6379 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2009 - acc: 0.8436 - val_loss: 0.6486 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1986 - acc: 0.8483 - val_loss: 0.6497 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1621 - acc: 0.9194 - val_loss: 0.6274 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1598 - acc: 0.9242 - val_loss: 0.6146 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 101ms/step - loss: 0.2769 - acc: 0.6975 - val_loss: 0.6581 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2154 - acc: 0.8152 - val_loss: 0.6496 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1513 - acc: 0.9384 - val_loss: 0.6283 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1481 - acc: 0.9431 - val_loss: 0.5875 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1669 - acc: 0.9052 - val_loss: 0.5771 - val_acc: 0.9057\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2007 - acc: 0.8436 - val_loss: 0.5975 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1978 - acc: 0.8483 - val_loss: 0.5989 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2070 - acc: 0.8341 - val_loss: 0.5908 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2140 - acc: 0.8246 - val_loss: 0.5974 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1336 - acc: 0.9526 - val_loss: 0.5183 - val_acc: 0.9811\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2303 - acc: 0.8009 - val_loss: 0.5840 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2208 - acc: 0.8104 - val_loss: 0.6103 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.2234 - acc: 0.8030 - val_loss: 0.6121 - val_acc: 0.8283\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4727 - acc: 0.3333 - val_loss: 0.6134 - val_acc: 1.0000\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2327 - acc: 0.7820 - val_loss: 0.6400 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2375 - acc: 0.7725 - val_loss: 0.6414 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2278 - acc: 0.7915 - val_loss: 0.6422 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1664 - acc: 0.9100 - val_loss: 0.6253 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1812 - acc: 0.8815 - val_loss: 0.6381 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1733 - acc: 0.8957 - val_loss: 0.6039 - val_acc: 0.9151\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2212 - acc: 0.8057 - val_loss: 0.6329 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1764 - acc: 0.8910 - val_loss: 0.6271 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1283 - acc: 0.9810 - val_loss: 0.5846 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1453 - acc: 0.9431 - val_loss: 0.5742 - val_acc: 0.9528\n",
      "6/6 [==============================] - 2s 63ms/step - loss: 0.3170 - acc: 0.6341 - val_loss: 0.6816 - val_acc: 0.5976\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2307 - acc: 0.7915 - val_loss: 0.6239 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1697 - acc: 0.9005 - val_loss: 0.6290 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2528 - acc: 0.7441 - val_loss: 0.6420 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 54ms/step - loss: 0.2288 - acc: 0.7915 - val_loss: 0.6148 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2375 - acc: 0.7725 - val_loss: 0.6370 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2246 - acc: 0.7962 - val_loss: 0.6581 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2503 - acc: 0.7441 - val_loss: 0.6591 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1713 - acc: 0.9052 - val_loss: 0.6324 - val_acc: 0.8962\n",
      "6/6 [==============================] - 2s 65ms/step - loss: 0.2251 - acc: 0.7944 - val_loss: 0.6700 - val_acc: 0.6889\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1306 - acc: 0.9810 - val_loss: 0.5971 - val_acc: 0.9434\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2573 - acc: 0.7299 - val_loss: 0.6573 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2338 - acc: 0.7773 - val_loss: 0.6516 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1517 - acc: 0.9384 - val_loss: 0.6003 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2356 - acc: 0.7773 - val_loss: 0.6450 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1906 - acc: 0.8626 - val_loss: 0.6142 - val_acc: 0.9151\n",
      "7/7 [==============================] - 3s 54ms/step - loss: 0.2377 - acc: 0.7692 - val_loss: 0.6449 - val_acc: 0.7885\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2368 - acc: 0.7725 - val_loss: 0.6492 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2198 - acc: 0.8057 - val_loss: 0.6441 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2082 - acc: 0.8294 - val_loss: 0.6762 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2352 - acc: 0.7725 - val_loss: 0.6600 - val_acc: 0.8302\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1330 - acc: 0.9810 - val_loss: 0.6187 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1905 - acc: 0.8626 - val_loss: 0.6329 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2688 - acc: 0.7062 - val_loss: 0.6762 - val_acc: 0.6509\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1921 - acc: 0.8578 - val_loss: 0.6075 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2316 - acc: 0.7820 - val_loss: 0.6533 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1817 - acc: 0.8815 - val_loss: 0.6236 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1762 - acc: 0.8910 - val_loss: 0.6155 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1647 - acc: 0.9100 - val_loss: 0.5831 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2204 - acc: 0.8104 - val_loss: 0.6122 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1871 - acc: 0.8673 - val_loss: 0.6186 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2392 - acc: 0.7773 - val_loss: 0.6432 - val_acc: 0.6981\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2228 - acc: 0.8009 - val_loss: 0.6374 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1671 - acc: 0.9052 - val_loss: 0.6053 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2206 - acc: 0.8104 - val_loss: 0.6222 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 51ms/step - loss: 0.2163 - acc: 0.8135 - val_loss: 0.6425 - val_acc: 0.8041\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1864 - acc: 0.8720 - val_loss: 0.6258 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2198 - acc: 0.8057 - val_loss: 0.6555 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2100 - acc: 0.8246 - val_loss: 0.6468 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.1626 - acc: 0.9139 - val_loss: 0.5762 - val_acc: 0.9423\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1797 - acc: 0.8815 - val_loss: 0.6300 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2539 - acc: 0.7393 - val_loss: 0.6292 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2599 - acc: 0.7251 - val_loss: 0.6661 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2071 - acc: 0.8294 - val_loss: 0.6618 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1796 - acc: 0.8863 - val_loss: 0.6110 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1913 - acc: 0.8626 - val_loss: 0.6184 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2241 - acc: 0.7962 - val_loss: 0.6454 - val_acc: 0.8208\n",
      "5/5 [==============================] - 2s 82ms/step - loss: 0.2832 - acc: 0.6757 - val_loss: 0.6809 - val_acc: 0.6081\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2248 - acc: 0.7962 - val_loss: 0.6421 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1328 - acc: 0.9810 - val_loss: 0.6011 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1748 - acc: 0.8957 - val_loss: 0.5983 - val_acc: 0.8762\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1783 - acc: 0.8863 - val_loss: 0.6024 - val_acc: 0.8491\n",
      "5/5 [==============================] - 2s 77ms/step - loss: 0.2768 - acc: 0.6959 - val_loss: 0.6516 - val_acc: 0.7568\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2284 - acc: 0.7915 - val_loss: 0.6268 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2135 - acc: 0.8199 - val_loss: 0.6324 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1640 - acc: 0.9100 - val_loss: 0.5372 - val_acc: 0.9906\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2052 - acc: 0.8341 - val_loss: 0.6183 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2147 - acc: 0.8199 - val_loss: 0.6253 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1614 - acc: 0.9147 - val_loss: 0.5763 - val_acc: 0.9528\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2066 - acc: 0.8356 - val_loss: 0.6264 - val_acc: 0.8082\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2046 - acc: 0.8389 - val_loss: 0.6077 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2206 - acc: 0.8057 - val_loss: 0.6383 - val_acc: 0.7925\n",
      "6/6 [==============================] - 2s 64ms/step - loss: 0.2164 - acc: 0.8167 - val_loss: 0.6571 - val_acc: 0.7111\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2320 - acc: 0.7820 - val_loss: 0.6373 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2357 - acc: 0.7773 - val_loss: 0.6184 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1867 - acc: 0.8720 - val_loss: 0.5954 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2348 - acc: 0.7773 - val_loss: 0.6477 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2630 - acc: 0.7299 - val_loss: 0.6395 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2341 - acc: 0.7773 - val_loss: 0.6663 - val_acc: 0.7358\n",
      "4/4 [==============================] - 2s 112ms/step - loss: 0.2115 - acc: 0.8235 - val_loss: 0.6681 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1845 - acc: 0.8768 - val_loss: 0.6117 - val_acc: 0.9245\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2367 - acc: 0.7725 - val_loss: 0.6459 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1867 - acc: 0.8768 - val_loss: 0.6510 - val_acc: 0.8381\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2082 - acc: 0.8294 - val_loss: 0.6296 - val_acc: 0.8585\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2414 - acc: 0.7647 - val_loss: 0.6452 - val_acc: 0.7941\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2183 - acc: 0.8104 - val_loss: 0.6325 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 151ms/step - loss: 0.2857 - acc: 0.6682 - val_loss: 0.6784 - val_acc: 0.6604\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2260 - acc: 0.7915 - val_loss: 0.6561 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2278 - acc: 0.7867 - val_loss: 0.6753 - val_acc: 0.7736\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.1799 - acc: 0.8895 - val_loss: 0.6460 - val_acc: 0.8352\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2085 - acc: 0.8294 - val_loss: 0.6478 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.2262 - acc: 0.7915 - val_loss: 0.6767 - val_acc: 0.7170\n",
      "6/6 [==============================] - 2s 66ms/step - loss: 0.1957 - acc: 0.8611 - val_loss: 0.6549 - val_acc: 0.8222\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1838 - acc: 0.8815 - val_loss: 0.6512 - val_acc: 0.8868\n",
      "5/5 [==============================] - 2s 83ms/step - loss: 0.2246 - acc: 0.7973 - val_loss: 0.6602 - val_acc: 0.7297\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2016 - acc: 0.8436 - val_loss: 0.6505 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1505 - acc: 0.9479 - val_loss: 0.6567 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 158ms/step - loss: 0.1731 - acc: 0.9005 - val_loss: 0.6300 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1805 - acc: 0.8815 - val_loss: 0.5998 - val_acc: 0.9434\n",
      "3/3 [==============================] - 2s 157ms/step - loss: 0.2200 - acc: 0.8046 - val_loss: 0.6515 - val_acc: 0.8182\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2279 - acc: 0.7915 - val_loss: 0.6352 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2198 - acc: 0.8057 - val_loss: 0.6589 - val_acc: 0.8113\n",
      "8/8 [==============================] - 3s 153ms/step - loss: 0.2099 - acc: 0.8246 - val_loss: 0.6515 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2111 - acc: 0.8246 - val_loss: 0.6624 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1993 - acc: 0.8483 - val_loss: 0.6446 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2705 - acc: 0.6967 - val_loss: 0.6690 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2130 - acc: 0.8199 - val_loss: 0.6555 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1987 - acc: 0.8483 - val_loss: 0.6685 - val_acc: 0.7642\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2046 - acc: 0.8389 - val_loss: 0.6547 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1819 - acc: 0.8815 - val_loss: 0.6537 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2209 - acc: 0.8009 - val_loss: 0.6610 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2466 - acc: 0.7488 - val_loss: 0.6660 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1702 - acc: 0.9100 - val_loss: 0.6544 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2153 - acc: 0.8152 - val_loss: 0.6504 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1718 - acc: 0.9005 - val_loss: 0.6006 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2024 - acc: 0.8436 - val_loss: 0.6517 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2390 - acc: 0.7678 - val_loss: 0.6619 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2653 - acc: 0.7109 - val_loss: 0.6732 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2203 - acc: 0.8057 - val_loss: 0.6506 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1602 - acc: 0.9242 - val_loss: 0.6221 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2422 - acc: 0.7630 - val_loss: 0.6609 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1475 - acc: 0.9431 - val_loss: 0.6007 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1613 - acc: 0.9147 - val_loss: 0.5877 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2596 - acc: 0.7299 - val_loss: 0.6436 - val_acc: 0.8019\n",
      "7/7 [==============================] - 3s 52ms/step - loss: 0.1724 - acc: 0.9024 - val_loss: 0.6406 - val_acc: 0.8544\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 0.2449 - acc: 0.7659 - val_loss: 0.6429 - val_acc: 0.7745\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1959 - acc: 0.8531 - val_loss: 0.6216 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1834 - acc: 0.8720 - val_loss: 0.6058 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2214 - acc: 0.8057 - val_loss: 0.6165 - val_acc: 0.8208\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2145 - acc: 0.8250 - val_loss: 0.6162 - val_acc: 0.8100\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1362 - acc: 0.9573 - val_loss: 0.5861 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1801 - acc: 0.8815 - val_loss: 0.5611 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2188 - acc: 0.8152 - val_loss: 0.6177 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1880 - acc: 0.8673 - val_loss: 0.6052 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2536 - acc: 0.7536 - val_loss: 0.6133 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2287 - acc: 0.7915 - val_loss: 0.6320 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1370 - acc: 0.9479 - val_loss: 0.5421 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1324 - acc: 0.9526 - val_loss: 0.5368 - val_acc: 0.9245\n",
      "3/3 [==============================] - 2s 153ms/step - loss: 0.1702 - acc: 0.8864 - val_loss: 0.4757 - val_acc: 0.9318\n",
      "6/6 [==============================] - 2s 202ms/step - loss: 0.2179 - acc: 0.8111 - val_loss: 0.5734 - val_acc: 0.8667\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1967 - acc: 0.8531 - val_loss: 0.5969 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1883 - acc: 0.8673 - val_loss: 0.5778 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2073 - acc: 0.8341 - val_loss: 0.6072 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2486 - acc: 0.7725 - val_loss: 0.6091 - val_acc: 0.7736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3366 - acc: 0.6296 - val_loss: 0.6348 - val_acc: 0.7692\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.1839 - acc: 0.8768 - val_loss: 0.6216 - val_acc: 0.8962\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.2452 - acc: 0.7571 - val_loss: 0.6528 - val_acc: 0.7333\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2791 - acc: 0.6919 - val_loss: 0.6762 - val_acc: 0.6321\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1763 - acc: 0.8910 - val_loss: 0.6135 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1377 - acc: 0.9573 - val_loss: 0.4809 - val_acc: 0.9623\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2026 - acc: 0.8436 - val_loss: 0.6123 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1466 - acc: 0.9479 - val_loss: 0.6160 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1707 - acc: 0.8957 - val_loss: 0.5694 - val_acc: 0.9245\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2067 - acc: 0.8322 - val_loss: 0.6224 - val_acc: 0.8108\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2277 - acc: 0.7962 - val_loss: 0.6247 - val_acc: 0.7736\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1280 - acc: 0.9621 - val_loss: 0.5248 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1752 - acc: 0.8910 - val_loss: 0.5084 - val_acc: 0.9717\n",
      "5/5 [==============================] - 2s 82ms/step - loss: 0.3068 - acc: 0.6667 - val_loss: 0.6491 - val_acc: 0.6774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1576 - acc: 0.9100 - val_loss: 0.5767 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2475 - acc: 0.7773 - val_loss: 0.5807 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 153ms/step - loss: 0.1331 - acc: 0.9479 - val_loss: 0.5186 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2519 - acc: 0.7583 - val_loss: 0.6132 - val_acc: 0.7736\n",
      "6/6 [==============================] - 2s 67ms/step - loss: 0.1579 - acc: 0.9097 - val_loss: 0.5499 - val_acc: 0.8590\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2359 - acc: 0.7820 - val_loss: 0.5980 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1758 - acc: 0.8768 - val_loss: 0.5266 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1940 - acc: 0.8578 - val_loss: 0.5226 - val_acc: 0.9151\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2088 - acc: 0.8294 - val_loss: 0.5314 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1843 - acc: 0.8720 - val_loss: 0.5843 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2683 - acc: 0.7488 - val_loss: 0.5885 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2092 - acc: 0.8246 - val_loss: 0.6259 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2011 - acc: 0.8436 - val_loss: 0.5682 - val_acc: 0.8774\n",
      "8/8 [==============================] - 3s 157ms/step - loss: 0.1666 - acc: 0.8957 - val_loss: 0.5109 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1976 - acc: 0.8436 - val_loss: 0.6123 - val_acc: 0.7830\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.2261 - acc: 0.8122 - val_loss: 0.6109 - val_acc: 0.7582\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1446 - acc: 0.9289 - val_loss: 0.5393 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2565 - acc: 0.7583 - val_loss: 0.6083 - val_acc: 0.7358\n",
      "5/5 [==============================] - 2s 82ms/step - loss: 0.1928 - acc: 0.8591 - val_loss: 0.5505 - val_acc: 0.8649\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1354 - acc: 0.9431 - val_loss: 0.5255 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1835 - acc: 0.8720 - val_loss: 0.5473 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1663 - acc: 0.8910 - val_loss: 0.5059 - val_acc: 0.9434\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.1833 - acc: 0.8732 - val_loss: 0.6023 - val_acc: 0.7864\n",
      "5/5 [==============================] - 2s 82ms/step - loss: 0.1547 - acc: 0.9133 - val_loss: 0.4685 - val_acc: 0.9733\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1967 - acc: 0.8531 - val_loss: 0.4832 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.3061 - acc: 0.6942 - val_loss: 0.5796 - val_acc: 0.8252\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2510 - acc: 0.7630 - val_loss: 0.6297 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2160 - acc: 0.8152 - val_loss: 0.5957 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2100 - acc: 0.8294 - val_loss: 0.6320 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1912 - acc: 0.8626 - val_loss: 0.5921 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2235 - acc: 0.8009 - val_loss: 0.6343 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2441 - acc: 0.7630 - val_loss: 0.6542 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2605 - acc: 0.7346 - val_loss: 0.6387 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2260 - acc: 0.7962 - val_loss: 0.6335 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2113 - acc: 0.8246 - val_loss: 0.6524 - val_acc: 0.7547\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.1517 - acc: 0.9384 - val_loss: 0.6285 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2592 - acc: 0.7299 - val_loss: 0.6462 - val_acc: 0.8019\n",
      "5/5 [==============================] - 2s 84ms/step - loss: 0.2595 - acc: 0.7248 - val_loss: 0.6578 - val_acc: 0.7297\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2362 - acc: 0.7733 - val_loss: 0.6272 - val_acc: 0.8267\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3521 - acc: 0.5417 - val_loss: 0.6582 - val_acc: 0.7500\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1803 - acc: 0.8863 - val_loss: 0.6140 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2286 - acc: 0.7962 - val_loss: 0.6138 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2618 - acc: 0.7251 - val_loss: 0.6566 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1769 - acc: 0.8910 - val_loss: 0.6174 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1819 - acc: 0.8815 - val_loss: 0.6352 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2282 - acc: 0.7915 - val_loss: 0.6558 - val_acc: 0.7170\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2825 - acc: 0.6777 - val_loss: 0.6682 - val_acc: 0.7075\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1966 - acc: 0.8531 - val_loss: 0.6608 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1987 - acc: 0.8483 - val_loss: 0.6351 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1548 - acc: 0.9384 - val_loss: 0.6333 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1898 - acc: 0.8673 - val_loss: 0.6447 - val_acc: 0.8208\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.1872 - acc: 0.8720 - val_loss: 0.6569 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2034 - acc: 0.8389 - val_loss: 0.6283 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1700 - acc: 0.9005 - val_loss: 0.6081 - val_acc: 0.9143\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1988 - acc: 0.8483 - val_loss: 0.6328 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1799 - acc: 0.8815 - val_loss: 0.5946 - val_acc: 0.9245\n",
      "8/8 [==============================] - 3s 150ms/step - loss: 0.1984 - acc: 0.8483 - val_loss: 0.5872 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2873 - acc: 0.6777 - val_loss: 0.6531 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2058 - acc: 0.8341 - val_loss: 0.6477 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1888 - acc: 0.8673 - val_loss: 0.6399 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1885 - acc: 0.8673 - val_loss: 0.6121 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2217 - acc: 0.8057 - val_loss: 0.6503 - val_acc: 0.7264\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2120 - acc: 0.8199 - val_loss: 0.6250 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1539 - acc: 0.9336 - val_loss: 0.6219 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1444 - acc: 0.9526 - val_loss: 0.6024 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2366 - acc: 0.7725 - val_loss: 0.6550 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1386 - acc: 0.9573 - val_loss: 0.5672 - val_acc: 0.9434\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2115 - acc: 0.8246 - val_loss: 0.6207 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.2037 - acc: 0.8398 - val_loss: 0.5917 - val_acc: 0.8571\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1944 - acc: 0.8531 - val_loss: 0.5771 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1763 - acc: 0.8863 - val_loss: 0.5746 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2348 - acc: 0.7867 - val_loss: 0.5844 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1709 - acc: 0.8957 - val_loss: 0.5734 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1046 - acc: 0.9810 - val_loss: 0.4918 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2381 - acc: 0.7820 - val_loss: 0.5673 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1006 - acc: 0.9953 - val_loss: 0.5236 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1926 - acc: 0.8531 - val_loss: 0.5888 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1743 - acc: 0.8768 - val_loss: 0.5244 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1718 - acc: 0.8957 - val_loss: 0.5107 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1474 - acc: 0.9242 - val_loss: 0.5232 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1476 - acc: 0.9242 - val_loss: 0.5617 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2196 - acc: 0.8104 - val_loss: 0.6092 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1820 - acc: 0.8720 - val_loss: 0.5812 - val_acc: 0.9434\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1917 - acc: 0.8531 - val_loss: 0.5811 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2086 - acc: 0.8294 - val_loss: 0.6227 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2463 - acc: 0.7583 - val_loss: 0.6244 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1863 - acc: 0.8768 - val_loss: 0.5989 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2210 - acc: 0.8104 - val_loss: 0.6417 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2107 - acc: 0.8246 - val_loss: 0.6443 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2237 - acc: 0.8009 - val_loss: 0.6558 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1769 - acc: 0.8910 - val_loss: 0.6206 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2571 - acc: 0.7346 - val_loss: 0.6556 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1571 - acc: 0.9336 - val_loss: 0.6089 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2001 - acc: 0.8483 - val_loss: 0.6424 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2180 - acc: 0.8104 - val_loss: 0.6336 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1783 - acc: 0.8863 - val_loss: 0.6142 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2101 - acc: 0.8294 - val_loss: 0.5971 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2031 - acc: 0.8389 - val_loss: 0.6245 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1657 - acc: 0.9005 - val_loss: 0.5783 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1838 - acc: 0.8720 - val_loss: 0.5289 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2230 - acc: 0.8246 - val_loss: 0.5839 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 57ms/step - loss: 0.2736 - acc: 0.7182 - val_loss: 0.6495 - val_acc: 0.6813\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2159 - acc: 0.8152 - val_loss: 0.6340 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1987 - acc: 0.8483 - val_loss: 0.6552 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2082 - acc: 0.8294 - val_loss: 0.6535 - val_acc: 0.7547\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1445 - acc: 0.9573 - val_loss: 0.6227 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 99ms/step - loss: 0.2281 - acc: 0.7899 - val_loss: 0.6615 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2539 - acc: 0.7393 - val_loss: 0.6379 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1370 - acc: 0.9479 - val_loss: 0.5218 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1943 - acc: 0.8531 - val_loss: 0.5793 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2483 - acc: 0.7536 - val_loss: 0.6447 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2313 - acc: 0.7867 - val_loss: 0.6531 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2032 - acc: 0.8389 - val_loss: 0.5996 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2258 - acc: 0.7962 - val_loss: 0.6614 - val_acc: 0.6981\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 0.2271 - acc: 0.7902 - val_loss: 0.6451 - val_acc: 0.8137\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.1702 - acc: 0.9045 - val_loss: 0.6231 - val_acc: 0.9400\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2058 - acc: 0.8294 - val_loss: 0.6106 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1876 - acc: 0.8673 - val_loss: 0.6561 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2219 - acc: 0.8009 - val_loss: 0.6227 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1780 - acc: 0.8910 - val_loss: 0.6361 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2127 - acc: 0.8199 - val_loss: 0.6376 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2514 - acc: 0.7441 - val_loss: 0.6542 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2256 - acc: 0.7962 - val_loss: 0.6630 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2026 - acc: 0.8436 - val_loss: 0.6580 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1985 - acc: 0.8483 - val_loss: 0.6589 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1598 - acc: 0.9194 - val_loss: 0.6038 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 146ms/step - loss: 0.1582 - acc: 0.9242 - val_loss: 0.5625 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 105ms/step - loss: 0.2891 - acc: 0.6975 - val_loss: 0.6358 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2149 - acc: 0.8152 - val_loss: 0.6453 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1463 - acc: 0.9384 - val_loss: 0.5918 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1327 - acc: 0.9431 - val_loss: 0.3992 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1659 - acc: 0.9052 - val_loss: 0.3708 - val_acc: 0.9057\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2134 - acc: 0.8436 - val_loss: 0.5451 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2007 - acc: 0.8483 - val_loss: 0.5680 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2069 - acc: 0.8341 - val_loss: 0.6180 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2144 - acc: 0.8246 - val_loss: 0.6145 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1332 - acc: 0.9526 - val_loss: 0.5103 - val_acc: 0.9811\n",
      "8/8 [==============================] - 3s 154ms/step - loss: 0.2270 - acc: 0.8009 - val_loss: 0.5966 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2247 - acc: 0.8104 - val_loss: 0.6166 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2281 - acc: 0.8030 - val_loss: 0.5757 - val_acc: 0.8283\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6496 - acc: 0.3333 - val_loss: 0.4617 - val_acc: 1.0000\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2336 - acc: 0.7820 - val_loss: 0.6223 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2398 - acc: 0.7725 - val_loss: 0.6287 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2282 - acc: 0.7915 - val_loss: 0.6374 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1659 - acc: 0.9100 - val_loss: 0.6145 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1819 - acc: 0.8815 - val_loss: 0.6363 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1712 - acc: 0.8957 - val_loss: 0.5988 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2208 - acc: 0.8057 - val_loss: 0.6287 - val_acc: 0.7830\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1754 - acc: 0.8910 - val_loss: 0.6152 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1223 - acc: 0.9810 - val_loss: 0.5474 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1389 - acc: 0.9431 - val_loss: 0.4894 - val_acc: 0.9528\n",
      "6/6 [==============================] - 2s 65ms/step - loss: 0.3341 - acc: 0.6341 - val_loss: 0.6866 - val_acc: 0.5976\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2338 - acc: 0.7915 - val_loss: 0.6099 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1675 - acc: 0.9005 - val_loss: 0.6254 - val_acc: 0.8208\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2532 - acc: 0.7441 - val_loss: 0.6436 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2291 - acc: 0.7915 - val_loss: 0.6137 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2376 - acc: 0.7725 - val_loss: 0.6288 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2249 - acc: 0.7962 - val_loss: 0.6589 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2522 - acc: 0.7441 - val_loss: 0.6558 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1716 - acc: 0.9052 - val_loss: 0.6093 - val_acc: 0.8962\n",
      "6/6 [==============================] - 2s 63ms/step - loss: 0.2255 - acc: 0.7944 - val_loss: 0.6684 - val_acc: 0.6889\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1245 - acc: 0.9810 - val_loss: 0.5615 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2633 - acc: 0.7299 - val_loss: 0.6441 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2353 - acc: 0.7773 - val_loss: 0.6406 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1492 - acc: 0.9384 - val_loss: 0.5793 - val_acc: 0.9528\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2371 - acc: 0.7773 - val_loss: 0.6301 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1898 - acc: 0.8626 - val_loss: 0.5854 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.2420 - acc: 0.7692 - val_loss: 0.6289 - val_acc: 0.7885\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2362 - acc: 0.7725 - val_loss: 0.6417 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2211 - acc: 0.8057 - val_loss: 0.6249 - val_acc: 0.7642\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2075 - acc: 0.8294 - val_loss: 0.6647 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2365 - acc: 0.7725 - val_loss: 0.6382 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1264 - acc: 0.9810 - val_loss: 0.5652 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1898 - acc: 0.8626 - val_loss: 0.6061 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2716 - acc: 0.7062 - val_loss: 0.6786 - val_acc: 0.6509\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1904 - acc: 0.8578 - val_loss: 0.5707 - val_acc: 0.9151\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2341 - acc: 0.7820 - val_loss: 0.6470 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1812 - acc: 0.8815 - val_loss: 0.6058 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1738 - acc: 0.8910 - val_loss: 0.6009 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1615 - acc: 0.9100 - val_loss: 0.5481 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2237 - acc: 0.8104 - val_loss: 0.6011 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.1856 - acc: 0.8673 - val_loss: 0.6038 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2435 - acc: 0.7773 - val_loss: 0.6320 - val_acc: 0.6981\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2235 - acc: 0.8009 - val_loss: 0.6249 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1657 - acc: 0.9052 - val_loss: 0.5869 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2236 - acc: 0.8104 - val_loss: 0.6070 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 51ms/step - loss: 0.2163 - acc: 0.8135 - val_loss: 0.6317 - val_acc: 0.8041\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1862 - acc: 0.8720 - val_loss: 0.6096 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2217 - acc: 0.8057 - val_loss: 0.6481 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2094 - acc: 0.8246 - val_loss: 0.6411 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.1586 - acc: 0.9139 - val_loss: 0.5465 - val_acc: 0.9423\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1787 - acc: 0.8815 - val_loss: 0.6211 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2546 - acc: 0.7393 - val_loss: 0.6180 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2597 - acc: 0.7251 - val_loss: 0.6648 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2070 - acc: 0.8294 - val_loss: 0.6546 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1793 - acc: 0.8863 - val_loss: 0.5976 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1937 - acc: 0.8626 - val_loss: 0.6068 - val_acc: 0.8868\n",
      "8/8 [==============================] - 3s 160ms/step - loss: 0.2239 - acc: 0.7962 - val_loss: 0.6425 - val_acc: 0.8208\n",
      "5/5 [==============================] - 2s 79ms/step - loss: 0.2838 - acc: 0.6757 - val_loss: 0.6771 - val_acc: 0.6081\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2249 - acc: 0.7962 - val_loss: 0.6234 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1269 - acc: 0.9810 - val_loss: 0.5462 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1745 - acc: 0.8957 - val_loss: 0.6036 - val_acc: 0.8762\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1771 - acc: 0.8863 - val_loss: 0.5753 - val_acc: 0.8491\n",
      "5/5 [==============================] - 3s 88ms/step - loss: 0.2766 - acc: 0.6959 - val_loss: 0.6582 - val_acc: 0.7568\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2300 - acc: 0.7915 - val_loss: 0.6319 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2127 - acc: 0.8199 - val_loss: 0.6449 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1694 - acc: 0.9100 - val_loss: 0.5802 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2050 - acc: 0.8341 - val_loss: 0.6374 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 154ms/step - loss: 0.2131 - acc: 0.8199 - val_loss: 0.6392 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1660 - acc: 0.9147 - val_loss: 0.6108 - val_acc: 0.9528\n",
      "5/5 [==============================] - 2s 79ms/step - loss: 0.2061 - acc: 0.8356 - val_loss: 0.6446 - val_acc: 0.8082\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2051 - acc: 0.8389 - val_loss: 0.6325 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2202 - acc: 0.8057 - val_loss: 0.6495 - val_acc: 0.7925\n",
      "6/6 [==============================] - 2s 61ms/step - loss: 0.2159 - acc: 0.8167 - val_loss: 0.6655 - val_acc: 0.7111\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2316 - acc: 0.7820 - val_loss: 0.6395 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2351 - acc: 0.7773 - val_loss: 0.6123 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1870 - acc: 0.8720 - val_loss: 0.6195 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2333 - acc: 0.7773 - val_loss: 0.6573 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2600 - acc: 0.7299 - val_loss: 0.6517 - val_acc: 0.7736\n",
      "8/8 [==============================] - 3s 155ms/step - loss: 0.2337 - acc: 0.7773 - val_loss: 0.6697 - val_acc: 0.7358\n",
      "4/4 [==============================] - 2s 110ms/step - loss: 0.2111 - acc: 0.8235 - val_loss: 0.6697 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1847 - acc: 0.8768 - val_loss: 0.6232 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2353 - acc: 0.7725 - val_loss: 0.6596 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1867 - acc: 0.8768 - val_loss: 0.6630 - val_acc: 0.8381\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2081 - acc: 0.8294 - val_loss: 0.6481 - val_acc: 0.8585\n",
      "7/7 [==============================] - 3s 55ms/step - loss: 0.2392 - acc: 0.7647 - val_loss: 0.6607 - val_acc: 0.7941\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2173 - acc: 0.8104 - val_loss: 0.6550 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2820 - acc: 0.6682 - val_loss: 0.6840 - val_acc: 0.6604\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2262 - acc: 0.7915 - val_loss: 0.6679 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2279 - acc: 0.7867 - val_loss: 0.6802 - val_acc: 0.7736\n",
      "7/7 [==============================] - 3s 55ms/step - loss: 0.1818 - acc: 0.8895 - val_loss: 0.6630 - val_acc: 0.8352\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2090 - acc: 0.8294 - val_loss: 0.6714 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2257 - acc: 0.7915 - val_loss: 0.6834 - val_acc: 0.7170\n",
      "6/6 [==============================] - 2s 61ms/step - loss: 0.1959 - acc: 0.8611 - val_loss: 0.6746 - val_acc: 0.8222\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1859 - acc: 0.8815 - val_loss: 0.6768 - val_acc: 0.8868\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2235 - acc: 0.7973 - val_loss: 0.6787 - val_acc: 0.7297\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2025 - acc: 0.8436 - val_loss: 0.6720 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1552 - acc: 0.9479 - val_loss: 0.6727 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1761 - acc: 0.9005 - val_loss: 0.6582 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1845 - acc: 0.8815 - val_loss: 0.6435 - val_acc: 0.9434\n",
      "3/3 [==============================] - 2s 163ms/step - loss: 0.2193 - acc: 0.8046 - val_loss: 0.6684 - val_acc: 0.8182\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2265 - acc: 0.7915 - val_loss: 0.6583 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2192 - acc: 0.8057 - val_loss: 0.6654 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2103 - acc: 0.8246 - val_loss: 0.6627 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2111 - acc: 0.8246 - val_loss: 0.6648 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1997 - acc: 0.8483 - val_loss: 0.6557 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2696 - acc: 0.6967 - val_loss: 0.6663 - val_acc: 0.8113\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2134 - acc: 0.8199 - val_loss: 0.6617 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1999 - acc: 0.8483 - val_loss: 0.6690 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2040 - acc: 0.8389 - val_loss: 0.6541 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1831 - acc: 0.8815 - val_loss: 0.6563 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2214 - acc: 0.8009 - val_loss: 0.6629 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2463 - acc: 0.7488 - val_loss: 0.6674 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1714 - acc: 0.9100 - val_loss: 0.6647 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2149 - acc: 0.8152 - val_loss: 0.6543 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1736 - acc: 0.9005 - val_loss: 0.6255 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2015 - acc: 0.8436 - val_loss: 0.6600 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2376 - acc: 0.7678 - val_loss: 0.6664 - val_acc: 0.7264\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2647 - acc: 0.7109 - val_loss: 0.6743 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2190 - acc: 0.8057 - val_loss: 0.6565 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1631 - acc: 0.9242 - val_loss: 0.6438 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2397 - acc: 0.7630 - val_loss: 0.6627 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1545 - acc: 0.9431 - val_loss: 0.6356 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1662 - acc: 0.9147 - val_loss: 0.6290 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2596 - acc: 0.7299 - val_loss: 0.6532 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.1714 - acc: 0.9024 - val_loss: 0.6340 - val_acc: 0.8544\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.2407 - acc: 0.7659 - val_loss: 0.6499 - val_acc: 0.7745\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1955 - acc: 0.8531 - val_loss: 0.6338 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1868 - acc: 0.8720 - val_loss: 0.6143 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2188 - acc: 0.8057 - val_loss: 0.6167 - val_acc: 0.8208\n",
      "7/7 [==============================] - 2s 57ms/step - loss: 0.2111 - acc: 0.8250 - val_loss: 0.6302 - val_acc: 0.8100\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1394 - acc: 0.9573 - val_loss: 0.6057 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1799 - acc: 0.8815 - val_loss: 0.5523 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2146 - acc: 0.8152 - val_loss: 0.5901 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1861 - acc: 0.8673 - val_loss: 0.5998 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2563 - acc: 0.7536 - val_loss: 0.6315 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2285 - acc: 0.7915 - val_loss: 0.6311 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1462 - acc: 0.9479 - val_loss: 0.5949 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1423 - acc: 0.9526 - val_loss: 0.5831 - val_acc: 0.9245\n",
      "3/3 [==============================] - 2s 154ms/step - loss: 0.1722 - acc: 0.8864 - val_loss: 0.5716 - val_acc: 0.9318\n",
      "6/6 [==============================] - 2s 65ms/step - loss: 0.2146 - acc: 0.8111 - val_loss: 0.5657 - val_acc: 0.8667\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1958 - acc: 0.8531 - val_loss: 0.6006 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1863 - acc: 0.8673 - val_loss: 0.5684 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2098 - acc: 0.8341 - val_loss: 0.5937 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2453 - acc: 0.7725 - val_loss: 0.6302 - val_acc: 0.7736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3544 - acc: 0.6296 - val_loss: 0.6334 - val_acc: 0.7692\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1808 - acc: 0.8768 - val_loss: 0.6133 - val_acc: 0.8962\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2450 - acc: 0.7571 - val_loss: 0.6666 - val_acc: 0.7333\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2756 - acc: 0.6919 - val_loss: 0.6794 - val_acc: 0.6321\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1801 - acc: 0.8910 - val_loss: 0.6560 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1439 - acc: 0.9573 - val_loss: 0.5656 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2015 - acc: 0.8436 - val_loss: 0.6451 - val_acc: 0.8868\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1499 - acc: 0.9479 - val_loss: 0.6346 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1730 - acc: 0.8957 - val_loss: 0.5960 - val_acc: 0.9245\n",
      "5/5 [==============================] - 2s 82ms/step - loss: 0.2060 - acc: 0.8322 - val_loss: 0.6282 - val_acc: 0.8108\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2257 - acc: 0.7962 - val_loss: 0.6333 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.1305 - acc: 0.9621 - val_loss: 0.5735 - val_acc: 0.9528\n",
      "8/8 [==============================] - 3s 162ms/step - loss: 0.1746 - acc: 0.8910 - val_loss: 0.5311 - val_acc: 0.9717\n",
      "5/5 [==============================] - 2s 87ms/step - loss: 0.3066 - acc: 0.6667 - val_loss: 0.6640 - val_acc: 0.6774\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1590 - acc: 0.9100 - val_loss: 0.5933 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2432 - acc: 0.7773 - val_loss: 0.5717 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1393 - acc: 0.9479 - val_loss: 0.5601 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2498 - acc: 0.7583 - val_loss: 0.6215 - val_acc: 0.7736\n",
      "6/6 [==============================] - 3s 68ms/step - loss: 0.1663 - acc: 0.9097 - val_loss: 0.6059 - val_acc: 0.8590\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2313 - acc: 0.7820 - val_loss: 0.6307 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1803 - acc: 0.8768 - val_loss: 0.6057 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1957 - acc: 0.8578 - val_loss: 0.6061 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2046 - acc: 0.8294 - val_loss: 0.6042 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1840 - acc: 0.8720 - val_loss: 0.6079 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2551 - acc: 0.7488 - val_loss: 0.6150 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2096 - acc: 0.8246 - val_loss: 0.6363 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2010 - acc: 0.8436 - val_loss: 0.6195 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1694 - acc: 0.8957 - val_loss: 0.5660 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1958 - acc: 0.8436 - val_loss: 0.6336 - val_acc: 0.7830\n",
      "7/7 [==============================] - 3s 56ms/step - loss: 0.2189 - acc: 0.8122 - val_loss: 0.6371 - val_acc: 0.7582\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1489 - acc: 0.9289 - val_loss: 0.5450 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2557 - acc: 0.7583 - val_loss: 0.6228 - val_acc: 0.7358\n",
      "5/5 [==============================] - 2s 83ms/step - loss: 0.1910 - acc: 0.8591 - val_loss: 0.5656 - val_acc: 0.8649\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1413 - acc: 0.9431 - val_loss: 0.5737 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1828 - acc: 0.8720 - val_loss: 0.6001 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 51ms/step - loss: 0.1696 - acc: 0.8910 - val_loss: 0.5789 - val_acc: 0.9434\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.1818 - acc: 0.8732 - val_loss: 0.5670 - val_acc: 0.7864\n",
      "5/5 [==============================] - 2s 79ms/step - loss: 0.1566 - acc: 0.9133 - val_loss: 0.4925 - val_acc: 0.9733\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1971 - acc: 0.8531 - val_loss: 0.5581 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 57ms/step - loss: 0.3004 - acc: 0.6942 - val_loss: 0.5995 - val_acc: 0.8252\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2526 - acc: 0.7630 - val_loss: 0.6296 - val_acc: 0.7264\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2176 - acc: 0.8152 - val_loss: 0.6008 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2106 - acc: 0.8294 - val_loss: 0.6400 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1937 - acc: 0.8626 - val_loss: 0.6398 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2230 - acc: 0.8009 - val_loss: 0.6446 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2420 - acc: 0.7630 - val_loss: 0.6583 - val_acc: 0.7264\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2562 - acc: 0.7346 - val_loss: 0.6463 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2250 - acc: 0.7962 - val_loss: 0.6591 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2127 - acc: 0.8246 - val_loss: 0.6646 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1540 - acc: 0.9384 - val_loss: 0.6559 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2573 - acc: 0.7299 - val_loss: 0.6420 - val_acc: 0.8019\n",
      "5/5 [==============================] - 2s 78ms/step - loss: 0.2584 - acc: 0.7248 - val_loss: 0.6587 - val_acc: 0.7297\n",
      "5/5 [==============================] - 2s 80ms/step - loss: 0.2348 - acc: 0.7733 - val_loss: 0.6524 - val_acc: 0.8267\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3470 - acc: 0.5417 - val_loss: 0.6530 - val_acc: 0.7500\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1809 - acc: 0.8863 - val_loss: 0.6331 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2250 - acc: 0.7962 - val_loss: 0.6604 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2591 - acc: 0.7251 - val_loss: 0.6692 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1775 - acc: 0.8910 - val_loss: 0.6323 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1841 - acc: 0.8815 - val_loss: 0.6620 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2268 - acc: 0.7915 - val_loss: 0.6618 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2814 - acc: 0.6777 - val_loss: 0.6666 - val_acc: 0.7075\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1970 - acc: 0.8531 - val_loss: 0.6648 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1989 - acc: 0.8483 - val_loss: 0.6200 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.1555 - acc: 0.9384 - val_loss: 0.6313 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1912 - acc: 0.8673 - val_loss: 0.6514 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1866 - acc: 0.8720 - val_loss: 0.6488 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2041 - acc: 0.8389 - val_loss: 0.6363 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1719 - acc: 0.9005 - val_loss: 0.6357 - val_acc: 0.9143\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1984 - acc: 0.8483 - val_loss: 0.6220 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1814 - acc: 0.8815 - val_loss: 0.6139 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1974 - acc: 0.8483 - val_loss: 0.6161 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2876 - acc: 0.6777 - val_loss: 0.6517 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2058 - acc: 0.8341 - val_loss: 0.6520 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1880 - acc: 0.8673 - val_loss: 0.6398 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 159ms/step - loss: 0.1901 - acc: 0.8673 - val_loss: 0.6305 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2199 - acc: 0.8057 - val_loss: 0.6698 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2129 - acc: 0.8199 - val_loss: 0.6373 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1562 - acc: 0.9336 - val_loss: 0.6285 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1415 - acc: 0.9526 - val_loss: 0.5931 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2365 - acc: 0.7725 - val_loss: 0.6501 - val_acc: 0.7170\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1413 - acc: 0.9573 - val_loss: 0.5985 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2103 - acc: 0.8246 - val_loss: 0.6311 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.2035 - acc: 0.8398 - val_loss: 0.6341 - val_acc: 0.8571\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1964 - acc: 0.8531 - val_loss: 0.6144 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1797 - acc: 0.8863 - val_loss: 0.6088 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2297 - acc: 0.7867 - val_loss: 0.6329 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1739 - acc: 0.8957 - val_loss: 0.6233 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1193 - acc: 0.9810 - val_loss: 0.5470 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2335 - acc: 0.7820 - val_loss: 0.5922 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1105 - acc: 0.9953 - val_loss: 0.5583 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1961 - acc: 0.8531 - val_loss: 0.6167 - val_acc: 0.8113\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1764 - acc: 0.8768 - val_loss: 0.5731 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1712 - acc: 0.8957 - val_loss: 0.5229 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1510 - acc: 0.9242 - val_loss: 0.5341 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1488 - acc: 0.9242 - val_loss: 0.5497 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2207 - acc: 0.8104 - val_loss: 0.5802 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1829 - acc: 0.8720 - val_loss: 0.5297 - val_acc: 0.9434\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1959 - acc: 0.8531 - val_loss: 0.5671 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2107 - acc: 0.8294 - val_loss: 0.5756 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2561 - acc: 0.7583 - val_loss: 0.6142 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1806 - acc: 0.8768 - val_loss: 0.5638 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2246 - acc: 0.8104 - val_loss: 0.5993 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2141 - acc: 0.8246 - val_loss: 0.6051 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2282 - acc: 0.8009 - val_loss: 0.6077 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1740 - acc: 0.8910 - val_loss: 0.5934 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2580 - acc: 0.7346 - val_loss: 0.6415 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1515 - acc: 0.9336 - val_loss: 0.5640 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1987 - acc: 0.8483 - val_loss: 0.6184 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 159ms/step - loss: 0.2189 - acc: 0.8104 - val_loss: 0.6147 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1765 - acc: 0.8863 - val_loss: 0.5938 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2088 - acc: 0.8294 - val_loss: 0.5928 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2041 - acc: 0.8389 - val_loss: 0.6160 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1675 - acc: 0.9005 - val_loss: 0.5973 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1845 - acc: 0.8720 - val_loss: 0.5805 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2112 - acc: 0.8246 - val_loss: 0.5760 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.2788 - acc: 0.7182 - val_loss: 0.6555 - val_acc: 0.6813\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2172 - acc: 0.8152 - val_loss: 0.6204 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1986 - acc: 0.8483 - val_loss: 0.6320 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2078 - acc: 0.8294 - val_loss: 0.6398 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1387 - acc: 0.9573 - val_loss: 0.6024 - val_acc: 0.9245\n",
      "4/4 [==============================] - 3s 107ms/step - loss: 0.2301 - acc: 0.7899 - val_loss: 0.6542 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2559 - acc: 0.7393 - val_loss: 0.6300 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1352 - acc: 0.9479 - val_loss: 0.5167 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1941 - acc: 0.8531 - val_loss: 0.5582 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2513 - acc: 0.7536 - val_loss: 0.6348 - val_acc: 0.7642\n",
      "8/8 [==============================] - 3s 154ms/step - loss: 0.2316 - acc: 0.7867 - val_loss: 0.6500 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2015 - acc: 0.8389 - val_loss: 0.6108 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2266 - acc: 0.7962 - val_loss: 0.6609 - val_acc: 0.6981\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 0.2274 - acc: 0.7902 - val_loss: 0.6366 - val_acc: 0.8137\n",
      "7/7 [==============================] - 2s 54ms/step - loss: 0.1691 - acc: 0.9045 - val_loss: 0.6176 - val_acc: 0.9400\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2075 - acc: 0.8294 - val_loss: 0.6104 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1889 - acc: 0.8673 - val_loss: 0.6545 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2241 - acc: 0.8009 - val_loss: 0.6099 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1761 - acc: 0.8910 - val_loss: 0.6140 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2113 - acc: 0.8199 - val_loss: 0.6172 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2555 - acc: 0.7441 - val_loss: 0.6365 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2269 - acc: 0.7962 - val_loss: 0.6261 - val_acc: 0.8302\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2005 - acc: 0.8436 - val_loss: 0.6315 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1974 - acc: 0.8483 - val_loss: 0.6384 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.1589 - acc: 0.9194 - val_loss: 0.5994 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1573 - acc: 0.9242 - val_loss: 0.5866 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 105ms/step - loss: 0.2815 - acc: 0.6975 - val_loss: 0.6571 - val_acc: 0.7119\n",
      "8/8 [==============================] - 3s 172ms/step - loss: 0.2157 - acc: 0.8152 - val_loss: 0.6453 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1501 - acc: 0.9384 - val_loss: 0.6224 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1475 - acc: 0.9431 - val_loss: 0.5959 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1659 - acc: 0.9052 - val_loss: 0.5526 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1997 - acc: 0.8436 - val_loss: 0.6126 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1989 - acc: 0.8483 - val_loss: 0.5981 - val_acc: 0.7925\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2051 - acc: 0.8341 - val_loss: 0.6054 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2151 - acc: 0.8246 - val_loss: 0.5994 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1325 - acc: 0.9526 - val_loss: 0.5061 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2289 - acc: 0.8009 - val_loss: 0.6052 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.2227 - acc: 0.8104 - val_loss: 0.6026 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2267 - acc: 0.8030 - val_loss: 0.5933 - val_acc: 0.8283\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4882 - acc: 0.3333 - val_loss: 0.6020 - val_acc: 1.0000\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.2361 - acc: 0.7820 - val_loss: 0.6116 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2395 - acc: 0.7725 - val_loss: 0.6175 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2301 - acc: 0.7915 - val_loss: 0.6172 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1641 - acc: 0.9100 - val_loss: 0.5934 - val_acc: 0.8868\n",
      "8/8 [==============================] - 3s 165ms/step - loss: 0.1817 - acc: 0.8815 - val_loss: 0.6251 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1715 - acc: 0.8957 - val_loss: 0.5844 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2234 - acc: 0.8057 - val_loss: 0.6162 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1744 - acc: 0.8910 - val_loss: 0.6096 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1244 - acc: 0.9810 - val_loss: 0.5532 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1420 - acc: 0.9431 - val_loss: 0.5415 - val_acc: 0.9528\n",
      "6/6 [==============================] - 3s 61ms/step - loss: 0.3227 - acc: 0.6341 - val_loss: 0.6777 - val_acc: 0.5976\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2323 - acc: 0.7915 - val_loss: 0.6117 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1668 - acc: 0.9005 - val_loss: 0.6003 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2579 - acc: 0.7441 - val_loss: 0.6149 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2315 - acc: 0.7915 - val_loss: 0.5825 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2400 - acc: 0.7725 - val_loss: 0.6207 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2257 - acc: 0.7962 - val_loss: 0.6470 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2522 - acc: 0.7441 - val_loss: 0.6442 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1690 - acc: 0.9052 - val_loss: 0.6077 - val_acc: 0.8962\n",
      "6/6 [==============================] - 2s 68ms/step - loss: 0.2265 - acc: 0.7944 - val_loss: 0.6605 - val_acc: 0.6889\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1258 - acc: 0.9810 - val_loss: 0.5576 - val_acc: 0.9434\n",
      "8/8 [==============================] - 3s 154ms/step - loss: 0.2604 - acc: 0.7299 - val_loss: 0.6422 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2361 - acc: 0.7773 - val_loss: 0.6409 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1488 - acc: 0.9384 - val_loss: 0.5721 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2371 - acc: 0.7773 - val_loss: 0.6283 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1908 - acc: 0.8626 - val_loss: 0.5952 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2402 - acc: 0.7692 - val_loss: 0.6365 - val_acc: 0.7885\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2365 - acc: 0.7725 - val_loss: 0.6423 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2175 - acc: 0.8057 - val_loss: 0.6477 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2080 - acc: 0.8294 - val_loss: 0.6715 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2354 - acc: 0.7725 - val_loss: 0.6550 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1337 - acc: 0.9810 - val_loss: 0.6001 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1910 - acc: 0.8626 - val_loss: 0.6338 - val_acc: 0.8585\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2687 - acc: 0.7062 - val_loss: 0.6759 - val_acc: 0.6509\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1932 - acc: 0.8578 - val_loss: 0.6206 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2319 - acc: 0.7820 - val_loss: 0.6495 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1827 - acc: 0.8815 - val_loss: 0.6323 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1775 - acc: 0.8910 - val_loss: 0.6267 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1654 - acc: 0.9100 - val_loss: 0.6062 - val_acc: 0.8774\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2193 - acc: 0.8104 - val_loss: 0.6261 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1878 - acc: 0.8673 - val_loss: 0.6233 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2380 - acc: 0.7773 - val_loss: 0.6497 - val_acc: 0.6981\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2242 - acc: 0.8009 - val_loss: 0.6361 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1667 - acc: 0.9052 - val_loss: 0.6119 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2189 - acc: 0.8104 - val_loss: 0.6215 - val_acc: 0.8019\n",
      "7/7 [==============================] - 3s 57ms/step - loss: 0.2161 - acc: 0.8135 - val_loss: 0.6407 - val_acc: 0.8041\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1856 - acc: 0.8720 - val_loss: 0.6156 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2200 - acc: 0.8057 - val_loss: 0.6472 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2109 - acc: 0.8246 - val_loss: 0.6311 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.1603 - acc: 0.9139 - val_loss: 0.5596 - val_acc: 0.9423\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1806 - acc: 0.8815 - val_loss: 0.6081 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2554 - acc: 0.7393 - val_loss: 0.6264 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2598 - acc: 0.7251 - val_loss: 0.6663 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2081 - acc: 0.8294 - val_loss: 0.6668 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1797 - acc: 0.8863 - val_loss: 0.6300 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1918 - acc: 0.8626 - val_loss: 0.6314 - val_acc: 0.8868\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2238 - acc: 0.7962 - val_loss: 0.6537 - val_acc: 0.8208\n",
      "5/5 [==============================] - 2s 80ms/step - loss: 0.2816 - acc: 0.6757 - val_loss: 0.6822 - val_acc: 0.6081\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.2242 - acc: 0.7962 - val_loss: 0.6486 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1335 - acc: 0.9810 - val_loss: 0.6075 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1753 - acc: 0.8957 - val_loss: 0.6310 - val_acc: 0.8762\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1789 - acc: 0.8863 - val_loss: 0.6159 - val_acc: 0.8491\n",
      "5/5 [==============================] - 3s 82ms/step - loss: 0.2742 - acc: 0.6959 - val_loss: 0.6574 - val_acc: 0.7568\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2280 - acc: 0.7915 - val_loss: 0.6374 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2126 - acc: 0.8199 - val_loss: 0.6451 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1681 - acc: 0.9100 - val_loss: 0.5840 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2056 - acc: 0.8341 - val_loss: 0.6356 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2140 - acc: 0.8199 - val_loss: 0.6440 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1658 - acc: 0.9147 - val_loss: 0.6073 - val_acc: 0.9528\n",
      "5/5 [==============================] - 2s 83ms/step - loss: 0.2058 - acc: 0.8356 - val_loss: 0.6356 - val_acc: 0.8082\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2040 - acc: 0.8389 - val_loss: 0.6237 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2202 - acc: 0.8057 - val_loss: 0.6442 - val_acc: 0.7925\n",
      "6/6 [==============================] - 2s 64ms/step - loss: 0.2164 - acc: 0.8167 - val_loss: 0.6596 - val_acc: 0.7111\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2333 - acc: 0.7820 - val_loss: 0.6357 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2358 - acc: 0.7773 - val_loss: 0.6198 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1865 - acc: 0.8720 - val_loss: 0.6146 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2342 - acc: 0.7773 - val_loss: 0.6493 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2618 - acc: 0.7299 - val_loss: 0.6414 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2344 - acc: 0.7773 - val_loss: 0.6631 - val_acc: 0.7358\n",
      "4/4 [==============================] - 3s 392ms/step - loss: 0.2110 - acc: 0.8235 - val_loss: 0.6669 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1843 - acc: 0.8768 - val_loss: 0.6275 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2358 - acc: 0.7725 - val_loss: 0.6552 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1861 - acc: 0.8768 - val_loss: 0.6576 - val_acc: 0.8381\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2091 - acc: 0.8294 - val_loss: 0.6426 - val_acc: 0.8585\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.2410 - acc: 0.7647 - val_loss: 0.6499 - val_acc: 0.7941\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2187 - acc: 0.8104 - val_loss: 0.6426 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2847 - acc: 0.6682 - val_loss: 0.6776 - val_acc: 0.6604\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2264 - acc: 0.7915 - val_loss: 0.6630 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2288 - acc: 0.7867 - val_loss: 0.6682 - val_acc: 0.7736\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.1794 - acc: 0.8895 - val_loss: 0.6487 - val_acc: 0.8352\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2089 - acc: 0.8294 - val_loss: 0.6476 - val_acc: 0.8585\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2266 - acc: 0.7915 - val_loss: 0.6694 - val_acc: 0.7170\n",
      "6/6 [==============================] - 2s 64ms/step - loss: 0.1933 - acc: 0.8611 - val_loss: 0.6606 - val_acc: 0.8222\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1842 - acc: 0.8815 - val_loss: 0.6512 - val_acc: 0.8868\n",
      "5/5 [==============================] - 2s 80ms/step - loss: 0.2245 - acc: 0.7973 - val_loss: 0.6672 - val_acc: 0.7297\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2012 - acc: 0.8436 - val_loss: 0.6510 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1527 - acc: 0.9479 - val_loss: 0.6534 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.1731 - acc: 0.9005 - val_loss: 0.6327 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1824 - acc: 0.8815 - val_loss: 0.6223 - val_acc: 0.9434\n",
      "3/3 [==============================] - 2s 150ms/step - loss: 0.2203 - acc: 0.8046 - val_loss: 0.6456 - val_acc: 0.8182\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.2280 - acc: 0.7915 - val_loss: 0.6344 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2193 - acc: 0.8057 - val_loss: 0.6419 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2106 - acc: 0.8246 - val_loss: 0.6452 - val_acc: 0.7830\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.2109 - acc: 0.8246 - val_loss: 0.6308 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1974 - acc: 0.8483 - val_loss: 0.6134 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2774 - acc: 0.6967 - val_loss: 0.6411 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2134 - acc: 0.8199 - val_loss: 0.6409 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1991 - acc: 0.8483 - val_loss: 0.6565 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2039 - acc: 0.8389 - val_loss: 0.6374 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1819 - acc: 0.8815 - val_loss: 0.6455 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2215 - acc: 0.8009 - val_loss: 0.6562 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2475 - acc: 0.7488 - val_loss: 0.6553 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1693 - acc: 0.9100 - val_loss: 0.6467 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2153 - acc: 0.8152 - val_loss: 0.6451 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1729 - acc: 0.9005 - val_loss: 0.6245 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2016 - acc: 0.8436 - val_loss: 0.6481 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2390 - acc: 0.7678 - val_loss: 0.6607 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2666 - acc: 0.7109 - val_loss: 0.6672 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2202 - acc: 0.8057 - val_loss: 0.6511 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1612 - acc: 0.9242 - val_loss: 0.6330 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 45ms/step - loss: 0.2405 - acc: 0.7630 - val_loss: 0.6633 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1498 - acc: 0.9431 - val_loss: 0.6129 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1612 - acc: 0.9147 - val_loss: 0.5944 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2607 - acc: 0.7299 - val_loss: 0.6366 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.1705 - acc: 0.9024 - val_loss: 0.6247 - val_acc: 0.8544\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.2437 - acc: 0.7659 - val_loss: 0.6333 - val_acc: 0.7745\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1955 - acc: 0.8531 - val_loss: 0.6051 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1834 - acc: 0.8720 - val_loss: 0.5855 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2203 - acc: 0.8057 - val_loss: 0.6225 - val_acc: 0.8208\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.2111 - acc: 0.8250 - val_loss: 0.6282 - val_acc: 0.8100\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1411 - acc: 0.9573 - val_loss: 0.6038 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1806 - acc: 0.8815 - val_loss: 0.5968 - val_acc: 0.9057\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2167 - acc: 0.8152 - val_loss: 0.6276 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1878 - acc: 0.8673 - val_loss: 0.6220 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2513 - acc: 0.7536 - val_loss: 0.6243 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2279 - acc: 0.7915 - val_loss: 0.6303 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1426 - acc: 0.9479 - val_loss: 0.5725 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1392 - acc: 0.9526 - val_loss: 0.5728 - val_acc: 0.9245\n",
      "3/3 [==============================] - 2s 166ms/step - loss: 0.1755 - acc: 0.8864 - val_loss: 0.5317 - val_acc: 0.9318\n",
      "6/6 [==============================] - 2s 65ms/step - loss: 0.2172 - acc: 0.8111 - val_loss: 0.6026 - val_acc: 0.8667\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1952 - acc: 0.8531 - val_loss: 0.6068 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1875 - acc: 0.8673 - val_loss: 0.5865 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2047 - acc: 0.8341 - val_loss: 0.6100 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2423 - acc: 0.7725 - val_loss: 0.6186 - val_acc: 0.7736\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3163 - acc: 0.6296 - val_loss: 0.6387 - val_acc: 0.7692\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1852 - acc: 0.8768 - val_loss: 0.6149 - val_acc: 0.8962\n",
      "7/7 [==============================] - 2s 61ms/step - loss: 0.2468 - acc: 0.7571 - val_loss: 0.6473 - val_acc: 0.7333\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2817 - acc: 0.6919 - val_loss: 0.6698 - val_acc: 0.6321\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1761 - acc: 0.8910 - val_loss: 0.6062 - val_acc: 0.9151\n",
      "8/8 [==============================] - 3s 165ms/step - loss: 0.1359 - acc: 0.9573 - val_loss: 0.4748 - val_acc: 0.9623\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2025 - acc: 0.8436 - val_loss: 0.6018 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1443 - acc: 0.9479 - val_loss: 0.6025 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1703 - acc: 0.8957 - val_loss: 0.5461 - val_acc: 0.9245\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2067 - acc: 0.8322 - val_loss: 0.6127 - val_acc: 0.8108\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2295 - acc: 0.7962 - val_loss: 0.6131 - val_acc: 0.7736\n",
      "8/8 [==============================] - 3s 169ms/step - loss: 0.1259 - acc: 0.9621 - val_loss: 0.5099 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1747 - acc: 0.8910 - val_loss: 0.4770 - val_acc: 0.9717\n",
      "5/5 [==============================] - 2s 83ms/step - loss: 0.3103 - acc: 0.6667 - val_loss: 0.6505 - val_acc: 0.6774\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1574 - acc: 0.9100 - val_loss: 0.5692 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2464 - acc: 0.7773 - val_loss: 0.5801 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1348 - acc: 0.9479 - val_loss: 0.5224 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2520 - acc: 0.7583 - val_loss: 0.6115 - val_acc: 0.7736\n",
      "6/6 [==============================] - 2s 66ms/step - loss: 0.1606 - acc: 0.9097 - val_loss: 0.5598 - val_acc: 0.8590\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2360 - acc: 0.7820 - val_loss: 0.5978 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1780 - acc: 0.8768 - val_loss: 0.5527 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1913 - acc: 0.8578 - val_loss: 0.5619 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2079 - acc: 0.8294 - val_loss: 0.5750 - val_acc: 0.9340\n",
      "8/8 [==============================] - 3s 163ms/step - loss: 0.1848 - acc: 0.8720 - val_loss: 0.6081 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2538 - acc: 0.7488 - val_loss: 0.6230 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2104 - acc: 0.8246 - val_loss: 0.6387 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2000 - acc: 0.8436 - val_loss: 0.6124 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1709 - acc: 0.8957 - val_loss: 0.5786 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1992 - acc: 0.8436 - val_loss: 0.6375 - val_acc: 0.7830\n",
      "7/7 [==============================] - 3s 187ms/step - loss: 0.2190 - acc: 0.8122 - val_loss: 0.6393 - val_acc: 0.7582\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1531 - acc: 0.9289 - val_loss: 0.5848 - val_acc: 0.9340\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2477 - acc: 0.7583 - val_loss: 0.6329 - val_acc: 0.7358\n",
      "5/5 [==============================] - 2s 79ms/step - loss: 0.1925 - acc: 0.8591 - val_loss: 0.6032 - val_acc: 0.8649\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1434 - acc: 0.9431 - val_loss: 0.5732 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1842 - acc: 0.8720 - val_loss: 0.5839 - val_acc: 0.8679\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1714 - acc: 0.8910 - val_loss: 0.5647 - val_acc: 0.9434\n",
      "7/7 [==============================] - 2s 52ms/step - loss: 0.1840 - acc: 0.8732 - val_loss: 0.6255 - val_acc: 0.7864\n",
      "5/5 [==============================] - 2s 80ms/step - loss: 0.1596 - acc: 0.9133 - val_loss: 0.5536 - val_acc: 0.9733\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1944 - acc: 0.8531 - val_loss: 0.5605 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.2871 - acc: 0.6942 - val_loss: 0.6080 - val_acc: 0.8252\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2467 - acc: 0.7630 - val_loss: 0.6400 - val_acc: 0.7264\n",
      "8/8 [==============================] - 3s 173ms/step - loss: 0.2165 - acc: 0.8152 - val_loss: 0.6125 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2090 - acc: 0.8294 - val_loss: 0.6372 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1916 - acc: 0.8626 - val_loss: 0.5955 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2239 - acc: 0.8009 - val_loss: 0.6427 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2445 - acc: 0.7630 - val_loss: 0.6470 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2595 - acc: 0.7346 - val_loss: 0.6385 - val_acc: 0.7830\n",
      "8/8 [==============================] - 3s 177ms/step - loss: 0.2264 - acc: 0.7962 - val_loss: 0.6267 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2109 - acc: 0.8246 - val_loss: 0.6421 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1524 - acc: 0.9384 - val_loss: 0.6365 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2583 - acc: 0.7299 - val_loss: 0.6488 - val_acc: 0.8019\n",
      "5/5 [==============================] - 2s 79ms/step - loss: 0.2608 - acc: 0.7248 - val_loss: 0.6607 - val_acc: 0.7297\n",
      "5/5 [==============================] - 2s 80ms/step - loss: 0.2358 - acc: 0.7733 - val_loss: 0.6393 - val_acc: 0.8267\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3513 - acc: 0.5417 - val_loss: 0.6581 - val_acc: 0.7500\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1803 - acc: 0.8863 - val_loss: 0.6323 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.2268 - acc: 0.7962 - val_loss: 0.6316 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2608 - acc: 0.7251 - val_loss: 0.6583 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1783 - acc: 0.8910 - val_loss: 0.6297 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1820 - acc: 0.8815 - val_loss: 0.6403 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2272 - acc: 0.7915 - val_loss: 0.6614 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2830 - acc: 0.6777 - val_loss: 0.6666 - val_acc: 0.7075\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1961 - acc: 0.8531 - val_loss: 0.6536 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1990 - acc: 0.8483 - val_loss: 0.6418 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1543 - acc: 0.9384 - val_loss: 0.6294 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1894 - acc: 0.8673 - val_loss: 0.6532 - val_acc: 0.8208\n",
      "8/8 [==============================] - 3s 165ms/step - loss: 0.1877 - acc: 0.8720 - val_loss: 0.6600 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2029 - acc: 0.8389 - val_loss: 0.6368 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1697 - acc: 0.9005 - val_loss: 0.6085 - val_acc: 0.9143\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1993 - acc: 0.8483 - val_loss: 0.6361 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.1796 - acc: 0.8815 - val_loss: 0.5995 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1983 - acc: 0.8483 - val_loss: 0.5957 - val_acc: 0.9057\n",
      "8/8 [==============================] - 3s 163ms/step - loss: 0.2871 - acc: 0.6777 - val_loss: 0.6571 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2063 - acc: 0.8341 - val_loss: 0.6504 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1889 - acc: 0.8673 - val_loss: 0.6397 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1879 - acc: 0.8673 - val_loss: 0.6152 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2209 - acc: 0.8057 - val_loss: 0.6477 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2115 - acc: 0.8199 - val_loss: 0.6071 - val_acc: 0.8585\n",
      "8/8 [==============================] - 3s 172ms/step - loss: 0.1514 - acc: 0.9336 - val_loss: 0.6068 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1418 - acc: 0.9526 - val_loss: 0.5728 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2392 - acc: 0.7725 - val_loss: 0.6484 - val_acc: 0.7170\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1328 - acc: 0.9573 - val_loss: 0.5178 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2107 - acc: 0.8246 - val_loss: 0.6005 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 58ms/step - loss: 0.2074 - acc: 0.8398 - val_loss: 0.5521 - val_acc: 0.8571\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1943 - acc: 0.8531 - val_loss: 0.5715 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1771 - acc: 0.8863 - val_loss: 0.5782 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2332 - acc: 0.7867 - val_loss: 0.6004 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1689 - acc: 0.8957 - val_loss: 0.5968 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1077 - acc: 0.9810 - val_loss: 0.4626 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2390 - acc: 0.7820 - val_loss: 0.5496 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.0831 - acc: 0.9953 - val_loss: 0.4599 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2038 - acc: 0.8531 - val_loss: 0.5659 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.1782 - acc: 0.8768 - val_loss: 0.4963 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1804 - acc: 0.8957 - val_loss: 0.4844 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1491 - acc: 0.9242 - val_loss: 0.4924 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1445 - acc: 0.9242 - val_loss: 0.5073 - val_acc: 0.8962\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2242 - acc: 0.8104 - val_loss: 0.5849 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1846 - acc: 0.8720 - val_loss: 0.5569 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1955 - acc: 0.8531 - val_loss: 0.5620 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2088 - acc: 0.8294 - val_loss: 0.5966 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2495 - acc: 0.7583 - val_loss: 0.6255 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1818 - acc: 0.8768 - val_loss: 0.5839 - val_acc: 0.8491\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.2190 - acc: 0.8104 - val_loss: 0.6221 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2105 - acc: 0.8246 - val_loss: 0.6309 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2239 - acc: 0.8009 - val_loss: 0.6381 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1750 - acc: 0.8910 - val_loss: 0.6152 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2594 - acc: 0.7346 - val_loss: 0.6413 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1520 - acc: 0.9336 - val_loss: 0.5842 - val_acc: 0.9528\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1981 - acc: 0.8483 - val_loss: 0.6161 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2187 - acc: 0.8104 - val_loss: 0.5976 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1763 - acc: 0.8863 - val_loss: 0.5715 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2097 - acc: 0.8294 - val_loss: 0.5808 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2037 - acc: 0.8389 - val_loss: 0.6079 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1681 - acc: 0.9005 - val_loss: 0.6085 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 173ms/step - loss: 0.1843 - acc: 0.8720 - val_loss: 0.5955 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2111 - acc: 0.8246 - val_loss: 0.6043 - val_acc: 0.8396\n",
      "7/7 [==============================] - 2s 56ms/step - loss: 0.2708 - acc: 0.7182 - val_loss: 0.6483 - val_acc: 0.6813\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2160 - acc: 0.8152 - val_loss: 0.6049 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1970 - acc: 0.8483 - val_loss: 0.6268 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2101 - acc: 0.8294 - val_loss: 0.6349 - val_acc: 0.7547\n",
      "8/8 [==============================] - 3s 166ms/step - loss: 0.1357 - acc: 0.9573 - val_loss: 0.5777 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 111ms/step - loss: 0.2332 - acc: 0.7899 - val_loss: 0.6398 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2651 - acc: 0.7393 - val_loss: 0.6097 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1289 - acc: 0.9479 - val_loss: 0.4747 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1957 - acc: 0.8531 - val_loss: 0.5958 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2473 - acc: 0.7536 - val_loss: 0.6452 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2296 - acc: 0.7867 - val_loss: 0.6544 - val_acc: 0.7453\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.2040 - acc: 0.8389 - val_loss: 0.6382 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2243 - acc: 0.7962 - val_loss: 0.6695 - val_acc: 0.6981\n",
      "7/7 [==============================] - 2s 57ms/step - loss: 0.2270 - acc: 0.7902 - val_loss: 0.6577 - val_acc: 0.8137\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.1714 - acc: 0.9045 - val_loss: 0.6403 - val_acc: 0.9400\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2079 - acc: 0.8294 - val_loss: 0.6452 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1895 - acc: 0.8673 - val_loss: 0.6642 - val_acc: 0.7170\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2220 - acc: 0.8009 - val_loss: 0.6389 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1783 - acc: 0.8910 - val_loss: 0.6446 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2129 - acc: 0.8199 - val_loss: 0.6373 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2513 - acc: 0.7441 - val_loss: 0.6498 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2250 - acc: 0.7962 - val_loss: 0.6476 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2008 - acc: 0.8436 - val_loss: 0.6480 - val_acc: 0.7830\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1990 - acc: 0.8483 - val_loss: 0.6486 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1617 - acc: 0.9194 - val_loss: 0.6230 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1609 - acc: 0.9242 - val_loss: 0.6179 - val_acc: 0.9245\n",
      "4/4 [==============================] - 2s 112ms/step - loss: 0.2760 - acc: 0.6975 - val_loss: 0.6591 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2149 - acc: 0.8152 - val_loss: 0.6562 - val_acc: 0.7453\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1519 - acc: 0.9384 - val_loss: 0.6366 - val_acc: 0.8302\n",
      "8/8 [==============================] - 3s 47ms/step - loss: 0.1486 - acc: 0.9431 - val_loss: 0.6005 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 53ms/step - loss: 0.1666 - acc: 0.9052 - val_loss: 0.5759 - val_acc: 0.9057\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2004 - acc: 0.8436 - val_loss: 0.6222 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1987 - acc: 0.8483 - val_loss: 0.6146 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 52ms/step - loss: 0.2053 - acc: 0.8341 - val_loss: 0.6209 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2127 - acc: 0.8246 - val_loss: 0.6200 - val_acc: 0.8113\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.1376 - acc: 0.9526 - val_loss: 0.5664 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2244 - acc: 0.8009 - val_loss: 0.6330 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2196 - acc: 0.8104 - val_loss: 0.6293 - val_acc: 0.7925\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2216 - acc: 0.8030 - val_loss: 0.6213 - val_acc: 0.8283\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4864 - acc: 0.3333 - val_loss: 0.5545 - val_acc: 1.0000\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2330 - acc: 0.7820 - val_loss: 0.6318 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 50ms/step - loss: 0.2379 - acc: 0.7725 - val_loss: 0.6411 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2270 - acc: 0.7915 - val_loss: 0.6410 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1668 - acc: 0.9100 - val_loss: 0.6195 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1820 - acc: 0.8815 - val_loss: 0.6406 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1732 - acc: 0.8957 - val_loss: 0.6076 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2220 - acc: 0.8057 - val_loss: 0.6319 - val_acc: 0.7830\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1763 - acc: 0.8910 - val_loss: 0.6267 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1296 - acc: 0.9810 - val_loss: 0.5886 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1470 - acc: 0.9431 - val_loss: 0.5813 - val_acc: 0.9528\n",
      "6/6 [==============================] - 2s 66ms/step - loss: 0.3168 - acc: 0.6341 - val_loss: 0.6752 - val_acc: 0.5976\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2284 - acc: 0.7915 - val_loss: 0.6295 - val_acc: 0.8113\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1702 - acc: 0.9005 - val_loss: 0.6221 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2539 - acc: 0.7441 - val_loss: 0.6314 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2277 - acc: 0.7915 - val_loss: 0.6150 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2390 - acc: 0.7725 - val_loss: 0.6316 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2248 - acc: 0.7962 - val_loss: 0.6469 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2522 - acc: 0.7441 - val_loss: 0.6449 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1690 - acc: 0.9052 - val_loss: 0.6125 - val_acc: 0.8962\n",
      "6/6 [==============================] - 2s 61ms/step - loss: 0.2258 - acc: 0.7944 - val_loss: 0.6638 - val_acc: 0.6889\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1278 - acc: 0.9810 - val_loss: 0.5795 - val_acc: 0.9434\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2589 - acc: 0.7299 - val_loss: 0.6503 - val_acc: 0.7547\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.2347 - acc: 0.7773 - val_loss: 0.6450 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 44ms/step - loss: 0.1492 - acc: 0.9384 - val_loss: 0.5729 - val_acc: 0.9528\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2394 - acc: 0.7773 - val_loss: 0.6278 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 45ms/step - loss: 0.1920 - acc: 0.8626 - val_loss: 0.5973 - val_acc: 0.9151\n",
      "7/7 [==============================] - 2s 53ms/step - loss: 0.2387 - acc: 0.7692 - val_loss: 0.6379 - val_acc: 0.7885\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2359 - acc: 0.7725 - val_loss: 0.6453 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2185 - acc: 0.8057 - val_loss: 0.6383 - val_acc: 0.7642\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2080 - acc: 0.8294 - val_loss: 0.6675 - val_acc: 0.6887\n",
      "8/8 [==============================] - 3s 168ms/step - loss: 0.2362 - acc: 0.7725 - val_loss: 0.6461 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1265 - acc: 0.9810 - val_loss: 0.5445 - val_acc: 0.9717\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1911 - acc: 0.8626 - val_loss: 0.6093 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2740 - acc: 0.7062 - val_loss: 0.6735 - val_acc: 0.6509\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1922 - acc: 0.8578 - val_loss: 0.6091 - val_acc: 0.9151\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2320 - acc: 0.7820 - val_loss: 0.6506 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1835 - acc: 0.8815 - val_loss: 0.6400 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.1787 - acc: 0.8910 - val_loss: 0.6340 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1664 - acc: 0.9100 - val_loss: 0.6077 - val_acc: 0.8774\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2191 - acc: 0.8104 - val_loss: 0.6370 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1891 - acc: 0.8673 - val_loss: 0.6362 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2379 - acc: 0.7773 - val_loss: 0.6544 - val_acc: 0.6981\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2215 - acc: 0.8009 - val_loss: 0.6503 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1667 - acc: 0.9052 - val_loss: 0.6226 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2193 - acc: 0.8104 - val_loss: 0.6285 - val_acc: 0.8019\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.2154 - acc: 0.8135 - val_loss: 0.6536 - val_acc: 0.8041\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1867 - acc: 0.8720 - val_loss: 0.6321 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.2194 - acc: 0.8057 - val_loss: 0.6570 - val_acc: 0.7830\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2099 - acc: 0.8246 - val_loss: 0.6437 - val_acc: 0.8396\n",
      "7/7 [==============================] - 3s 56ms/step - loss: 0.1612 - acc: 0.9139 - val_loss: 0.5714 - val_acc: 0.9423\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1796 - acc: 0.8815 - val_loss: 0.6129 - val_acc: 0.8679\n",
      "8/8 [==============================] - 2s 53ms/step - loss: 0.2523 - acc: 0.7393 - val_loss: 0.6292 - val_acc: 0.8302\n",
      "8/8 [==============================] - 2s 59ms/step - loss: 0.2593 - acc: 0.7251 - val_loss: 0.6700 - val_acc: 0.7264\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2072 - acc: 0.8294 - val_loss: 0.6615 - val_acc: 0.6887\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1798 - acc: 0.8863 - val_loss: 0.5997 - val_acc: 0.9245\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1922 - acc: 0.8626 - val_loss: 0.6136 - val_acc: 0.8868\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2234 - acc: 0.7962 - val_loss: 0.6411 - val_acc: 0.8208\n",
      "5/5 [==============================] - 2s 81ms/step - loss: 0.2817 - acc: 0.6757 - val_loss: 0.6802 - val_acc: 0.6081\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2242 - acc: 0.7962 - val_loss: 0.6251 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1277 - acc: 0.9810 - val_loss: 0.5404 - val_acc: 0.9811\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1743 - acc: 0.8957 - val_loss: 0.5918 - val_acc: 0.8762\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.1775 - acc: 0.8863 - val_loss: 0.5469 - val_acc: 0.8491\n",
      "5/5 [==============================] - 2s 83ms/step - loss: 0.2785 - acc: 0.6959 - val_loss: 0.6537 - val_acc: 0.7568\n",
      "8/8 [==============================] - 2s 51ms/step - loss: 0.2315 - acc: 0.7915 - val_loss: 0.6083 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2131 - acc: 0.8199 - val_loss: 0.6385 - val_acc: 0.8585\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.1672 - acc: 0.9100 - val_loss: 0.5685 - val_acc: 0.9906\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2055 - acc: 0.8341 - val_loss: 0.6296 - val_acc: 0.8019\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2143 - acc: 0.8199 - val_loss: 0.6441 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1646 - acc: 0.9147 - val_loss: 0.5938 - val_acc: 0.9528\n",
      "5/5 [==============================] - 2s 82ms/step - loss: 0.2053 - acc: 0.8356 - val_loss: 0.6209 - val_acc: 0.8082\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2050 - acc: 0.8389 - val_loss: 0.6229 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2209 - acc: 0.8057 - val_loss: 0.6498 - val_acc: 0.7925\n",
      "6/6 [==============================] - 2s 64ms/step - loss: 0.2156 - acc: 0.8167 - val_loss: 0.6602 - val_acc: 0.7111\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2349 - acc: 0.7820 - val_loss: 0.6357 - val_acc: 0.8396\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2361 - acc: 0.7773 - val_loss: 0.6117 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1866 - acc: 0.8720 - val_loss: 0.6013 - val_acc: 0.8962\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2339 - acc: 0.7773 - val_loss: 0.6442 - val_acc: 0.8019\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2662 - acc: 0.7299 - val_loss: 0.6377 - val_acc: 0.7736\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2356 - acc: 0.7773 - val_loss: 0.6642 - val_acc: 0.7358\n",
      "4/4 [==============================] - 3s 112ms/step - loss: 0.2117 - acc: 0.8235 - val_loss: 0.6688 - val_acc: 0.7119\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1841 - acc: 0.8768 - val_loss: 0.6195 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.2361 - acc: 0.7725 - val_loss: 0.6477 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 49ms/step - loss: 0.1856 - acc: 0.8768 - val_loss: 0.6485 - val_acc: 0.8381\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2082 - acc: 0.8294 - val_loss: 0.6264 - val_acc: 0.8585\n",
      "7/7 [==============================] - 2s 57ms/step - loss: 0.2414 - acc: 0.7647 - val_loss: 0.6413 - val_acc: 0.7941\n",
      "8/8 [==============================] - 3s 49ms/step - loss: 0.2185 - acc: 0.8104 - val_loss: 0.6345 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2857 - acc: 0.6682 - val_loss: 0.6764 - val_acc: 0.6604\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2266 - acc: 0.7915 - val_loss: 0.6554 - val_acc: 0.7925\n",
      "8/8 [==============================] - 2s 50ms/step - loss: 0.2290 - acc: 0.7867 - val_loss: 0.6694 - val_acc: 0.7736\n",
      "7/7 [==============================] - 2s 55ms/step - loss: 0.1791 - acc: 0.8895 - val_loss: 0.6429 - val_acc: 0.8352\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2092 - acc: 0.8294 - val_loss: 0.6430 - val_acc: 0.8585\n",
      "8/8 [==============================] - 3s 48ms/step - loss: 0.2269 - acc: 0.7915 - val_loss: 0.6688 - val_acc: 0.7170\n",
      "6/6 [==============================] - 2s 64ms/step - loss: 0.1942 - acc: 0.8611 - val_loss: 0.6578 - val_acc: 0.8222\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1833 - acc: 0.8815 - val_loss: 0.6456 - val_acc: 0.8868\n",
      "5/5 [==============================] - 2s 85ms/step - loss: 0.2256 - acc: 0.7973 - val_loss: 0.6612 - val_acc: 0.7297\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.2010 - acc: 0.8436 - val_loss: 0.6422 - val_acc: 0.8491\n",
      "8/8 [==============================] - 2s 46ms/step - loss: 0.1497 - acc: 0.9479 - val_loss: 0.6481 - val_acc: 0.8396\n",
      "8/8 [==============================] - 3s 46ms/step - loss: 0.1721 - acc: 0.9005 - val_loss: 0.6129 - val_acc: 0.9245\n",
      "8/8 [==============================] - 2s 48ms/step - loss: 0.1804 - acc: 0.8815 - val_loss: 0.5871 - val_acc: 0.9434\n",
      "3/3 [==============================] - 2s 167ms/step - loss: 0.2216 - acc: 0.8046 - val_loss: 0.6319 - val_acc: 0.8182\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2299 - acc: 0.7915 - val_loss: 0.6166 - val_acc: 0.8208\n",
      "8/8 [==============================] - 2s 47ms/step - loss: 0.2206 - acc: 0.8057 - val_loss: 0.6298 - val_acc: 0.8113\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "\n",
    "log_df = pd.DataFrame(columns=['loss', 'acc', 'val_loss', 'val_acc'])       # 전체 진행은 3-1과 동일함\n",
    "epoch = 5\n",
    "isFirst = True\n",
    "\n",
    "x_valid = list()\n",
    "y_valid = list()\n",
    "\n",
    "for i in range(epoch):\n",
    "  for x, y in train_data:\n",
    "    x_train = x[:int(len(x)*0.5)]\n",
    "    x_test = x[int(len(x)*0.5):int(len(x)*0.75)]\n",
    "    y_train = y[:int(len(y)*0.5)]\n",
    "    y_test = y[int(len(y)*0.5):int(len(y)*0.75)]\n",
    "\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(20, input_shape=(10, 20)))               # 위의 실행에서 모델이 충분히 복잡하지 못하다 판단, 모델을 조금 더 복잡하게 만듦\n",
    "    model.add(Dense(128, activation='relu'))                # hidden layer에 dense층을 여러개 삽입, 마지막 층은 sigmoid 이용한 한개의 dense층\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    if not isFirst:\n",
    "      model.load_weights('model_weight/v3')\n",
    "    else:\n",
    "      isFirst = False\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=1, batch_size=30, verbose=1, validation_data = (x_test, y_test), class_weight={True:0.8, False:0.2})\n",
    "    log_df = pd.concat([log_df, pd.DataFrame(history.history)])\n",
    "    model.save_weights('model_weight/v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25c145a9f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(221)                                        # 3-1과 동일한 방식의 그래프 그리기\n",
    "plt.plot(list(log_df['loss']), label='loss')\n",
    "plt.plot(list(log_df['val_loss']), label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(list(log_df['acc']), label='acc')\n",
    "plt.plot(list(log_df['val_acc']), label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "log_df.to_csv(\"lstm2/log_for_test.csv\", index=False)\n",
    "\n",
    "l1 = log_df[:299]\n",
    "l2 = log_df[299:598]\n",
    "l3 = log_df[598:897]\n",
    "l4 = log_df[897:1196]\n",
    "l5 = log_df[1196:]\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot([1,2,3,4,5], [l1['loss'].mean(), l2['loss'].mean(), l3['loss'].mean(), l4['loss'].mean(), l5['loss'].mean()], label='loss')\n",
    "plt.plot([1,2,3,4,5], [l1['val_loss'].mean(), l2['val_loss'].mean(), l3['val_loss'].mean(), l4['val_loss'].mean(), l5['val_loss'].mean()], label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot([1,2,3,4,5], [l1['acc'].mean(), l2['acc'].mean(), l3['acc'].mean(), l4['acc'].mean(), l5['acc'].mean()], label='acc')\n",
    "plt.plot([1,2,3,4,5], [l1['val_acc'].mean(), l2['val_acc'].mean(), l3['val_acc'].mean(), l4['val_acc'].mean(), l5['val_acc'].mean()], label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3-3. Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                3280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               2688      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,849\n",
      "Trainable params: 16,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "RESULTS\n",
      "True Positive\tFalse Negative\tFalse Positive\tTrue Negative\n",
      "313\t\t\t\t5678\t\t\t\t1164\t\t\t\t23437\n",
      "------------------------------------------------------------\n",
      "\t\t precision\t\trecall\t\t\tf1 score\t\tsupport\n",
      "risk    :0.21\t\t\t0.05\t\t\t0.08\t\t\t5991\n",
      "no risk :0.80\t\t\t0.95\t\t\t0.87\t\t\t24601\n",
      "accuracy: 0.7763467573221757\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "\n",
    "x = np.load('x_valid.npy', allow_pickle=True)       # valid 부분\n",
    "y = np.load('y_valid.npy', allow_pickle=True)\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(10, 20)))           # 3-2모델을 가져옴\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "model.load_weights('model_weights/v3')              # 3-2에서 학습한 가중치 복구\n",
    "model.save_weights('model_weights/v3')\n",
    "\n",
    "results = {'TP' : 0, 'TN' : 0, 'FP' : 0, 'FN' : 0}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    predicts = model.predict(x[i], verbose=0)\n",
    "    predicts = [True if x>0.5 else False for [x] in predicts]\n",
    "    for j in range(len(predicts)):\n",
    "        if y[i][j] and predicts[j]:\n",
    "            results['TP'] += 1      # TP = true positive\n",
    "        elif y[i][j]:\n",
    "            results['FN'] += 1      # FN = false negative\n",
    "        elif predicts[j]:\n",
    "            results['FP'] += 1      # FP = false positive\n",
    "        else:\n",
    "            results['TN'] += 1      # TN = true negative\n",
    "\n",
    "print('\\nRESULTS')              # 출력부분, precision / recall / f1 score / support 직접 계산\n",
    "print('True Positive\\tFalse Negative\\tFalse Positive\\tTrue Negative')\n",
    "print('{}\\t\\t\\t\\t{}\\t\\t\\t\\t{}\\t\\t\\t\\t{}'.format(results['TP'], results['FN'], results['FP'],  results['TN']))\n",
    "print('------------------------------------------------------------')\n",
    "print('\\t\\t precision\\t\\trecall\\t\\t\\tf1 score\\t\\tsupport')\n",
    "precision = results['TP'] / (results['TP'] + results['FP'])\n",
    "recall = results['TP'] / (results['TP'] + results['FN'])\n",
    "print('risk    :{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{}'.format(precision, recall, 2*precision*recall / (precision+recall), results['TP'] + results['FN']))\n",
    "precision = results['TN'] / (results['TN'] + results['FN'])\n",
    "recall = results['TN'] / (results['TN'] + results['FP'])\n",
    "print('no risk :{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{}'.format(precision, recall, 2*precision*recall / (precision+recall), results['FP'] + results['TN']))\n",
    "print('accuracy: {}'.format((results['TP'] + results['TN']) / (results['TP'] + results['TN'] + results['FP'] + results['FN'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50baee28870d358e64fb0672623b67d817f95823ed506d49d40f53415dd6866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
