{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = ['?', '??', 'N/A', 'NA', 'nan', 'NaN', '-nan', '-NaN', 'null', '-']\n",
    "x_train = pd.read_csv('./data/track1/features/x_train_normal.csv', na_values = null_values)\n",
    "x_valid = pd.read_csv('./data/track1/features/x_valid_normal.csv', na_values = null_values)\n",
    "x_test = pd.read_csv('./data/track1/features/x_test_normal.csv', na_values = null_values)\n",
    "y_train = pd.read_csv('./data/track1/features/y_train_normal.csv', na_values = null_values)\n",
    "y_valid = pd.read_csv('./data/track1/features/y_valid_normal.csv', na_values = null_values)\n",
    "y_test = pd.read_csv('./data/track1/features/y_test_normal.csv', na_values = null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = x_train.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_valid_features = x_valid.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "x_test_features = x_test.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "y_train_bool = y_train['Y'] <-2.0\n",
    "y_valid_bool = y_valid['Y'] <-2.0\n",
    "y_test_bool = y_test['Y'] <-2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tree Base Simple Classifers\n",
    "Tree 기반 분류 모델인 Decision Tree와 Random Forest 를 사용하여 재무데이터로 리스크 주식 분류 모델을 만들기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-1 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test/models/decisionTree.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier(\n",
    "    max_depth=15,\n",
    "    min_samples_split=100,\n",
    "    class_weight={True: 10, False: 1}\n",
    ")\n",
    "decisionTree.fit(x_train_features, y_train_bool)\n",
    "\n",
    "joblib.dump(decisionTree, './test/models/decisionTree.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.98      0.34      0.50     63391\n",
      "        risk       0.22      0.96      0.36     12724\n",
      "\n",
      "    accuracy                           0.44     76115\n",
      "   macro avg       0.60      0.65      0.43     76115\n",
      "weighted avg       0.85      0.44      0.48     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = decisionTree.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_train_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.90      0.31      0.46     21052\n",
      "        risk       0.20      0.83      0.32      4344\n",
      "\n",
      "    accuracy                           0.40     25396\n",
      "   macro avg       0.55      0.57      0.39     25396\n",
      "weighted avg       0.78      0.40      0.44     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = decisionTree.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_valid_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.90      0.31      0.46     21040\n",
      "        risk       0.20      0.83      0.32      4311\n",
      "\n",
      "    accuracy                           0.40     25351\n",
      "   macro avg       0.55      0.57      0.39     25351\n",
      "weighted avg       0.78      0.40      0.43     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = decisionTree.predict(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test/models/randomForest.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    criterion='entropy', \n",
    "    min_samples_split = 100,\n",
    "    bootstrap=True,\n",
    "    max_depth=20,\n",
    "    class_weight={True: 10, False: 1}\n",
    "    )\n",
    "rf.fit(x_train_features, y_train_bool)\n",
    "\n",
    "joblib.dump(rf, './test/models/randomForest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.97      0.40      0.57     63391\n",
      "        risk       0.24      0.95      0.38     12724\n",
      "\n",
      "    accuracy                           0.49     76115\n",
      "   macro avg       0.61      0.67      0.48     76115\n",
      "weighted avg       0.85      0.49      0.54     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = rf.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_train_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.38      0.53     21052\n",
      "        risk       0.22      0.83      0.34      4344\n",
      "\n",
      "    accuracy                           0.45     25396\n",
      "   macro avg       0.56      0.60      0.44     25396\n",
      "weighted avg       0.79      0.45      0.50     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.38      0.54     21040\n",
      "        risk       0.21      0.82      0.34      4311\n",
      "\n",
      "    accuracy                           0.46     25351\n",
      "   macro avg       0.56      0.60      0.44     25351\n",
      "weighted avg       0.79      0.46      0.50     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Selection\n",
    "Forward, Backward 방식으로 Feature Selection을 시도 하였음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여기 채워 주면 될듯"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LightGBM and Weak Bagging\n",
    "gradient boosting 기반의 tree classifier인 lightGBM 모델을 사용하여 Risk 종목 분류를 하였습니다\n",
    "LightGBM 모델들을 Feature selection 결과를 기반으로 Bagging의 아이디어를 활용하여 앙상블 하는 모델을 만들었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[1]\ttraining's binary_logloss: 0.68593\n",
      "[2]\ttraining's binary_logloss: 0.680011\n",
      "[3]\ttraining's binary_logloss: 0.674916\n",
      "[4]\ttraining's binary_logloss: 0.670883\n",
      "[5]\ttraining's binary_logloss: 0.667258\n",
      "[6]\ttraining's binary_logloss: 0.664002\n",
      "[7]\ttraining's binary_logloss: 0.6611\n",
      "[8]\ttraining's binary_logloss: 0.658771\n",
      "[9]\ttraining's binary_logloss: 0.656329\n",
      "[10]\ttraining's binary_logloss: 0.654196\n",
      "[11]\ttraining's binary_logloss: 0.652471\n",
      "[12]\ttraining's binary_logloss: 0.650882\n",
      "[13]\ttraining's binary_logloss: 0.649265\n",
      "[14]\ttraining's binary_logloss: 0.64794\n",
      "[15]\ttraining's binary_logloss: 0.646684\n",
      "[16]\ttraining's binary_logloss: 0.645487\n",
      "[17]\ttraining's binary_logloss: 0.644293\n",
      "[18]\ttraining's binary_logloss: 0.643337\n",
      "[19]\ttraining's binary_logloss: 0.642276\n",
      "[20]\ttraining's binary_logloss: 0.64132\n",
      "[21]\ttraining's binary_logloss: 0.640491\n",
      "[22]\ttraining's binary_logloss: 0.639765\n",
      "[23]\ttraining's binary_logloss: 0.63902\n",
      "[24]\ttraining's binary_logloss: 0.638244\n",
      "[25]\ttraining's binary_logloss: 0.637302\n",
      "[26]\ttraining's binary_logloss: 0.636636\n",
      "[27]\ttraining's binary_logloss: 0.635949\n",
      "[28]\ttraining's binary_logloss: 0.635384\n",
      "[29]\ttraining's binary_logloss: 0.634798\n",
      "[30]\ttraining's binary_logloss: 0.634222\n",
      "[31]\ttraining's binary_logloss: 0.6336\n",
      "[32]\ttraining's binary_logloss: 0.633031\n",
      "[33]\ttraining's binary_logloss: 0.632468\n",
      "[34]\ttraining's binary_logloss: 0.63191\n",
      "[35]\ttraining's binary_logloss: 0.631295\n",
      "[36]\ttraining's binary_logloss: 0.630744\n",
      "[37]\ttraining's binary_logloss: 0.630266\n",
      "[38]\ttraining's binary_logloss: 0.62976\n",
      "[39]\ttraining's binary_logloss: 0.629294\n",
      "[40]\ttraining's binary_logloss: 0.628809\n",
      "[41]\ttraining's binary_logloss: 0.628359\n",
      "[42]\ttraining's binary_logloss: 0.627802\n",
      "[43]\ttraining's binary_logloss: 0.627349\n",
      "[44]\ttraining's binary_logloss: 0.626987\n",
      "[45]\ttraining's binary_logloss: 0.626615\n",
      "[46]\ttraining's binary_logloss: 0.626166\n",
      "[47]\ttraining's binary_logloss: 0.625772\n",
      "[48]\ttraining's binary_logloss: 0.625331\n",
      "[49]\ttraining's binary_logloss: 0.624874\n",
      "[50]\ttraining's binary_logloss: 0.624458\n",
      "[51]\ttraining's binary_logloss: 0.624062\n",
      "[52]\ttraining's binary_logloss: 0.623689\n",
      "[53]\ttraining's binary_logloss: 0.623388\n",
      "[54]\ttraining's binary_logloss: 0.623039\n",
      "[55]\ttraining's binary_logloss: 0.622688\n",
      "[56]\ttraining's binary_logloss: 0.622268\n",
      "[57]\ttraining's binary_logloss: 0.621922\n",
      "[58]\ttraining's binary_logloss: 0.621554\n",
      "[59]\ttraining's binary_logloss: 0.621217\n",
      "[60]\ttraining's binary_logloss: 0.62082\n",
      "[61]\ttraining's binary_logloss: 0.620498\n",
      "[62]\ttraining's binary_logloss: 0.620156\n",
      "[63]\ttraining's binary_logloss: 0.619893\n",
      "[64]\ttraining's binary_logloss: 0.619571\n",
      "[65]\ttraining's binary_logloss: 0.619234\n",
      "[66]\ttraining's binary_logloss: 0.618855\n",
      "[67]\ttraining's binary_logloss: 0.61856\n",
      "[68]\ttraining's binary_logloss: 0.618223\n",
      "[69]\ttraining's binary_logloss: 0.617968\n",
      "[70]\ttraining's binary_logloss: 0.617621\n",
      "[71]\ttraining's binary_logloss: 0.617319\n",
      "[72]\ttraining's binary_logloss: 0.616981\n",
      "[73]\ttraining's binary_logloss: 0.616706\n",
      "[74]\ttraining's binary_logloss: 0.616457\n",
      "[75]\ttraining's binary_logloss: 0.616208\n",
      "[76]\ttraining's binary_logloss: 0.615883\n",
      "[77]\ttraining's binary_logloss: 0.615556\n",
      "[78]\ttraining's binary_logloss: 0.615371\n",
      "[79]\ttraining's binary_logloss: 0.615055\n",
      "[80]\ttraining's binary_logloss: 0.614686\n",
      "[81]\ttraining's binary_logloss: 0.614459\n",
      "[82]\ttraining's binary_logloss: 0.614091\n",
      "[83]\ttraining's binary_logloss: 0.613841\n",
      "[84]\ttraining's binary_logloss: 0.613623\n",
      "[85]\ttraining's binary_logloss: 0.613307\n",
      "[86]\ttraining's binary_logloss: 0.613013\n",
      "[87]\ttraining's binary_logloss: 0.612715\n",
      "[88]\ttraining's binary_logloss: 0.612431\n",
      "[89]\ttraining's binary_logloss: 0.612069\n",
      "[90]\ttraining's binary_logloss: 0.611823\n",
      "[91]\ttraining's binary_logloss: 0.611565\n",
      "[92]\ttraining's binary_logloss: 0.611329\n",
      "[93]\ttraining's binary_logloss: 0.611135\n",
      "[94]\ttraining's binary_logloss: 0.610805\n",
      "[95]\ttraining's binary_logloss: 0.610513\n",
      "[96]\ttraining's binary_logloss: 0.610248\n",
      "[97]\ttraining's binary_logloss: 0.609962\n",
      "[98]\ttraining's binary_logloss: 0.609671\n",
      "[99]\ttraining's binary_logloss: 0.60942\n",
      "[100]\ttraining's binary_logloss: 0.609174\n",
      "[101]\ttraining's binary_logloss: 0.608977\n",
      "[102]\ttraining's binary_logloss: 0.608805\n",
      "[103]\ttraining's binary_logloss: 0.608544\n",
      "[104]\ttraining's binary_logloss: 0.608228\n",
      "[105]\ttraining's binary_logloss: 0.607933\n",
      "[106]\ttraining's binary_logloss: 0.607618\n",
      "[107]\ttraining's binary_logloss: 0.607336\n",
      "[108]\ttraining's binary_logloss: 0.607116\n",
      "[109]\ttraining's binary_logloss: 0.606894\n",
      "[110]\ttraining's binary_logloss: 0.606611\n",
      "[111]\ttraining's binary_logloss: 0.606302\n",
      "[112]\ttraining's binary_logloss: 0.606125\n",
      "[113]\ttraining's binary_logloss: 0.605839\n",
      "[114]\ttraining's binary_logloss: 0.605649\n",
      "[115]\ttraining's binary_logloss: 0.605361\n",
      "[116]\ttraining's binary_logloss: 0.60511\n",
      "[117]\ttraining's binary_logloss: 0.604884\n",
      "[118]\ttraining's binary_logloss: 0.604604\n",
      "[119]\ttraining's binary_logloss: 0.604375\n",
      "[120]\ttraining's binary_logloss: 0.6041\n",
      "[121]\ttraining's binary_logloss: 0.603779\n",
      "[122]\ttraining's binary_logloss: 0.603629\n",
      "[123]\ttraining's binary_logloss: 0.603461\n",
      "[124]\ttraining's binary_logloss: 0.603289\n",
      "[125]\ttraining's binary_logloss: 0.602998\n",
      "[126]\ttraining's binary_logloss: 0.602701\n",
      "[127]\ttraining's binary_logloss: 0.60246\n",
      "[128]\ttraining's binary_logloss: 0.602203\n",
      "[129]\ttraining's binary_logloss: 0.601995\n",
      "[130]\ttraining's binary_logloss: 0.601805\n",
      "[131]\ttraining's binary_logloss: 0.601578\n",
      "[132]\ttraining's binary_logloss: 0.601397\n",
      "[133]\ttraining's binary_logloss: 0.601212\n",
      "[134]\ttraining's binary_logloss: 0.600968\n",
      "[135]\ttraining's binary_logloss: 0.600763\n",
      "[136]\ttraining's binary_logloss: 0.600595\n",
      "[137]\ttraining's binary_logloss: 0.60031\n",
      "[138]\ttraining's binary_logloss: 0.600084\n",
      "[139]\ttraining's binary_logloss: 0.599851\n",
      "[140]\ttraining's binary_logloss: 0.599619\n",
      "[141]\ttraining's binary_logloss: 0.599404\n",
      "[142]\ttraining's binary_logloss: 0.599202\n",
      "[143]\ttraining's binary_logloss: 0.598906\n",
      "[144]\ttraining's binary_logloss: 0.598676\n",
      "[145]\ttraining's binary_logloss: 0.598526\n",
      "[146]\ttraining's binary_logloss: 0.598298\n",
      "[147]\ttraining's binary_logloss: 0.598124\n",
      "[148]\ttraining's binary_logloss: 0.597913\n",
      "[149]\ttraining's binary_logloss: 0.597705\n",
      "[150]\ttraining's binary_logloss: 0.597582\n",
      "[151]\ttraining's binary_logloss: 0.597411\n",
      "[152]\ttraining's binary_logloss: 0.597217\n",
      "[153]\ttraining's binary_logloss: 0.596968\n",
      "[154]\ttraining's binary_logloss: 0.59674\n",
      "[155]\ttraining's binary_logloss: 0.59652\n",
      "[156]\ttraining's binary_logloss: 0.596376\n",
      "[157]\ttraining's binary_logloss: 0.596211\n",
      "[158]\ttraining's binary_logloss: 0.596088\n",
      "[159]\ttraining's binary_logloss: 0.595926\n",
      "[160]\ttraining's binary_logloss: 0.595741\n",
      "[161]\ttraining's binary_logloss: 0.595483\n",
      "[162]\ttraining's binary_logloss: 0.595257\n",
      "[163]\ttraining's binary_logloss: 0.595059\n",
      "[164]\ttraining's binary_logloss: 0.594812\n",
      "[165]\ttraining's binary_logloss: 0.594553\n",
      "[166]\ttraining's binary_logloss: 0.594357\n",
      "[167]\ttraining's binary_logloss: 0.594218\n",
      "[168]\ttraining's binary_logloss: 0.593991\n",
      "[169]\ttraining's binary_logloss: 0.593817\n",
      "[170]\ttraining's binary_logloss: 0.593606\n",
      "[171]\ttraining's binary_logloss: 0.593372\n",
      "[172]\ttraining's binary_logloss: 0.593193\n",
      "[173]\ttraining's binary_logloss: 0.592962\n",
      "[174]\ttraining's binary_logloss: 0.592772\n",
      "[175]\ttraining's binary_logloss: 0.592576\n",
      "[176]\ttraining's binary_logloss: 0.592354\n",
      "[177]\ttraining's binary_logloss: 0.592098\n",
      "[178]\ttraining's binary_logloss: 0.591914\n",
      "[179]\ttraining's binary_logloss: 0.591679\n",
      "[180]\ttraining's binary_logloss: 0.591473\n",
      "[181]\ttraining's binary_logloss: 0.591278\n",
      "[182]\ttraining's binary_logloss: 0.591103\n",
      "[183]\ttraining's binary_logloss: 0.590926\n",
      "[184]\ttraining's binary_logloss: 0.590685\n",
      "[185]\ttraining's binary_logloss: 0.590501\n",
      "[186]\ttraining's binary_logloss: 0.590272\n",
      "[187]\ttraining's binary_logloss: 0.590108\n",
      "[188]\ttraining's binary_logloss: 0.589808\n",
      "[189]\ttraining's binary_logloss: 0.589601\n",
      "[190]\ttraining's binary_logloss: 0.589375\n",
      "[191]\ttraining's binary_logloss: 0.58913\n",
      "[192]\ttraining's binary_logloss: 0.588848\n",
      "[193]\ttraining's binary_logloss: 0.588667\n",
      "[194]\ttraining's binary_logloss: 0.588509\n",
      "[195]\ttraining's binary_logloss: 0.588292\n",
      "[196]\ttraining's binary_logloss: 0.588132\n",
      "[197]\ttraining's binary_logloss: 0.587962\n",
      "[198]\ttraining's binary_logloss: 0.587736\n",
      "[199]\ttraining's binary_logloss: 0.587568\n",
      "[200]\ttraining's binary_logloss: 0.587288\n",
      "[201]\ttraining's binary_logloss: 0.587052\n",
      "[202]\ttraining's binary_logloss: 0.586921\n",
      "[203]\ttraining's binary_logloss: 0.586727\n",
      "[204]\ttraining's binary_logloss: 0.586535\n",
      "[205]\ttraining's binary_logloss: 0.586345\n",
      "[206]\ttraining's binary_logloss: 0.586207\n",
      "[207]\ttraining's binary_logloss: 0.586009\n",
      "[208]\ttraining's binary_logloss: 0.585883\n",
      "[209]\ttraining's binary_logloss: 0.58563\n",
      "[210]\ttraining's binary_logloss: 0.585446\n",
      "[211]\ttraining's binary_logloss: 0.58519\n",
      "[212]\ttraining's binary_logloss: 0.584945\n",
      "[213]\ttraining's binary_logloss: 0.584688\n",
      "[214]\ttraining's binary_logloss: 0.584455\n",
      "[215]\ttraining's binary_logloss: 0.584174\n",
      "[216]\ttraining's binary_logloss: 0.58403\n",
      "[217]\ttraining's binary_logloss: 0.583908\n",
      "[218]\ttraining's binary_logloss: 0.583693\n",
      "[219]\ttraining's binary_logloss: 0.583536\n",
      "[220]\ttraining's binary_logloss: 0.58332\n",
      "[221]\ttraining's binary_logloss: 0.583125\n",
      "[222]\ttraining's binary_logloss: 0.582936\n",
      "[223]\ttraining's binary_logloss: 0.582777\n",
      "[224]\ttraining's binary_logloss: 0.582577\n",
      "[225]\ttraining's binary_logloss: 0.582435\n",
      "[226]\ttraining's binary_logloss: 0.582211\n",
      "[227]\ttraining's binary_logloss: 0.581968\n",
      "[228]\ttraining's binary_logloss: 0.581762\n",
      "[229]\ttraining's binary_logloss: 0.581472\n",
      "[230]\ttraining's binary_logloss: 0.581305\n",
      "[231]\ttraining's binary_logloss: 0.581068\n",
      "[232]\ttraining's binary_logloss: 0.580849\n",
      "[233]\ttraining's binary_logloss: 0.580636\n",
      "[234]\ttraining's binary_logloss: 0.580513\n",
      "[235]\ttraining's binary_logloss: 0.580318\n",
      "[236]\ttraining's binary_logloss: 0.580184\n",
      "[237]\ttraining's binary_logloss: 0.579969\n",
      "[238]\ttraining's binary_logloss: 0.579763\n",
      "[239]\ttraining's binary_logloss: 0.579578\n",
      "[240]\ttraining's binary_logloss: 0.579359\n",
      "[241]\ttraining's binary_logloss: 0.579174\n",
      "[242]\ttraining's binary_logloss: 0.579063\n",
      "[243]\ttraining's binary_logloss: 0.578901\n",
      "[244]\ttraining's binary_logloss: 0.578647\n",
      "[245]\ttraining's binary_logloss: 0.578388\n",
      "[246]\ttraining's binary_logloss: 0.57821\n",
      "[247]\ttraining's binary_logloss: 0.578003\n",
      "[248]\ttraining's binary_logloss: 0.577786\n",
      "[249]\ttraining's binary_logloss: 0.577586\n",
      "[250]\ttraining's binary_logloss: 0.577375\n",
      "[251]\ttraining's binary_logloss: 0.577189\n",
      "[252]\ttraining's binary_logloss: 0.576934\n",
      "[253]\ttraining's binary_logloss: 0.576655\n",
      "[254]\ttraining's binary_logloss: 0.57639\n",
      "[255]\ttraining's binary_logloss: 0.576204\n",
      "[256]\ttraining's binary_logloss: 0.576046\n",
      "[257]\ttraining's binary_logloss: 0.575875\n",
      "[258]\ttraining's binary_logloss: 0.575678\n",
      "[259]\ttraining's binary_logloss: 0.575489\n",
      "[260]\ttraining's binary_logloss: 0.575276\n",
      "[261]\ttraining's binary_logloss: 0.575116\n",
      "[262]\ttraining's binary_logloss: 0.574943\n",
      "[263]\ttraining's binary_logloss: 0.574681\n",
      "[264]\ttraining's binary_logloss: 0.574532\n",
      "[265]\ttraining's binary_logloss: 0.574361\n",
      "[266]\ttraining's binary_logloss: 0.574215\n",
      "[267]\ttraining's binary_logloss: 0.574042\n",
      "[268]\ttraining's binary_logloss: 0.573921\n",
      "[269]\ttraining's binary_logloss: 0.573721\n",
      "[270]\ttraining's binary_logloss: 0.573536\n",
      "[271]\ttraining's binary_logloss: 0.573326\n",
      "[272]\ttraining's binary_logloss: 0.573115\n",
      "[273]\ttraining's binary_logloss: 0.572925\n",
      "[274]\ttraining's binary_logloss: 0.572737\n",
      "[275]\ttraining's binary_logloss: 0.572558\n",
      "[276]\ttraining's binary_logloss: 0.5724\n",
      "[277]\ttraining's binary_logloss: 0.572193\n",
      "[278]\ttraining's binary_logloss: 0.572076\n",
      "[279]\ttraining's binary_logloss: 0.57188\n",
      "[280]\ttraining's binary_logloss: 0.571628\n",
      "[281]\ttraining's binary_logloss: 0.571431\n",
      "[282]\ttraining's binary_logloss: 0.571308\n",
      "[283]\ttraining's binary_logloss: 0.571108\n",
      "[284]\ttraining's binary_logloss: 0.570932\n",
      "[285]\ttraining's binary_logloss: 0.570766\n",
      "[286]\ttraining's binary_logloss: 0.570643\n",
      "[287]\ttraining's binary_logloss: 0.570452\n",
      "[288]\ttraining's binary_logloss: 0.570336\n",
      "[289]\ttraining's binary_logloss: 0.570203\n",
      "[290]\ttraining's binary_logloss: 0.570092\n",
      "[291]\ttraining's binary_logloss: 0.569923\n",
      "[292]\ttraining's binary_logloss: 0.569745\n",
      "[293]\ttraining's binary_logloss: 0.569553\n",
      "[294]\ttraining's binary_logloss: 0.569325\n",
      "[295]\ttraining's binary_logloss: 0.569085\n",
      "[296]\ttraining's binary_logloss: 0.568966\n",
      "[297]\ttraining's binary_logloss: 0.568797\n",
      "[298]\ttraining's binary_logloss: 0.568562\n",
      "[299]\ttraining's binary_logloss: 0.568372\n",
      "[300]\ttraining's binary_logloss: 0.568192\n",
      "[301]\ttraining's binary_logloss: 0.568009\n",
      "[302]\ttraining's binary_logloss: 0.567908\n",
      "[303]\ttraining's binary_logloss: 0.567804\n",
      "[304]\ttraining's binary_logloss: 0.567717\n",
      "[305]\ttraining's binary_logloss: 0.567568\n",
      "[306]\ttraining's binary_logloss: 0.567407\n",
      "[307]\ttraining's binary_logloss: 0.567218\n",
      "[308]\ttraining's binary_logloss: 0.567089\n",
      "[309]\ttraining's binary_logloss: 0.566902\n",
      "[310]\ttraining's binary_logloss: 0.566756\n",
      "[311]\ttraining's binary_logloss: 0.566577\n",
      "[312]\ttraining's binary_logloss: 0.566499\n",
      "[313]\ttraining's binary_logloss: 0.56634\n",
      "[314]\ttraining's binary_logloss: 0.566136\n",
      "[315]\ttraining's binary_logloss: 0.565915\n",
      "[316]\ttraining's binary_logloss: 0.565683\n",
      "[317]\ttraining's binary_logloss: 0.565591\n",
      "[318]\ttraining's binary_logloss: 0.565477\n",
      "[319]\ttraining's binary_logloss: 0.565346\n",
      "[320]\ttraining's binary_logloss: 0.565174\n",
      "[321]\ttraining's binary_logloss: 0.565047\n",
      "[322]\ttraining's binary_logloss: 0.564891\n",
      "[323]\ttraining's binary_logloss: 0.564701\n",
      "[324]\ttraining's binary_logloss: 0.564528\n",
      "[325]\ttraining's binary_logloss: 0.56437\n",
      "[326]\ttraining's binary_logloss: 0.564235\n",
      "[327]\ttraining's binary_logloss: 0.56409\n",
      "[328]\ttraining's binary_logloss: 0.563948\n",
      "[329]\ttraining's binary_logloss: 0.563741\n",
      "[330]\ttraining's binary_logloss: 0.563566\n",
      "[331]\ttraining's binary_logloss: 0.563398\n",
      "[332]\ttraining's binary_logloss: 0.563245\n",
      "[333]\ttraining's binary_logloss: 0.56307\n",
      "[334]\ttraining's binary_logloss: 0.5629\n",
      "[335]\ttraining's binary_logloss: 0.562681\n",
      "[336]\ttraining's binary_logloss: 0.562469\n",
      "[337]\ttraining's binary_logloss: 0.562311\n",
      "[338]\ttraining's binary_logloss: 0.562193\n",
      "[339]\ttraining's binary_logloss: 0.561994\n",
      "[340]\ttraining's binary_logloss: 0.561824\n",
      "[341]\ttraining's binary_logloss: 0.561665\n",
      "[342]\ttraining's binary_logloss: 0.561473\n",
      "[343]\ttraining's binary_logloss: 0.561367\n",
      "[344]\ttraining's binary_logloss: 0.561246\n",
      "[345]\ttraining's binary_logloss: 0.561062\n",
      "[346]\ttraining's binary_logloss: 0.560937\n",
      "[347]\ttraining's binary_logloss: 0.560769\n",
      "[348]\ttraining's binary_logloss: 0.560547\n",
      "[349]\ttraining's binary_logloss: 0.560375\n",
      "[350]\ttraining's binary_logloss: 0.560227\n",
      "[351]\ttraining's binary_logloss: 0.560014\n",
      "[352]\ttraining's binary_logloss: 0.559838\n",
      "[353]\ttraining's binary_logloss: 0.559701\n",
      "[354]\ttraining's binary_logloss: 0.559549\n",
      "[355]\ttraining's binary_logloss: 0.559399\n",
      "[356]\ttraining's binary_logloss: 0.559254\n",
      "[357]\ttraining's binary_logloss: 0.559092\n",
      "[358]\ttraining's binary_logloss: 0.558952\n",
      "[359]\ttraining's binary_logloss: 0.558728\n",
      "[360]\ttraining's binary_logloss: 0.558554\n",
      "[361]\ttraining's binary_logloss: 0.558437\n",
      "[362]\ttraining's binary_logloss: 0.55827\n",
      "[363]\ttraining's binary_logloss: 0.558122\n",
      "[364]\ttraining's binary_logloss: 0.557914\n",
      "[365]\ttraining's binary_logloss: 0.557751\n",
      "[366]\ttraining's binary_logloss: 0.557656\n",
      "[367]\ttraining's binary_logloss: 0.557494\n",
      "[368]\ttraining's binary_logloss: 0.557267\n",
      "[369]\ttraining's binary_logloss: 0.557069\n",
      "[370]\ttraining's binary_logloss: 0.556969\n",
      "[371]\ttraining's binary_logloss: 0.556882\n",
      "[372]\ttraining's binary_logloss: 0.556743\n",
      "[373]\ttraining's binary_logloss: 0.556623\n",
      "[374]\ttraining's binary_logloss: 0.55646\n",
      "[375]\ttraining's binary_logloss: 0.556357\n",
      "[376]\ttraining's binary_logloss: 0.55623\n",
      "[377]\ttraining's binary_logloss: 0.55604\n",
      "[378]\ttraining's binary_logloss: 0.555951\n",
      "[379]\ttraining's binary_logloss: 0.55576\n",
      "[380]\ttraining's binary_logloss: 0.55557\n",
      "[381]\ttraining's binary_logloss: 0.555431\n",
      "[382]\ttraining's binary_logloss: 0.555238\n",
      "[383]\ttraining's binary_logloss: 0.555031\n",
      "[384]\ttraining's binary_logloss: 0.554881\n",
      "[385]\ttraining's binary_logloss: 0.554691\n",
      "[386]\ttraining's binary_logloss: 0.554523\n",
      "[387]\ttraining's binary_logloss: 0.554349\n",
      "[388]\ttraining's binary_logloss: 0.554223\n",
      "[389]\ttraining's binary_logloss: 0.554037\n",
      "[390]\ttraining's binary_logloss: 0.553855\n",
      "[391]\ttraining's binary_logloss: 0.553687\n",
      "[392]\ttraining's binary_logloss: 0.553536\n",
      "[393]\ttraining's binary_logloss: 0.553312\n",
      "[394]\ttraining's binary_logloss: 0.553106\n",
      "[395]\ttraining's binary_logloss: 0.552971\n",
      "[396]\ttraining's binary_logloss: 0.552824\n",
      "[397]\ttraining's binary_logloss: 0.55271\n",
      "[398]\ttraining's binary_logloss: 0.552585\n",
      "[399]\ttraining's binary_logloss: 0.552392\n",
      "[400]\ttraining's binary_logloss: 0.55222\n",
      "[401]\ttraining's binary_logloss: 0.552066\n",
      "[402]\ttraining's binary_logloss: 0.551998\n",
      "[403]\ttraining's binary_logloss: 0.551868\n",
      "[404]\ttraining's binary_logloss: 0.551677\n",
      "[405]\ttraining's binary_logloss: 0.551538\n",
      "[406]\ttraining's binary_logloss: 0.551431\n",
      "[407]\ttraining's binary_logloss: 0.551319\n",
      "[408]\ttraining's binary_logloss: 0.551158\n",
      "[409]\ttraining's binary_logloss: 0.551018\n",
      "[410]\ttraining's binary_logloss: 0.550902\n",
      "[411]\ttraining's binary_logloss: 0.550738\n",
      "[412]\ttraining's binary_logloss: 0.55052\n",
      "[413]\ttraining's binary_logloss: 0.550316\n",
      "[414]\ttraining's binary_logloss: 0.55014\n",
      "[415]\ttraining's binary_logloss: 0.550034\n",
      "[416]\ttraining's binary_logloss: 0.54993\n",
      "[417]\ttraining's binary_logloss: 0.549854\n",
      "[418]\ttraining's binary_logloss: 0.549729\n",
      "[419]\ttraining's binary_logloss: 0.549606\n",
      "[420]\ttraining's binary_logloss: 0.549431\n",
      "[421]\ttraining's binary_logloss: 0.549314\n",
      "[422]\ttraining's binary_logloss: 0.549091\n",
      "[423]\ttraining's binary_logloss: 0.548946\n",
      "[424]\ttraining's binary_logloss: 0.548788\n",
      "[425]\ttraining's binary_logloss: 0.548685\n",
      "[426]\ttraining's binary_logloss: 0.548577\n",
      "[427]\ttraining's binary_logloss: 0.548388\n",
      "[428]\ttraining's binary_logloss: 0.548275\n",
      "[429]\ttraining's binary_logloss: 0.548137\n",
      "[430]\ttraining's binary_logloss: 0.547983\n",
      "[431]\ttraining's binary_logloss: 0.547876\n",
      "[432]\ttraining's binary_logloss: 0.547765\n",
      "[433]\ttraining's binary_logloss: 0.547591\n",
      "[434]\ttraining's binary_logloss: 0.547404\n",
      "[435]\ttraining's binary_logloss: 0.547209\n",
      "[436]\ttraining's binary_logloss: 0.547033\n",
      "[437]\ttraining's binary_logloss: 0.546901\n",
      "[438]\ttraining's binary_logloss: 0.546805\n",
      "[439]\ttraining's binary_logloss: 0.546615\n",
      "[440]\ttraining's binary_logloss: 0.546429\n",
      "[441]\ttraining's binary_logloss: 0.546289\n",
      "[442]\ttraining's binary_logloss: 0.546124\n",
      "[443]\ttraining's binary_logloss: 0.545912\n",
      "[444]\ttraining's binary_logloss: 0.545735\n",
      "[445]\ttraining's binary_logloss: 0.545625\n",
      "[446]\ttraining's binary_logloss: 0.545466\n",
      "[447]\ttraining's binary_logloss: 0.545308\n",
      "[448]\ttraining's binary_logloss: 0.545164\n",
      "[449]\ttraining's binary_logloss: 0.545041\n",
      "[450]\ttraining's binary_logloss: 0.544936\n",
      "[451]\ttraining's binary_logloss: 0.544847\n",
      "[452]\ttraining's binary_logloss: 0.544697\n",
      "[453]\ttraining's binary_logloss: 0.544481\n",
      "[454]\ttraining's binary_logloss: 0.544345\n",
      "[455]\ttraining's binary_logloss: 0.544202\n",
      "[456]\ttraining's binary_logloss: 0.54406\n",
      "[457]\ttraining's binary_logloss: 0.544005\n",
      "[458]\ttraining's binary_logloss: 0.543853\n",
      "[459]\ttraining's binary_logloss: 0.543664\n",
      "[460]\ttraining's binary_logloss: 0.543486\n",
      "[461]\ttraining's binary_logloss: 0.543346\n",
      "[462]\ttraining's binary_logloss: 0.543116\n",
      "[463]\ttraining's binary_logloss: 0.54299\n",
      "[464]\ttraining's binary_logloss: 0.542785\n",
      "[465]\ttraining's binary_logloss: 0.54263\n",
      "[466]\ttraining's binary_logloss: 0.542451\n",
      "[467]\ttraining's binary_logloss: 0.542347\n",
      "[468]\ttraining's binary_logloss: 0.542232\n",
      "[469]\ttraining's binary_logloss: 0.542138\n",
      "[470]\ttraining's binary_logloss: 0.542053\n",
      "[471]\ttraining's binary_logloss: 0.541937\n",
      "[472]\ttraining's binary_logloss: 0.541762\n",
      "[473]\ttraining's binary_logloss: 0.541641\n",
      "[474]\ttraining's binary_logloss: 0.541561\n",
      "[475]\ttraining's binary_logloss: 0.541362\n",
      "[476]\ttraining's binary_logloss: 0.541211\n",
      "[477]\ttraining's binary_logloss: 0.541133\n",
      "[478]\ttraining's binary_logloss: 0.541053\n",
      "[479]\ttraining's binary_logloss: 0.540882\n",
      "[480]\ttraining's binary_logloss: 0.540698\n",
      "[481]\ttraining's binary_logloss: 0.540557\n",
      "[482]\ttraining's binary_logloss: 0.540429\n",
      "[483]\ttraining's binary_logloss: 0.54033\n",
      "[484]\ttraining's binary_logloss: 0.540198\n",
      "[485]\ttraining's binary_logloss: 0.540119\n",
      "[486]\ttraining's binary_logloss: 0.540006\n",
      "[487]\ttraining's binary_logloss: 0.539838\n",
      "[488]\ttraining's binary_logloss: 0.539695\n",
      "[489]\ttraining's binary_logloss: 0.539561\n",
      "[490]\ttraining's binary_logloss: 0.539421\n",
      "[491]\ttraining's binary_logloss: 0.5393\n",
      "[492]\ttraining's binary_logloss: 0.539163\n",
      "[493]\ttraining's binary_logloss: 0.538971\n",
      "[494]\ttraining's binary_logloss: 0.538888\n",
      "[495]\ttraining's binary_logloss: 0.538791\n",
      "[496]\ttraining's binary_logloss: 0.538721\n",
      "[497]\ttraining's binary_logloss: 0.538629\n",
      "[498]\ttraining's binary_logloss: 0.538465\n",
      "[499]\ttraining's binary_logloss: 0.538345\n",
      "[500]\ttraining's binary_logloss: 0.538209\n",
      "[501]\ttraining's binary_logloss: 0.538091\n",
      "[502]\ttraining's binary_logloss: 0.53801\n",
      "[503]\ttraining's binary_logloss: 0.537939\n",
      "[504]\ttraining's binary_logloss: 0.537834\n",
      "[505]\ttraining's binary_logloss: 0.537685\n",
      "[506]\ttraining's binary_logloss: 0.537461\n",
      "[507]\ttraining's binary_logloss: 0.537356\n",
      "[508]\ttraining's binary_logloss: 0.537173\n",
      "[509]\ttraining's binary_logloss: 0.536957\n",
      "[510]\ttraining's binary_logloss: 0.536826\n",
      "[511]\ttraining's binary_logloss: 0.536744\n",
      "[512]\ttraining's binary_logloss: 0.536606\n",
      "[513]\ttraining's binary_logloss: 0.536512\n",
      "[514]\ttraining's binary_logloss: 0.536384\n",
      "[515]\ttraining's binary_logloss: 0.536272\n",
      "[516]\ttraining's binary_logloss: 0.536097\n",
      "[517]\ttraining's binary_logloss: 0.535947\n",
      "[518]\ttraining's binary_logloss: 0.535826\n",
      "[519]\ttraining's binary_logloss: 0.535596\n",
      "[520]\ttraining's binary_logloss: 0.535473\n",
      "[521]\ttraining's binary_logloss: 0.535313\n",
      "[522]\ttraining's binary_logloss: 0.535139\n",
      "[523]\ttraining's binary_logloss: 0.534927\n",
      "[524]\ttraining's binary_logloss: 0.534831\n",
      "[525]\ttraining's binary_logloss: 0.534676\n",
      "[526]\ttraining's binary_logloss: 0.534549\n",
      "[527]\ttraining's binary_logloss: 0.534408\n",
      "[528]\ttraining's binary_logloss: 0.53429\n",
      "[529]\ttraining's binary_logloss: 0.534168\n",
      "[530]\ttraining's binary_logloss: 0.534077\n",
      "[531]\ttraining's binary_logloss: 0.53399\n",
      "[532]\ttraining's binary_logloss: 0.533895\n",
      "[533]\ttraining's binary_logloss: 0.533755\n",
      "[534]\ttraining's binary_logloss: 0.533652\n",
      "[535]\ttraining's binary_logloss: 0.533479\n",
      "[536]\ttraining's binary_logloss: 0.53333\n",
      "[537]\ttraining's binary_logloss: 0.533169\n",
      "[538]\ttraining's binary_logloss: 0.532995\n",
      "[539]\ttraining's binary_logloss: 0.53285\n",
      "[540]\ttraining's binary_logloss: 0.532707\n",
      "[541]\ttraining's binary_logloss: 0.532542\n",
      "[542]\ttraining's binary_logloss: 0.532388\n",
      "[543]\ttraining's binary_logloss: 0.532225\n",
      "[544]\ttraining's binary_logloss: 0.532097\n",
      "[545]\ttraining's binary_logloss: 0.531912\n",
      "[546]\ttraining's binary_logloss: 0.531814\n",
      "[547]\ttraining's binary_logloss: 0.531721\n",
      "[548]\ttraining's binary_logloss: 0.531612\n",
      "[549]\ttraining's binary_logloss: 0.531497\n",
      "[550]\ttraining's binary_logloss: 0.531411\n",
      "[551]\ttraining's binary_logloss: 0.531252\n",
      "[552]\ttraining's binary_logloss: 0.531129\n",
      "[553]\ttraining's binary_logloss: 0.530931\n",
      "[554]\ttraining's binary_logloss: 0.530782\n",
      "[555]\ttraining's binary_logloss: 0.530634\n",
      "[556]\ttraining's binary_logloss: 0.530463\n",
      "[557]\ttraining's binary_logloss: 0.530334\n",
      "[558]\ttraining's binary_logloss: 0.530188\n",
      "[559]\ttraining's binary_logloss: 0.530108\n",
      "[560]\ttraining's binary_logloss: 0.529943\n",
      "[561]\ttraining's binary_logloss: 0.529843\n",
      "[562]\ttraining's binary_logloss: 0.529642\n",
      "[563]\ttraining's binary_logloss: 0.529558\n",
      "[564]\ttraining's binary_logloss: 0.529456\n",
      "[565]\ttraining's binary_logloss: 0.529284\n",
      "[566]\ttraining's binary_logloss: 0.529156\n",
      "[567]\ttraining's binary_logloss: 0.528997\n",
      "[568]\ttraining's binary_logloss: 0.52885\n",
      "[569]\ttraining's binary_logloss: 0.528776\n",
      "[570]\ttraining's binary_logloss: 0.528662\n",
      "[571]\ttraining's binary_logloss: 0.528517\n",
      "[572]\ttraining's binary_logloss: 0.528347\n",
      "[573]\ttraining's binary_logloss: 0.528173\n",
      "[574]\ttraining's binary_logloss: 0.527992\n",
      "[575]\ttraining's binary_logloss: 0.527844\n",
      "[576]\ttraining's binary_logloss: 0.527714\n",
      "[577]\ttraining's binary_logloss: 0.527498\n",
      "[578]\ttraining's binary_logloss: 0.527314\n",
      "[579]\ttraining's binary_logloss: 0.527201\n",
      "[580]\ttraining's binary_logloss: 0.527131\n",
      "[581]\ttraining's binary_logloss: 0.526939\n",
      "[582]\ttraining's binary_logloss: 0.526768\n",
      "[583]\ttraining's binary_logloss: 0.526589\n",
      "[584]\ttraining's binary_logloss: 0.526426\n",
      "[585]\ttraining's binary_logloss: 0.526289\n",
      "[586]\ttraining's binary_logloss: 0.526127\n",
      "[587]\ttraining's binary_logloss: 0.526005\n",
      "[588]\ttraining's binary_logloss: 0.525842\n",
      "[589]\ttraining's binary_logloss: 0.525717\n",
      "[590]\ttraining's binary_logloss: 0.525558\n",
      "[591]\ttraining's binary_logloss: 0.525419\n",
      "[592]\ttraining's binary_logloss: 0.525303\n",
      "[593]\ttraining's binary_logloss: 0.525192\n",
      "[594]\ttraining's binary_logloss: 0.524974\n",
      "[595]\ttraining's binary_logloss: 0.52491\n",
      "[596]\ttraining's binary_logloss: 0.524714\n",
      "[597]\ttraining's binary_logloss: 0.524556\n",
      "[598]\ttraining's binary_logloss: 0.524424\n",
      "[599]\ttraining's binary_logloss: 0.524294\n",
      "[600]\ttraining's binary_logloss: 0.524163\n",
      "[601]\ttraining's binary_logloss: 0.524033\n",
      "[602]\ttraining's binary_logloss: 0.523855\n",
      "[603]\ttraining's binary_logloss: 0.523706\n",
      "[604]\ttraining's binary_logloss: 0.523595\n",
      "[605]\ttraining's binary_logloss: 0.523478\n",
      "[606]\ttraining's binary_logloss: 0.523345\n",
      "[607]\ttraining's binary_logloss: 0.523241\n",
      "[608]\ttraining's binary_logloss: 0.523097\n",
      "[609]\ttraining's binary_logloss: 0.522949\n",
      "[610]\ttraining's binary_logloss: 0.522815\n",
      "[611]\ttraining's binary_logloss: 0.522738\n",
      "[612]\ttraining's binary_logloss: 0.52262\n",
      "[613]\ttraining's binary_logloss: 0.522516\n",
      "[614]\ttraining's binary_logloss: 0.522325\n",
      "[615]\ttraining's binary_logloss: 0.522227\n",
      "[616]\ttraining's binary_logloss: 0.52215\n",
      "[617]\ttraining's binary_logloss: 0.521994\n",
      "[618]\ttraining's binary_logloss: 0.521851\n",
      "[619]\ttraining's binary_logloss: 0.521722\n",
      "[620]\ttraining's binary_logloss: 0.521507\n",
      "[621]\ttraining's binary_logloss: 0.521388\n",
      "[622]\ttraining's binary_logloss: 0.521261\n",
      "[623]\ttraining's binary_logloss: 0.521169\n",
      "[624]\ttraining's binary_logloss: 0.520996\n",
      "[625]\ttraining's binary_logloss: 0.52081\n",
      "[626]\ttraining's binary_logloss: 0.520679\n",
      "[627]\ttraining's binary_logloss: 0.52051\n",
      "[628]\ttraining's binary_logloss: 0.520413\n",
      "[629]\ttraining's binary_logloss: 0.5203\n",
      "[630]\ttraining's binary_logloss: 0.520163\n",
      "[631]\ttraining's binary_logloss: 0.520043\n",
      "[632]\ttraining's binary_logloss: 0.519976\n",
      "[633]\ttraining's binary_logloss: 0.519857\n",
      "[634]\ttraining's binary_logloss: 0.519709\n",
      "[635]\ttraining's binary_logloss: 0.519592\n",
      "[636]\ttraining's binary_logloss: 0.519391\n",
      "[637]\ttraining's binary_logloss: 0.519292\n",
      "[638]\ttraining's binary_logloss: 0.519209\n",
      "[639]\ttraining's binary_logloss: 0.519021\n",
      "[640]\ttraining's binary_logloss: 0.51891\n",
      "[641]\ttraining's binary_logloss: 0.518774\n",
      "[642]\ttraining's binary_logloss: 0.518672\n",
      "[643]\ttraining's binary_logloss: 0.518559\n",
      "[644]\ttraining's binary_logloss: 0.518411\n",
      "[645]\ttraining's binary_logloss: 0.518252\n",
      "[646]\ttraining's binary_logloss: 0.518161\n",
      "[647]\ttraining's binary_logloss: 0.517934\n",
      "[648]\ttraining's binary_logloss: 0.517829\n",
      "[649]\ttraining's binary_logloss: 0.517732\n",
      "[650]\ttraining's binary_logloss: 0.517629\n",
      "[651]\ttraining's binary_logloss: 0.517541\n",
      "[652]\ttraining's binary_logloss: 0.517364\n",
      "[653]\ttraining's binary_logloss: 0.517244\n",
      "[654]\ttraining's binary_logloss: 0.517127\n",
      "[655]\ttraining's binary_logloss: 0.516978\n",
      "[656]\ttraining's binary_logloss: 0.516868\n",
      "[657]\ttraining's binary_logloss: 0.516701\n",
      "[658]\ttraining's binary_logloss: 0.516565\n",
      "[659]\ttraining's binary_logloss: 0.516444\n",
      "[660]\ttraining's binary_logloss: 0.516364\n",
      "[661]\ttraining's binary_logloss: 0.51625\n",
      "[662]\ttraining's binary_logloss: 0.516135\n",
      "[663]\ttraining's binary_logloss: 0.516053\n",
      "[664]\ttraining's binary_logloss: 0.515894\n",
      "[665]\ttraining's binary_logloss: 0.515804\n",
      "[666]\ttraining's binary_logloss: 0.51568\n",
      "[667]\ttraining's binary_logloss: 0.515523\n",
      "[668]\ttraining's binary_logloss: 0.515389\n",
      "[669]\ttraining's binary_logloss: 0.515302\n",
      "[670]\ttraining's binary_logloss: 0.515214\n",
      "[671]\ttraining's binary_logloss: 0.515052\n",
      "[672]\ttraining's binary_logloss: 0.514966\n",
      "[673]\ttraining's binary_logloss: 0.514902\n",
      "[674]\ttraining's binary_logloss: 0.514803\n",
      "[675]\ttraining's binary_logloss: 0.514691\n",
      "[676]\ttraining's binary_logloss: 0.514491\n",
      "[677]\ttraining's binary_logloss: 0.514376\n",
      "[678]\ttraining's binary_logloss: 0.514172\n",
      "[679]\ttraining's binary_logloss: 0.5141\n",
      "[680]\ttraining's binary_logloss: 0.514008\n",
      "[681]\ttraining's binary_logloss: 0.513903\n",
      "[682]\ttraining's binary_logloss: 0.513771\n",
      "[683]\ttraining's binary_logloss: 0.513587\n",
      "[684]\ttraining's binary_logloss: 0.513457\n",
      "[685]\ttraining's binary_logloss: 0.513332\n",
      "[686]\ttraining's binary_logloss: 0.513245\n",
      "[687]\ttraining's binary_logloss: 0.513132\n",
      "[688]\ttraining's binary_logloss: 0.512979\n",
      "[689]\ttraining's binary_logloss: 0.512826\n",
      "[690]\ttraining's binary_logloss: 0.512692\n",
      "[691]\ttraining's binary_logloss: 0.51249\n",
      "[692]\ttraining's binary_logloss: 0.512386\n",
      "[693]\ttraining's binary_logloss: 0.512233\n",
      "[694]\ttraining's binary_logloss: 0.512113\n",
      "[695]\ttraining's binary_logloss: 0.511952\n",
      "[696]\ttraining's binary_logloss: 0.511796\n",
      "[697]\ttraining's binary_logloss: 0.511672\n",
      "[698]\ttraining's binary_logloss: 0.511572\n",
      "[699]\ttraining's binary_logloss: 0.511417\n",
      "[700]\ttraining's binary_logloss: 0.511316\n",
      "[701]\ttraining's binary_logloss: 0.511206\n",
      "[702]\ttraining's binary_logloss: 0.511102\n",
      "[703]\ttraining's binary_logloss: 0.511012\n",
      "[704]\ttraining's binary_logloss: 0.510834\n",
      "[705]\ttraining's binary_logloss: 0.510695\n",
      "[706]\ttraining's binary_logloss: 0.51058\n",
      "[707]\ttraining's binary_logloss: 0.510398\n",
      "[708]\ttraining's binary_logloss: 0.510256\n",
      "[709]\ttraining's binary_logloss: 0.510153\n",
      "[710]\ttraining's binary_logloss: 0.510012\n",
      "[711]\ttraining's binary_logloss: 0.509902\n",
      "[712]\ttraining's binary_logloss: 0.50971\n",
      "[713]\ttraining's binary_logloss: 0.509593\n",
      "[714]\ttraining's binary_logloss: 0.509456\n",
      "[715]\ttraining's binary_logloss: 0.509375\n",
      "[716]\ttraining's binary_logloss: 0.509289\n",
      "[717]\ttraining's binary_logloss: 0.509132\n",
      "[718]\ttraining's binary_logloss: 0.509001\n",
      "[719]\ttraining's binary_logloss: 0.508848\n",
      "[720]\ttraining's binary_logloss: 0.508742\n",
      "[721]\ttraining's binary_logloss: 0.508625\n",
      "[722]\ttraining's binary_logloss: 0.508504\n",
      "[723]\ttraining's binary_logloss: 0.50833\n",
      "[724]\ttraining's binary_logloss: 0.508212\n",
      "[725]\ttraining's binary_logloss: 0.508081\n",
      "[726]\ttraining's binary_logloss: 0.507975\n",
      "[727]\ttraining's binary_logloss: 0.50787\n",
      "[728]\ttraining's binary_logloss: 0.507743\n",
      "[729]\ttraining's binary_logloss: 0.50763\n",
      "[730]\ttraining's binary_logloss: 0.507543\n",
      "[731]\ttraining's binary_logloss: 0.507386\n",
      "[732]\ttraining's binary_logloss: 0.507253\n",
      "[733]\ttraining's binary_logloss: 0.507136\n",
      "[734]\ttraining's binary_logloss: 0.506953\n",
      "[735]\ttraining's binary_logloss: 0.506819\n",
      "[736]\ttraining's binary_logloss: 0.50674\n",
      "[737]\ttraining's binary_logloss: 0.506574\n",
      "[738]\ttraining's binary_logloss: 0.506454\n",
      "[739]\ttraining's binary_logloss: 0.5064\n",
      "[740]\ttraining's binary_logloss: 0.506296\n",
      "[741]\ttraining's binary_logloss: 0.506101\n",
      "[742]\ttraining's binary_logloss: 0.50599\n",
      "[743]\ttraining's binary_logloss: 0.505866\n",
      "[744]\ttraining's binary_logloss: 0.505711\n",
      "[745]\ttraining's binary_logloss: 0.505521\n",
      "[746]\ttraining's binary_logloss: 0.505358\n",
      "[747]\ttraining's binary_logloss: 0.505174\n",
      "[748]\ttraining's binary_logloss: 0.504993\n",
      "[749]\ttraining's binary_logloss: 0.504912\n",
      "[750]\ttraining's binary_logloss: 0.504767\n",
      "[751]\ttraining's binary_logloss: 0.504622\n",
      "[752]\ttraining's binary_logloss: 0.504529\n",
      "[753]\ttraining's binary_logloss: 0.504411\n",
      "[754]\ttraining's binary_logloss: 0.504278\n",
      "[755]\ttraining's binary_logloss: 0.50416\n",
      "[756]\ttraining's binary_logloss: 0.504031\n",
      "[757]\ttraining's binary_logloss: 0.503906\n",
      "[758]\ttraining's binary_logloss: 0.503703\n",
      "[759]\ttraining's binary_logloss: 0.503543\n",
      "[760]\ttraining's binary_logloss: 0.503414\n",
      "[761]\ttraining's binary_logloss: 0.503344\n",
      "[762]\ttraining's binary_logloss: 0.503279\n",
      "[763]\ttraining's binary_logloss: 0.503205\n",
      "[764]\ttraining's binary_logloss: 0.50313\n",
      "[765]\ttraining's binary_logloss: 0.502981\n",
      "[766]\ttraining's binary_logloss: 0.502886\n",
      "[767]\ttraining's binary_logloss: 0.502755\n",
      "[768]\ttraining's binary_logloss: 0.502651\n",
      "[769]\ttraining's binary_logloss: 0.502492\n",
      "[770]\ttraining's binary_logloss: 0.502344\n",
      "[771]\ttraining's binary_logloss: 0.502238\n",
      "[772]\ttraining's binary_logloss: 0.50208\n",
      "[773]\ttraining's binary_logloss: 0.501888\n",
      "[774]\ttraining's binary_logloss: 0.501674\n",
      "[775]\ttraining's binary_logloss: 0.501558\n",
      "[776]\ttraining's binary_logloss: 0.501433\n",
      "[777]\ttraining's binary_logloss: 0.501281\n",
      "[778]\ttraining's binary_logloss: 0.501188\n",
      "[779]\ttraining's binary_logloss: 0.501072\n",
      "[780]\ttraining's binary_logloss: 0.500946\n",
      "[781]\ttraining's binary_logloss: 0.500874\n",
      "[782]\ttraining's binary_logloss: 0.500791\n",
      "[783]\ttraining's binary_logloss: 0.500705\n",
      "[784]\ttraining's binary_logloss: 0.500544\n",
      "[785]\ttraining's binary_logloss: 0.500486\n",
      "[786]\ttraining's binary_logloss: 0.500338\n",
      "[787]\ttraining's binary_logloss: 0.50023\n",
      "[788]\ttraining's binary_logloss: 0.500038\n",
      "[789]\ttraining's binary_logloss: 0.499907\n",
      "[790]\ttraining's binary_logloss: 0.499801\n",
      "[791]\ttraining's binary_logloss: 0.499624\n",
      "[792]\ttraining's binary_logloss: 0.499515\n",
      "[793]\ttraining's binary_logloss: 0.499394\n",
      "[794]\ttraining's binary_logloss: 0.49926\n",
      "[795]\ttraining's binary_logloss: 0.499141\n",
      "[796]\ttraining's binary_logloss: 0.499051\n",
      "[797]\ttraining's binary_logloss: 0.498974\n",
      "[798]\ttraining's binary_logloss: 0.498791\n",
      "[799]\ttraining's binary_logloss: 0.498636\n",
      "[800]\ttraining's binary_logloss: 0.498569\n",
      "[801]\ttraining's binary_logloss: 0.498425\n",
      "[802]\ttraining's binary_logloss: 0.49831\n",
      "[803]\ttraining's binary_logloss: 0.498186\n",
      "[804]\ttraining's binary_logloss: 0.498071\n",
      "[805]\ttraining's binary_logloss: 0.497998\n",
      "[806]\ttraining's binary_logloss: 0.497918\n",
      "[807]\ttraining's binary_logloss: 0.497829\n",
      "[808]\ttraining's binary_logloss: 0.49775\n",
      "[809]\ttraining's binary_logloss: 0.497612\n",
      "[810]\ttraining's binary_logloss: 0.497526\n",
      "[811]\ttraining's binary_logloss: 0.497465\n",
      "[812]\ttraining's binary_logloss: 0.497417\n",
      "[813]\ttraining's binary_logloss: 0.497249\n",
      "[814]\ttraining's binary_logloss: 0.497136\n",
      "[815]\ttraining's binary_logloss: 0.49696\n",
      "[816]\ttraining's binary_logloss: 0.49678\n",
      "[817]\ttraining's binary_logloss: 0.4967\n",
      "[818]\ttraining's binary_logloss: 0.496625\n",
      "[819]\ttraining's binary_logloss: 0.496518\n",
      "[820]\ttraining's binary_logloss: 0.496405\n",
      "[821]\ttraining's binary_logloss: 0.496203\n",
      "[822]\ttraining's binary_logloss: 0.496104\n",
      "[823]\ttraining's binary_logloss: 0.495996\n",
      "[824]\ttraining's binary_logloss: 0.495924\n",
      "[825]\ttraining's binary_logloss: 0.495794\n",
      "[826]\ttraining's binary_logloss: 0.495679\n",
      "[827]\ttraining's binary_logloss: 0.495549\n",
      "[828]\ttraining's binary_logloss: 0.49547\n",
      "[829]\ttraining's binary_logloss: 0.495381\n",
      "[830]\ttraining's binary_logloss: 0.49531\n",
      "[831]\ttraining's binary_logloss: 0.495167\n",
      "[832]\ttraining's binary_logloss: 0.495109\n",
      "[833]\ttraining's binary_logloss: 0.495009\n",
      "[834]\ttraining's binary_logloss: 0.494894\n",
      "[835]\ttraining's binary_logloss: 0.494813\n",
      "[836]\ttraining's binary_logloss: 0.49471\n",
      "[837]\ttraining's binary_logloss: 0.494623\n",
      "[838]\ttraining's binary_logloss: 0.494516\n",
      "[839]\ttraining's binary_logloss: 0.494404\n",
      "[840]\ttraining's binary_logloss: 0.494239\n",
      "[841]\ttraining's binary_logloss: 0.494083\n",
      "[842]\ttraining's binary_logloss: 0.493968\n",
      "[843]\ttraining's binary_logloss: 0.493876\n",
      "[844]\ttraining's binary_logloss: 0.493734\n",
      "[845]\ttraining's binary_logloss: 0.493669\n",
      "[846]\ttraining's binary_logloss: 0.493607\n",
      "[847]\ttraining's binary_logloss: 0.493478\n",
      "[848]\ttraining's binary_logloss: 0.493291\n",
      "[849]\ttraining's binary_logloss: 0.49321\n",
      "[850]\ttraining's binary_logloss: 0.493147\n",
      "[851]\ttraining's binary_logloss: 0.493057\n",
      "[852]\ttraining's binary_logloss: 0.492897\n",
      "[853]\ttraining's binary_logloss: 0.492751\n",
      "[854]\ttraining's binary_logloss: 0.492667\n",
      "[855]\ttraining's binary_logloss: 0.492568\n",
      "[856]\ttraining's binary_logloss: 0.492456\n",
      "[857]\ttraining's binary_logloss: 0.492394\n",
      "[858]\ttraining's binary_logloss: 0.492255\n",
      "[859]\ttraining's binary_logloss: 0.492133\n",
      "[860]\ttraining's binary_logloss: 0.491974\n",
      "[861]\ttraining's binary_logloss: 0.491893\n",
      "[862]\ttraining's binary_logloss: 0.491788\n",
      "[863]\ttraining's binary_logloss: 0.491702\n",
      "[864]\ttraining's binary_logloss: 0.491617\n",
      "[865]\ttraining's binary_logloss: 0.491568\n",
      "[866]\ttraining's binary_logloss: 0.49146\n",
      "[867]\ttraining's binary_logloss: 0.49134\n",
      "[868]\ttraining's binary_logloss: 0.491276\n",
      "[869]\ttraining's binary_logloss: 0.491196\n",
      "[870]\ttraining's binary_logloss: 0.491008\n",
      "[871]\ttraining's binary_logloss: 0.490906\n",
      "[872]\ttraining's binary_logloss: 0.490829\n",
      "[873]\ttraining's binary_logloss: 0.490754\n",
      "[874]\ttraining's binary_logloss: 0.490611\n",
      "[875]\ttraining's binary_logloss: 0.490396\n",
      "[876]\ttraining's binary_logloss: 0.49028\n",
      "[877]\ttraining's binary_logloss: 0.490165\n",
      "[878]\ttraining's binary_logloss: 0.490001\n",
      "[879]\ttraining's binary_logloss: 0.489943\n",
      "[880]\ttraining's binary_logloss: 0.489845\n",
      "[881]\ttraining's binary_logloss: 0.489728\n",
      "[882]\ttraining's binary_logloss: 0.489654\n",
      "[883]\ttraining's binary_logloss: 0.489449\n",
      "[884]\ttraining's binary_logloss: 0.489271\n",
      "[885]\ttraining's binary_logloss: 0.489173\n",
      "[886]\ttraining's binary_logloss: 0.489054\n",
      "[887]\ttraining's binary_logloss: 0.48894\n",
      "[888]\ttraining's binary_logloss: 0.488879\n",
      "[889]\ttraining's binary_logloss: 0.488731\n",
      "[890]\ttraining's binary_logloss: 0.488673\n",
      "[891]\ttraining's binary_logloss: 0.488592\n",
      "[892]\ttraining's binary_logloss: 0.488539\n",
      "[893]\ttraining's binary_logloss: 0.488422\n",
      "[894]\ttraining's binary_logloss: 0.488262\n",
      "[895]\ttraining's binary_logloss: 0.488105\n",
      "[896]\ttraining's binary_logloss: 0.487983\n",
      "[897]\ttraining's binary_logloss: 0.487846\n",
      "[898]\ttraining's binary_logloss: 0.48772\n",
      "[899]\ttraining's binary_logloss: 0.487599\n",
      "[900]\ttraining's binary_logloss: 0.487495\n",
      "[901]\ttraining's binary_logloss: 0.487337\n",
      "[902]\ttraining's binary_logloss: 0.487174\n",
      "[903]\ttraining's binary_logloss: 0.487027\n",
      "[904]\ttraining's binary_logloss: 0.486967\n",
      "[905]\ttraining's binary_logloss: 0.486873\n",
      "[906]\ttraining's binary_logloss: 0.486747\n",
      "[907]\ttraining's binary_logloss: 0.486639\n",
      "[908]\ttraining's binary_logloss: 0.48653\n",
      "[909]\ttraining's binary_logloss: 0.486469\n",
      "[910]\ttraining's binary_logloss: 0.486372\n",
      "[911]\ttraining's binary_logloss: 0.486304\n",
      "[912]\ttraining's binary_logloss: 0.486185\n",
      "[913]\ttraining's binary_logloss: 0.486064\n",
      "[914]\ttraining's binary_logloss: 0.485956\n",
      "[915]\ttraining's binary_logloss: 0.485872\n",
      "[916]\ttraining's binary_logloss: 0.485779\n",
      "[917]\ttraining's binary_logloss: 0.485689\n",
      "[918]\ttraining's binary_logloss: 0.485586\n",
      "[919]\ttraining's binary_logloss: 0.48552\n",
      "[920]\ttraining's binary_logloss: 0.485393\n",
      "[921]\ttraining's binary_logloss: 0.485279\n",
      "[922]\ttraining's binary_logloss: 0.485202\n",
      "[923]\ttraining's binary_logloss: 0.485128\n",
      "[924]\ttraining's binary_logloss: 0.485015\n",
      "[925]\ttraining's binary_logloss: 0.484943\n",
      "[926]\ttraining's binary_logloss: 0.48489\n",
      "[927]\ttraining's binary_logloss: 0.484812\n",
      "[928]\ttraining's binary_logloss: 0.484638\n",
      "[929]\ttraining's binary_logloss: 0.484505\n",
      "[930]\ttraining's binary_logloss: 0.484455\n",
      "[931]\ttraining's binary_logloss: 0.48437\n",
      "[932]\ttraining's binary_logloss: 0.484229\n",
      "[933]\ttraining's binary_logloss: 0.484078\n",
      "[934]\ttraining's binary_logloss: 0.484006\n",
      "[935]\ttraining's binary_logloss: 0.483925\n",
      "[936]\ttraining's binary_logloss: 0.483885\n",
      "[937]\ttraining's binary_logloss: 0.48374\n",
      "[938]\ttraining's binary_logloss: 0.483647\n",
      "[939]\ttraining's binary_logloss: 0.483545\n",
      "[940]\ttraining's binary_logloss: 0.483468\n",
      "[941]\ttraining's binary_logloss: 0.483386\n",
      "[942]\ttraining's binary_logloss: 0.483311\n",
      "[943]\ttraining's binary_logloss: 0.483266\n",
      "[944]\ttraining's binary_logloss: 0.483201\n",
      "[945]\ttraining's binary_logloss: 0.483105\n",
      "[946]\ttraining's binary_logloss: 0.483042\n",
      "[947]\ttraining's binary_logloss: 0.482947\n",
      "[948]\ttraining's binary_logloss: 0.482782\n",
      "[949]\ttraining's binary_logloss: 0.482701\n",
      "[950]\ttraining's binary_logloss: 0.482583\n",
      "[951]\ttraining's binary_logloss: 0.482538\n",
      "[952]\ttraining's binary_logloss: 0.482467\n",
      "[953]\ttraining's binary_logloss: 0.482351\n",
      "[954]\ttraining's binary_logloss: 0.482187\n",
      "[955]\ttraining's binary_logloss: 0.482091\n",
      "[956]\ttraining's binary_logloss: 0.482004\n",
      "[957]\ttraining's binary_logloss: 0.481834\n",
      "[958]\ttraining's binary_logloss: 0.481714\n",
      "[959]\ttraining's binary_logloss: 0.481611\n",
      "[960]\ttraining's binary_logloss: 0.481528\n",
      "[961]\ttraining's binary_logloss: 0.481377\n",
      "[962]\ttraining's binary_logloss: 0.481267\n",
      "[963]\ttraining's binary_logloss: 0.481171\n",
      "[964]\ttraining's binary_logloss: 0.481092\n",
      "[965]\ttraining's binary_logloss: 0.481025\n",
      "[966]\ttraining's binary_logloss: 0.480949\n",
      "[967]\ttraining's binary_logloss: 0.480807\n",
      "[968]\ttraining's binary_logloss: 0.480667\n",
      "[969]\ttraining's binary_logloss: 0.480568\n",
      "[970]\ttraining's binary_logloss: 0.480437\n",
      "[971]\ttraining's binary_logloss: 0.480338\n",
      "[972]\ttraining's binary_logloss: 0.480195\n",
      "[973]\ttraining's binary_logloss: 0.480084\n",
      "[974]\ttraining's binary_logloss: 0.47997\n",
      "[975]\ttraining's binary_logloss: 0.479805\n",
      "[976]\ttraining's binary_logloss: 0.479709\n",
      "[977]\ttraining's binary_logloss: 0.479625\n",
      "[978]\ttraining's binary_logloss: 0.479536\n",
      "[979]\ttraining's binary_logloss: 0.479334\n",
      "[980]\ttraining's binary_logloss: 0.479242\n",
      "[981]\ttraining's binary_logloss: 0.479174\n",
      "[982]\ttraining's binary_logloss: 0.479028\n",
      "[983]\ttraining's binary_logloss: 0.478916\n",
      "[984]\ttraining's binary_logloss: 0.478773\n",
      "[985]\ttraining's binary_logloss: 0.478668\n",
      "[986]\ttraining's binary_logloss: 0.478632\n",
      "[987]\ttraining's binary_logloss: 0.478542\n",
      "[988]\ttraining's binary_logloss: 0.478409\n",
      "[989]\ttraining's binary_logloss: 0.478283\n",
      "[990]\ttraining's binary_logloss: 0.478207\n",
      "[991]\ttraining's binary_logloss: 0.478095\n",
      "[992]\ttraining's binary_logloss: 0.477992\n",
      "[993]\ttraining's binary_logloss: 0.477878\n",
      "[994]\ttraining's binary_logloss: 0.477749\n",
      "[995]\ttraining's binary_logloss: 0.477686\n",
      "[996]\ttraining's binary_logloss: 0.477598\n",
      "[997]\ttraining's binary_logloss: 0.477523\n",
      "[998]\ttraining's binary_logloss: 0.477343\n",
      "[999]\ttraining's binary_logloss: 0.477273\n",
      "[1000]\ttraining's binary_logloss: 0.477162\n",
      "[1001]\ttraining's binary_logloss: 0.477061\n",
      "[1002]\ttraining's binary_logloss: 0.477001\n",
      "[1003]\ttraining's binary_logloss: 0.47688\n",
      "[1004]\ttraining's binary_logloss: 0.476838\n",
      "[1005]\ttraining's binary_logloss: 0.476792\n",
      "[1006]\ttraining's binary_logloss: 0.476635\n",
      "[1007]\ttraining's binary_logloss: 0.476488\n",
      "[1008]\ttraining's binary_logloss: 0.476423\n",
      "[1009]\ttraining's binary_logloss: 0.476257\n",
      "[1010]\ttraining's binary_logloss: 0.476149\n",
      "[1011]\ttraining's binary_logloss: 0.476075\n",
      "[1012]\ttraining's binary_logloss: 0.475925\n",
      "[1013]\ttraining's binary_logloss: 0.475861\n",
      "[1014]\ttraining's binary_logloss: 0.475808\n",
      "[1015]\ttraining's binary_logloss: 0.475709\n",
      "[1016]\ttraining's binary_logloss: 0.475586\n",
      "[1017]\ttraining's binary_logloss: 0.475506\n",
      "[1018]\ttraining's binary_logloss: 0.475406\n",
      "[1019]\ttraining's binary_logloss: 0.475337\n",
      "[1020]\ttraining's binary_logloss: 0.475267\n",
      "[1021]\ttraining's binary_logloss: 0.475131\n",
      "[1022]\ttraining's binary_logloss: 0.475024\n",
      "[1023]\ttraining's binary_logloss: 0.474909\n",
      "[1024]\ttraining's binary_logloss: 0.4748\n",
      "[1025]\ttraining's binary_logloss: 0.474587\n",
      "[1026]\ttraining's binary_logloss: 0.474503\n",
      "[1027]\ttraining's binary_logloss: 0.47439\n",
      "[1028]\ttraining's binary_logloss: 0.474295\n",
      "[1029]\ttraining's binary_logloss: 0.474136\n",
      "[1030]\ttraining's binary_logloss: 0.474051\n",
      "[1031]\ttraining's binary_logloss: 0.473944\n",
      "[1032]\ttraining's binary_logloss: 0.473812\n",
      "[1033]\ttraining's binary_logloss: 0.473742\n",
      "[1034]\ttraining's binary_logloss: 0.473685\n",
      "[1035]\ttraining's binary_logloss: 0.473632\n",
      "[1036]\ttraining's binary_logloss: 0.473533\n",
      "[1037]\ttraining's binary_logloss: 0.473439\n",
      "[1038]\ttraining's binary_logloss: 0.473355\n",
      "[1039]\ttraining's binary_logloss: 0.473303\n",
      "[1040]\ttraining's binary_logloss: 0.473182\n",
      "[1041]\ttraining's binary_logloss: 0.47309\n",
      "[1042]\ttraining's binary_logloss: 0.473021\n",
      "[1043]\ttraining's binary_logloss: 0.47295\n",
      "[1044]\ttraining's binary_logloss: 0.472897\n",
      "[1045]\ttraining's binary_logloss: 0.472756\n",
      "[1046]\ttraining's binary_logloss: 0.47264\n",
      "[1047]\ttraining's binary_logloss: 0.472571\n",
      "[1048]\ttraining's binary_logloss: 0.472522\n",
      "[1049]\ttraining's binary_logloss: 0.472468\n",
      "[1050]\ttraining's binary_logloss: 0.472387\n",
      "[1051]\ttraining's binary_logloss: 0.472248\n",
      "[1052]\ttraining's binary_logloss: 0.472174\n",
      "[1053]\ttraining's binary_logloss: 0.47204\n",
      "[1054]\ttraining's binary_logloss: 0.471953\n",
      "[1055]\ttraining's binary_logloss: 0.471833\n",
      "[1056]\ttraining's binary_logloss: 0.471701\n",
      "[1057]\ttraining's binary_logloss: 0.4716\n",
      "[1058]\ttraining's binary_logloss: 0.471514\n",
      "[1059]\ttraining's binary_logloss: 0.471411\n",
      "[1060]\ttraining's binary_logloss: 0.471297\n",
      "[1061]\ttraining's binary_logloss: 0.471246\n",
      "[1062]\ttraining's binary_logloss: 0.471142\n",
      "[1063]\ttraining's binary_logloss: 0.471091\n",
      "[1064]\ttraining's binary_logloss: 0.470954\n",
      "[1065]\ttraining's binary_logloss: 0.470833\n",
      "[1066]\ttraining's binary_logloss: 0.470737\n",
      "[1067]\ttraining's binary_logloss: 0.470605\n",
      "[1068]\ttraining's binary_logloss: 0.470494\n",
      "[1069]\ttraining's binary_logloss: 0.470373\n",
      "[1070]\ttraining's binary_logloss: 0.470297\n",
      "[1071]\ttraining's binary_logloss: 0.47012\n",
      "[1072]\ttraining's binary_logloss: 0.470058\n",
      "[1073]\ttraining's binary_logloss: 0.469976\n",
      "[1074]\ttraining's binary_logloss: 0.469908\n",
      "[1075]\ttraining's binary_logloss: 0.469795\n",
      "[1076]\ttraining's binary_logloss: 0.469691\n",
      "[1077]\ttraining's binary_logloss: 0.469545\n",
      "[1078]\ttraining's binary_logloss: 0.469456\n",
      "[1079]\ttraining's binary_logloss: 0.469349\n",
      "[1080]\ttraining's binary_logloss: 0.469237\n",
      "[1081]\ttraining's binary_logloss: 0.469148\n",
      "[1082]\ttraining's binary_logloss: 0.469067\n",
      "[1083]\ttraining's binary_logloss: 0.46886\n",
      "[1084]\ttraining's binary_logloss: 0.468741\n",
      "[1085]\ttraining's binary_logloss: 0.468655\n",
      "[1086]\ttraining's binary_logloss: 0.468581\n",
      "[1087]\ttraining's binary_logloss: 0.468466\n",
      "[1088]\ttraining's binary_logloss: 0.46833\n",
      "[1089]\ttraining's binary_logloss: 0.468278\n",
      "[1090]\ttraining's binary_logloss: 0.468178\n",
      "[1091]\ttraining's binary_logloss: 0.468039\n",
      "[1092]\ttraining's binary_logloss: 0.46796\n",
      "[1093]\ttraining's binary_logloss: 0.467882\n",
      "[1094]\ttraining's binary_logloss: 0.467789\n",
      "[1095]\ttraining's binary_logloss: 0.46772\n",
      "[1096]\ttraining's binary_logloss: 0.46765\n",
      "[1097]\ttraining's binary_logloss: 0.467522\n",
      "[1098]\ttraining's binary_logloss: 0.467358\n",
      "[1099]\ttraining's binary_logloss: 0.467254\n",
      "[1100]\ttraining's binary_logloss: 0.467149\n",
      "[1101]\ttraining's binary_logloss: 0.466998\n",
      "[1102]\ttraining's binary_logloss: 0.466875\n",
      "[1103]\ttraining's binary_logloss: 0.466793\n",
      "[1104]\ttraining's binary_logloss: 0.466668\n",
      "[1105]\ttraining's binary_logloss: 0.46656\n",
      "[1106]\ttraining's binary_logloss: 0.466438\n",
      "[1107]\ttraining's binary_logloss: 0.466374\n",
      "[1108]\ttraining's binary_logloss: 0.466264\n",
      "[1109]\ttraining's binary_logloss: 0.466123\n",
      "[1110]\ttraining's binary_logloss: 0.466023\n",
      "[1111]\ttraining's binary_logloss: 0.465942\n",
      "[1112]\ttraining's binary_logloss: 0.465867\n",
      "[1113]\ttraining's binary_logloss: 0.465785\n",
      "[1114]\ttraining's binary_logloss: 0.465697\n",
      "[1115]\ttraining's binary_logloss: 0.465655\n",
      "[1116]\ttraining's binary_logloss: 0.465561\n",
      "[1117]\ttraining's binary_logloss: 0.465455\n",
      "[1118]\ttraining's binary_logloss: 0.465328\n",
      "[1119]\ttraining's binary_logloss: 0.465126\n",
      "[1120]\ttraining's binary_logloss: 0.464989\n",
      "[1121]\ttraining's binary_logloss: 0.46482\n",
      "[1122]\ttraining's binary_logloss: 0.464702\n",
      "[1123]\ttraining's binary_logloss: 0.464589\n",
      "[1124]\ttraining's binary_logloss: 0.464484\n",
      "[1125]\ttraining's binary_logloss: 0.464388\n",
      "[1126]\ttraining's binary_logloss: 0.464338\n",
      "[1127]\ttraining's binary_logloss: 0.464214\n",
      "[1128]\ttraining's binary_logloss: 0.464151\n",
      "[1129]\ttraining's binary_logloss: 0.464088\n",
      "[1130]\ttraining's binary_logloss: 0.463973\n",
      "[1131]\ttraining's binary_logloss: 0.463879\n",
      "[1132]\ttraining's binary_logloss: 0.463795\n",
      "[1133]\ttraining's binary_logloss: 0.463679\n",
      "[1134]\ttraining's binary_logloss: 0.463624\n",
      "[1135]\ttraining's binary_logloss: 0.463568\n",
      "[1136]\ttraining's binary_logloss: 0.463461\n",
      "[1137]\ttraining's binary_logloss: 0.463355\n",
      "[1138]\ttraining's binary_logloss: 0.463193\n",
      "[1139]\ttraining's binary_logloss: 0.463071\n",
      "[1140]\ttraining's binary_logloss: 0.462896\n",
      "[1141]\ttraining's binary_logloss: 0.462841\n",
      "[1142]\ttraining's binary_logloss: 0.462761\n",
      "[1143]\ttraining's binary_logloss: 0.462679\n",
      "[1144]\ttraining's binary_logloss: 0.462636\n",
      "[1145]\ttraining's binary_logloss: 0.462586\n",
      "[1146]\ttraining's binary_logloss: 0.462534\n",
      "[1147]\ttraining's binary_logloss: 0.462425\n",
      "[1148]\ttraining's binary_logloss: 0.462346\n",
      "[1149]\ttraining's binary_logloss: 0.462287\n",
      "[1150]\ttraining's binary_logloss: 0.462172\n",
      "[1151]\ttraining's binary_logloss: 0.462105\n",
      "[1152]\ttraining's binary_logloss: 0.461992\n",
      "[1153]\ttraining's binary_logloss: 0.461924\n",
      "[1154]\ttraining's binary_logloss: 0.461859\n",
      "[1155]\ttraining's binary_logloss: 0.461786\n",
      "[1156]\ttraining's binary_logloss: 0.46169\n",
      "[1157]\ttraining's binary_logloss: 0.461612\n",
      "[1158]\ttraining's binary_logloss: 0.46154\n",
      "[1159]\ttraining's binary_logloss: 0.461444\n",
      "[1160]\ttraining's binary_logloss: 0.461346\n",
      "[1161]\ttraining's binary_logloss: 0.461211\n",
      "[1162]\ttraining's binary_logloss: 0.461105\n",
      "[1163]\ttraining's binary_logloss: 0.460966\n",
      "[1164]\ttraining's binary_logloss: 0.460888\n",
      "[1165]\ttraining's binary_logloss: 0.460735\n",
      "[1166]\ttraining's binary_logloss: 0.460577\n",
      "[1167]\ttraining's binary_logloss: 0.460519\n",
      "[1168]\ttraining's binary_logloss: 0.460456\n",
      "[1169]\ttraining's binary_logloss: 0.460405\n",
      "[1170]\ttraining's binary_logloss: 0.460338\n",
      "[1171]\ttraining's binary_logloss: 0.460304\n",
      "[1172]\ttraining's binary_logloss: 0.46023\n",
      "[1173]\ttraining's binary_logloss: 0.460099\n",
      "[1174]\ttraining's binary_logloss: 0.460042\n",
      "[1175]\ttraining's binary_logloss: 0.459938\n",
      "[1176]\ttraining's binary_logloss: 0.459881\n",
      "[1177]\ttraining's binary_logloss: 0.459805\n",
      "[1178]\ttraining's binary_logloss: 0.459698\n",
      "[1179]\ttraining's binary_logloss: 0.459608\n",
      "[1180]\ttraining's binary_logloss: 0.459556\n",
      "[1181]\ttraining's binary_logloss: 0.459519\n",
      "[1182]\ttraining's binary_logloss: 0.459418\n",
      "[1183]\ttraining's binary_logloss: 0.459305\n",
      "[1184]\ttraining's binary_logloss: 0.459187\n",
      "[1185]\ttraining's binary_logloss: 0.459046\n",
      "[1186]\ttraining's binary_logloss: 0.458951\n",
      "[1187]\ttraining's binary_logloss: 0.4589\n",
      "[1188]\ttraining's binary_logloss: 0.458768\n",
      "[1189]\ttraining's binary_logloss: 0.45866\n",
      "[1190]\ttraining's binary_logloss: 0.458532\n",
      "[1191]\ttraining's binary_logloss: 0.458415\n",
      "[1192]\ttraining's binary_logloss: 0.458328\n",
      "[1193]\ttraining's binary_logloss: 0.45827\n",
      "[1194]\ttraining's binary_logloss: 0.458174\n",
      "[1195]\ttraining's binary_logloss: 0.458102\n",
      "[1196]\ttraining's binary_logloss: 0.458003\n",
      "[1197]\ttraining's binary_logloss: 0.45792\n",
      "[1198]\ttraining's binary_logloss: 0.4578\n",
      "[1199]\ttraining's binary_logloss: 0.457689\n",
      "[1200]\ttraining's binary_logloss: 0.45761\n",
      "[1201]\ttraining's binary_logloss: 0.45755\n",
      "[1202]\ttraining's binary_logloss: 0.457426\n",
      "[1203]\ttraining's binary_logloss: 0.457302\n",
      "[1204]\ttraining's binary_logloss: 0.45723\n",
      "[1205]\ttraining's binary_logloss: 0.457141\n",
      "[1206]\ttraining's binary_logloss: 0.45708\n",
      "[1207]\ttraining's binary_logloss: 0.457024\n",
      "[1208]\ttraining's binary_logloss: 0.45687\n",
      "[1209]\ttraining's binary_logloss: 0.456801\n",
      "[1210]\ttraining's binary_logloss: 0.456699\n",
      "[1211]\ttraining's binary_logloss: 0.456557\n",
      "[1212]\ttraining's binary_logloss: 0.456424\n",
      "[1213]\ttraining's binary_logloss: 0.456324\n",
      "[1214]\ttraining's binary_logloss: 0.456226\n",
      "[1215]\ttraining's binary_logloss: 0.456187\n",
      "[1216]\ttraining's binary_logloss: 0.456108\n",
      "[1217]\ttraining's binary_logloss: 0.456042\n",
      "[1218]\ttraining's binary_logloss: 0.455923\n",
      "[1219]\ttraining's binary_logloss: 0.455813\n",
      "[1220]\ttraining's binary_logloss: 0.455676\n",
      "[1221]\ttraining's binary_logloss: 0.455555\n",
      "[1222]\ttraining's binary_logloss: 0.455461\n",
      "[1223]\ttraining's binary_logloss: 0.455375\n",
      "[1224]\ttraining's binary_logloss: 0.455283\n",
      "[1225]\ttraining's binary_logloss: 0.455237\n",
      "[1226]\ttraining's binary_logloss: 0.455176\n",
      "[1227]\ttraining's binary_logloss: 0.455095\n",
      "[1228]\ttraining's binary_logloss: 0.45499\n",
      "[1229]\ttraining's binary_logloss: 0.454892\n",
      "[1230]\ttraining's binary_logloss: 0.45479\n",
      "[1231]\ttraining's binary_logloss: 0.454648\n",
      "[1232]\ttraining's binary_logloss: 0.454534\n",
      "[1233]\ttraining's binary_logloss: 0.454471\n",
      "[1234]\ttraining's binary_logloss: 0.454367\n",
      "[1235]\ttraining's binary_logloss: 0.454309\n",
      "[1236]\ttraining's binary_logloss: 0.45421\n",
      "[1237]\ttraining's binary_logloss: 0.454124\n",
      "[1238]\ttraining's binary_logloss: 0.454029\n",
      "[1239]\ttraining's binary_logloss: 0.453954\n",
      "[1240]\ttraining's binary_logloss: 0.453861\n",
      "[1241]\ttraining's binary_logloss: 0.453777\n",
      "[1242]\ttraining's binary_logloss: 0.453735\n",
      "[1243]\ttraining's binary_logloss: 0.453661\n",
      "[1244]\ttraining's binary_logloss: 0.453588\n",
      "[1245]\ttraining's binary_logloss: 0.4535\n",
      "[1246]\ttraining's binary_logloss: 0.453444\n",
      "[1247]\ttraining's binary_logloss: 0.453345\n",
      "[1248]\ttraining's binary_logloss: 0.453242\n",
      "[1249]\ttraining's binary_logloss: 0.45312\n",
      "[1250]\ttraining's binary_logloss: 0.453038\n",
      "[1251]\ttraining's binary_logloss: 0.452945\n",
      "[1252]\ttraining's binary_logloss: 0.452838\n",
      "[1253]\ttraining's binary_logloss: 0.452763\n",
      "[1254]\ttraining's binary_logloss: 0.452675\n",
      "[1255]\ttraining's binary_logloss: 0.452628\n",
      "[1256]\ttraining's binary_logloss: 0.452583\n",
      "[1257]\ttraining's binary_logloss: 0.452535\n",
      "[1258]\ttraining's binary_logloss: 0.452437\n",
      "[1259]\ttraining's binary_logloss: 0.452325\n",
      "[1260]\ttraining's binary_logloss: 0.452205\n",
      "[1261]\ttraining's binary_logloss: 0.452108\n",
      "[1262]\ttraining's binary_logloss: 0.452029\n",
      "[1263]\ttraining's binary_logloss: 0.451928\n",
      "[1264]\ttraining's binary_logloss: 0.451828\n",
      "[1265]\ttraining's binary_logloss: 0.451724\n",
      "[1266]\ttraining's binary_logloss: 0.45161\n",
      "[1267]\ttraining's binary_logloss: 0.45147\n",
      "[1268]\ttraining's binary_logloss: 0.451319\n",
      "[1269]\ttraining's binary_logloss: 0.451238\n",
      "[1270]\ttraining's binary_logloss: 0.451158\n",
      "[1271]\ttraining's binary_logloss: 0.451075\n",
      "[1272]\ttraining's binary_logloss: 0.450963\n",
      "[1273]\ttraining's binary_logloss: 0.450856\n",
      "[1274]\ttraining's binary_logloss: 0.450756\n",
      "[1275]\ttraining's binary_logloss: 0.450667\n",
      "[1276]\ttraining's binary_logloss: 0.450556\n",
      "[1277]\ttraining's binary_logloss: 0.450432\n",
      "[1278]\ttraining's binary_logloss: 0.450367\n",
      "[1279]\ttraining's binary_logloss: 0.450234\n",
      "[1280]\ttraining's binary_logloss: 0.450143\n",
      "[1281]\ttraining's binary_logloss: 0.450016\n",
      "[1282]\ttraining's binary_logloss: 0.449922\n",
      "[1283]\ttraining's binary_logloss: 0.449837\n",
      "[1284]\ttraining's binary_logloss: 0.449772\n",
      "[1285]\ttraining's binary_logloss: 0.449677\n",
      "[1286]\ttraining's binary_logloss: 0.449631\n",
      "[1287]\ttraining's binary_logloss: 0.449551\n",
      "[1288]\ttraining's binary_logloss: 0.449444\n",
      "[1289]\ttraining's binary_logloss: 0.449368\n",
      "[1290]\ttraining's binary_logloss: 0.449307\n",
      "[1291]\ttraining's binary_logloss: 0.449258\n",
      "[1292]\ttraining's binary_logloss: 0.449221\n",
      "[1293]\ttraining's binary_logloss: 0.449136\n",
      "[1294]\ttraining's binary_logloss: 0.449101\n",
      "[1295]\ttraining's binary_logloss: 0.449034\n",
      "[1296]\ttraining's binary_logloss: 0.448981\n",
      "[1297]\ttraining's binary_logloss: 0.448853\n",
      "[1298]\ttraining's binary_logloss: 0.448731\n",
      "[1299]\ttraining's binary_logloss: 0.448605\n",
      "[1300]\ttraining's binary_logloss: 0.448515\n",
      "[1301]\ttraining's binary_logloss: 0.448409\n",
      "[1302]\ttraining's binary_logloss: 0.448329\n",
      "[1303]\ttraining's binary_logloss: 0.448273\n",
      "[1304]\ttraining's binary_logloss: 0.44811\n",
      "[1305]\ttraining's binary_logloss: 0.448009\n",
      "[1306]\ttraining's binary_logloss: 0.447951\n",
      "[1307]\ttraining's binary_logloss: 0.447895\n",
      "[1308]\ttraining's binary_logloss: 0.447803\n",
      "[1309]\ttraining's binary_logloss: 0.447681\n",
      "[1310]\ttraining's binary_logloss: 0.447624\n",
      "[1311]\ttraining's binary_logloss: 0.447531\n",
      "[1312]\ttraining's binary_logloss: 0.44745\n",
      "[1313]\ttraining's binary_logloss: 0.447389\n",
      "[1314]\ttraining's binary_logloss: 0.447292\n",
      "[1315]\ttraining's binary_logloss: 0.447184\n",
      "[1316]\ttraining's binary_logloss: 0.447095\n",
      "[1317]\ttraining's binary_logloss: 0.447037\n",
      "[1318]\ttraining's binary_logloss: 0.446983\n",
      "[1319]\ttraining's binary_logloss: 0.446833\n",
      "[1320]\ttraining's binary_logloss: 0.446777\n",
      "[1321]\ttraining's binary_logloss: 0.446744\n",
      "[1322]\ttraining's binary_logloss: 0.446648\n",
      "[1323]\ttraining's binary_logloss: 0.446515\n",
      "[1324]\ttraining's binary_logloss: 0.446418\n",
      "[1325]\ttraining's binary_logloss: 0.446366\n",
      "[1326]\ttraining's binary_logloss: 0.446291\n",
      "[1327]\ttraining's binary_logloss: 0.446179\n",
      "[1328]\ttraining's binary_logloss: 0.446121\n",
      "[1329]\ttraining's binary_logloss: 0.446027\n",
      "[1330]\ttraining's binary_logloss: 0.445992\n",
      "[1331]\ttraining's binary_logloss: 0.445911\n",
      "[1332]\ttraining's binary_logloss: 0.44584\n",
      "[1333]\ttraining's binary_logloss: 0.445773\n",
      "[1334]\ttraining's binary_logloss: 0.445668\n",
      "[1335]\ttraining's binary_logloss: 0.445588\n",
      "[1336]\ttraining's binary_logloss: 0.445535\n",
      "[1337]\ttraining's binary_logloss: 0.445477\n",
      "[1338]\ttraining's binary_logloss: 0.445437\n",
      "[1339]\ttraining's binary_logloss: 0.445331\n",
      "[1340]\ttraining's binary_logloss: 0.445264\n",
      "[1341]\ttraining's binary_logloss: 0.445179\n",
      "[1342]\ttraining's binary_logloss: 0.445121\n",
      "[1343]\ttraining's binary_logloss: 0.445021\n",
      "[1344]\ttraining's binary_logloss: 0.444931\n",
      "[1345]\ttraining's binary_logloss: 0.444865\n",
      "[1346]\ttraining's binary_logloss: 0.444761\n",
      "[1347]\ttraining's binary_logloss: 0.444615\n",
      "[1348]\ttraining's binary_logloss: 0.444549\n",
      "[1349]\ttraining's binary_logloss: 0.444396\n",
      "[1350]\ttraining's binary_logloss: 0.444288\n",
      "[1351]\ttraining's binary_logloss: 0.444214\n",
      "[1352]\ttraining's binary_logloss: 0.444119\n",
      "[1353]\ttraining's binary_logloss: 0.444037\n",
      "[1354]\ttraining's binary_logloss: 0.443983\n",
      "[1355]\ttraining's binary_logloss: 0.443919\n",
      "[1356]\ttraining's binary_logloss: 0.443827\n",
      "[1357]\ttraining's binary_logloss: 0.443732\n",
      "[1358]\ttraining's binary_logloss: 0.443656\n",
      "[1359]\ttraining's binary_logloss: 0.443559\n",
      "[1360]\ttraining's binary_logloss: 0.443475\n",
      "[1361]\ttraining's binary_logloss: 0.443398\n",
      "[1362]\ttraining's binary_logloss: 0.443304\n",
      "[1363]\ttraining's binary_logloss: 0.443217\n",
      "[1364]\ttraining's binary_logloss: 0.443142\n",
      "[1365]\ttraining's binary_logloss: 0.44309\n",
      "[1366]\ttraining's binary_logloss: 0.443014\n",
      "[1367]\ttraining's binary_logloss: 0.442922\n",
      "[1368]\ttraining's binary_logloss: 0.44284\n",
      "[1369]\ttraining's binary_logloss: 0.442709\n",
      "[1370]\ttraining's binary_logloss: 0.442632\n",
      "[1371]\ttraining's binary_logloss: 0.44254\n",
      "[1372]\ttraining's binary_logloss: 0.442455\n",
      "[1373]\ttraining's binary_logloss: 0.442344\n",
      "[1374]\ttraining's binary_logloss: 0.442271\n",
      "[1375]\ttraining's binary_logloss: 0.442156\n",
      "[1376]\ttraining's binary_logloss: 0.442058\n",
      "[1377]\ttraining's binary_logloss: 0.441984\n",
      "[1378]\ttraining's binary_logloss: 0.441926\n",
      "[1379]\ttraining's binary_logloss: 0.441855\n",
      "[1380]\ttraining's binary_logloss: 0.441698\n",
      "[1381]\ttraining's binary_logloss: 0.44161\n",
      "[1382]\ttraining's binary_logloss: 0.441504\n",
      "[1383]\ttraining's binary_logloss: 0.44144\n",
      "[1384]\ttraining's binary_logloss: 0.441333\n",
      "[1385]\ttraining's binary_logloss: 0.441237\n",
      "[1386]\ttraining's binary_logloss: 0.441146\n",
      "[1387]\ttraining's binary_logloss: 0.441064\n",
      "[1388]\ttraining's binary_logloss: 0.44099\n",
      "[1389]\ttraining's binary_logloss: 0.44094\n",
      "[1390]\ttraining's binary_logloss: 0.440866\n",
      "[1391]\ttraining's binary_logloss: 0.440744\n",
      "[1392]\ttraining's binary_logloss: 0.44065\n",
      "[1393]\ttraining's binary_logloss: 0.440594\n",
      "[1394]\ttraining's binary_logloss: 0.440478\n",
      "[1395]\ttraining's binary_logloss: 0.440395\n",
      "[1396]\ttraining's binary_logloss: 0.440359\n",
      "[1397]\ttraining's binary_logloss: 0.440213\n",
      "[1398]\ttraining's binary_logloss: 0.440063\n",
      "[1399]\ttraining's binary_logloss: 0.439983\n",
      "[1400]\ttraining's binary_logloss: 0.439932\n",
      "[1401]\ttraining's binary_logloss: 0.439892\n",
      "[1402]\ttraining's binary_logloss: 0.439798\n",
      "[1403]\ttraining's binary_logloss: 0.439715\n",
      "[1404]\ttraining's binary_logloss: 0.439651\n",
      "[1405]\ttraining's binary_logloss: 0.439594\n",
      "[1406]\ttraining's binary_logloss: 0.439544\n",
      "[1407]\ttraining's binary_logloss: 0.439477\n",
      "[1408]\ttraining's binary_logloss: 0.439388\n",
      "[1409]\ttraining's binary_logloss: 0.439262\n",
      "[1410]\ttraining's binary_logloss: 0.439119\n",
      "[1411]\ttraining's binary_logloss: 0.439036\n",
      "[1412]\ttraining's binary_logloss: 0.438925\n",
      "[1413]\ttraining's binary_logloss: 0.43884\n",
      "[1414]\ttraining's binary_logloss: 0.438723\n",
      "[1415]\ttraining's binary_logloss: 0.438688\n",
      "[1416]\ttraining's binary_logloss: 0.438609\n",
      "[1417]\ttraining's binary_logloss: 0.438494\n",
      "[1418]\ttraining's binary_logloss: 0.438417\n",
      "[1419]\ttraining's binary_logloss: 0.438343\n",
      "[1420]\ttraining's binary_logloss: 0.438298\n",
      "[1421]\ttraining's binary_logloss: 0.438259\n",
      "[1422]\ttraining's binary_logloss: 0.438152\n",
      "[1423]\ttraining's binary_logloss: 0.438059\n",
      "[1424]\ttraining's binary_logloss: 0.437949\n",
      "[1425]\ttraining's binary_logloss: 0.437861\n",
      "[1426]\ttraining's binary_logloss: 0.437769\n",
      "[1427]\ttraining's binary_logloss: 0.437697\n",
      "[1428]\ttraining's binary_logloss: 0.437629\n",
      "[1429]\ttraining's binary_logloss: 0.437536\n",
      "[1430]\ttraining's binary_logloss: 0.437474\n",
      "[1431]\ttraining's binary_logloss: 0.437404\n",
      "[1432]\ttraining's binary_logloss: 0.4373\n",
      "[1433]\ttraining's binary_logloss: 0.437166\n",
      "[1434]\ttraining's binary_logloss: 0.437082\n",
      "[1435]\ttraining's binary_logloss: 0.436979\n",
      "[1436]\ttraining's binary_logloss: 0.436904\n",
      "[1437]\ttraining's binary_logloss: 0.436783\n",
      "[1438]\ttraining's binary_logloss: 0.436735\n",
      "[1439]\ttraining's binary_logloss: 0.436629\n",
      "[1440]\ttraining's binary_logloss: 0.436548\n",
      "[1441]\ttraining's binary_logloss: 0.436437\n",
      "[1442]\ttraining's binary_logloss: 0.436371\n",
      "[1443]\ttraining's binary_logloss: 0.436272\n",
      "[1444]\ttraining's binary_logloss: 0.436209\n",
      "[1445]\ttraining's binary_logloss: 0.436075\n",
      "[1446]\ttraining's binary_logloss: 0.435915\n",
      "[1447]\ttraining's binary_logloss: 0.435862\n",
      "[1448]\ttraining's binary_logloss: 0.435781\n",
      "[1449]\ttraining's binary_logloss: 0.435726\n",
      "[1450]\ttraining's binary_logloss: 0.435554\n",
      "[1451]\ttraining's binary_logloss: 0.43551\n",
      "[1452]\ttraining's binary_logloss: 0.435411\n",
      "[1453]\ttraining's binary_logloss: 0.43533\n",
      "[1454]\ttraining's binary_logloss: 0.435288\n",
      "[1455]\ttraining's binary_logloss: 0.435235\n",
      "[1456]\ttraining's binary_logloss: 0.435117\n",
      "[1457]\ttraining's binary_logloss: 0.435054\n",
      "[1458]\ttraining's binary_logloss: 0.434985\n",
      "[1459]\ttraining's binary_logloss: 0.43489\n",
      "[1460]\ttraining's binary_logloss: 0.434814\n",
      "[1461]\ttraining's binary_logloss: 0.434757\n",
      "[1462]\ttraining's binary_logloss: 0.434693\n",
      "[1463]\ttraining's binary_logloss: 0.434632\n",
      "[1464]\ttraining's binary_logloss: 0.434552\n",
      "[1465]\ttraining's binary_logloss: 0.434436\n",
      "[1466]\ttraining's binary_logloss: 0.434345\n",
      "[1467]\ttraining's binary_logloss: 0.434234\n",
      "[1468]\ttraining's binary_logloss: 0.434173\n",
      "[1469]\ttraining's binary_logloss: 0.434097\n",
      "[1470]\ttraining's binary_logloss: 0.434038\n",
      "[1471]\ttraining's binary_logloss: 0.43395\n",
      "[1472]\ttraining's binary_logloss: 0.433874\n",
      "[1473]\ttraining's binary_logloss: 0.433819\n",
      "[1474]\ttraining's binary_logloss: 0.433767\n",
      "[1475]\ttraining's binary_logloss: 0.433657\n",
      "[1476]\ttraining's binary_logloss: 0.433579\n",
      "[1477]\ttraining's binary_logloss: 0.433519\n",
      "[1478]\ttraining's binary_logloss: 0.433473\n",
      "[1479]\ttraining's binary_logloss: 0.433399\n",
      "[1480]\ttraining's binary_logloss: 0.433332\n",
      "[1481]\ttraining's binary_logloss: 0.433267\n",
      "[1482]\ttraining's binary_logloss: 0.433161\n",
      "[1483]\ttraining's binary_logloss: 0.433123\n",
      "[1484]\ttraining's binary_logloss: 0.433041\n",
      "[1485]\ttraining's binary_logloss: 0.432954\n",
      "[1486]\ttraining's binary_logloss: 0.432851\n",
      "[1487]\ttraining's binary_logloss: 0.43276\n",
      "[1488]\ttraining's binary_logloss: 0.432717\n",
      "[1489]\ttraining's binary_logloss: 0.432663\n",
      "[1490]\ttraining's binary_logloss: 0.4326\n",
      "[1491]\ttraining's binary_logloss: 0.432548\n",
      "[1492]\ttraining's binary_logloss: 0.432508\n",
      "[1493]\ttraining's binary_logloss: 0.43241\n",
      "[1494]\ttraining's binary_logloss: 0.432298\n",
      "[1495]\ttraining's binary_logloss: 0.432206\n",
      "[1496]\ttraining's binary_logloss: 0.43213\n",
      "[1497]\ttraining's binary_logloss: 0.432051\n",
      "[1498]\ttraining's binary_logloss: 0.432016\n",
      "[1499]\ttraining's binary_logloss: 0.431901\n",
      "[1500]\ttraining's binary_logloss: 0.431788\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as LightGBM\n",
    "\n",
    "lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=1500,\n",
    "                               max_depth = 50,\n",
    "                               min_data_in_leaf = 50,\n",
    "                               class_weight={True: 5, False: 1}\n",
    "                              ) \n",
    "\n",
    "evals = [(x_train_features, y_train_bool)]\n",
    "lgbm.fit(x_train_features, y_train_bool, eval_metric='logloss', eval_set=evals)\n",
    "y_pred = lgbm.predict(x_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.98      0.78      0.87     63391\n",
      "        risk       0.45      0.91      0.60     12724\n",
      "\n",
      "    accuracy                           0.80     76115\n",
      "   macro avg       0.71      0.84      0.73     76115\n",
      "weighted avg       0.89      0.80      0.82     76115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_train_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_train_bool, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.87      0.70      0.77     21052\n",
      "        risk       0.25      0.48      0.33      4344\n",
      "\n",
      "    accuracy                           0.66     25396\n",
      "   macro avg       0.56      0.59      0.55     25396\n",
      "weighted avg       0.76      0.66      0.70     25396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = lgbm.predict(x_valid_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "\n",
    "print(classification_report(y_valid_bool, y, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50baee28870d358e64fb0672623b67d817f95823ed506d49d40f53415dd6866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
