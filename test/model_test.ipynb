{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "null_values = ['?', '??', 'N/A', 'NA', 'nan', 'NaN', '-nan', '-NaN', 'null', '-']\n",
    "x_test = pd.read_csv('./data/x_test_normal.csv', na_values = null_values)\n",
    "y_test = pd.read_csv('./data/y_test_normal.csv', na_values = null_values)\n",
    "x_test_features = x_test.drop(columns=['날짜', 'CODE', '종가'], inplace=False)\n",
    "y_test_bool = y_test['Y'] <-2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tree Base Simple Classifers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-1 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.90      0.31      0.46     21040\n",
      "        risk       0.20      0.83      0.32      4311\n",
      "\n",
      "    accuracy                           0.40     25351\n",
      "   macro avg       0.55      0.57      0.39     25351\n",
      "weighted avg       0.78      0.40      0.43     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier(\n",
    "    max_depth=15,\n",
    "    min_samples_split=100,\n",
    "    class_weight={True: 10, False: 1}\n",
    ")\n",
    "\n",
    "decisionTree = joblib.load('./models/decisionTree.pkl') \n",
    "y_pred = decisionTree.predict(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.38      0.54     21040\n",
      "        risk       0.21      0.82      0.34      4311\n",
      "\n",
      "    accuracy                           0.46     25351\n",
      "   macro avg       0.56      0.60      0.44     25351\n",
      "weighted avg       0.79      0.46      0.50     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    criterion='entropy', \n",
    "    min_samples_split = 100,\n",
    "    bootstrap=True,\n",
    "    max_depth=20,\n",
    "    class_weight={True: 10, False: 1}\n",
    "    )\n",
    "rf = joblib.load('./models/randomForest.pkl') \n",
    "y_pred = rf.predict(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LightGBM and Weak Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lightgbm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-1 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.40      0.55     21040\n",
      "        risk       0.21      0.80      0.34      4311\n",
      "\n",
      "    accuracy                           0.47     25351\n",
      "   macro avg       0.56      0.60      0.44     25351\n",
      "weighted avg       0.79      0.47      0.52     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as LightGBM\n",
    "\n",
    "lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=600,\n",
    "                               max_depth = 50,\n",
    "                               min_data_in_leaf = 50,\n",
    "                               class_weight={True: 10, False: 1},\n",
    "                               learning_rate= 0.1\n",
    "                              ) \n",
    "\n",
    "rf = joblib.load('./models/LightGBM.pkl') \n",
    "y_pred = rf.predict(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-2 LightGBM and Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no risk       0.91      0.37      0.52     21040\n",
      "        risk       0.21      0.82      0.34      4311\n",
      "\n",
      "    accuracy                           0.45     25351\n",
      "   macro avg       0.56      0.60      0.43     25351\n",
      "weighted avg       0.79      0.45      0.49     25351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as LightGBM\n",
    "\n",
    "# light gbm 앙상블을 위해 feature set을 생성하는 코드\n",
    "rfecv_feature_list = ['BPS', 'PBR', 'DIV', '거래량', '시가총액', '금리', '자산총계', '이익잉여금', '자본총계']\n",
    "sfs_feature_list = ['BPS', 'DIV', '거래량', '금리', '비유동자산', '자산총계', '부채총계', '법인세차감전 순이익', '당기순이익']\n",
    "\n",
    "\n",
    "def make_feature_set(x) :\n",
    "    x_whole = x\n",
    "    x_rfecv = x[rfecv_feature_list]\n",
    "    x_sfs = x[sfs_feature_list]\n",
    "    return x_whole, x_rfecv, x_sfs\n",
    "\n",
    "feature_set = []\n",
    "feature_set = make_feature_set(x_test_features)\n",
    "model = []\n",
    "\n",
    "## train\n",
    "i = 0\n",
    "for x in feature_set :\n",
    "    lgbm = LightGBM.LGBMClassifier(early_stopping_rounds=100,\n",
    "                               reg_lambda = 0.25, \n",
    "                               n_estimators=600,\n",
    "                               max_depth = 50,\n",
    "                               min_data_in_leaf = 50,\n",
    "                               class_weight={True: 10, False: 1},\n",
    "                               learning_rate= 0.1\n",
    "                              ) \n",
    "    lgbm = joblib.load('./models/lgbm_ensembles' +str(i) + '.pkl') \n",
    "    i = i+1\n",
    "    \n",
    "    model.append(lgbm)\n",
    "\n",
    "## prediction\n",
    "def predict_ensemble_model(x_) :\n",
    "    feature_set = make_feature_set(x_)\n",
    "    y_pred = []\n",
    "    i = 0\n",
    "    for x in feature_set :\n",
    "        print(type(model[i]))\n",
    "        pred = model[i].predict(x)\n",
    "        y_pred.append(pred)\n",
    "        i = i+1\n",
    "\n",
    "    y_pred_sum = y_pred[0] | (y_pred[1] & y_pred[2])# & y_pred[3] & y_pred[4])\n",
    "    return y_pred_sum\n",
    "\n",
    "y_pred = predict_ensemble_model(x_test_features)\n",
    "target_names = ['no risk', 'risk']\n",
    "print(classification_report(y_test_bool, y_pred, target_names = target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi Layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch\n",
    "! pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x.iloc[idx])\n",
    "        y = torch.FloatTensor(self.y.iloc[idx])\n",
    "        return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Ncua0eWeOaCR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class Simple_MLP_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_MLP_Net, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(22, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(32, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def embedding_output(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :  tensor(0.8299) loss :  0.43261614170941437\n"
     ]
    }
   ],
   "source": [
    "#from torcheval.metrics import BinaryAccuracy\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "\n",
    "y_test_int = pd.DataFrame()\n",
    "y_test_int['y'] = y_test_bool.astype(int)\n",
    "valid_dataset = StockDataset(x_test_features, y_test_int)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Simple_MLP_Net().to(device)\n",
    "\n",
    "PATH = './models/mlp_net_checkpoint19.pth'\n",
    "checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "\n",
    "model.eval()\n",
    "total_acc = 0\n",
    "total_loss = 0\n",
    "num_batch = 0\n",
    "for x, y in valid_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        metric = BinaryAccuracy().to(device)\n",
    "        metric(outputs, y)\n",
    "        acc = metric.compute()\n",
    "        total_acc += acc\n",
    "        total_loss += loss.cpu().item()\n",
    "        num_batch = num_batch + 1\n",
    "        \n",
    "total_acc = total_acc/(num_batch) \n",
    "total_loss = total_loss/(num_batch)\n",
    "\n",
    "print(\"acc : \", total_acc, \"loss : \" , total_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2 Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1670484617333,
     "user": {
      "displayName": "전병국",
      "userId": "04716627198164050618"
     },
     "user_tz": -540
    },
    "id": "vNinSxd-U9Up"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class Encoder_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(22, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 22, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def calcEncoding(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5514,
     "status": "ok",
     "timestamp": 1670485292125,
     "user": {
      "displayName": "전병국",
      "userId": "04716627198164050618"
     },
     "user_tz": -540
    },
    "id": "zWFzLGjdiG9T",
    "outputId": "ba04d695-9b53-46c9-e152-dbce6312709f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.002253052545711398\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = StockDataset(x_test_features, x_test_features)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Encoder_Decoder().to(device)\n",
    "\n",
    "PATH = './models/embedding_net5_150_checkpoint.pth'\n",
    "checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "total_loss = 0\n",
    "num_batch = 0\n",
    "for x, y in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        total_loss += loss\n",
    "        \n",
    "total_loss = total_loss / len(valid_dataloader)\n",
    "print(\"test loss : \" + str(float(total_loss)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.11.0\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bkjeo\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 flatbuffers-22.12.6 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 libclang-14.0.6 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.29.0 termcolor-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                3280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               2688      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,849\n",
      "Trainable params: 16,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "RESULTS\n",
      "True Positive\tFalse Negative\tFalse Positive\tTrue Negative\n",
      "313\t\t\t\t5678\t\t\t\t1164\t\t\t\t23437\n",
      "------------------------------------------------------------\n",
      "\t\t precision\t\trecall\t\t\tf1 score\t\tsupport\n",
      "risk    :0.21\t\t\t0.05\t\t\t0.08\t\t\t5991\n",
      "no risk :0.80\t\t\t0.95\t\t\t0.87\t\t\t24601\n",
      "accuracy: 0.7763467573221757\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "\n",
    "x = np.load('./models/valid_info/x_valid.npy', allow_pickle=True)               # 전체 데이터 중 25%에 해당하는 학습에 사용되지 않은 validation data 로드\n",
    "y = np.load('./models/valid_info/y_valid.npy', allow_pickle=True)               # x_valid는 (기업수)*(기업당 데이터 수)*(window 크기)*(feature 갯수) = 299*106*10*20\n",
    "                                                                       # y_valid는 (기업수)*(결과 값) = 299*106\n",
    "K.clear_session()\n",
    "model = Sequential()                                                            # 학습에 사용한 모델의 구조 복구\n",
    "model.add(LSTM(20, input_shape=(10, 20)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "model.load_weights('./models/valid_info/v3')                                                        # 학습된 weights 불러와 적용\n",
    "model.save_weights('./models/valid_info/v3')\n",
    "\n",
    "results = {'TP' : 0, 'TN' : 0, 'FP' : 0, 'FN' : 0}\n",
    "\n",
    "for i in range(len(y)):\n",
    "    predicts = model.predict(x[i], verbose=0)                                   # validation 진행\n",
    "    predicts = [True if x>0.5 else False for [x] in predicts]                   # classification 결과값 환산\n",
    "    for j in range(len(predicts)):\n",
    "        if y[i][j] and predicts[j]:\n",
    "            results['TP'] += 1          # TP : true positive\n",
    "        elif y[i][j]:\n",
    "            results['FN'] += 1          # TN : true negative\n",
    "        elif predicts[j]:\n",
    "            results['FP'] += 1          # FP = false positive\n",
    "        else:\n",
    "            results['TN'] += 1          # FN = false negative\n",
    "\n",
    "print('\\nRESULTS')                                                          # 결과값 출력부분, precision, recall, f1 score, support 출력\n",
    "print('True Positive\\tFalse Negative\\tFalse Positive\\tTrue Negative')\n",
    "print('{}\\t\\t\\t\\t{}\\t\\t\\t\\t{}\\t\\t\\t\\t{}'.format(results['TP'], results['FN'], results['FP'],  results['TN']))\n",
    "print('------------------------------------------------------------')\n",
    "print('\\t\\t precision\\t\\trecall\\t\\t\\tf1 score\\t\\tsupport')\n",
    "precision = results['TP'] / (results['TP'] + results['FP'])             # precision값과 recall값, f1-score 직접 계산\n",
    "recall = results['TP'] / (results['TP'] + results['FN'])\n",
    "print('risk    :{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{}'.format(precision, recall, 2*precision*recall / (precision+recall), results['TP'] + results['FN']))\n",
    "precision = results['TN'] / (results['TN'] + results['FN'])\n",
    "recall = results['TN'] / (results['TN'] + results['FP'])\n",
    "print('no risk :{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{:.2f}\\t\\t\\t{}'.format(precision, recall, 2*precision*recall / (precision+recall), results['FP'] + results['TN']))\n",
    "print('accuracy: {}'.format((results['TP'] + results['TN']) / (results['TP'] + results['TN'] + results['FP'] + results['FN'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50baee28870d358e64fb0672623b67d817f95823ed506d49d40f53415dd6866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
